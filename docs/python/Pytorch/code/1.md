# 那些你应该会的Pytorch操作


### 【Chapter 1】：自定义求导/反向传播方式

```python
import torch
from torch import nn
from torch.nn import functional as F

class Exmaple(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x): #
    ''' 定义激活函数的计算过程 '''
        ctx.save_for_backward(x)
        # 将输入保存起来，在backward时使用
        output = x.clamp(min=0)
        return output

    @staticmethod
    def backward(ctx, grad_output):
    ''' 
    根据链式求导，dloss / dx = (dloss / doutput) * (doutput / dx)
    
    其中的dloss / doutput就是输入的参数grad_output
    '''
        x, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input_ < 0] = 0               
        return grad_input

```

下面展示```EfficientNet```中自定义的```Swish```激活函数

$$
    Swish(x) = x \cdot sigmoid(x)    
$$

```python
# An ordinary implementation of Swish function
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)
 
 
# A memory-efficient implementation of Swish function
class SwishImplementation(torch.autograd.Function):
    @staticmethod
    def forward(ctx, i):
        result = i * torch.sigmoid(i)
        ctx.save_for_backward(i)
        return result
 
    @staticmethod
    def backward(ctx, grad_output):
        i = ctx.saved_tensors[0]
        sigmoid_i = torch.sigmoid(i)
        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))
 
class MemoryEfficientSwish(nn.Module):
    def forward(self, x):
        return SwishImplementation.apply(x)
```

- ```ctx```是一个上下文对象，可以利用```save_for_backward```来保存```tensors```，在```backward```阶段可以进行获取

- ```ctx.needs_input_grad```作为一个```bool```也可以用来控制每一个```input```是否需要计算梯度：```ctx.needs_input_grad[0] = False```，表示```forward```里的第一个```input```不需要梯度，若此时我们```return```这个位置的梯度值的话，为```None```即可


例1：编写$y = e^x$作为激活函数：

```python
class Expfunc(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        output = torch.exp(x)
        ctx.save_for_backward(x)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        x, =ctx.saved_tensors
        return grad_output * torch.exp(x)
```

例2：编写$y = xW^T + b$的反向传播过程

```python
class LinearFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, w, bias=None):
        ctx.save_for_backward(x, w, bias)
        if bias != None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        x, w, bias = ctx.saved_tensors
        if ctx.needs_input_grad[0]:
            grad_input = grad_output @ weight   # 复合函数求导，链式法则
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t() @ x   # 复合函数求导，链式法则
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0).squeeze(0)
 
        return grad_input, grad_weight, grad_bias
```

