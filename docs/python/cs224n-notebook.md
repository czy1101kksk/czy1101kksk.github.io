# ğŸ›£[Deep Learning]Stanford CS224n:Natural Language Processing  
---
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

!!! info "æƒ³è¯´çš„è¯ğŸ‡"
    <font size = 3.5>
    åˆæ¬¡æ¥è§¦æ·±åº¦å­¦ä¹ æ˜¯åœ¨å¤§ä¸€å¯’å‡å‡†å¤‡æ‰“æ•°æ¨¡æ¯”èµ›çš„æ—¶å€™,æ–­æ–­ç»­ç»­ä¹Ÿå°±å­¦äº†ä¸€å°æ®µæ—¶é—´,å…ˆå°è¯•äº†ææ²çš„[Dive into deep learing](https://zh-v2.d2l.ai)åé¢åœ¨ç¡®å®šæœ¬ç§‘ç§‘ç ”æ–¹å‘åå†³å®šçœ‹cs224nå’ŒHugging faceçš„[NLPcourse](https://huggingface.co/learn/nlp-course/chapter1/1)æŒæ¡ä¸€äº›åŸºç¡€çš„å†…å®¹,å› æ­¤ä¸“é—¨å†™ä¸‹æ¥ç”¨ä½œå¤ä¹ (å¤§ä½¬è½»å–·ğŸ˜¥)
    
    ğŸ”è¯¾ç¨‹ç½‘ç«™ï¼šhttp://web.stanford.edu/class/cs224n/index.html
    
    ğŸ‘€ä¸€äº›èµ„æº: 
    https://www.bilibili.com/video/BV1jt421L7ui/(Bç«™åŒè¯­ç²¾ç¿»)
    https://www.showmeai.tech/tutorials/36?articleId=231(è¯¾ä»¶ç¿»è¯‘+çŸ¥è¯†æ¢³ç†)

    </font>

![](img/cs224n.png)

## ğŸ¥—Lecture 1 
---
<font size = 4>

!!! note "Representing words as discrete symbols - ont-hot vectors"
    <font size = 4>
    The tradional NLP use <B>one-hot vectors</B> to represent words as discrete symbol,

    ```python
    from sklearn.preprocessing import OneHotEncoder

    encoder = OneHotEncoder()

    X = [['Male', 1], ['Female', 3], ['Female', 2]]

    encoder.fit(X)
    encoded_data = encoder.transform(X)

    dense_array = encoded_data.toarray()

    print(dense_array)

    #: [[0. 1. 1. 0. 0.]
    #   [1. 0. 0. 0. 1.]
    #   [1. 0. 0. 1. 0.]]
    ``` 
    </font>

- All the one-hot vectors are orthogonal, and there is no natural notion of similarity for them ,making the dimension of the one-hot vectors too large.



</font>