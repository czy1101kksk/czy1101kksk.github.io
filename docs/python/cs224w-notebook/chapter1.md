# ğŸ›£[Deep Learning]Stanford CS224w:Machine Learning with Graphs
---
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

!!! info "æƒ³è¯´çš„è¯ğŸ‡"
    <font size = 3.5>
    
    ğŸ”è¯¾ç¨‹ç½‘ç«™ï¼šhttp://web.stanford.edu/class/cs224w/
    
    ğŸ‘€ä¸€äº›èµ„æº: 
    Bç«™ç²¾è®²ï¼šhttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&vd_source=280e4970f2995a05fdeab972a42bfdd0
    
    https://github.com/TommyZihao/zihao_course/tree/main/CS224W
    
    Slides: http://web.stanford.edu/class/cs224w/slides
    
    </font>

## Node Embeddings
---

![](./img/c1.png)

åœ¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æµç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¯¹åŸå§‹æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹```feature engineering```ï¼ˆæ¯”å¦‚æå–ç‰¹å¾ç­‰ï¼‰ï¼Œä½†æ˜¯ç°åœ¨æˆ‘ä»¬ä½¿ç”¨è¡¨ç¤ºå­¦ä¹ ```representation learning```çš„æ–¹å¼æ¥è‡ªåŠ¨å­¦ä¹ åˆ°æ•°æ®çš„ç‰¹å¾ï¼Œç›´æ¥åº”ç”¨äºä¸‹æµé¢„æµ‹ä»»åŠ¡ã€‚

å›¾çš„è¡¨ç¤ºå­¦ä¹ ï¼šMap nodes into an embedding space, similarity of embeddings between nodes indicates their similarity in the network.For exmaple : Both nodes are close to each other(connected by an edge).

![](./img/c2.png)

![](./img/c6.png)

we assume an graph $G$: $V$ is the vertex set and $A$ is the adjacency matrix (assume binary).

> the adjacency matrix(é‚»æ¥çŸ©é˜µ): è¡¨ç¤ºé¡¶ç‚¹ä¹‹é—´ç›¸é‚»å…³ç³»çš„çŸ©é˜µï¼Œè‹¥ä¸¤ä¸ªé¡¶ç‚¹ç›¸é‚»ï¼Œåˆ™å¯¹åº”ä½ç½®çš„å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚

![](./img/c3.png)

Our goal is to encode nodes making the similarity in the embedding space reflect the similarity in the graph.

![](./img/c4.png)

So <B>the definition of similarity function</B> is the key(a 
measure of similarity in the original network).

$$
similarity(u,v)  \approx  \mathbf{z}_v^T \mathbf{z}_u
$$

<B>Encoder</B>: maps each node to a low-dimensional vector $\mathbf{z}_v$.we assume $Z \in \mathbb{R}^{d \times |V|}$ as the matrix of embeddings and $v \in \mathbb{I}^{|V|}$ as indicator vector.

$$
ENC(v) = \mathbf{z}_v \stackrel{simplest}{=} \mathbf{Z} \cdot v
$$

![](./img/c5.png)

### Random Walks

![](./img/c7.png)

> $P(v|\mathbf{z}_u)$æ˜¯ä»$u$å¼€å§‹éšæœºæ¸¸èµ°èƒ½åˆ°$v$çš„æ¦‚ç‡ï¼Œè¡¡é‡$u$å’Œ$v$çš„ç›¸ä¼¼åº¦ï¼Œç”¨èŠ‚ç‚¹embeddingå‘é‡ç›¸ä¼¼æ€§ç®—æ¦‚ç‡ã€‚

![](./img/c8.png)

$$
\mathbf{z}^T_u \mathbf{z}_v = \text{the probability that u and v c0-occur on a random walk over the graph}
$$

![](./img/c9.png)

> Why random walks?
    
    - Expressivity: Flexible stochastic definition of node similarity that incorporates both local and higher-order neighborhood information
    
    - Idea: if random walk starting from node ğ’– visits ğ’— with high probability, ğ’– and ğ’— are similar (high-order multi-hop information)
    
    - Efficiency: Do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks

The definition of nearby nodes and our goal to learn a mapping:

- $N_R(u)$: neighbourhood of $u$ which can be obtained by random walk

- $f:u â†’ \mathbb{R}^d:f(u)=\mathbf{z}_u$ 

-  Log-likelihood objective:

    $\mathop{\arg\max}\limits_{z} \mathop{\sum}\limits_{u \in V}  \log P(N_R(u) | \mathbf{z}_u) $

    Equivalently,

    $\mathop{\arg\min}\limits_{z} â„’ = \mathop{\sum}\limits_{u \in V} \mathop{\sum}\limits_{v \in N_R(u)} - \log P(v | \mathbf{z}_u)$

    > å³ä½¿å¾—ç›¸é‚»çš„nodesä¹‹é—´çš„ç›¸ä¼¼åº¦æœ€å¤§åŒ–

![](./img/c10.png)

- Parameterize $P(v|\mathbf{z}_u)$ as a softmax function:

    $$
    P(v|\mathbf{z}_u) = \frac{\exp(\mathbf{z}_u^T \mathbf{z}_v)}{\sum_{n \in V} \exp(\mathbf{z}_{u}^T \mathbf{z}_n)}
    $$

so,the log-likelihood objective can transfer to:

$$
â„’ = \mathop{\sum}\limits_{u \in V} \mathop{\sum}\limits_{v \in N_R(u)} - \log \frac{\exp(\mathbf{z}_u^T \mathbf{z}_v)}{\sum_{n \in V} \exp(\mathbf{z}_{u}^T \mathbf{z}_n)}
$$

![](./img/c11.png)

> Obviously $O(|V|^2)$ complexity, doing this naively is too expensive.

![](./img/c12.png)

![](./img/c13.png)

> ä¸Šè¿°çš„éšæœºæ¸¸èµ°ç­–ç•¥æ˜¯å®Œå…¨éšæœºçš„ï¼Œå›ºå®šé•¿åº¦çš„æ¸¸èµ°ï¼Œæ˜¯å¦éœ€è¦æ”¹è¿›ï¼Ÿ

### Node2Vecï¼šBiased Walks

> Node2Vecå’Œéšæœºæ¸¸èµ°çš„åŒºåˆ«æ˜¯å¦‚ä½•å®šä¹‰ç›¸é‚»èŠ‚ç‚¹é›†â€”â€”ä»¥åŠå¦‚ä½•å®šä¹‰éšæœºæ¸¸èµ°çš„ç­–ç•¥

![](./img/c14.png)

Node2Vec gives two parameters to control the random walk:

- Return parameter $p$: probability of returning to the previous node

- In-out parameter $q$: the â€œratioâ€ of BFS vs. DFS

![](./img/c15.png)
![](./img/c16.png)
![](./img/c17.png)

>Core idea: Embedding nodes so that distances in embedding space reflect node similarities in the original network.

![](./img/c18.png)

### Matrix Factorization

![](./img/c19.png)
![](./img/c20.png)

### Limitations

- æ— æ³•ç«‹åˆ»æ³›åŒ–åˆ°æ–°åŠ å…¥çš„èŠ‚ç‚¹ï¼Œæ— æ³•å¤„ç†åŠ¨æ€ç½‘ç»œï¼ˆCannot obtain embeddings for nodes not in the training set. Cannot apply to new graphs, evolving graphsï¼‰

- Cannot capture structural similarity

![](./img/c21.png)

- ä»…ä»…ä½¿ç”¨äº†èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥ä¿¡æ¯(Cannot utilize node, edge and graph features)

### Embedding Entire Graphs

The Goal: Embed a subgraph(å­å›¾) $G$ into a low-dimensional space $\mathbb{R}^d$

- Approach 1: ç›´æ¥å¯¹æ‰€æœ‰èŠ‚ç‚¹é”®å…¥æ±‚å’Œ/å¹³å‡

$$
z_G = \sum_{v \in G} z_v
$$

- Approach 2: å¼•å…¥ä¸€ä¸ªè™šæ‹ŸèŠ‚ç‚¹ï¼ˆvirtual nodeï¼‰ï¼Œæ±‚å‡ºè™šæ‹ŸèŠ‚ç‚¹çš„åµŒå…¥æ¥ä»£æ›¿å­å›¾çš„åµŒå…¥

![](./img/a1.png)

- Approach 3: Anonymous Walks(åŒ¿åéšæœºæ¸¸èµ°)




![](./img/c22%20(1).png)

![](./img/c22%20(2).png)