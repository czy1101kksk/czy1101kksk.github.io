# ğŸ›£[Deep Learning]Stanford CS224w:Machine Learning with Graphs
---
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

!!! info "æƒ³è¯´çš„è¯ğŸ‡"
    <font size = 3.5>
    
    ğŸ”è¯¾ç¨‹ç½‘ç«™ï¼šhttp://web.stanford.edu/class/cs224w/
    
    ğŸ‘€ä¸€äº›èµ„æº: 
    Bç«™ç²¾è®²ï¼šhttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&vd_source=280e4970f2995a05fdeab972a42bfdd0
    
    https://github.com/TommyZihao/zihao_course/tree/main/CS224W
    
    Slides: http://web.stanford.edu/class/cs224w/slides

    Optional Readingsï¼š
    
    [DeepWalk: Online Learning of Social Representations](https://arxiv.org/pdf/1403.6652)

    [node2vec: Scalable Feature Learning for Networks](https://arxiv.org/pdf/1607.00653)

    [Network Embedding as Matrix Factorization](https://arxiv.org/pdf/1710.02971.pdf)
    
    </font>


## Node Embeddings
---

![](./img/c1.png)

åœ¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æµç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¯¹åŸå§‹æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹```feature engineering```ï¼ˆæ¯”å¦‚æå–ç‰¹å¾ç­‰ï¼‰ï¼Œä½†æ˜¯ç°åœ¨æˆ‘ä»¬ä½¿ç”¨è¡¨ç¤ºå­¦ä¹ ```representation learning```çš„æ–¹å¼æ¥è‡ªåŠ¨å­¦ä¹ åˆ°æ•°æ®çš„ç‰¹å¾ï¼Œç›´æ¥åº”ç”¨äºä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡ã€‚

å›¾çš„è¡¨ç¤ºå­¦ä¹ ï¼šMap nodes into an embedding space, similarity of embeddings between nodes indicates their similarity in the network.For exmaple : Both nodes are close to each other(connected by an edge).

![](./img/c2.png)

![](./img/c6.png)

we assume an graph $G$: $V$ is the vertex set and $A$ is the adjacency matrix (assume binary).

> the adjacency matrix(é‚»æ¥çŸ©é˜µ): è¡¨ç¤ºé¡¶ç‚¹ä¹‹é—´ç›¸é‚»å…³ç³»çš„çŸ©é˜µï¼Œè‹¥ä¸¤ä¸ªé¡¶ç‚¹ç›¸é‚»ï¼Œåˆ™å¯¹åº”ä½ç½®çš„å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚

![](./img/c3.png)

Our goal is to encode nodes making the similarity in the embedding space reflect the similarity in the graph.

![](./img/c4.png)

So <B>the definition of similarity function</B> is the key(a 
measure of similarity in the original network).

$$
similarity(u,v)  \approx  \mathbf{z}_v^T \mathbf{z}_u
$$

<B>Encoder</B>: maps each node to a low-dimensional vector $\mathbf{z}_v$.we assume $Z \in \mathbb{R}^{d \times |V|}$ as the matrix of embeddings and $v \in \mathbb{I}^{|V|}$ as indicator vector.

$$
ENC(v) = \mathbf{z}_v \stackrel{simplest}{=} \mathbf{Z} \cdot v
$$

![](./img/c5.png)

### Random Walks

![](./img/c7.png)

> $P(v|\mathbf{z}_u)$æ˜¯ä»$u$å¼€å§‹éšæœºæ¸¸èµ°èƒ½åˆ°$v$çš„æ¦‚ç‡ï¼Œè¡¡é‡$u$å’Œ$v$çš„ç›¸ä¼¼åº¦ï¼Œç”¨èŠ‚ç‚¹embeddingå‘é‡ç›¸ä¼¼æ€§ç®—æ¦‚ç‡ã€‚ 

![](./img/c8.png)

$$
\mathbf{z}^T_u \mathbf{z}_v = \text{the probability that u and v co-occur on a random walk over the graph}
$$

![](./img/c9.png)

> Why random walks?
    
    - Expressivity: Flexible stochastic definition of node similarity that incorporates both local and higher-order neighborhood information
    
    - Idea: if random walk starting from node ğ’– visits ğ’— with high probability, ğ’– and ğ’— are similar (high-order multi-hop information)
    
    - Efficiency: Do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks

The definition of nearby nodes and our goal to learn a mapping:

- $N_R(u)$: neighbourhood of $u$ which can be obtained by random walk

- $f:u â†’ \mathbb{R}^d:f(u)=\mathbf{z}_u$ 

-  Log-likelihood objective:

    $\mathop{\arg\max}\limits_{z} \mathop{\sum}\limits_{u \in V}  \log P(N_R(u) | \mathbf{z}_u) $

    Equivalently,

    $\mathop{\arg\min}\limits_{z} â„’ = \mathop{\sum}\limits_{u \in V} \mathop{\sum}\limits_{v \in N_R(u)} - \log P(v | \mathbf{z}_u)$

    > ä½¿ç›¸é‚»çš„nodesä¹‹é—´çš„ç›¸ä¼¼åº¦æœ€å¤§åŒ–

![](./img/c10.png)

- Parameterize $P(v|\mathbf{z}_u)$ as a softmax function:

    $$
    P(v|\mathbf{z}_u) = \frac{\exp(\mathbf{z}_u^T \mathbf{z}_v)}{\sum_{n \in V} \exp(\mathbf{z}_{u}^T \mathbf{z}_n)}
    $$

so,the log-likelihood objective can transfer to:

$$
â„’ = \mathop{\sum}\limits_{u \in V} \mathop{\sum}\limits_{v \in N_R(u)} - \log \frac{\exp(\mathbf{z}_u^T \mathbf{z}_v)}{\sum_{n \in V} \exp(\mathbf{z}_{u}^T \mathbf{z}_n)}
$$

![](./img/c11.png)

> Obviously $O(|V|^2)$ complexity, doing this naively is too expensive.

![](./img/c12.png)

![](./img/c13.png)

> å±‚æ¬¡Softmaxï¼ˆHierarchical Softmaxï¼‰ä¼˜åŒ–ç®—æ³•ï¼Œé¿å…è®¡ç®—æ‰€æœ‰è¯çš„softmax

> ä¸Šè¿°çš„éšæœºæ¸¸èµ°ç­–ç•¥æ˜¯å®Œå…¨éšæœºçš„ï¼Œå›ºå®šé•¿åº¦çš„æ¸¸èµ°ï¼Œæ˜¯å¦éœ€è¦æ”¹è¿›ï¼Ÿ

### DeepWalkï¼šRandWalk + Skip-Gram
---

Codeï¼š[https://github.com/phanein/deepwalk](https://github.com/phanein/deepwalk)

DeepWalkå°†å›¾æ•°æ®ä¸è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼ˆWord2Vecï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡éšæœºæ¸¸èµ°å°†å›¾ç»“æ„è½¬åŒ–ä¸ºèŠ‚ç‚¹åºåˆ—ï¼Œç„¶åä½¿ç”¨Skip-Gramæ¨¡å‹è®­ç»ƒè¯åµŒå…¥ï¼Œç”¨äºå­¦ä¹ ç½‘ç»œä¸­é¡¶ç‚¹çš„æ½œåœ¨è¡¨ç¤ºã€‚

![](./img/deepwalk.png)

â‘ ä»ç½‘ç»œä¸­çš„æ¯ä¸ªèŠ‚ç‚¹å¼€å§‹åˆ†åˆ«è¿›è¡ŒRandomWalké‡‡æ ·ï¼Œå¾—åˆ°å±€éƒ¨ç›¸å…³è”çš„è®­ç»ƒæ•°æ®ï¼›

â‘¡å¯¹é‡‡æ ·æ•°æ®è¿›è¡ŒSkipGramè®­ç»ƒï¼Œå°†ç¦»æ•£çš„ç½‘ç»œèŠ‚ç‚¹è¡¨ç¤ºæˆå‘é‡åŒ–ï¼Œæœ€å¤§åŒ–èŠ‚ç‚¹å…±ç°ï¼Œä½¿ç”¨Hierarchical Softmaxæ¥åšè¶…å¤§è§„æ¨¡åˆ†ç±»çš„åˆ†ç±»å™¨



### Node2Vecï¼šBiased Walks
---

Codeï¼šhttps://github.com/aditya-grover/node2vec

> Node2Vecå’Œéšæœºæ¸¸èµ°çš„åŒºåˆ«æ˜¯å¦‚ä½•å®šä¹‰ç›¸é‚»èŠ‚ç‚¹é›†â€”â€”ä»¥åŠå¦‚ä½•å®šä¹‰éšæœºæ¸¸èµ°çš„ç­–ç•¥(åéšæœºæ¸¸èµ°)

![](./img/c14.png)

- BFSï¼šèŠ‚ç‚¹åŠŸèƒ½è§’è‰²structural equivalence

- DFSï¼šåŒè´¨ç¤¾ç¾¤homophily


Node2Vec gives two parameters to control the random walk:

- Return parameter $p$: probability of returning to the previous node

- In-out parameter $q$: the â€œratioâ€ of BFS vs. DFS

å¼•å…¥è¿™ä¸¤ä¸ªè¶…å‚æ•°$pï¼Œq$ï¼Œæ¥æ§åˆ¶éšæœºæ¸¸èµ°çš„ç­–ç•¥ã€‚å‡è®¾å½“å‰éšæœºæ¸¸èµ°ç»è¿‡è¾¹$(t,v)$åˆ°è¾¾èŠ‚ç‚¹$v$ã€‚åˆ™è½¬ç§»ç­–ç•¥éµå¾ªä»¥ä¸‹å…¬å¼ï¼š$\pi_{vx}=\alpha_{pq}(t,x) \cdot w_{vx}$ï¼Œè½¬ç§»ç­–ç•¥ä¸º$\alpha_{pq}(t,x)$ï¼Œ$w_vx$æ˜¯èŠ‚ç‚¹$v$ä¸$x$ä¹‹é—´çš„è¾¹æƒã€‚$d_{tx}$ä¸ºèŠ‚ç‚¹$t$å’Œ$x$ä¹‹é—´çš„æœ€çŸ­è·¯å¾„è·ç¦»ï¼š

$$
\alpha_{pq}(t,x) = 
\begin{cases}
\frac{1}{p}, & if \quad d_{tx}=0 \\
1, & if \quad d_{tx}=1  \\
\frac{1}{q}, & if \quad d_{tx}=2 \\
\end{cases}
$$

![](./img/c15.png)
![](./img/c16.png)
![](./img/c17.png)

>Core idea: Embedding nodes so that distances in embedding space reflect node similarities in the original network.

![](./img/c18.png)

### Matrix Factorization

![](./img/c19.png)
![](./img/c20.png)

### Limitations

- æ— æ³•ç«‹åˆ»æ³›åŒ–åˆ°æ–°åŠ å…¥çš„èŠ‚ç‚¹ï¼Œæ— æ³•å¤„ç†åŠ¨æ€ç½‘ç»œï¼ˆCannot obtain embeddings for nodes not in the training set. Cannot apply to new graphs, evolving graphsï¼‰

- Cannot capture structural similarity

![](./img/c21.png)

- ä»…ä»…ä½¿ç”¨äº†èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥ä¿¡æ¯(Cannot utilize node, edge and graph features)

### Embedding Entire Graphs

The Goal: Embed a subgraph(å­å›¾) $G$ into a low-dimensional space $\mathbb{R}^d$

- Approach 1: ç›´æ¥å¯¹æ‰€æœ‰èŠ‚ç‚¹é”®å…¥æ±‚å’Œ/å¹³å‡

$$
z_G = \sum_{v \in G} z_v
$$

- Approach 2: å¼•å…¥ä¸€ä¸ªè™šæ‹ŸèŠ‚ç‚¹ï¼ˆvirtual nodeï¼‰ï¼Œæ±‚å‡ºè™šæ‹ŸèŠ‚ç‚¹çš„åµŒå…¥æ¥ä»£æ›¿å­å›¾çš„åµŒå…¥

![](./img/a1.png)

- Approach 3: Anonymous Walks(åŒ¿åéšæœºæ¸¸èµ°)


![](./img/c22%20(1).png)

![](./img/c22%20(2).png)