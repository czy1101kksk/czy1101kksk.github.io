# ğŸ›£[Deep Learning]Stanford CS224w:Machine Learning with Graphs
---
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

!!! info "æƒ³è¯´çš„è¯ğŸ‡"
    <font size = 3.5>
    
    ğŸ”è¯¾ç¨‹ç½‘ç«™ï¼šhttp://web.stanford.edu/class/cs224w/
    
    ğŸ‘€ä¸€äº›èµ„æº: 
    Bç«™ç²¾è®²ï¼šhttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&vd_source=280e4970f2995a05fdeab972a42bfdd0
    
    https://github.com/TommyZihao/zihao_course/tree/main/CS224W
    
    Slides: http://web.stanford.edu/class/cs224w/slides
    
    </font>

### The limitation of node embedding 

- $O(|V|d)$ parameters are neededï¼šNo sharing of parameters between nodesï¼Œevery node has its own unique embedding

- Have no ability to generate embeddings for nodes that are not in the training set

- Do not incorporate structural node features (e.g. node type, node degree)

### Permutation Invariance(ç½®æ¢ä¸å˜æ€§)

![](./img/a2.png)

For order plan 1 and order plan 2, graph and node representation should be the same, but the node embeddings are different.

Consider we learn a function $f:\mathbb{R}^{|V| \times m}\times \mathbb{R}^{|V| \times |V|}$ to map the graph $G=(A,X)$ to a vector $\mathbb{R}^d$, then the function $f$ should be <B>permutation invariant</B>: $f(A,X) = f(A',X')=f(PAP^T,PX)$ for any permutation $P$.

>  Permutation ğ‘ƒ: a shuffle of the node order.Example:$(A,B,C)->(B,C,A)$.

> for different order of nodes, the adjacency matrix $A$ is different, but the output of $f$ should be the same!.

![](./img/a3.png)

### Permutation Equivariant(ç½®æ¢ç­‰å˜æ€§)

![](./img/a4.png)

Consider we learn a function $f:\mathbb{R}^{|V| \times m}\times \mathbb{R}^{|V| \times |V|}$ to map the graph $G=(A,X)$ to a vector $\mathbb{R}^{|V| \times d}$.then the function $f$ should be <B>permutation equivariant</B>: $Pf(A,X) =f(PAP^T,PX)$ for any permutation $P$.

![](./img/a5.png)
![](./img/a6.png)
![](./img/a7.png)

> Idea: Nodeâ€™s neighborhood defines a computation graph

![](./img/a8.png)
![](./img/a8%20(2).png)

![](./img/c9%20(2).png)

![](./img/a11.png)

### Graph Neural Networks

$$
\begin{aligned}
h_v^{(0)} =& x_v \\
h_v^{(k+1)} =& \sigma(W_k \sum_{u\in N(v)} \frac{h_u^{(k)}}{|N(v)|} + B_k h_v^{(k)}), âˆ€k \in \{ 0,...,k-1 \}\\
z_v =& h_v^{(K)}(\text{Final node embedding})\\
\end{aligned}
$$

è®¾$H^{(k)}=[h_1^{(k)},...,h_{|V|}^{(k)}]^T$ï¼Œåˆ™$\sum_{u \in N_v} h_u^{(k)}=A_{v,:}H^{(k)}$

> A ä¸ºä¸€ä¸ªç¨€ç–çš„å•ä½çŸ©é˜µï¼ŒExample:$\begin{bmatrix} 1 & 0 & ... & 0 & 1 & 0 \\ 1 & 0 & ... & 0 & 1 & 0 \\ ...  \\ 1 & 0 & ... & 0 & 1 & 0 \\ \end{bmatrix}$

è®¾å¯¹è§’çŸ©é˜µï¼ˆdiagonal matrixï¼‰$D$,å³$D_{v,v}=Deg(v)=|N(v)|$,åˆ™$D_{v,v}^{-1}=1/|N(v)|$.

Therefore,$\sum_{u \in N(v)} \frac{h_u^{(k-1)}}{|N(v)|} \rightarrow H^{(k+1)} = D^{-1}AH^{(k)}$

soï¼Œ$H^{(k+1)} = \sigma (D^{-1} A H^{(k)} W_k^T + H^{(k)} B_k^T) $

![](./img/g1.png)

#### Graph unsupervised training

![](./img/g2.png)

#### Graph supervised training

![](./img/g3.png)

```python
import torch
from torch.nn import Linear
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        torch.manual_seed(1234)
        self.conv1 = GCNConv(dataset.num_features, 4)
        self.conv2 = GCNConv(4, 4)
        self.conv3 = GCNConv(4, 2)
        self.classifier = Linear(2, dataset.num_classes)

    def forward(self, x, edge_index):
        h = self.conv1(x, edge_index)
        h = h.tanh()
        h = self.conv2(h, edge_index)
        h = h.tanh()
        h = self.conv3(h, edge_index)
        h = h.tanh()  # Final GNN embedding space.

        # Apply a final (linear) classifier.
        out = self.classifier(h)

        return out, h

model = GCN()

_, h = model(data.x, data.edge_index)
print(f'Embedding shape: {list(h.shape)}')

visualize(h, color=data.y)
```

æ ¹æ®ä¸Šè¿°ï¼ŒGNNçš„ç›®æ ‡æ˜¯è·å–ä¸€ä¸ªè¾“å…¥å›¾$G=(\mathbb{V,E})$ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹$v_i \in \mathbb{V}$éƒ½æœ‰ä¸€ä¸ªè¾“å…¥ç‰¹å¾å‘é‡$X_i$ï¼Œä»¥æ­¤å­¦ä¹ ä¸€ä¸ªå‡½æ•°$f_G : \mathbb{V} \times \mathbb{R}^{d_1} \to \mathbb{R}^{d_2} $ï¼Œè¯¥å‡½æ•°æ¥æ”¶ä¸€ä¸ªèŠ‚ç‚¹åŠå…¶ç‰¹å¾å‘é‡ä»¥åŠå›¾ç»“æ„ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªåµŒå…¥ï¼Œå³ä¸€ä¸ªä»¥å¯¹æˆ‘ä»¬çš„ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„æ–¹å¼è¡¨ç¤ºè¯¥èŠ‚ç‚¹çš„å‘é‡ã€‚

```python
criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.

def train(data):
    optimizer.zero_grad()  # Clear gradients.
    out, h = model(data.x, data.edge_index)  # Perform a single forward pass.
    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.
    loss.backward()  # Derive gradients.
    optimizer.step()  # Update parameters based on gradients.

    accuracy = {}
    # Calculate training accuracy on our four examples
    predicted_classes = torch.argmax(out[data.train_mask], axis=1) # [-0.6, 0.2, 0.7, 0.1] -> 2
    target_classes = data.y[data.train_mask]
    accuracy['train'] = torch.mean(
        torch.where(predicted_classes == target_classes, 1, 0).float())

    # Calculate validation accuracy on the whole graph
    predicted_classes = torch.argmax(out, axis=1)
    target_classes = data.y
    accuracy['val'] = torch.mean(
        torch.where(predicted_classes == target_classes, 1, 0).float())

    return loss, h, accuracy

for epoch in range(1,501):
    loss, h, accuracy = train(data)
    # Visualize the node embeddings every 10 epochs
    if epoch % 10 == 0:
        visualize(h, color=data.y, epoch=epoch, loss=loss, accuracy=accuracy)
```

![](./img2/gnnclassifier.png)

### comparison with other methods

![](./img/f1.png)
![](./img/f2.png)

ä»¥ ```karate club```ä¸ºä¾‹ï¼š

```python
G = nx.karate_club_graph()
```

Q1:Average Degree if a graph
---

```python
def average_degree(num_edges, num_nodes):
    average_degree = 0
    average_degree = round(2* num_edges / num_nodes)
    return average_degree

num_edges = G.number_of_edges()
num_nodes = G.number_of_nodes()
avg_degree = average_degree(num_edges, num_nodes)
print(f'Average Degree:{avg_degree}')
```

Q2:the average clustering coefficient of the graph
---

>å¹³å‡èšç±»ç³»æ•°(average clustering coefficient):æè¿°ä¸€ä¸ªå›¾ä¸­çš„é¡¶ç‚¹ä¹‹é—´é›†æˆ<B>å›¢</B>(clique)çš„ç¨‹åº¦ç³»æ•°ã€‚å³ä¸€ä¸ªç‚¹çš„é‚»æ¥ç‚¹ä¹‹é—´ç›¸äº’è¿æ¥çš„ç¨‹åº¦ã€‚è‹¥ä¸€ä¸ªèŠ‚ç‚¹$i$çš„åº¦ä¸º$k_i$ï¼Œ$e_i$ä¸ºè¯¥èŠ‚ç‚¹ä¸é‚»å±…ä¹‹é—´å­˜åœ¨çš„è¾¹æ•°ï¼Œåˆ™$$C_i=\frac{2e_i}{k_i(k_i-1)}$$

```python
def average_clustering_coefficient(G):
    avg_clustering_coef = 0
    avg_clustering_coef = round(nx.average_clustering(G), 2)
    return avg_clustering_coef

avg_cluster_coef = average_clustering_coefficient(G)
```

Q3:PageRank 
---

- ç»å…¸å›¾ç®—æ³•ï¼šPageRank

PageRank measures importance of nodes in a graph based on its link structure.

Core idea:

- The more pages link to this page, the more important it is;

- A link from an important page is worth more.

If a page $i$ with importance $r_i$ has $d_i$ out-links, then each link gets $\frac{r_i}{d_i}$ votes.Thus, the importance of a page $j$, represented as $r_j$ is the sum of the votes on its in links.

$$
r_j = \sum_{i \rightarrow j} \frac{r_i}{d_i}
$$

, where $d_i$ is the out degree of node $i$.

The PageRank algorithm (used by Google) outputs a <B>probability distribution</B> which represent the likelihood of a random surfer clicking on links will arrive at any particular page.

At each time step, the random surfer has two options:

- with prob. $\beta$, follow a link at random

- with prob. $1- \beta$, jump to a random page

$$
r_j = \beta \sum_{i \rightarrow j} \frac{r_i}{d_i} + (1-\beta) \frac{1}{N}
$$

What is the PageRank value for node 0 after one PageRank iteration?

```python
def one_iter_pagerank(G, beta, r0, node_id):
    r1 = (1 - beta) / G.number_of_nodes()
    for neighbor in G.neighbors(node_id):
        r1 += beta * r0 / G.degree(neighbor)
    # PageRank_list = nx.pagerank(G, alpha=beta)
    return round(r1, 2)

beta = 0.8
r0 = 1 / G.number_of_nodes()
node = 0
r1 = one_iter_pagerank(G, beta, r0, node)
```

Q4:the (raw) closeness centrality
---

> é‚»è¿‘ä¸­å¿ƒåº¦(closeness centrality)è¡¡é‡ç½‘ç»œä¸­èŠ‚ç‚¹åˆ°å…¶ä»–èŠ‚ç‚¹çš„å¹³å‡è·ç¦»,è·ç¦»è¶ŠçŸ­è¡¨ç¤ºèŠ‚ç‚¹è¶Šæ¥è¿‘ç½‘ç»œä¸­çš„å…¶ä»–èŠ‚ç‚¹,å…¶Closeness Centralityå€¼è¶Šé«˜ã€‚$$c(v) = \frac{1}{\sum_{u \neq v}\text{shortest path length between } u \text{ and } v}$$

```python
def closeness_centrality(G, node=5):
    clossness = nx.clossness_centrality(G, node) / (len(nx.node_connected_component(G, node)) - 1)
    clossness = round(clossness, 2)
    return clossness

node = 5
closeness = closeness_centrality(G,  node=node)
```

> ```nx.clossness_centrality```è¾“å‡ºçš„æ˜¯$\frac{n-1}{\sum_{u \neq v} d(u,v)}$ï¼Œå…¶ä¸­$d(u,v)$è¡¨ç¤ºèŠ‚ç‚¹$u$å’Œ$v$ä¹‹é—´çš„æœ€çŸ­è·¯å¾„é•¿åº¦ã€‚

Q5:get the edge_list ,transform it into torch.LongTensor
---

```python
def graph_to_edge_list(G):
    return [edge for edge in G.edges()]
def edge_list_to_tensor(edge_list):
    return torch.LongTensor(edge_list).t()

pos_edge_list = graph_to_edge_list(G)
pos_edge_index = edge_list_to_tensor(pos_edge_list)
```

Q6:negative sampling
---

> "Negative" edges æŒ‡çš„æ˜¯å›¾ä¸­ä¸å­˜åœ¨çš„è¾¹ï¼Œå¯ä»¥ä½œä¸ºè´Ÿæ ·æœ¬

```python
import random

def sample_negative_edges(G, num_neg_samples):
    neg_edge_list = [random.sample(list(enumerate(nx.non_edges(G))), num_neg_samples)[i][1] for i in range(num_neg_samples)]
    return neg_edge_list

def can_be_negative(G, edge):
    return not G.has_edge(*edge)
```