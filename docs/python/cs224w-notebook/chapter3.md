# ğŸ›£[Deep Learning]Stanford CS224w:Machine Learning with Graphs
---
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

!!! info "æƒ³è¯´çš„è¯ğŸ‡"
    <font size = 3.5>
    
    ğŸ”è¯¾ç¨‹ç½‘ç«™ï¼šhttp://web.stanford.edu/class/cs224w/
    
    ğŸ‘€ä¸€äº›èµ„æº: 
    Bç«™ç²¾è®²ï¼šhttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&vd_source=280e4970f2995a05fdeab972a42bfdd0
    
    https://github.com/TommyZihao/zihao_course/tree/main/CS224W
    
    Slides: http://web.stanford.edu/class/cs224w/slides
    
    </font>

### A GNN Layer

![](./img/k1.png)

### Message Computation

- Message function: $m_u^{(l)} = MSG^{(l)} (h_u^{(l-1)})$, For example: A Linear layer $m_u^{(l)} = W^{(l)} h^{(l-1)} $

>  Intuition: Each node will create a message, which will be sent to other nodes later

![](./img/k2.png)

### Aggregation 

- Aggregation function: $h_v^{(l)} = AGG^{(l)} (m_{v1}^{(l)}, m_{v2}^{(l)}, \ldots, m_{vk}^{(l)})$, For example: Summing up all messages $h_v^{(l)} = \sum_{u \in N(v)} m_u^{(l)}$

>  Intuition: Node ğ‘£ will aggregate the messages from its neighbors 

In order to consider the node $v$ itself, we can include $h_v^{(l-1)}$ when computing $h_v^{(l)}$.For example: 

$$
h_v^{(l)} = Concat(AGG({m_u^{(l)}}, u \in N(v)), m_v^{(l)})
$$

![](./img/k4.png)

### Graph Convolutional Network (GCN)

![](./img/k3.png)

è®ºæ–‡æ ‡é¢˜: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)

å‡è®¾ä¸€ä¸ªä¸‹å›¾è¿™æ ·çš„å›¾ç»“æ„ï¼Œå­˜åœ¨ABCDEäº”ä¸ªèŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹ä¹‹é—´å­˜åœ¨è¿æ¥

![](./img/k8.png)

å› æ­¤ï¼Œå¯ä»¥è¿™æ ·èšåˆä¿¡æ¯ï¼š

![](./img/k9.png)

ä½†æ˜¾ç„¶è¿™æ ·çš„èšåˆæ–¹å¼è¿‡äºç®€å•ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°èŠ‚ç‚¹è‡ªèº«ä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ”¹è¿›é‚»æ¥çŸ©é˜µï¼š

$$
\widetilde{A} = A + I
$$

å°†èŠ‚ç‚¹è‡ªèº«ä¸å…¶è¿‘é‚»ç²—æš´çš„åŠ å’Œçš„èšåˆæ–¹æ³•æ˜¾ç„¶æ˜¯æœ‰é—®é¢˜çš„ï¼Œç›¸å½“äºæˆ‘ä»¬å˜ç›¸çš„æ”¹å˜äº†ç‰¹å¾çš„é‡çº§ï¼Œéšç€è¿­ä»£çš„å¢åŠ ï¼Œç‰¹å¾é‡çº§ä¼šå˜å¾—è¶Šæ¥è¶Šå¤§ã€‚å› æ­¤æˆ‘ä»¬å¼•å…¥$D=Deg(N(v))= \sum_j A_{ij}$çš„åº¦çŸ©é˜µï¼ˆä¸è¯¥èŠ‚ç‚¹ç›¸é‚»èŠ‚ç‚¹çš„æ•°æ®ï¼‰,å¹¶ä¸”è€ƒè™‘è‡ªèº«ä¿¡æ¯ï¼Œåœ¨$D$ä¸ŠåŠ å…¥å•ä½çŸ©é˜µ$I$ï¼š$\widetilde{D}=D+I=\sum_j \widetilde{A}_{ij}$

æœ€ç»ˆï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š$\widetilde{D}^{-1} \widetilde{A} X$

> $\widetilde{D}^{-1}$ä½œä¸ºä¸€ä¸ªåˆç­‰çŸ©é˜µï¼Œå·¦ä¹˜$\widetilde{D}^{-1}$ç›¸å½“äºè¡Œå˜æ¢ï¼Œå³$\widetilde{A} X$ æ¯ä¸€è¡Œé™¤ä»¥åº¦.

- <B>Renormalization trick</B>: $\widetilde{D}^{-\frac{1}{2}} \widetilde{A} \widetilde{D}^{-\frac{1}{2}} X$

$\widetilde{A}$å·¦å³éƒ½ä¹˜ä»¥ä¸€ä¸ªåˆç­‰çŸ©é˜µ$\widetilde{D}^{-\frac{1}{2}}$ï¼Œ$\widetilde{A}_{ij}$è¡¨ç¤º$\widetilde{A}$ä¸­ç¬¬$i$è¡Œç¬¬$j$åˆ—çš„å…ƒç´ ï¼Œå³ä¸º$i$èŠ‚ç‚¹ä¸$j$èŠ‚ç‚¹çš„å…³ç³»ã€‚å·¦ä¾§çš„$\widetilde{D}^{-\frac{1}{2}}$å¾ˆå¥½ç†è§£ï¼Œå³æ˜¯å¯¹é‡åº¦çš„å½’ä¸€åŒ–ï¼Œè€Œå³ä¾§çš„$\widetilde{D}^{-\frac{1}{2}}$åˆ™æ˜¯å¯¹èŠ‚ç‚¹é—´å…³ç³»çš„å½’ä¸€åŒ–ï¼Œå³æ˜¯å°†èŠ‚ç‚¹çš„é‚»æ¥èŠ‚ç‚¹å¯¹å…¶çš„â€œè´¡çŒ®â€è¿›è¡Œæ ‡å‡†åŒ–ã€‚

> æ¯”å¦‚è¯´Appä¸Šçš„ç”¨æˆ·ä¹‹é—´çš„å…³æ³¨å…³ç³»æ˜¯å¤©ç„¶çš„åŒæ„å›¾ï¼Œå‡è®¾æˆ‘ä»¬è¦åšèŠ‚ç‚¹åˆ†ç±»åˆ¤å®šæŸä¸ªç”¨æˆ·Aæ˜¯ä¸æ˜¯ç®—æ³•å·¥ç¨‹å¸ˆï¼Œå¹¶ä¸”å‡è®¾Aç”¨æˆ·ä»…ä»…å’Œå¦ä¸€ä¸ªç®—æ³•å·¥ç¨‹å¸ˆBä»¥åŠ10ä¸ªçŒå¤´æœ‰å…³æ³¨çš„å…³ç³»ã€‚ç›´è§‚ä¸Šï¼ŒçŒå¤´å¯¹ç”¨æˆ·Açš„èŠ‚ç‚¹ç±»åˆ«çš„åˆ¤å®šçš„è´¡çŒ®åº”è¯¥æ˜¯å¾ˆå°çš„ï¼Œå› ä¸ºçŒå¤´å¾€å¾€ä¼šå’Œç®—æ³•ï¼Œå¼€å‘ï¼Œæµ‹è¯•ï¼Œäº§å“ç­‰ä¸åŒç±»å‹çš„ç”¨æˆ·æœ‰å…³è”å…³ç³»ï¼Œä»–ä»¬å¯¹äºç”¨æˆ·Açš„â€œå¿ è¯šå…³è”åº¦â€æ˜¯å¾ˆä½çš„ï¼Œè€Œå¯¹äºç®—æ³•å·¥ç¨‹å¸ˆBè€Œè¨€ï¼Œå‡è®¾ä»–ä»…ä»…å’ŒAæœ‰å…³è”ï¼Œé‚£ä¹ˆæ˜æ˜¾Bå¯¹Açš„â€œå¿ è¯šå…³è”åº¦â€æ˜¯å¾ˆé«˜çš„ï¼ŒBçš„Node Featuresä»¥åŠBå’ŒAçš„å…³è”å…³ç³»åœ¨å¯¹Aè¿›è¡ŒèŠ‚ç‚¹åˆ†ç±»çš„æ—¶å€™åº”è¯¥æ˜¯æ›´é‡è¦çš„ï¼

- GCNç®—æ³•æµç¨‹

å›¾ç½‘ç»œçš„è®¡ç®—å°±æ˜¯ä¸æ–­è€ƒè™‘é‚»å±…åŠè‡ªèº«ä¿¡æ¯çš„ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œæ¯è¿›è¡Œä¸€æ¬¡è¿­ä»£å°±æ˜¯ä¸€æ¬¡ç‰¹å¾é‡ç»„ï¼Œä¸‹ä¸€å±‚çš„ç‰¹å¾ä¸ºä¸Šä¸€å±‚ç‰¹å¾çš„å›¾å·ç§¯ï¼š

$$
X^{k+1} = \widetilde{D}^{-\frac{1}{2}} \widetilde{A} \widetilde{D}^{-\frac{1}{2}} X^k
$$

ä»¤renormalizationä¹‹åçš„çŸ©é˜µä¸º$\hat{A}$:

$$
X^{k+1} = \hat{A} X^k
$$

ä¸€ä¸ªä¸¤å±‚çš„GCNç½‘ç»œä¸º($W$ä¸ºæƒé‡çŸ©é˜µ)ï¼š

$$
Z = \hat{A} \sigma (\hat{A} X^k W^{(0)}) W^{(1)}
$$

<details> 
<summary>utils</summary>

```python
def encode_onehot(labels): #ç‹¬çƒ­ç¼–ç 
    classes = set(labels)
    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)} #å¯¹è§’å•ä½çŸ©é˜µ
    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)
    return labels_onehot


def load_data(path="../data/cora/", dataset="cora"):
    """Load citation network dataset (cora only for now)"""
    print('Loading {} dataset...'.format(dataset))

    idx_features_labels = np.genfromtxt(f"{path}{dataset}.content", dtype=np.dtype(str))
    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) #ç¨€ç–çŸ©é˜µ
    labels = encode_onehot(idx_features_labels[:, -1])

    # build graph
    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)
    idx_map = {j: i for i, j in enumerate(idx)}
    edges_unordered = np.genfromtxt(f"{path}{dataset}.cites", dtype=np.int32)
    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),
                     dtype=np.int32).reshape(edges_unordered.shape)
    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)
    # sp.coo_matrix((data,(row,col)), shape=, dtype=)
    # build symmetric adjacency matrix
    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)
    # å¯¹ç§°åŒ–é‚»æ¥çŸ©é˜µ(æ— å‘å›¾)
    features = normalize(features)
    adj = normalize(adj + sp.eye(adj.shape[0]))

    idx_train = range(140)
    idx_val = range(200, 500)
    idx_test = range(500, 1500)

    features = torch.FloatTensor(np.array(features.todense()))
    labels = torch.LongTensor(np.where(labels)[1])
    adj = sparse_mx_to_torch_sparse_tensor(adj)

    idx_train = torch.LongTensor(idx_train)
    idx_val = torch.LongTensor(idx_val)
    idx_test = torch.LongTensor(idx_test)

    return adj, features, labels, idx_train, idx_val, idx_test


def normalize(mx):
    """Row-normalize sparse matrix"""
    rowsum = np.array(mx.sum(1))
    r_inv = np.power(rowsum, -1/2).flatten()
    r_inv[np.isinf(r_inv)] = 0.
    r_mat_inv = sp.diags(r_inv) #å¯¹è§’çŸ©é˜µ
    mx = r_mat_inv.dot(mx.dot(r_mat_inv))
    return mx

def accuracy(output, labels):
    preds = output.max(1)[1].type_as(labels)
    correct = preds.eq(labels).double()
    correct = correct.sum()
    return correct / len(labels)

def sparse_mx_to_torch_sparse_tensor(sparse_mx):
    """Convert a scipy sparse matrix to a torch sparse tensor."""
    sparse_mx = sparse_mx.tocoo().astype(np.float32)
    # COOæ ¼å¼(.tocoo())ç”¨ä¸‰ä¸ªæ•°ç»„æ¥è¡¨ç¤ºçŸ©é˜µï¼š
    # è¡Œç´¢å¼•æ•°ç»„.rowã€åˆ—ç´¢å¼•æ•°ç»„.colã€æ•°æ®æ•°ç»„.data
    indices = torch.from_numpy(
        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))
    values = torch.from_numpy(sparse_mx.data)
    shape = torch.Size(sparse_mx.shape)
    return torch.sparse.FloatTensor(indices, values, shape)
```

</details>

<details> 
<summary>Model</summary>

```python
class GraphConvolution(Module):
    """
    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907
    """
    def __init__(self, in_features, out_features, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.FloatTensor(in_features, out_features))
        if bias:
            self.bias = Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self): # åˆå§‹åŒ–æƒé‡å’Œåç½®å‚æ•°
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input, adj):
        support = torch.mm(input, self.weight)
        output = torch.spmm(adj, support)  #torch.spmmç¨€ç–çŸ©é˜µä¹˜æ³•
        if self.bias is not None:
            return output + self.bias
        else:
            return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
               + str(self.in_features) + ' -> ' \
               + str(self.out_features) + ')'

class GCN(nn.Module):
    def __init__(self, nfeat, nhid, nclass, dropout):
        super(GCN, self).__init__()

        self.gc1 = GraphConvolution(nfeat, nhid)
        self.gc2 = GraphConvolution(nhid, nclass)
        self.dropout = dropout

    def forward(self, x, adj):
        x = F.relu(self.gc1(x, adj))
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.gc2(x, adj)
        return F.log_softmax(x, dim=1)
```
</details> 

<details> 
<summary>Train</summary>

```python
from __future__ import division
from __future__ import print_function
import time
import argparse
import numpy as np

import torch
import torch.nn.functional as F
import torch.optim as optim

from pygcn.utils import load_data, accuracy
from pygcn.models import GCN

# Training settings
parser = argparse.ArgumentParser()
parser.add_argument('--no-cuda', action='store_true', default=False,
                    help='Disables CUDA training.')
parser.add_argument('--fastmode', action='store_true', default=False,
                    help='Validate during training pass.')
parser.add_argument('--seed', type=int, default=42, help='Random seed.')
parser.add_argument('--epochs', type=int, default=200,
                    help='Number of epochs to train.')
parser.add_argument('--lr', type=float, default=0.01,
                    help='Initial learning rate.')
parser.add_argument('--weight_decay', type=float, default=5e-4,
                    help='Weight decay (L2 loss on parameters).')
parser.add_argument('--hidden', type=int, default=16,
                    help='Number of hidden units.')
parser.add_argument('--dropout', type=float, default=0.5,
                    help='Dropout rate (1 - keep probability).')

args = parser.parse_args()
args.cuda = not args.no_cuda and torch.cuda.is_available()

np.random.seed(args.seed)
torch.manual_seed(args.seed)
if args.cuda:
    torch.cuda.manual_seed(args.seed)

# Load data
adj, features, labels, idx_train, idx_val, idx_test = load_data()

# Model and optimizer
model = GCN(nfeat=features.shape[1],
            nhid=args.hidden,
            nclass=labels.max().item() + 1,
            dropout=args.dropout)
optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

if args.cuda:
    model.cuda()
    features = features.cuda()
    adj = adj.cuda()
    labels = labels.cuda()
    idx_train = idx_train.cuda()
    idx_val = idx_val.cuda()
    idx_test = idx_test.cuda()


def train(epoch):
    t = time.time()
    model.train()
    optimizer.zero_grad()
    output = model(features, adj)
    loss_train = F.nll_loss(output[idx_train], labels[idx_train])
    acc_train = accuracy(output[idx_train], labels[idx_train])
    loss_train.backward()
    optimizer.step()

    if not args.fastmode:
        # Evaluate validation set performance separately,
        # deactivates dropout during validation run.
        model.eval()
        output = model(features, adj)

    loss_val = F.nll_loss(output[idx_val], labels[idx_val])
    acc_val = accuracy(output[idx_val], labels[idx_val])
    print('Epoch: {:04d}'.format(epoch+1),
          'loss_train: {:.4f}'.format(loss_train.item()),
          'acc_train: {:.4f}'.format(acc_train.item()),
          'loss_val: {:.4f}'.format(loss_val.item()),
          'acc_val: {:.4f}'.format(acc_val.item()),
          'time: {:.4f}s'.format(time.time() - t))


def test():
    model.eval()
    output = model(features, adj)
    loss_test = F.nll_loss(output[idx_test], labels[idx_test])
    acc_test = accuracy(output[idx_test], labels[idx_test])
    print("Test set results:",
          "loss= {:.4f}".format(loss_test.item()),
          "accuracy= {:.4f}".format(acc_test.item()))


# Train model
t_total = time.time()
for epoch in range(args.epochs):
    train(epoch)
print("Optimization Finished!")
print("Total time elapsed: {:.4f}s".format(time.time() - t_total))

# Testing
test()
```
</details> 


### GraphSAGEï¼ˆGraph Sample and Aggregateï¼‰

![](./img/k6.png)
![](./img/k7.png)



è®ºæ–‡æ ‡é¢˜: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1609.02907)

```GCN```æœ¬èº«æœ‰ä¸€ä¸ªå±€é™ï¼Œå³æ²¡æ³•å¿«é€Ÿè¡¨ç¤ºæ–°èŠ‚ç‚¹ã€‚```GCN```éœ€è¦æŠŠæ‰€æœ‰èŠ‚ç‚¹éƒ½å‚ä¸è®­ç»ƒï¼ˆæ•´ä¸ªå›¾éƒ½ä¸¢è¿›å»è®­ç»ƒï¼‰æ‰èƒ½å¾—åˆ°```node embedding```ï¼Œå¦‚æœæ–°```node```æ¥äº†ï¼Œæ²¡æ³•å¾—åˆ°æ–°```node```çš„```embedding```ã€‚æ‰€ä»¥è¯´ï¼Œ```GCN```æ˜¯```transductive```çš„ã€‚ï¼ˆ```Transductive```ä»»åŠ¡æ˜¯æŒ‡ï¼šè®­ç»ƒé˜¶æ®µä¸æµ‹è¯•é˜¶æ®µéƒ½åŸºäºåŒæ ·çš„å›¾ç»“æ„ï¼‰

è€Œ```GraphSAGE```æ˜¯```inductive```çš„ã€‚```inductive```ä»»åŠ¡æ˜¯æŒ‡ï¼šè®­ç»ƒé˜¶æ®µä¸æµ‹è¯•é˜¶æ®µéœ€è¦å¤„ç†çš„graphä¸åŒã€‚é€šå¸¸æ˜¯è®­ç»ƒé˜¶æ®µåªæ˜¯åœ¨å­å›¾ï¼ˆ```subgraph```ï¼‰ä¸Šè¿›è¡Œï¼Œæµ‹è¯•é˜¶æ®µéœ€è¦å¤„ç†æœªçŸ¥çš„é¡¶ç‚¹ã€‚

è¦æƒ³å¾—åˆ°æ–°èŠ‚ç‚¹çš„è¡¨ç¤ºï¼Œéœ€è¦è®©æ–°çš„```node```æˆ–è€…```subgraph```å»å’Œå·²ç»ä¼˜åŒ–å¥½çš„```node embedding```å»â€œå¯¹é½â€ã€‚ç„¶è€Œæ¯ä¸ªèŠ‚ç‚¹çš„è¡¨ç¤ºéƒ½æ˜¯å—åˆ°å…¶ä»–èŠ‚ç‚¹çš„å½±å“ï¼ˆç‰µä¸€å‘è€ŒåŠ¨å…¨èº«ï¼‰ï¼Œå› æ­¤æ·»åŠ ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ„å‘³ç€è®¸è®¸å¤šå¤šä¸ä¹‹ç›¸å…³çš„èŠ‚ç‚¹çš„è¡¨ç¤ºéƒ½åº”è¯¥è°ƒæ•´ã€‚

```GraphSAGE```æ–¹æ³•æä¾›äº†ä¸€ç§é€šç”¨çš„å½’çº³å¼æ¡†æ¶ï¼Œä½¿ç”¨ç»“ç‚¹ä¿¡æ¯ç‰¹å¾ä¸ºæœªå‡ºç°è¿‡çš„ï¼ˆunseenï¼‰ç»“ç‚¹ç”Ÿæˆç»“ç‚¹å‘é‡ï¼Œè¿™ä¸€æ–¹æ³•ä¸ºåæ¥çš„ ```PinSage```ï¼ˆ```GCN``` åœ¨å•†ä¸šæ¨èç³»ç»Ÿé¦–æ¬¡æˆåŠŸåº”ç”¨ï¼‰æä¾›äº†åŸºç¡€ã€‚

GraphSAGEçš„æ ¸å¿ƒæ€æƒ³åœ¨äº<B>å…ˆä½¿ç”¨é‡‡æ ·çš„æ–¹æ³•ï¼Œé‡‡æ ·å›ºå®šæ•°é‡çš„é‚»å±…ç»“ç‚¹ï¼›ç„¶åè¿›è¡Œèšåˆ</B>

![](./img/r1.png)

- åœ¨å›¾ä¸­éšæœºé‡‡æ ·è‹¥å¹²ç»“ç‚¹ï¼Œç»“ç‚¹æ•°ä¸ºè¶…å‚æ•°```batch_size```ï¼Œå¯¹äºæ¯ä¸€ä¸ªè¢«é‡‡æ ·çš„ç»“ç‚¹åˆéšæœºé€‰å–å®ƒä»¬å›ºå®šä¸ªæ•°çš„é‚»å±…èŠ‚ç‚¹ï¼Œè¿™ä¸ªå¯ä»¥æ˜¯ä¸€é˜¶é‚»å±…ä¹Ÿå¯ä»¥æ˜¯äºŒé˜¶é‚»å±…ï¼Œæœ€åæ„æˆè¿›è¡Œå·ç§¯æ“ä½œçš„å›¾ã€‚

- å°†é‚»å±…èŠ‚ç‚¹çš„ä¿¡æ¯é€šè¿‡```aggregate```å‡½æ•°èšåˆèµ·æ¥æ›´æ–°åˆšåˆšé‡‡æ ·çš„ç»“ç‚¹ã€‚

- è®¡ç®—é‡‡æ ·ç»“ç‚¹å¤„çš„æŸå¤±ï¼Œå¦‚æœæ˜¯æ— ç›‘ç£ä»»åŠ¡ï¼Œåˆ™ç›®æ ‡è®¾ä¸º<B>å›¾ä¸Šé‚»å±…ç»“ç‚¹çš„ç¼–ç ç›¸ä¼¼</B>ï¼ˆåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›é‚»å±…èŠ‚ç‚¹çš„ç¼–ç ç›¸ä¼¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥å°†èšåˆåçš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å¾—å®ƒä»¬åœ¨å‘é‡ç©ºé—´ä¸­çš„è·ç¦»æ›´åŠ æ¥è¿‘ï¼Œä»è€Œå¢å¼ºèŠ‚ç‚¹è¡¨ç¤ºçš„ç›¸ä¼¼æ€§ï¼‰ï¼›å¦‚æœæ˜¯æœ‰ç›‘ç£ä»»åŠ¡ï¼Œåˆ™æ ¹æ®æœ‰ç›‘ç£ç»“ç‚¹çš„æ ‡ç­¾å’Œæœ€åçš„å€¼è®¡ç®—lossåä¼ æ›´æ–°ã€‚

<B>é‚»å±…èŠ‚ç‚¹çš„é€‰å–</B>

æœ¬æ–‡ä¸­é‡‡ç”¨çš„æ˜¯å¯¹é‚»å±…èŠ‚ç‚¹çš„å‡åŒ€é‡‡æ ·ï¼ˆå›ºå®šå¤§å°ï¼‰ï¼Œæ¯ä¸€è·³æŠ½æ ·çš„é‚»å±…æ•°é‡ä¸å¤šäº$S_K$ä¸ªï¼Œå¦‚æœä¸æ˜¯å›ºå®šé‡‡æ ·çš„è¯ï¼Œå•ä¸ªæ‰¹æ¬¡çš„å†…å­˜å’Œé¢„æœŸè¿è¡Œæ—¶é—´éƒ½æ˜¯ä¸å¯é¢„æµ‹çš„ï¼Œæœ€åæƒ…å†µä¸‹æ˜¯$O(|V|)$ï¼Œå½“å›¾è§„æ¨¡å¾ˆå¤§æ—¶å¾ˆå®¹æ˜“è®­ç»ƒä¸åŠ¨æ¨¡å‹ã€‚

![](./img/r2.png)

> éšç€å±‚æ•°Kçš„å¢åŠ ï¼Œå¯ä»¥<B>èšåˆè¶Šæ¥è¶Šè¿œè·ç¦»çš„ä¿¡æ¯</B>ã€‚è¿™æ˜¯å› ä¸ºï¼Œè™½ç„¶æ¯æ¬¡é€‰æ‹©é‚»å±…çš„æ—¶å€™å°±æ˜¯ä»å‘¨å›´çš„ä¸€é˜¶é‚»å±…ä¸­å‡åŒ€åœ°é‡‡æ ·å›ºå®šä¸ªæ•°ä¸ªé‚»å±…ï¼Œä½†æ˜¯ç”±äºèŠ‚ç‚¹çš„é‚»å±…ä¹Ÿèšåˆäº†å…¶é‚»å±…çš„ä¿¡æ¯ï¼Œè¿™æ ·ï¼Œåœ¨ä¸‹ä¸€æ¬¡èšåˆæ—¶ï¼Œè¯¥èŠ‚ç‚¹å°±ä¼šæ¥æ”¶åˆ°å…¶é‚»å±…çš„é‚»å±…çš„ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯èšåˆåˆ°äº†äºŒé˜¶é‚»å±…çš„ä¿¡æ¯äº†ã€‚


- Mean Aggregate

$$
h_v^k \leftarrow  \sigma(W \cdot mean(\{h_v^{k-1}\} \cup \{h_u^{k-1},\forall u \in N(v)\}) )
$$

> å½“å‰èŠ‚ç‚¹$v$æœ¬èº«å’Œå®ƒæ‰€æœ‰çš„é‚»å±…åœ¨$k-1$å±‚çš„```embedding```çš„```mean```ï¼Œç„¶åç»è¿‡```MLP+sigmoid```ã€‚ï¼ˆè¿™ä¸ªèšåˆå‡½æ•°æ¯”è¾ƒç²—ç³™ï¼‰

- LSTM Aggregate

ä½¿ç”¨```LSTM```å¯¹é‚»å±…ç»“ç‚¹ä¿¡æ¯è¿›è¡Œèšåˆï¼Œæ‹¥æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„åœ°æ˜¯ï¼Œå› ä¸º LSTM çš„åºåˆ—æ€§ï¼Œè¿™ä¸ªèšåˆå‡½æ•°ä¸å…·å¤‡å¯¹ç§°æ€§ã€‚æ–‡ç« ä¸­ä½¿ç”¨å¯¹é‚»å±…ç»“ç‚¹éšæœºæ’åˆ—çš„æ–¹æ³•æ¥å°†å…¶åº”ç”¨äºæ— åºé›†åˆã€‚ 

- pooling Aggregate

$$
Aggregate^{pool}_k = max({\sigma(W_{pool} h^k_{u_i} + b), \cup \{h_u^{k-1},\forall u \in N(v)})
$$

> æŠŠèŠ‚ç‚¹$v$çš„æ‰€æœ‰é‚»å±…èŠ‚ç‚¹éƒ½ç‹¬ç«‹åœ°è¾“å…¥ä¸€ä¸ª```MLP+sigmoid```å¾—åˆ°ä¸€ä¸ªå‘é‡ï¼Œæœ€åæŠŠæ‰€æœ‰é‚»å±…çš„å‘é‡åšä¸€ä¸ª```element-wise```(é€å…ƒç´ )çš„```max-pooling```ã€‚(å®éªŒå‘ç°```max pooling```å’Œ```mean pooling```æ•ˆæœä¸ŠåŸºæœ¬ä¸€è‡´)

![](./img/r3.png)

![](./img/r4.png)

å¯¹äºé‚»å±…èŠ‚ç‚¹çš„é‡‡æ ·ï¼Œæ»¡è¶³$K=2,S_1 \cdot S_2 <= 500$è¡¨ç°æ¯”è¾ƒå¥½

å¯¹äºèšåˆå‡½æ•°çš„æ¯”è¾ƒä¸Šï¼Œ```LSTM aggregator``` å’Œ ``Pooling aggregator`` è¡¨ç°æœ€å¥½ï¼Œä½†æ˜¯å‰è€…æ¯”åè€…æ…¢å¤§çº¦ä¸¤å€ã€‚

- GraphSAGEçš„<B>æ— ç›‘ç£å­¦ä¹ </B>

å¯¹äº<B>æ— ç›‘ç£å­¦ä¹ </B>ï¼Œè®¾è®¡çš„æŸå¤±å‡½æ•°åº”è¯¥è®©ä¸´è¿‘çš„èŠ‚ç‚¹çš„æ‹¥æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºï¼Œåä¹‹åº”è¯¥è¡¨ç¤ºå¤§ä¸ç›¸åŒ

$$
J_G(z_u)=-log(\sigma(z_u^T z_v)) -Q\cdot \mathbb{E}_{v_n âˆ¼ P_n(v)} log(\sigma(z_u^T z_{v_n}))
$$

> $P_n$:è´Ÿé‡‡æ ·åˆ†å¸ƒï¼Œ$Q$:è´Ÿé‡‡æ ·æ ·æœ¬çš„æ•°é‡


å¯¹äºæœ‰ç›‘ç£å­¦ä¹ ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨cross-entropy lossç­‰å¸¸è§„æŸå¤±å‡½æ•°ï¼Œä¸Šé¢çš„è¿™ä¸ªlossä¹Ÿå¯ä»¥ä½œä¸ºä¸€ä¸ªè¾…åŠ©lossã€‚

<details> 
<summary>model</summary>

```python
class Classification(nn.Module):

	def __init__(self, emb_size, num_classes):
		super(Classification, self).__init__()

		#self.weight = nn.Parameter(torch.FloatTensor(emb_size, num_classes))
		self.layer = nn.Sequential(
								nn.Linear(emb_size, num_classes)	  
								#nn.ReLU()
							)
		self.init_params() #åˆå§‹åŒ–

	def init_params(self):
		for param in self.parameters():
			if len(param.size()) == 2:
				nn.init.xavier_uniform_(param)

	def forward(self, embeds):
		logists = torch.log_softmax(self.layer(embeds), 1)
		return logists

# class Classification(nn.Module):

# 	def __init__(self, emb_size, num_classes):
# 		super(Classification, self).__init__()

# 		self.weight = nn.Parameter(torch.FloatTensor(emb_size, num_classes))
# 		self.init_params()

# 	def init_params(self):
# 		for param in self.parameters():
# 			nn.init.xavier_uniform_(param)

# 	def forward(self, embeds):
# 		logists = torch.log_softmax(torch.mm(embeds,self.weight), 1)
# 		return logists

class UnsupervisedLoss(object):
	"""docstring for UnsupervisedLoss"""
	def __init__(self, adj_lists, train_nodes, device):
		super(UnsupervisedLoss, self).__init__()
		'''
        Args:
            Q: è°ƒæ•´è´Ÿæ ·æœ¬å¾—åˆ†çš„ç¼©æ”¾
            N_WALKS: éšæœºæ¸¸èµ°çš„æ¬¡æ•°
            WALK_LEN: éšæœºæ¸¸èµ°çš„é•¿åº¦
            N_WALK_LEN: è®¡ç®—è´Ÿæ ·æœ¬æ—¶ï¼Œå…è®¸æ‰©å±•çš„æ­¥æ•°ï¼Œç”¨äºå¯»æ‰¾è¿œç¦»å½“å‰èŠ‚ç‚¹çš„è´Ÿæ ·æœ¬
            MARGIN: è¾¹ç•Œå‚æ•°
            adj_lists: è®¿é—®èŠ‚ç‚¹çš„é‚»æ¥å…³ç³»
            train_nodes: è®­ç»ƒèŠ‚ç‚¹
        '''
        
        self.Q = 10
		self.N_WALKS = 6
		self.WALK_LEN = 1
		self.N_WALK_LEN = 5
		self.MARGIN = 3
		self.adj_lists = adj_lists
		self.train_nodes = train_nodes
		self.device = device

		self.target_nodes = None
		self.positive_pairs = []
		self.negtive_pairs = []
		self.node_positive_pairs = {}
		self.node_negtive_pairs = {}
		self.unique_nodes_batch = []

	def get_loss_sage(self, embeddings, nodes): # åŸºäºSAGEæ¨¡å‹è®¡ç®—æŸå¤±
		assert len(embeddings) == len(self.unique_nodes_batch)
		assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]
		node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}

		nodes_score = []
		assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)
		for node in self.node_positive_pairs:
			pps = self.node_positive_pairs[node]
			nps = self.node_negtive_pairs[node]
			if len(pps) == 0 or len(nps) == 0:
				continue

			# Q * Exception(negative score)
			indexs = [list(x) for x in zip(*nps)]
			node_indexs = [node2index[x] for x in indexs[0]]
			neighb_indexs = [node2index[x] for x in indexs[1]]
			neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])
			neg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)), 0)
			#print(neg_score)

			# multiple positive score
			indexs = [list(x) for x in zip(*pps)]
			node_indexs = [node2index[x] for x in indexs[0]]
			neighb_indexs = [node2index[x] for x in indexs[1]]
			pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])
			pos_score = torch.log(torch.sigmoid(pos_score))
			#print(pos_score)

			nodes_score.append(torch.mean(- pos_score - neg_score).view(1,-1))
				
		loss = torch.mean(torch.cat(nodes_score, 0))
		
		return loss

	def get_loss_margin(self, embeddings, nodes):
		assert len(embeddings) == len(self.unique_nodes_batch)
		assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]
		node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}

		nodes_score = []
		assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)
		for node in self.node_positive_pairs:
			pps = self.node_positive_pairs[node]
			nps = self.node_negtive_pairs[node]
			if len(pps) == 0 or len(nps) == 0:
				continue

			indexs = [list(x) for x in zip(*pps)]
			node_indexs = [node2index[x] for x in indexs[0]]
			neighb_indexs = [node2index[x] for x in indexs[1]]
			pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])
			pos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)

			indexs = [list(x) for x in zip(*nps)]
			node_indexs = [node2index[x] for x in indexs[0]]
			neighb_indexs = [node2index[x] for x in indexs[1]]
			neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])
			neg_score, _ = torch.max(torch.log(torch.sigmoid(neg_score)), 0)

			nodes_score.append(torch.max(torch.tensor(0.0).to(self.device), neg_score-pos_score+self.MARGIN).view(1,-1))
			# nodes_score.append((-pos_score - neg_score).view(1,-1))

		loss = torch.mean(torch.cat(nodes_score, 0),0)

		# loss = -torch.log(torch.sigmoid(pos_score))-4*torch.log(torch.sigmoid(-neg_score))
		
		return loss

	def extend_nodes(self, nodes, num_neg=6): 
        ''' 
        ä¸ºæ¯ä¸ªç›®æ ‡èŠ‚ç‚¹ç”Ÿæˆæ­£è´Ÿæ ·æœ¬èŠ‚ç‚¹å¯¹
        '''
		self.positive_pairs = []
		self.node_positive_pairs = {}
		self.negtive_pairs = []
		self.node_negtive_pairs = {}

		self.target_nodes = nodes
		self.get_positive_nodes(nodes)
		# print(self.positive_pairs)
		self.get_negtive_nodes(nodes, num_neg)
		# print(self.negtive_pairs)
		self.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x]) | set([i for x in self.negtive_pairs for i in x]))
		assert set(self.target_nodes) < set(self.unique_nodes_batch)
		return self.unique_nodes_batch

	def get_positive_nodes(self, nodes):
		return self._run_random_walks(nodes)

	def get_negtive_nodes(self, nodes, num_neg):
		for node in nodes:
			neighbors = set([node])
			frontier = set([node])
			for i in range(self.N_WALK_LEN):
				current = set()
				for outer in frontier:
					current |= self.adj_lists[int(outer)]
				frontier = current - neighbors
				neighbors |= current
			far_nodes = set(self.train_nodes) - neighbors
			neg_samples = random.sample(far_nodes, num_neg) if num_neg < len(far_nodes) else far_nodes
			self.negtive_pairs.extend([(node, neg_node) for neg_node in neg_samples])
			self.node_negtive_pairs[node] = [(node, neg_node) for neg_node in neg_samples]
		return self.negtive_pairs

	def _run_random_walks(self, nodes):
		for node in nodes:
			if len(self.adj_lists[int(node)]) == 0:
				continue
			cur_pairs = []
			for i in range(self.N_WALKS):
				curr_node = node
				for j in range(self.WALK_LEN):
					neighs = self.adj_lists[int(curr_node)]
					next_node = random.choice(list(neighs))
					# self co-occurrences are useless
					if next_node != node and next_node in self.train_nodes:
						self.positive_pairs.append((node,next_node))
						cur_pairs.append((node,next_node))
					curr_node = next_node

			self.node_positive_pairs[node] = cur_pairs
		return self.positive_pairs
		
class SageLayer(nn.Module):
	"""
	Encodes a node's using 'convolutional' GraphSage approach
	"""
	def __init__(self, input_size, out_size, gcn=False): 
		super(SageLayer, self).__init__()

		self.input_size = input_size
		self.out_size = out_size
		self.gcn = gcn
		self.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gcn else 2 * self.input_size))

		self.init_params()

	def init_params(self):
		for param in self.parameters():
			nn.init.xavier_uniform_(param)

	def forward(self, self_feats, aggregate_feats, neighs=None):
		"""
		Generates embeddings for a batch of nodes.

		nodes	 -- list of nodes
		"""
		if not self.gcn:
			combined = torch.cat([self_feats, aggregate_feats], dim=1)
		else:
			combined = aggregate_feats
		combined = F.relu(self.weight.mm(combined.t())).t()
		return combined

class GraphSage(nn.Module):
	"""docstring for GraphSage"""
	def __init__(self, num_layers, input_size, out_size, raw_features, adj_lists, device, gcn=False, agg_func='MEAN'):
		super(GraphSage, self).__init__()

		self.input_size = input_size
		self.out_size = out_size
		self.num_layers = num_layers
		self.gcn = gcn
		self.device = device
		self.agg_func = agg_func
		self.raw_features = raw_features
		self.adj_lists = adj_lists

		for index in range(1, num_layers+1):
			layer_size = out_size if index != 1 else input_size
			setattr(self, 'sage_layer'+str(index), SageLayer(layer_size, out_size, gcn=self.gcn))

	def forward(self, nodes_batch):
		"""
		Generates embeddings for a batch of nodes.
		nodes_batch	-- batch of nodes to learn the embeddings
		"""
		lower_layer_nodes = list(nodes_batch)
		nodes_batch_layers = [(lower_layer_nodes,)]
		# self.dc.logger.info('get_unique_neighs.')
		for i in range(self.num_layers):
			lower_samp_neighs, lower_layer_nodes_dict, lower_layer_nodes= self._get_unique_neighs_list(lower_layer_nodes)
			nodes_batch_layers.insert(0, (lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict))

		assert len(nodes_batch_layers) == self.num_layers + 1

		pre_hidden_embs = self.raw_features
		for index in range(1, self.num_layers+1):
			nb = nodes_batch_layers[index][0]
			pre_neighs = nodes_batch_layers[index-1]
			# self.dc.logger.info('aggregate_feats.')
			aggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs) # 
			sage_layer = getattr(self, 'sage_layer'+str(index))
			if index > 1:
				nb = self._nodes_map(nb, pre_hidden_embs, pre_neighs)
			# self.dc.logger.info('sage_layer.')
			cur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb],
										aggregate_feats=aggregate_feats)
			pre_hidden_embs = cur_hidden_embs

		return pre_hidden_embs

	def _nodes_map(self, nodes, hidden_embs, neighs):
		layer_nodes, samp_neighs, layer_nodes_dict = neighs
		assert len(samp_neighs) == len(nodes)
		index = [layer_nodes_dict[x] for x in nodes]
		return index

	def _get_unique_neighs_list(self, nodes, num_sample=10) -> list(set), dict, list:
        '''
        è¿›è¡Œé‚»å±…é‡‡æ ·ï¼Œè¿”å›ï¼š
            samp_neighs: æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…é›†åˆåˆ—è¡¨ï¼ˆåŒ…å«è‡ªèº«ï¼‰
            unique_nodes: è¢«é€‰ä¸­çš„é‚»å±…èŠ‚ç‚¹ï¼ˆå­—å…¸ï¼‰
            _unique_nodes_list: è¢«é€‰ä¸­çš„é‚»å±…èŠ‚ç‚¹ï¼ˆåˆ—è¡¨ï¼‰
        '''
		_set = set
		to_neighs = [self.adj_lists[int(node)] for node in nodes]
		if not num_sample is None:
			_sample = random.sample
			samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs] #éšæœºé‡‡æ ·
		else:
			samp_neighs = to_neighs
		samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)] # å°†æ¯ä¸ªèŠ‚ç‚¹è‡ªèº«ä¹ŸåŠ å…¥å…¶é‚»å±…é›†åˆ
		_unique_nodes_list = list(set.union(*samp_neighs)) # å¹¶é›†ï¼Œå¾—åˆ°æ‰€æœ‰é‚»å±…èŠ‚ç‚¹
		i = list(range(len(_unique_nodes_list)))
		unique_nodes = dict(list(zip(_unique_nodes_list, i)))
		return samp_neighs, unique_nodes, _unique_nodes_list # lower_samp_neighs, lower_layer_nodes_dict, lower_layer_nodes

	def aggregate(self, nodes, pre_hidden_embs, pre_neighs, num_sample=10):
		# èšåˆå‡½æ•°
		'''
		pre_hidden_embs: æ‰€æœ‰èŠ‚ç‚¹çš„é¢„è®­ç»ƒåµŒå…¥çŸ©é˜µ
		unique_nodesï¼šæ‰€æœ‰å”¯ä¸€é‚»å±…èŠ‚ç‚¹çš„ ID åˆ°å…¶åœ¨ unique_nodes_list ä¸­ç´¢å¼•çš„æ˜ å°„ï¼ˆå­—å…¸ï¼‰ idx -> index
		unique_nodes_listï¼šæ‰€æœ‰å”¯ä¸€é‚»å±…èŠ‚ç‚¹çš„ ID åˆ—è¡¨
		samp_neighsï¼šæ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„é‡‡æ ·é‚»å±…èŠ‚ç‚¹ ID åˆ—è¡¨
		'''
		unique_nodes_list, samp_neighs, unique_nodes = pre_neighs

		assert len(nodes) == len(samp_neighs)
		indicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))]
		assert (False not in indicator)

		if not self.gcn:
			samp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]
		# self.dc.logger.info('2')
		if len(pre_hidden_embs) == len(unique_nodes):
			embed_matrix = pre_hidden_embs
		else:
			embed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)] #å–éœ€è¦çš„èŠ‚ç‚¹åµŒå…¥çŸ©é˜µ
		# self.dc.logger.info('3')
		mask = torch.zeros(len(samp_neighs), len(unique_nodes))
		# æ©ç æŒ‡ç¤ºå“ªäº›é‚»å±…èŠ‚ç‚¹å‚ä¸èšåˆ
		# mask çŸ©é˜µçš„æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå¾…å¤„ç†èŠ‚ç‚¹ï¼Œæ¯ä¸€åˆ—å¯¹åº”ä¸€ä¸ªå”¯ä¸€çš„é‚»å±…èŠ‚ç‚¹ã€‚
		column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh] # æ‰€æœ‰éœ€è¦å‚ä¸èšåˆçš„é‚»å±…èŠ‚ç‚¹åœ¨ unique_nodesä¸­çš„ç´¢å¼•
		row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))] # æ‰€æœ‰éœ€è¦è¿›è¡Œé‚»å±…èŠ‚ç‚¹ç‰¹å¾èšåˆçš„èŠ‚ç‚¹åœ¨ samp_neighs ä¸­çš„ç´¢å¼•
		'''
		row_indices -> [[0,0,0,...],[1,1,1,...],[2,2,2,...],...]	
		'''
		mask[row_indices, column_indices] = 1
		# self.dc.logger.info('4')
		# Example:èŠ‚ç‚¹içš„é‚»å±…èŠ‚ç‚¹åˆ—è¡¨ä¸­åŒ…å«èŠ‚ç‚¹nï¼Œé‚£ä¹ˆ mask[i, unique_nodes[n]]è¢«è®¾ç½®ä¸º 1ï¼Œè¡¨ç¤ºè¯¥é‚»å±…èŠ‚ç‚¹å‚ä¸èŠ‚ç‚¹içš„ç‰¹å¾èšåˆã€‚

		if self.agg_func == 'MEAN':
			num_neigh = mask.sum(1, keepdim=True)
			mask = mask.div(num_neigh).to(embed_matrix.device)
			aggregate_feats = mask.mm(embed_matrix)

		elif self.agg_func == 'MAX':
			# print(mask)
			indexs = [x.nonzero() for x in mask==1]
			# nonzero() æ–¹æ³•ä¼šè¿”å›æ•°ç»„ä¸­éé›¶å…ƒç´ çš„ç´¢å¼•ï¼Œä»¥å…ƒç»„çš„å½¢å¼è¿”å›
			aggregate_feats = []
			# self.dc.logger.info('5')
			for feat in [embed_matrix[x.squeeze()] for x in indexs]:
				if len(feat.size()) == 1:
					aggregate_feats.append(feat.view(1, -1))
				else:
					aggregate_feats.append(torch.max(feat,0)[0].view(1, -1))
			aggregate_feats = torch.cat(aggregate_feats, 0)
		# self.dc.logger.info('6')
		return aggregate_feats
```
</details>

### Graph Attention Networks

![](./img/r5%20(1).png)

![](./img/r5%20(2).png)

è®ºæ–‡æ ‡é¢˜: [Graph Attention Networks](https://arxiv.org/abs/1710.10903)

![](./img/r7.png)

- Attention coefficients $e_{vu}$ across pairs of nodes $u$ and $v$ based on their features $h_u$ and $h_v$.

$$
e_{vu} = a(W^{l}h_u{l-1}, W^{l}h_v{l-1})
$$

> $e_{vu}$ indicates the importance of node $v$ to node $u$.

- Normalize the attention coefficients to obtain the attention coefficients $\alpha_{vu}$.

$$
a_{vu} = \frac{exp(e_{vu})}{\sum_{k \in N(v)} exp(e_{vk})}
$$

- Weighted sum based on the final attention weights $\alpha_{vu}$.

$$
h_v^l = \sigma( \sum_{u \in N(v)} \alpha_{vu} W^l h_u^{l-1})
$$

![](./img/r6.png)
![](./img/r.png)
![](./img/r9.png)

æœ¬æ–‡æå‡ºçš„Graph Attention Layersçš„è¾“å…¥ä¸ºä¸€ç»„èŠ‚ç‚¹çš„ç‰¹å¾ï¼š$\mathbf{h} = \{ h_1,h_2,...,h_N \},h_i \in \mathbb{R}^F$ï¼Œ$N$ä¸ºèŠ‚ç‚¹ä¸ªæ•°ï¼Œ$F$ä¸ºæ¯ä¸ªèŠ‚ç‚¹çš„ç‰¹å¾æ•°ã€‚è¯¥å±‚è¾“å‡ºä¸ºä¸€ç»„æ–°çš„èŠ‚ç‚¹ç‰¹å¾$\mathbf{h}' = \{ h_1',h_2',...,h_N' \},h_i' \in \mathbb{R}^{F'} $

å°†è¾“å…¥ç‰¹å¾è½¬æ¢ä¸ºé«˜å±‚ç‰¹å¾ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªå…±äº«å‚æ•°çš„çº¿æ€§å˜æ¢$\mathbf{W}$:

$$
e_{ij} = a(\mathbf{W} h_i, \mathbf{W} h_j)
$$

> $e_{ij}$æ˜¯ä»ä¸€ä¸ªèŠ‚ç‚¹åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹çš„æ³¨æ„åŠ›åˆ†æ•°ï¼ˆimportanceï¼‰ï¼Œ$a$æ˜¯è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œ$a \in \mathbf{R}^{2F'}$

$$
e_{ij} = LeakyReLU(\mathbf{a}^T [\mathbf{W} h_i || \mathbf{W} h_j])
$$

å› æ­¤:

$$
\alpha_{ij} = \frac{exp (LeakyReLU(\mathbf{a}^T [\mathbf{W} h_i || \mathbf{W} h_j]))}{\sum_{k \in \mathcal{N}(i)} exp(LeakyReLU(\mathbf{a}^T [\mathbf{W} h_i || \mathbf{W} h_k]))}
$$

å¾—åˆ°å„ä¸ªé‚»èŠ‚ç‚¹çš„$\alpha_{ij}$åï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡ŒåŠ æƒæ±‚å’Œ:

$$
h_i' = \sigma(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} \mathbf{W} h_j)
$$

- Multi-head Graph Attention

ä¸ºäº†ç¨³å®š ```attention``` çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæˆ‘ä»¬å‘ç°å°†æˆ‘ä»¬çš„æœºåˆ¶æ‹“å±•åˆ° ```multi-head attention``` æ˜¯æœ‰å¥½å¤„çš„:

$$
h_i' = \mathop{\Big|\Big|}\limits_{k=1}^K  \sigma(\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k \mathbf{W}^k h_j)
$$

å€˜è‹¥æˆ‘ä»¬åœ¨æœ€åè¾“å‡ºå±‚æ‰§è¡Œè¯¥ ```multi-head attention```åˆ™```concat```æ“ä½œä¸æ˜¯å¿…é¡»çš„ï¼Œå¯ä»¥ä½¿ç”¨```Average```ä»£æ›¿ï¼Œæ¨è¿Ÿæ‰§è¡Œæœ€ç»ˆéçº¿æ€§:

$$
h_i' =  \sigma( \frac{1}{K} \sum_{k=1}^{K}  \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k \mathbf{W}^k h_j)
$$

<details> 
<summary>layers</summary>

```python
class GraphAttentionLayer(nn.Module):
    """
    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903
    """
    def __init__(self, in_features, out_features, dropout, alpha, concat=True):
        super(GraphAttentionLayer, self).__init__()
        self.dropout = dropout
        self.in_features = in_features
        self.out_features = out_features
        self.alpha = alpha
        self.concat = concat

        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))
        nn.init.xavier_uniform_(self.W.data, gain=1.414)
        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))
        nn.init.xavier_uniform_(self.a.data, gain=1.414)

        self.leakyrelu = nn.LeakyReLU(self.alpha)

    def forward(self, h, adj):
        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)
        e = self._prepare_attentional_mechanism_input(Wh)

        zero_vec = -9e15*torch.ones_like(e)
        attention = torch.where(adj > 0, e, zero_vec) # é€å…ƒç´ æ“ä½œ e if adj > 0 else -9e15
        attention = F.softmax(attention, dim=1)
        attention = F.dropout(attention, self.dropout, training=self.training)
        h_prime = torch.matmul(attention, Wh)

        if self.concat:
            return F.elu(h_prime)
        else:
            return h_prime

    def _prepare_attentional_mechanism_input(self, Wh):
        # Wh.shape (N, out_feature)
        # self.a.shape (2 * out_feature, 1)
        # Wh1&2.shape (N, 1)
        # e.shape (N, N)
        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])
        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])
        # broadcast add
		'''
		a = torch.tensor([[1],[2], [3], [4], [5]])
		b = torch.tensor([[1], [2], [3], [4], [5]])
		a+b.T
		>>> tensor([[ 2,  3,  4,  5,  6],
					[ 3,  4,  5,  6,  7],
					[ 4,  5,  6,  7,  8],
					[ 5,  6,  7,  8,  9],
					[ 6,  7,  8,  9, 10]])
		'''
        e = Wh1 + Wh2.T
        return self.leakyrelu(e)

    def __repr__(self):
        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'

class SpecialSpmmFunction(torch.autograd.Function): 
	# åœ¨ ç¨€ç–çŸ©é˜µä¹˜æ³• ä¸­åªå¯¹ ç¨€ç–åŒºåŸŸ è¿›è¡Œ åå‘ä¼ æ’­ è®¡ç®—
    """Special function for only sparse region backpropataion layer."""
    @staticmethod
    def forward(ctx, indices, values, shape, b):
        assert indices.requires_grad == False
        a = torch.sparse_coo_tensor(indices, values, shape)  #ç¨€ç–å¼ é‡
        ctx.save_for_backward(a, b)
        ctx.N = shape[0] # å°†ç¨€ç–çŸ©é˜µçš„ç¬¬ä¸€ç»´å¤§å°ä¿å­˜åˆ° ctx.N ä¸­ï¼Œç”¨äºåç»­è®¡ç®—
        return torch.matmul(a, b)

    @staticmethod
    def backward(ctx, grad_output):
        # grad_outputè¾“å‡ºå¼ é‡çš„æ¢¯åº¦
		a, b = ctx.saved_tensors
        grad_values = grad_b = None # åˆ†åˆ«è¡¨ç¤ºvaluesï¼ˆç¨€ç–çŸ©é˜µä¸­éé›¶å…ƒç´ ï¼‰çš„æ¢¯åº¦å’Œbçš„æ¢¯åº¦ã€‚
        if ctx.needs_input_grad[1]: # æ£€æŸ¥æ˜¯å¦éœ€è¦è®¡ç®—valuesçš„æ¢¯åº¦
            grad_a_dense = grad_output.matmul(b.t()) # è®¡ç®—ç¨€ç–çŸ©é˜µaçš„æ¢¯åº¦, a_denseè¡¨ç¤ºaçš„å¯†é›†å½¢å¼ã€‚
			# æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œaçš„æ¢¯åº¦ç­‰äºè¾“å‡ºæ¢¯åº¦ grad_output ä¹˜ä»¥ output å¯¹ a çš„åå¯¼æ•°
            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :] 
			# è½¬æ¢ä¸ºçº¿æ€§ç´¢å¼•
            grad_values = grad_a_dense.view(-1)[edge_idx]
        if ctx.needs_input_grad[3]: # æ£€æŸ¥æ˜¯å¦éœ€è¦è®¡ç®—bçš„æ¢¯åº¦ã€‚
            grad_b = a.t().matmul(grad_output)
        return None, grad_values, None, grad_b


class SpecialSpmm(nn.Module):
    def forward(self, indices, values, shape, b):
        return SpecialSpmmFunction.apply(indices, values, shape, b)

    
class SpGraphAttentionLayer(nn.Module):
    """
    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903
    """

    def __init__(self, in_features, out_features, dropout, alpha, concat=True):
        super(SpGraphAttentionLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.alpha = alpha
        self.concat = concat

        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))
        nn.init.xavier_normal_(self.W.data, gain=1.414)
                
        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))
        nn.init.xavier_normal_(self.a.data, gain=1.414)

        self.dropout = nn.Dropout(dropout)
        self.leakyrelu = nn.LeakyReLU(self.alpha)
        self.special_spmm = SpecialSpmm()

    def forward(self, input, adj):
        dv = 'cuda' if input.is_cuda else 'cpu'

        N = input.size()[0]
        edge = adj.nonzero().t() 
		# [æ‰€æœ‰è¾¹çš„èµ·ç‚¹èŠ‚ç‚¹ç´¢å¼•,
		#	æ‰€æœ‰è¾¹çš„ç»ˆç‚¹èŠ‚ç‚¹ç´¢å¼•]

        h = torch.mm(input, self.W)
        # h: N x out
        assert not torch.isnan(h).any()

        # Self-attention on the nodes - Shared attention mechanism
        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t() 
        # edge: 2*D x E 
		# h[edge[0, :], :] å–å‡ºæ‰€æœ‰èµ·ç‚¹èŠ‚ç‚¹çš„ç‰¹å¾ï¼Œh[edge[1, :], :] å–å‡ºæ‰€æœ‰ç»ˆç‚¹èŠ‚ç‚¹çš„ç‰¹å¾ã€‚

        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))
        assert not torch.isnan(edge_e).any()
        # edge_e: E

        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv)) 
		# æ¯ä¸ªèŠ‚ç‚¹çš„æ³¨æ„åŠ›æƒé‡ä¹‹å’Œ e_rowsum
        # e_rowsum: N x 1

        edge_e = self.dropout(edge_e)
        # edge_e: E

        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h) 
		# åŠ æƒé‚»å±…ç‰¹å¾ä¹‹å’Œ h_prime
        assert not torch.isnan(h_prime).any()
        # h_prime: N x out
        
        h_prime = h_prime.div(e_rowsum)
        # h_prime: N x out
        assert not torch.isnan(h_prime).any()

        if self.concat:
            # if this layer is not last layer,
            return F.elu(h_prime)
        else:
            # if this layer is last layer,
            return h_prime

    def __repr__(self):
        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'
```
</details> 

<details> 
<summary>model</summary>

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from layers import GraphAttentionLayer, SpGraphAttentionLayer


class GAT(nn.Module):
    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):
        """Dense version of GAT."""
        super(GAT, self).__init__()
        self.dropout = dropout

        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]
        for i, attention in enumerate(self.attentions):
            self.add_module('attention_{}'.format(i), attention)
			# .add_module()å°†æ¯ä¸ªå›¾æ³¨æ„åŠ›å±‚æ·»åŠ åˆ°æ¨¡å—ä¸­ï¼Œå¹¶ä¸ºæ¯ä¸ªå›¾æ³¨æ„åŠ›å±‚æ·»åŠ ä¸€ä¸ªå”¯ä¸€çš„åç§°ã€‚

        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)
		# å°†nheadsä¸ªå›¾æ³¨æ„åŠ›å±‚çš„è¾“å‡ºç‰¹å¾æ‹¼æ¥èµ·æ¥ï¼Œå¹¶æ˜ å°„åˆ° nclass ç»´çš„è¾“å‡ºç‰¹å¾

    def forward(self, x, adj):
        x = F.dropout(x, self.dropout, training=self.training)
        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)
        x = F.dropout(x, self.dropout, training=self.training)
        x = F.elu(self.out_att(x, adj))
        return F.log_softmax(x, dim=1)


class SpGAT(nn.Module):
    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):
        """Sparse version of GAT."""
        super(SpGAT, self).__init__()
        self.dropout = dropout

        self.attentions = [SpGraphAttentionLayer(nfeat, 
                                                 nhid, 
                                                 dropout=dropout, 
                                                 alpha=alpha, 
                                                 concat=True) for _ in range(nheads)]
        for i, attention in enumerate(self.attentions):
            self.add_module('attention_{}'.format(i), attention)

        self.out_att = SpGraphAttentionLayer(nhid * nheads, 
                                             nclass, 
                                             dropout=dropout, 
                                             alpha=alpha, 
                                             concat=False)

    def forward(self, x, adj):
        x = F.dropout(x, self.dropout, training=self.training)
        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)
        x = F.dropout(x, self.dropout, training=self.training)
        x = F.elu(self.out_att(x, adj))
        return F.log_softmax(x, dim=1)
```
</details> 

### The over-smoothing problem

When we stack many GNNLayers together, the output of the network will suffer from <B>the over-smoothing problem:all the node embeddings converge to the same value</B>

But we want to see the diffierences between different nodes.

Why does this problem happen: <B>Receptive field of a GNN</B>

- <B>Receptive field of a GNN</B>: the set of nodes that determine the embedding of a node of interest.

![](./img/y1.png)

In a 3-layer GNN, the receptive field overlap for two nodes(æ„Ÿå—é‡é‡å è¿‡å¤§). The shared neighbors quickly grows when the number of GNNlayers $>=$ 3.So we should be cautious when adding GNNLayers.

> If two nodes have highly-overlapped receptive fields, then their embeddings are highly similar

- <B>Goal:Making a shallow GNN more expressive.</B>

![](./img/t1.png)

![](./img/t2.png)

![](./img/t3.png)

![](./img/t4.png)
![](./img/t4%20(2).png)

![](./img/t5.png)

### Manipulate Graphs

![](./img/t6.png)

![](./img/t7.png)

### Feature Augmentation on Graphs

![](./img/t8%20(1).png)
![](./img/t8%20(2).png)

![](./img/t9.png)

### Add Virtual Nodes/Edges: Augment sparse graphs

- Add virtual edges

![](./img/t10%20(1).png)

- Add virtual nodes

![](./img/t11.png)

### Node Neighborhood Sampling

![](./img/t12.png)

![](./img/t15%20(1).png)

![](./img/t15%20(2).png)

![](./img/t15%20(3).png)