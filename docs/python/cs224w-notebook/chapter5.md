# ğŸ›£[Deep Learning]Stanford CS224w:Machine Learning with Graphs
---
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

!!! info "æƒ³è¯´çš„è¯ğŸ‡"
    <font size = 3.5>
    
    ğŸ”è¯¾ç¨‹ç½‘ç«™ï¼šhttp://web.stanford.edu/class/cs224w/
    
    ğŸ‘€ä¸€äº›èµ„æº: 
    Bç«™ç²¾è®²ï¼šhttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&vd_source=280e4970f2995a05fdeab972a42bfdd0
    
    https://github.com/TommyZihao/zihao_course/tree/main/CS224W
    
    Slides: http://web.stanford.edu/class/cs224w/slides
    
    </font>

![](./img/u1.png)

### Neighborhood Aggregation

- Observation: Neighbor aggregation can be abstracted as a function over a multi-set (a set with repeating elements).

![](./img/u2.png)

![](./img/u4.png)

- GCN

![](./img/u5.png)

- GraphSAGE

![](./img/u6.png)

![](./img/u7.png)

![](./img/mm.png)
![](./img/666%20(4).png)
![](./img/666%20(5).png)

![](./img/u8.png)

### Designing Most Expressive GNNs

![](./img/u9.png)

![](./img/u7%20(1).png)

![](./img/u7%20(2).png)

### Graph Isomorphism Network(GIN)

GIN(å›¾åŒæ„ç½‘ç»œ)'s neighbor aggregation function is <B>injective</B>, so GIN is the most expressive GNN

![](./img/u10.png)

- 1-Weisfeiler-Lehmanï¼ˆColor refinement algorithmï¼‰ç®—æ³•

![](./img/uu.png)

![](./img/666%20(1).png)
![](./img/666%20(2).png)
![](./img/666%20(3).png)

GIN uses a NN to model the injective HASH function

$$
\begin{aligned}
&GINconv(c^{(k)}(v),\{ c^{(k)}(u)_{u \in N(v)} \}) \\
=& MLP_{\phi} \Big( (1+\epsilon)MLP_f (c^{(k)}(v))+ \sum_{u\in N(v)} MLP_f (c^{(k)}(u)) \Big)\\
\end{aligned}
$$

> where $\epsilon$ is a learnable parameter.

![](./img/aaa.png)

![](./img/45.png)

![](./img/22.png)

### General tips

![](./img/33.png)

### Understand GIN 

è®ºæ–‡åœ°å€ï¼š[How Powerful are Graph Neural Networks](https://arxiv.org/abs/1810.00826)

```python
class S2VGraph(object):
    def __init__(self, g, label, node_tags=None, node_features=None):
        '''
            g: a networkx graph
            label: an integer graph label
            node_tags: a list of integer node tags
            node_features: a torch float tensor, one-hot representation of the tag that is used as input to neural nets
            edge_mat: a torch long tensor, contain edge list, will be used to create torch sparse tensor
            neighbors: list of neighbors (without self-loop)
        '''
        self.label = label # å›¾çš„æ ‡ç­¾
        self.g = g # networkx graph type
        self.node_tags = node_tags # èŠ‚ç‚¹æ ‡ç­¾
        self.neighbors = []
        self.node_features = 0 # èŠ‚ç‚¹ç‰¹å¾
        self.edge_mat = 0
        self.max_neighbor = 0
'''
COLLAB æ˜¯ä¸€ä¸ªç§‘å­¦åˆä½œæ•°æ®é›†ã€‚å›¾å¯¹åº”äºç ”ç©¶äººå‘˜çš„è‡ªæˆ‘ç½‘ç»œï¼Œå³ç ”ç©¶äººå‘˜åŠå…¶åˆä½œè€…æ˜¯èŠ‚ç‚¹ï¼Œè¾¹è¡¨ç¤ºä¸¤ä¸ªç ”ç©¶äººå‘˜ä¹‹é—´çš„åˆä½œã€‚ç ”ç©¶äººå‘˜çš„è‡ªæˆ‘ç½‘ç»œæœ‰ä¸‰ä¸ªå¯èƒ½çš„æ ‡ç­¾ï¼Œå³é«˜èƒ½ç‰©â€‹â€‹ç†ã€å‡èšæ€ç‰©ç†å’Œå¤©ä½“ç‰©ç†ï¼Œè¿™äº›æ˜¯ç ”ç©¶äººå‘˜æ‰€å±çš„é¢†åŸŸã€‚è¯¥æ•°æ®é›†æœ‰ 5,000ä¸ªå›¾ï¼Œæ¯ä¸ªå›¾ï¼ˆgraphsï¼‰éƒ½æœ‰æ ‡ç­¾ 0ã€1 æˆ– 2ã€‚

COLLABæ•°æ®é›†ï¼ˆ.txtï¼‰æ ¼å¼ï¼š
5000 # å›¾çš„æ•°é‡
45 0 # ç¬¬ä¸€ä¸ªå›¾çš„èŠ‚ç‚¹æ•°å’Œæ ‡ç­¾
0 44 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44
0 44 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44
0 44 0 1 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44
.. ... ... 

ç¬¬ä¸€æ¬¡æ¥è§¦å›¾ç¥ç»ç½‘ç»œçš„æ•°æ®é›†ï¼Œå¿…é¡»ææ¸…æ¥šæ•°æ®é›†çš„æ ¼å¼ï¼Œæ‰èƒ½æ­£ç¡®åœ°è¯»å–æ•°æ®ã€‚
'''


def load_data(dataset, degree_as_tag):
    '''
        dataset: name of dataset
        test_proportion: ratio of test train split
        seed: random seed for random splitting of dataset
    '''
    print('loading data')
    g_list = [] # å­˜æ”¾å›¾å¯¹è±¡
    label_dict = {} # å­˜æ”¾æ ‡ç­¾æ˜ å°„
    feat_dict = {} # å­˜æ”¾èŠ‚ç‚¹ç‰¹å¾æ˜ å°„

    with open('dataset/%s/%s.txt' % (dataset, dataset), 'r') as f:
        n_g = int(f.readline().strip())
        for i in range(n_g):
            row = f.readline().strip().split()
            n, l = [int(w) for w in row] # number of nodes and label
            if not l in label_dict:
                mapped = len(label_dict)
                label_dict[l] = mapped
            g = nx.Graph() # NetworkX
            node_tags = [] # èŠ‚ç‚¹æ ‡ç­¾åˆ—è¡¨
            node_features = [] # èŠ‚ç‚¹ç‰¹å¾åˆ—è¡¨
            n_edges = 0 # è¾¹æ•°
            for j in range(n):
                g.add_node(j) # .add_nodeæ·»åŠ èŠ‚ç‚¹
                row = f.readline().strip().split()
                tmp = int(row[1]) + 2 # ç”¨äºç¡®å®šèŠ‚ç‚¹å±æ€§çš„æ•°é‡
                if tmp == len(row):
                    # no node attributes æ²¡æœ‰èŠ‚ç‚¹å±æ€§
                    row = [int(w) for w in row]
                    attr = None
                else:
                    row, attr = [int(w) for w in row[:tmp]], np.array([float(w) for w in row[tmp:]])
                if not row[0] in feat_dict:
                    mapped = len(feat_dict)
                    feat_dict[row[0]] = mapped
                node_tags.append(feat_dict[row[0]]) # èŠ‚ç‚¹æ ‡ç­¾

                if tmp > len(row): 
                    node_features.append(attr)

                n_edges += row[1] # åŠ è¾¹
                for k in range(2, len(row)):
                    g.add_edge(j, row[k]) # .add_edgeæ·»åŠ è¾¹
            '''
            g.add_node
            g.add_edge
            '''

            if node_features != []:
                node_features = np.stack(node_features)
                node_feature_flag = True
            else:
                node_features = None
                node_feature_flag = False

            assert len(g) == n

            g_list.append(S2VGraph(g, l, node_tags)) #(å›¾ï¼Œå›¾æ ‡ç­¾ï¼ŒèŠ‚ç‚¹æ ‡ç­¾)

    #add labels and edge_mat       
    for g in g_list:
        g.neighbors = [[] for i in range(len(g.g))]
        # ä¸ºæ¯ä¸ªå›¾çš„æ¯ä¸ªèŠ‚ç‚¹åˆå§‹åŒ–é‚»å±…åˆ—è¡¨
        for i, j in g.g.edges():
            g.neighbors[i].append(j)
            g.neighbors[j].append(i)
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹å¡«å……é‚»å±…åˆ—è¡¨ï¼ˆæ— å‘å›¾ï¼‰
        degree_list = []
        for i in range(len(g.g)):
            g.neighbors[i] = g.neighbors[i]
            degree_list.append(len(g.neighbors[i])) # æ¯ä¸ªèŠ‚ç‚¹çš„åº¦æ•°
        g.max_neighbor = max(degree_list)

        g.label = label_dict[g.label]

        edges = [list(pair) for pair in g.g.edges()]
        # è¾¹çš„èŠ‚ç‚¹å¯¹
        edges.extend([[i, j] for j, i in edges])
        # æ— å‘å›¾
        deg_list = list(dict(g.g.degree(range(len(g.g)))).values())
        # è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„åº¦æ•°ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
        g.edge_mat = torch.LongTensor(edges).transpose(0,1)
        # ç”¨äºç”Ÿæˆç¨€ç–çŸ©é˜µA

    if degree_as_tag:
        for g in g_list:
            g.node_tags = list(dict(g.g.degree).values())

    #Extracting unique tag labels   
    tagset = set([])
    for g in g_list:
        tagset = tagset.union(set(g.node_tags))

    tagset = list(tagset)
    tag2index = {tagset[i]:i for i in range(len(tagset))}

    for g in g_list:
        g.node_features = torch.zeros(len(g.node_tags), len(tagset))
        g.node_features[range(len(g.node_tags)), [tag2index[tag] for tag in g.node_tags]] = 1


    print('# classes: %d' % len(label_dict))
    print('# maximum node tag: %d' % len(tagset))

    print("# data: %d" % len(g_list))

    return g_list, len(label_dict)

def separate_data(graph_list, seed, fold_idx):
    assert 0 <= fold_idx and fold_idx < 10, "fold_idx must be from 0 to 9."
    skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = seed)

    labels = [graph.label for graph in graph_list]
    idx_list = []
    for idx in skf.split(np.zeros(len(labels)), labels):
        idx_list.append(idx)
    train_idx, test_idx = idx_list[fold_idx]

    train_graph_list = [graph_list[i] for i in train_idx]
    test_graph_list = [graph_list[i] for i in test_idx]

    return train_graph_list, test_graph_list

class MLP(nn.Module):
    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):
        '''
            num_layers: number of layers in the neural networks (EXCLUDING the input layer). If num_layers=1, this reduces to linear model.
            input_dim: dimensionality of input features
            hidden_dim: dimensionality of hidden units at ALL layers
            output_dim: number of classes for prediction
            device: which device to use
        '''
    
        super(MLP, self).__init__()

        self.linear_or_not = True #default is linear model
        self.num_layers = num_layers

        if num_layers < 1:
            raise ValueError("number of layers should be positive!")
        elif num_layers == 1:
            #Linear model
            self.linear = nn.Linear(input_dim, output_dim)
        else:
            #Multi-layer model
            self.linear_or_not = False
            self.linears = torch.nn.ModuleList()
            self.batch_norms = torch.nn.ModuleList()
        
            self.linears.append(nn.Linear(input_dim, hidden_dim))
            for layer in range(num_layers - 2):
                self.linears.append(nn.Linear(hidden_dim, hidden_dim))
            self.linears.append(nn.Linear(hidden_dim, output_dim))

            for layer in range(num_layers - 1):
                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))

    def forward(self, x):
        if self.linear_or_not:
            #If linear model
            return self.linear(x)
        else:
            #If MLP
            h = x
            for layer in range(self.num_layers - 1):
                h = F.relu(self.batch_norms[layer](self.linears[layer](h)))
            return self.linears[self.num_layers - 1](h)

class GraphCNN(nn.Module):
    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, output_dim, final_dropout, learn_eps, graph_pooling_type, neighbor_pooling_type, device):
        '''
            num_layers: number of layers in the neural networks (INCLUDING the input layer)
            num_mlp_layers: number of layers in mlps (EXCLUDING the input layer)
            input_dim: dimensionality of input features
            hidden_dim: dimensionality of hidden units at ALL layers
            output_dim: number of classes for prediction
            final_dropout: dropout ratio on the final linear layer
            learn_eps: If True, learn epsilon to distinguish center nodes from neighboring nodes. If False, aggregate neighbors and center nodes altogether. 
            neighbor_pooling_type: how to aggregate neighbors (mean, average, or max)
            graph_pooling_type: how to aggregate entire nodes in a graph (mean, average)
            device: which device to use
        '''

        super(GraphCNN, self).__init__()

        self.final_dropout = final_dropout
        self.device = device
        self.num_layers = num_layers
        self.graph_pooling_type = graph_pooling_type
        self.neighbor_pooling_type = neighbor_pooling_type
        self.learn_eps = learn_eps
        self.eps = nn.Parameter(torch.zeros(self.num_layers-1))
        # learn_eps æ˜¯å¦æ·»åŠ è‡ªç¯

        ###List of MLPs
        self.mlps = torch.nn.ModuleList()

        ###List of batchnorms applied to the output of MLP (input of the final prediction linear layer)
        self.batch_norms = torch.nn.ModuleList()

        for layer in range(self.num_layers-1):
            if layer == 0:
                self.mlps.append(MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim))
            else:
                self.mlps.append(MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim))

            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        #Linear function that maps the hidden representation at dofferemt layers into a prediction score
        self.linears_prediction = torch.nn.ModuleList()
        for layer in range(num_layers):
            if layer == 0:
                self.linears_prediction.append(nn.Linear(input_dim, output_dim))
            else:
                self.linears_prediction.append(nn.Linear(hidden_dim, output_dim))


    def __preprocess_neighbors_maxpool(self, batch_graph): 
        #ä¸ºæœ€å¤§æ± åŒ–åˆ›å»ºå¡«å……çš„é‚»å±…åˆ—è¡¨
        ###create padded_neighbor_list in concatenated graph

        #compute the maximum number of neighbors within the graphs in the current minibatch
        max_deg = max([graph.max_neighbor for graph in batch_graph])

        padded_neighbor_list = []
        start_idx = [0]
        '''
        ä¸ºä»€ä¹ˆè¦ä½¿ç”¨èŠ‚ç‚¹ç´¢å¼•åç§»é‡ï¼Ÿ
        
        ä¸ºæ¯ä¸ªå›¾çš„é‚»å±…èŠ‚ç‚¹æ·»åŠ åç§»é‡æ˜¯ä¸ºäº†ç¡®ä¿é‚»å±…èŠ‚ç‚¹åœ¨æ‰¹å¤„ç†ä¸­çš„å…¨å±€ç´¢å¼•æ˜¯å”¯ä¸€çš„å’Œæ­£ç¡®çš„

        åœ¨å¤„ç†å¤šä¸ªå›¾æ—¶ï¼Œå›¾çš„èŠ‚ç‚¹ç´¢å¼•æ˜¯å±€éƒ¨çš„ã€‚æ¯ä¸ªå›¾çš„èŠ‚ç‚¹ç´¢å¼•ä»0å¼€å§‹ï¼Œè€Œåœ¨æ‰¹å¤„ç†çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæ‰€æœ‰å›¾çš„èŠ‚ç‚¹ç´¢å¼•éœ€è¦åˆå¹¶æˆä¸€ä¸ªç»Ÿä¸€çš„ç´¢å¼•ç©ºé—´ã€‚é€šè¿‡åŠ ä¸Š start_idx[i]ï¼Œå¯ä»¥å°†å½“å‰å›¾çš„é‚»å±…èŠ‚ç‚¹ç´¢å¼•è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•ï¼Œé¿å…ç´¢å¼•å†²çªã€‚
        '''

        for i, graph in enumerate(batch_graph):
            start_idx.append(start_idx[i] + len(graph.g))
            #æ›´æ–° start_idxåˆ—è¡¨ï¼Œå°†å½“å‰å›¾çš„èŠ‚ç‚¹æ•°æ·»åŠ åˆ°å‰ä¸€ä¸ªå›¾çš„ç»“æŸç´¢å¼•ï¼Œä»¥ä¾¿ä¸ºä¸‹ä¸€ä¸ªå›¾çš„èŠ‚ç‚¹åˆ†é…æ­£ç¡®çš„ç´¢å¼•ã€‚
            padded_neighbors = []
            for j in range(len(graph.neighbors)):
                #ç¬¬iä¸ªå›¾ç¬¬jä¸ªèŠ‚ç‚¹çš„é‚»å±…åˆ—è¡¨
                #add off-set values to the neighbor indices
                pad = [n + start_idx[i] for n in graph.neighbors[j]]
                #padding, dummy data is assumed to be stored in -1
                #ä¸ºæ¯ä¸ªé‚»å±…èŠ‚ç‚¹æ·»åŠ åç§»é‡ï¼Œç¡®ä¿å®ƒä»¬çš„ç´¢å¼•åœ¨æ•´ä¸ªæ‰¹æ¬¡ä¸­çš„æ­£ç¡®ä½ç½®ã€‚(å°†é‚»å±…èŠ‚ç‚¹çš„ç´¢å¼•è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•)

                pad.extend([-1]*(max_deg - len(pad)))
                #å°†é‚»å±…åˆ—è¡¨å¡«å……åˆ° max_deg çš„é•¿åº¦ï¼Œå¡«å……éƒ¨åˆ†ç”¨ -1 è¡¨ç¤ºï¼Œè¡¨ç¤ºæ— æ•ˆçš„é‚»å±…
                #Add center nodes in the maxpooling if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.
                if not self.learn_eps:
                    pad.append(j + start_idx[i])

                padded_neighbors.append(pad)
            padded_neighbor_list.extend(padded_neighbors)
        '''
        output:padded_neighbor_list->[num_nodes, max_degree]
        '''
        return torch.LongTensor(padded_neighbor_list)


    def __preprocess_neighbors_sumavepool(self, batch_graph):
        ###create block diagonal sparse matrix
        # æ„å»ºä¸€ä¸ªç”¨äºæ±‚å’Œæˆ–å¹³å‡æ± åŒ–çš„å—å¯¹è§’ç¨€ç–çŸ©é˜µ
        edge_mat_list = []
        start_idx = [0]
        for i, graph in enumerate(batch_graph):
            start_idx.append(start_idx[i] + len(graph.g))
            edge_mat_list.append(graph.edge_mat + start_idx[i])
            # å°†å½“å‰å›¾çš„è¾¹çŸ©é˜µ graph.edge_mat çš„ç´¢å¼•åŠ ä¸Šå½“å‰å›¾çš„èµ·å§‹ç´¢å¼•ï¼Œä»¥ä¾¿å°†æ‰€æœ‰å›¾åˆå¹¶åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç´¢å¼•ç©ºé—´ã€‚
        Adj_block_idx = torch.cat(edge_mat_list, 1)
        Adj_block_elem = torch.ones(Adj_block_idx.shape[1])

        #Add self-loops in the adjacency matrix if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.

        if not self.learn_eps:
            num_node = start_idx[-1]
            self_loop_edge = torch.LongTensor([range(num_node), range(num_node)])
            elem = torch.ones(num_node)
            Adj_block_idx = torch.cat([Adj_block_idx, self_loop_edge], 1)
            Adj_block_elem = torch.cat([Adj_block_elem, elem], 0)

        Adj_block = torch.sparse.FloatTensor(Adj_block_idx, Adj_block_elem, torch.Size([start_idx[-1],start_idx[-1]]))

        return Adj_block.to(self.device)


    def __preprocess_graphpool(self, batch_graph):
        ###create sum or average pooling sparse matrix over entire nodes in each graph (num graphs x num nodes)
        
        start_idx = [0]

        #compute the padded neighbor list
        for i, graph in enumerate(batch_graph):
            start_idx.append(start_idx[i] + len(graph.g))

        idx = []
        elem = []
        #idxç”¨äºå­˜å‚¨ç¨€ç–çŸ©é˜µä¸­éé›¶å…ƒç´ çš„ç´¢å¼•ï¼Œelemç”¨äºå­˜å‚¨å¯¹åº”çš„æƒé‡
        for i, graph in enumerate(batch_graph):
            ###average pooling   æƒé‡ä¸º 1 / len(graph.g)
            if self.graph_pooling_type == "average":
                elem.extend([1./len(graph.g)]*len(graph.g))
            
            else:
            ###sum pooling   æƒé‡ä¸º 1 
                elem.extend([1]*len(graph.g))

            idx.extend([[i, j] for j in range(start_idx[i], start_idx[i+1], 1)])
            #å°†æ¯ä¸ªèŠ‚ç‚¹çš„å…¨å±€ç´¢å¼•æ·»åŠ åˆ°idxåˆ—è¡¨ä¸­ï¼Œæ ¼å¼ä¸º: 
            # [[å›¾ç´¢å¼•, èŠ‚ç‚¹ç´¢å¼•], ...]ï¼Œå›¾ä¸å…¶çš„èŠ‚ç‚¹ç´¢å¼•å¯¹åº”
        elem = torch.FloatTensor(elem)
        idx = torch.LongTensor(idx).transpose(0,1)
        graph_pool = torch.sparse.FloatTensor(idx, elem, torch.Size([len(batch_graph), start_idx[-1]]))
        '''
        output:graph_pool å…³äºå›¾ä¸èŠ‚ç‚¹å…³ç³»çš„ç¨€ç–çŸ©é˜µï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºä¸€ä¸ªèŠ‚ç‚¹å­˜åœ¨äºå“ªä¸ªå›¾
        '''
        return graph_pool.to(self.device)

    def maxpool(self, h, padded_neighbor_list):
        ###Element-wise minimum will never affect max-pooling
        #å¯¹å›¾ä¸­çš„èŠ‚ç‚¹ç‰¹å¾è¿›è¡Œæœ€å¤§æ± åŒ–
        '''
        h: ç‰¹å¾çŸ©é˜µ
        padded_neighbor_list: é‚»å±…åˆ—è¡¨
        '''
        dummy = torch.min(h, dim = 0)[0]
        h_with_dummy = torch.cat([h, dummy.reshape((1, -1)).to(self.device)])
        #å°†åŸç‰¹å¾çŸ©é˜µhå’Œè™šæ‹ŸèŠ‚ç‚¹dummyç»“åˆï¼Œå½¢æˆæ–°çš„ç‰¹å¾çŸ©é˜µ h_with_dummy
        pooled_rep = torch.max(h_with_dummy[padded_neighbor_list], dim = 1)[0] 
        # dummyå¯¹åº”padded_neighbor_listä¸­çš„å…ƒç´ -1
        '''
        output: pooled_rep é€šè¿‡æœ€å¤§æ± åŒ–ä»é‚»èŠ‚ç‚¹ç‰¹å¾ä¸­æå–ä¿¡æ¯
        '''
        return pooled_rep

    def next_layer_eps(self, h, layer, padded_neighbor_list = None, Adj_block = None):
        ###pooling neighboring nodes and center nodes separately by epsilon reweighting. 

        if self.neighbor_pooling_type == "max":
            ##If max pooling
            pooled = self.maxpool(h, padded_neighbor_list)
        else:
            #If sum or average pooling
            pooled = torch.spmm(Adj_block, h)
            if self.neighbor_pooling_type == "average":
                #If average pooling
                degree = torch.spmm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))
                pooled = pooled/degree

        #Reweights the center node representation when aggregating it with its neighbors
        pooled = pooled + (1 + self.eps[layer])*h
        pooled_rep = self.mlps[layer](pooled)
        h = self.batch_norms[layer](pooled_rep)

        #non-linearity
        h = F.relu(h)
        return h

    def next_layer(self, h, layer, padded_neighbor_list = None, Adj_block = None):
        ###pooling neighboring nodes and center nodes altogether  
        # å°†é‚»å±…èŠ‚ç‚¹å’Œä¸­å¿ƒèŠ‚ç‚¹çš„è¡¨ç¤ºä¸€èµ·å¤„ç†    
        if self.neighbor_pooling_type == "max":
            ##If max pooling
            pooled = self.maxpool(h, padded_neighbor_list)
        else:
            #If sum or average pooling
            pooled = torch.spmm(Adj_block, h)
            if self.neighbor_pooling_type == "average":
                #If average pooling
                degree = torch.spmm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))
                pooled = pooled/degree

        #representation of neighboring and center nodes 
        pooled_rep = self.mlps[layer](pooled)

        h = self.batch_norms[layer](pooled_rep)

        #non-linearity
        h = F.relu(h)
        return h

    '''
    - next_layer_eps åœ¨èšåˆæ—¶å…³æ³¨äºä¸­å¿ƒèŠ‚ç‚¹çš„é‡æƒé‡ï¼Œç¡®ä¿åœ¨æ± åŒ–æ—¶è€ƒè™‘ä¸­å¿ƒèŠ‚ç‚¹çš„ç‰¹å¾ã€‚
    - next_layer åˆ™æ˜¯å°†é‚»å±…å’Œä¸­å¿ƒèŠ‚ç‚¹çš„ç‰¹å¾ç›´æ¥åˆå¹¶è¿›è¡Œå¤„ç†ã€‚
    '''

    def forward(self, batch_graph):
        X_concat = torch.cat([graph.node_features for graph in batch_graph], 0).to(self.device)
        graph_pool = self.__preprocess_graphpool(batch_graph)

        if self.neighbor_pooling_type == "max":
            padded_neighbor_list = self.__preprocess_neighbors_maxpool(batch_graph)
        else:
            Adj_block = self.__preprocess_neighbors_sumavepool(batch_graph)

        #list of hidden representation at each layer (including input)
        hidden_rep = [X_concat]
        h = X_concat

        for layer in range(self.num_layers-1):
            if self.neighbor_pooling_type == "max" and self.learn_eps:
                h = self.next_layer_eps(h, layer, padded_neighbor_list = padded_neighbor_list)
            elif not self.neighbor_pooling_type == "max" and self.learn_eps:
                h = self.next_layer_eps(h, layer, Adj_block = Adj_block)
            elif self.neighbor_pooling_type == "max" and not self.learn_eps:
                h = self.next_layer(h, layer, padded_neighbor_list = padded_neighbor_list)
            elif not self.neighbor_pooling_type == "max" and not self.learn_eps:
                h = self.next_layer(h, layer, Adj_block = Adj_block)

            hidden_rep.append(h)

        score_over_layer = 0
    
        #perform pooling over all nodes in each graph in every layer
        for layer, h in enumerate(hidden_rep):
            pooled_h = torch.spmm(graph_pool, h)
            #graph_poolæ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œè¡¨ç¤ºèŠ‚ç‚¹åœ¨å›¾ä¸­çš„èšåˆæ–¹å¼ã€‚é€šè¿‡çŸ©é˜µä¹˜æ³•torch.spmmï¼ˆç¨€ç–çŸ©é˜µä¸ç¨ å¯†çŸ©é˜µçš„ä¹˜æ³•ï¼‰ï¼Œå°†å½“å‰å±‚çš„éšè—è¡¨ç¤ºhè¿›è¡Œæ± åŒ–ã€‚
            score_over_layer += F.dropout(self.linears_prediction[layer](pooled_h), self.final_dropout, training = self.training)

        return score_over_layer
```

å¯¹äºèŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ï¼ŒèŠ‚ç‚¹è¡¨ç¤º$h_v^{(k)}$ä½œä¸ºé¢„æµ‹çš„è¾“å…¥ï¼›å¯¹äºå›¾åˆ†ç±»ä»»åŠ¡ï¼ŒREADOUTå‡½æ•°èšåˆäº†æœ€åä¸€ä¾§è¿­ä»£è¾“å‡ºçš„èŠ‚ç‚¹è¡¨ç¤º$h_v^{(k)}$,å¹¶ç”Ÿæˆå›¾è¡¨ç¤º$h_G$:

$$
h_G = READOUT( \{ h_v^{(k)} | v \in G \} )
$$

> READOUTå‡½æ•°æ˜¯å…·æœ‰æ’åˆ—ä¸å˜æ€§çš„å‡½æ•°ï¼Œå¦‚sum,average, max-pooling...

è€Œæœ¬æ–‡æå‡ºREADOUTå‡½æ•°ä½¿ç”¨```Concat+Sum```,å¯¹æ¯æ¬¡è¿­ä»£æ‰€å¾—åˆ°çš„æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾æ±‚å’Œä»¥å¾—åˆ°å›¾çš„ç‰¹å¾ï¼Œæœ€åæ‹¼æ¥èµ·æ¥

$$
h_G = Concat(Sum(\{ h_v^{(k)} | v \in G \})| k = 0,1,...,K) 
$$

ä¸‰ç§ä¸åŒçš„aggregateå‡½æ•°ï¼š

- sumï¼šå­¦ä¹ å…¨éƒ¨çš„æ ‡ç­¾ä»¥åŠæ•°é‡ï¼Œå¯ä»¥å­¦ä¹ ç²¾ç¡®çš„ç»“æ„ä¿¡æ¯ï¼ˆä¸ä»…ä¿å­˜äº†åˆ†å¸ƒä¿¡æ¯ï¼Œè¿˜ä¿å­˜äº†ç±»åˆ«ä¿¡æ¯ï¼‰

- meanï¼šå­¦ä¹ æ ‡ç­¾çš„æ¯”ä¾‹ï¼ˆæ¯”å¦‚ä¸¤ä¸ªå›¾æ ‡ç­¾æ¯”ä¾‹ç›¸åŒï¼Œä½†æ˜¯èŠ‚ç‚¹æœ‰å€æ•°å…³ç³»ï¼‰ï¼Œåå‘å­¦ä¹ åˆ†å¸ƒä¿¡æ¯

- maxï¼šå­¦ä¹ æœ€å¤§æ ‡ç­¾ï¼Œå¿½ç•¥å¤šæ ·ï¼Œåå‘å­¦ä¹ æœ‰ä»£è¡¨æ€§çš„å…ƒç´ ä¿¡æ¯

![](./img2/20.png)

![](./img2/21.png)

- <B>å€ŸåŠ©```DGL```åº“çš„ç®€æ´å®ç°</B>

```python
import argparse

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from dgl.data import GINDataset
from dgl.dataloading import GraphDataLoader
from dgl.nn.pytorch.conv import GINConv
from dgl.nn.pytorch.glob import SumPooling
from sklearn.model_selection import StratifiedKFold
from torch.utils.data.sampler import SubsetRandomSampler


class MLP(nn.Module):
    """Construct two-layer MLP-type aggreator for GIN model"""

    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.linears = nn.ModuleList()
        # two-layer MLP
        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))
        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))
        self.batch_norm = nn.BatchNorm1d((hidden_dim))

    def forward(self, x):
        h = x
        h = F.relu(self.batch_norm(self.linears[0](h)))
        return self.linears[1](h)


class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.ginlayers = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        num_layers = 5
        # five-layer GCN with two-layer MLP aggregator and sum-neighbor-pooling scheme
        for layer in range(num_layers - 1):  # excluding the input layer
            if layer == 0:
                mlp = MLP(input_dim, hidden_dim, hidden_dim)
            else:
                mlp = MLP(hidden_dim, hidden_dim, hidden_dim)
            self.ginlayers.append(
                GINConv(mlp, learn_eps=False)
            )  # set to True if learning epsilon
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))
        # linear functions for graph sum poolings of output of each layer
        self.linear_prediction = nn.ModuleList()
        for layer in range(num_layers):
            if layer == 0:
                self.linear_prediction.append(nn.Linear(input_dim, output_dim))
            else:
                self.linear_prediction.append(nn.Linear(hidden_dim, output_dim))
        self.drop = nn.Dropout(0.5)
        self.pool = (
            SumPooling()
        )  # change to mean readout (AvgPooling) on social network datasets

    def forward(self, g, h):
        # list of hidden representation at each layer (including the input layer)
        hidden_rep = [h]
        for i, layer in enumerate(self.ginlayers):
            h = layer(g, h)
            h = self.batch_norms[i](h)
            h = F.relu(h)
            hidden_rep.append(h)
        score_over_layer = 0
        # perform graph sum pooling over all nodes in each layer
        for i, h in enumerate(hidden_rep):
            pooled_h = self.pool(g, h)
            score_over_layer += self.drop(self.linear_prediction[i](pooled_h))
        return score_over_layer


def split_fold10(labels, fold_idx=0):
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)
    idx_list = []
    for idx in skf.split(np.zeros(len(labels)), labels):
        idx_list.append(idx)
    train_idx, valid_idx = idx_list[fold_idx]
    return train_idx, valid_idx


def evaluate(dataloader, device, model):
    model.eval()
    total = 0
    total_correct = 0
    for batched_graph, labels in dataloader:
        batched_graph = batched_graph.to(device)
        labels = labels.to(device)
        feat = batched_graph.ndata.pop("attr")
        total += len(labels)
        logits = model(batched_graph, feat)
        _, predicted = torch.max(logits, 1)
        total_correct += (predicted == labels).sum().item()
    acc = 1.0 * total_correct / total
    return acc


def train(train_loader, val_loader, device, model):
    # loss function, optimizer and scheduler
    loss_fcn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)

    # training loop
    for epoch in range(350):
        model.train()
        total_loss = 0
        for batch, (batched_graph, labels) in enumerate(train_loader):
            batched_graph = batched_graph.to(device)
            labels = labels.to(device)
            feat = batched_graph.ndata.pop("attr")
            logits = model(batched_graph, feat)
            loss = loss_fcn(logits, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        scheduler.step() #æ›´æ–°optimizerçš„learning rate
        train_acc = evaluate(train_loader, device, model)
        valid_acc = evaluate(val_loader, device, model)
        print(
            "Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} ".format(
                epoch, total_loss / (batch + 1), train_acc, valid_acc
            )
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--dataset",
        type=str,
        default="MUTAG",
        choices=["MUTAG", "PTC", "NCI1", "PROTEINS"],
        help="name of dataset (default: MUTAG)",
    )
    args = parser.parse_args()
    print(f"Training with DGL built-in GINConv module with a fixed epsilon = 0")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # load and split dataset
    dataset = GINDataset(
        args.dataset, self_loop=True, degree_as_nlabel=False
    )  # add self_loop and disable one-hot encoding for input features
    labels = [l for _, l in dataset]
    train_idx, val_idx = split_fold10(labels)

    # create dataloader
    train_loader = GraphDataLoader(
        dataset,
        sampler=SubsetRandomSampler(train_idx),
        batch_size=128,
        pin_memory=torch.cuda.is_available(),
    )
    val_loader = GraphDataLoader(
        dataset,
        sampler=SubsetRandomSampler(val_idx),
        batch_size=128,
        pin_memory=torch.cuda.is_available(),
    )

    # create GIN model
    in_size = dataset.dim_nfeats
    out_size = dataset.gclasses
    model = GIN(in_size, 16, out_size).to(device)

    # model training/validating
    print("Training...")
    train(train_loader, val_loader, device, model)
```