# ğŸ›£[Deep Learning]Stanford CS224w:Machine Learning with Graphs
---
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

!!! info "æƒ³è¯´çš„è¯ğŸ‡"
    <font size = 3.5>
    
    ğŸ”è¯¾ç¨‹ç½‘ç«™ï¼šhttp://web.stanford.edu/class/cs224w/
    
    ğŸ‘€ä¸€äº›èµ„æº: 
    Bç«™ç²¾è®²ï¼šhttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&vd_source=280e4970f2995a05fdeab972a42bfdd0
    
    https://github.com/TommyZihao/zihao_course/tree/main/CS224W
    
    Slides: http://web.stanford.edu/class/cs224w/slides
    
    </font>

###  Heterogeneous graphs(å¼‚æ„å›¾)

> å¼‚è´¨å›¾(heterogeneous graphï¼Œä¹Ÿç§°å¼‚æ„å›¾)ï¼Œåˆè¢«ç§°ä¸ºå¼‚è´¨ä¿¡æ¯ç½‘ç»œ(heterogeneous information networkï¼Œä¹Ÿç§°å¼‚æ„ä¿¡æ¯ç½‘ç»œ)ã€‚åŒºåˆ«äºåŒè´¨å›¾(homogeneous graph)ï¼Œå®ƒæ˜¯ä¸€ç§å…·æœ‰å¤šç§èŠ‚ç‚¹ç±»å‹æˆ–å¤šç§è¾¹ç±»å‹çš„å›¾æ•°æ®ç»“æ„( graphs with multiple nodes or edge types)ï¼Œç”¨äºåˆ»ç”»å¤æ‚å¼‚è´¨å¯¹è±¡åŠå…¶äº¤äº’ï¼Œå…·æœ‰ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¸ºå›¾æ•°æ®æŒ–æ˜æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å»ºæ¨¡å·¥å…·å’Œåˆ†ææ–¹æ³•ã€‚åŒæ—¶ï¼Œå¼‚è´¨å›¾æ•°æ®ä¹Ÿæ˜¯ä¸€ç§å¹¿æ³›å­˜åœ¨çš„æ•°æ®ç±»å‹ï¼Œä¾‹å¦‚çŸ¥è¯†å›¾è°±ï¼Œç¤¾äº¤ç½‘ç»œæ•°æ®ã€‚

- Relational GCNs

- Heterogeneous Graph Transformer

- Design space for heterogeneous GNNs

![](./img/tu1.png)

![](./img/tu2.png)

A heterogeneous graph can be defined as :

$$
\mathbf{G} = \{\mathbf{V}, \mathbf{E}, \tau,\phi \}
$$

- Node with node types: $v \in \mathbf{V}$

- Node types for node: $v:\tau (v)$

- Edge with edge types: $(u,v) \in \mathbf{E}$

- Edge types: $(u,v): \phi (u,v)$

heterogeneous graph's relation type for edge $e$ is a tuple:$r(u,v) = (\tau(u,v), \phi(u,v), \tau(v))$

![](./img/tu3.png)

### Relation GCN(RGCN)

> How do we extend GCNmodel to handle heterogeneous graphs?

![](./img/tu4.png)

$$
h^{(l+1)}_v = \sigma( \sum_{r \in R} \sum_{u \in N^r_v} \frac{1}{c^{r}_{v}} W_r^{(l)} h^{(l)}_u + W_0^{(l)} h^{(l)}_v )
$$

- $c^{r}_{v}$ is the number of relations of $v$, for normalization.

- Each neighbor of a given relation:

$$
m_{u,r}^{(l)} = \frac{1}{c_{v,r}} W_r^{(l)} h^{(l)}_u
$$

- Self-loop

$$
m_{v}^{(l)} = W_0^{(l)} h^{(l)}_v
$$

- Aggregation:

$$
h^{(l+1)}_v = \sigma(Sum(\{{m_{u,r}^{(l)} ï¼Œ u \in N^r_v}\}) âˆª \{m_{v}^{(l)}\})
$$

![](./img/tu5.png)

### Block Diagonal Matrixï¼ˆåˆ†å—å¯¹è§’çŸ©é˜µï¼‰

![](./img/tu6.png)

### Basis Learning

![](./img/tu8.png)

### RGCN for Link Prediction

Firstly, assume $(E, r_3, A)$ is training supervision edge, all the other edges are training message edges

![](./img2/1.png)

- Use RGCN to score the training supervision edge $(E, r_3, A)$

- Create a negative edge by perturbing the supervision edge , e.g. $(E, r_3, B)$, $(E, r_3, D)$

> Note that negative edges should not belong to training message edges or training supervision edges.

- Use GNN model to score negative edge

- Optimize a standatd CrossEntropy loss 

1. Maximize the score of training supervision edge

2. Minimize the score of negative edges

$$
\mathcal{l} = -log \sigma (f_{r3} (h_E,h_A)) - log (1-\sigma(f_{r3} (h_E-h_B)))
$$

![](./img2/2.png)

![](./img2/3.png)

> Hits@k:ä¸€ä¸ªè¯„ä¼°ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæ€§èƒ½çš„æŒ‡æ ‡ï¼Œå¸¸ç”¨äºæ¨èç³»ç»Ÿã€æœç´¢å¼•æ“å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç­‰åœºæ™¯.å¯¹äºç»™å®šçš„é¢„æµ‹åˆ—è¡¨ï¼Œå¦‚æœçœŸå®çš„å…ƒç´ å‡ºç°åœ¨åˆ—è¡¨çš„å‰ k ä¸ªä½ç½®ä¸­ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ªæˆåŠŸçš„é¢„æµ‹ï¼ˆHitï¼‰ã€‚Hits@k æŒ‡æ ‡è®¡ç®—çš„æ˜¯æ‰€æœ‰æˆåŠŸçš„é¢„æµ‹å æ€»é¢„æµ‹æ¬¡æ•°çš„æ¯”ä¾‹ã€‚

> Reciprocal Rank (RR):ä¸€ä¸ªè¡¡é‡ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæ€§èƒ½çš„æŒ‡æ ‡ï¼Œå®ƒç‰¹åˆ«å…³æ³¨æœ€ç›¸å…³ç»“æœçš„æ’åã€‚åœ¨æ¨èç³»ç»Ÿã€æœç´¢å¼•æ“ã€å›¾ç¥ç»ç½‘ç»œç­‰é¢†åŸŸä¸­ï¼Œè¿™ä¸ªæŒ‡æ ‡ç”¨æ¥è¯„ä¼°æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚åœ¨ç»™å®šçš„æŸ¥è¯¢ä¸­ï¼Œå–æœ€ç›¸å…³ç»“æœçš„æ’åçš„å€’æ•°ã€‚å¦‚æœæœ€ç›¸å…³çš„ç»“æœæ’åè¶Šé å‰ï¼Œå…¶å€’æ•°ï¼ˆå³ Reciprocal Rankï¼‰å°±è¶Šå¤§ï¼Œè¡¨ç¤ºç³»ç»Ÿçš„æ€§èƒ½è¶Šå¥½ã€‚

![](./img2/4.png)

### Understanding RGCN(paper)

é˜…è¯»åœ°å€ï¼š[Modeling Relational Data with Graph Convolutional Networks](https://arxiv.org/abs/1703.06103)

å‡è®¾æœ‰å‘æ ‡è®°å›¾$G=(V,\epsilon,R)$ï¼Œå…¶ä¸­èŠ‚ç‚¹$v_i \in V$ï¼Œæœ‰æ ‡ç­¾çš„è¾¹ä¸º$(v_i,r,v_j) \in \epsilon$ï¼Œå…³ç³»ç±»å‹$r \in R$.

æœ¬æ–‡æœ€åˆçš„åŠ¨æœºæ˜¯å°†æœ¬åœ°å›¾é‚»åŸŸä¸Šè¿è¡Œçš„GCNæ‰©å±•åˆ°å¤§è§„æ¨¡å…³ç³»æ•°æ®ã€‚è¿™äº›åŠç›¸å…³æ–¹æ³•(å¦‚GNN)å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªç®€å•çš„å¯å¾®æ¶ˆæ¯ä¼ é€’æ¡†æ¶çš„ç‰¹æ®Šæƒ…å†µï¼š

$$
h_i^{(l+1)} = \sigma \Big( \sum_{m \in \mathcal{M}_i} g_m (h_i^{(l)}, h_j^{(l)}) \Big)
$$

