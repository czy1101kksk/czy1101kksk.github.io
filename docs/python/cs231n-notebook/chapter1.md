# ğŸ›£Stanford CS231n:Deep Learning for Computer Vision  
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

!!! info "æƒ³è¯´çš„è¯ğŸ‡"
    
    <font size = 3.5>
    
    ğŸ”è¯¾ç¨‹ç½‘ç«™ï¼š[https://cs231n.stanford.edu/](https://cs231n.stanford.edu/)
    
    2024ç‰ˆPPT: [https://cs231n.stanford.edu/slides/2024/](https://cs231n.stanford.edu/slides/2024/)
    
    </font>

## Nearest Neighbor Classifier
---

Ké‚»è¿‘ç®—æ³•æ˜¯ä¸€ç§ç®€å•çš„æƒ°æ€§ç®—æ³•ï¼ˆä¸éœ€è¦è®­ç»ƒï¼Œéå‚æ•°ï¼Œä¸å­¦ä¹ ç‰¹å¾ï¼‰ï¼Œå¯ç›´æ¥è¿›è¡Œé¢„æµ‹è®¡ç®—ã€‚

![](./cs231-img/Knearest.png)

è¡¡é‡ä¸¤ä¸ªæ ·æœ¬çš„è·ç¦»:

- $L_1$ distance: $d_1(I_1, I_2) = \sum_{p} |I_1^p - I_2^p|$

![](./cs231-img/k2.png)

- $L_2$ distance: $d_2(I_1, I_2) = \sqrt{\sum_{p} (I_1^p - I_2^p)^2}$

```distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))```

- Ké‚»è¿‘å®ç°ï¼š

```python
import numpy as np

# input x.shape [num_examples, y, x]

#  [ [[1, 2, 3],
#     [2, 3, 4],
#     [4, 5, 6]],
#
#    [[1, 2, 3],
#     [2, 3, 4],
#     [4, 5, 6]] ]

class KNearestNeighbor:
    def __init__(self, k):
        super().__init__()
        self.k = k
    
    def train(self, X, Y):
        self.X_train = X
        self.y_train = Y

    def distance(self, x_1, x_2):
        dis = np.sum(np.abs(x_1 - x_2), axis=1)
        return np.sum(dis, axis=1)
    
    def predict(self, X):
        num_examples = X.shape[0]
        y_predict = np.zeros(num_examples, dtype=self.y_train.dtype)
        for i in range(num_examples):
            Distance = self.distance(self.X_train, X[i,:])
            sorted_indices = np.argsort(Distance)
            k_nearest_labels = self.y_train[sorted_indices]
            y_predict[i] = collections.Counter(k_nearest_labels[0:self.k]).most_common(1)[0][0]

        return y_predict
```

!!! note "$L_1$å’Œ$L_2$æ¯”è¾ƒ"

    <font size = 3.5>
    åœ¨ $L_1$ è·ç¦»æ›´ä¾èµ–äº<B>åæ ‡è½´çš„é€‰å®š(æ—‹è½¬è§’åº¦)</B>ï¼Œåæ ‡è½´é€‰æ‹©ä¸åŒ $L_1$ è·ç¦»ä¹Ÿä¼šè·Ÿç€å˜åŒ–ï¼Œåˆ¤å®šçš„æ•°æ®å½’ç±»çš„è¾¹ç•Œä¼šæ›´è¶‹å‘äºè´´è¿‘åæ ‡ç³»çš„è½´æ¥åˆ†å‰²æ‰€å±åŒºåŸŸï¼Œè€Œ $L_2$ çš„è¯ç›¸å¯¹æ¥è¯´ä¸åæ ‡ç³»çš„å…³è”åº¦æ²¡é‚£ä¹ˆå¤§ï¼Œä¼šå½¢æˆä¸€ä¸ªåœ†ï¼Œä¸è·Ÿéšåæ ‡è½´å˜åŒ–ã€‚

    - åœ¨é¢å¯¹ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å·®å¼‚æ—¶ï¼Œ$L_2$ æ¯” $L_1$ æ›´åŠ ä¸èƒ½å®¹å¿è¿™äº›å·®å¼‚ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç›¸å¯¹äº1ä¸ªå·¨å¤§çš„å·®å¼‚ï¼Œ$L_2$ è·ç¦»æ›´å€¾å‘äºæ¥å—å¤šä¸ªä¸­ç­‰ç¨‹åº¦çš„å·®å¼‚
    </font>

![](./cs231-img/l_2.png)

ç”±ä¸Šå‡ å¼ è¢«åˆ»æ„å¤„ç†çš„å›¾(ä¿è¯äº†å…¶ä¸åŸå›¾çš„$L_2$distanceç›¸åŒ)å¯çŸ¥ï¼Œä½¿ç”¨åƒç´ å·®å¼‚ï¼ˆä¸¤ä¸ªåƒç´ å€¼ä¹‹å·®ï¼‰æ¥æ¯”è¾ƒå›¾åƒæ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼Œæ„Ÿå®˜ä¸Šä¸åŒçš„ä¸¤å¼ å›¾ç‰‡ï¼Œå¯èƒ½æœ‰ç›¸åŒçš„$L_2$distanceï¼Œä¸å›¾åƒçš„è¯­ä¹‰å†…å®¹å…³è”ä¸å¤§ã€‚

## Linear Classifier
---

KNN æ¨¡å‹ä¸­è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰ä½¿ç”¨ä»»ä½•å‚æ•°ï¼Œåªæ˜¯å•çº¯çš„æŠŠè®­ç»ƒæ•°æ®å­˜å‚¨èµ·æ¥ï¼Œè€Œä¸ä¹‹ç›¸å¯¹çš„æ˜¯<B>å‚æ•°æ¨¡å‹</B>ï¼Œæœ€ç®€å•çš„å‚æ•°æ¨¡å‹æ˜¯çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼ˆLinear classifierï¼‰:

\[
    f(x_i,W,b) = W x_i + b    
\]

![](./cs231-img/linear.png)

<B>å®é™…ä¸Šï¼Œä¸Šå›¾å‚æ•°çŸ©é˜µ$W$ç›¸å½“äºæ˜¯ä¸‰ä¸ªåˆ†ç±»å™¨çš„ç»„åˆï¼Œ$W$çš„æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œåˆ†åˆ«å¯¹åº”'çŒ«'ã€'ç‹—'ã€'èˆ¹'</B>

!!! info "å°†çº¿æ€§åˆ†ç±»å™¨çœ‹åšæ¨¡æ¿åŒ¹é…"
    
    <font size = 3.5>
    æŠŠæƒé‡$W$çš„æ¯ä¸€è¡Œçœ‹ä½œä¸€ä¸ªåˆ†ç±»çš„æ¨¡æ¿ï¼Œä¸€å¼ å›¾åƒå¯¹åº”ä¸åŒåˆ†ç±»çš„å¾—åˆ†ï¼Œæ˜¯é€šè¿‡ä½¿ç”¨å†…ç§¯ï¼ˆä¹Ÿå«ç‚¹ç§¯ï¼‰æ¥æ¯”è¾ƒå›¾åƒå’Œæ¨¡æ¿ï¼Œç„¶åæ‰¾åˆ°å’Œå“ªä¸ªæ¨¡æ¿æœ€ç›¸ä¼¼ã€‚

    å¯ä»¥çœ‹åˆ°ï¼š

    ![](./cs231-img/w.png)

    </font>

!!! info "å°†å›¾åƒçœ‹åšé«˜ç»´ç©ºé—´çš„ç‚¹"
    
    <font size = 3.5>
    æŠŠå›¾åƒçœ‹ä½œé«˜ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œçº¿æ€§åˆ†ç±»å™¨å¯¹æ•´ä¸ªç©ºé—´è¿›è¡Œåˆ†å‰²ï¼Œå¯¹åº”ä¸€ä¸ªä¸ªç±»åˆ«

    ![](./cs231-img/high.png)

    </font>

## Loss Function
---

å¯¹äºæœ‰Nä¸ªè®­ç»ƒæ ·æœ¬å¯¹åº”Nä¸ªæ ‡ç­¾çš„è®­ç»ƒé›†æ•°æ®$(x_i, y_i)$ï¼ŒæŸå¤±å‡½æ•°å®šä¹‰ä¸º:

\[
    L =  \frac{1}{N} \sum_{i=1}^{N} L_i(f(x_i, W), y_i)  
\]


- å¤šç±»æ”¯æŒå‘é‡æœºæŸå¤± (Multiclass Support Vector Machine Loss)

    ```SVM```çš„æŸå¤±å‡½æ•°æƒ³è¦ ```SVM```åœ¨æ­£ç¡®åˆ†ç±»ä¸Šçš„å¾—åˆ†å§‹ç»ˆæ¯”ä¸æ­£ç¡®åˆ†ç±»ä¸Šçš„å¾—åˆ†é«˜å‡ºä¸€ä¸ªè¾¹ç•Œå€¼ $\Delta$

    ![](./cs231-img/svm.png)

    å¯¹äºä¸Šè¿°ç¬¬1å¼ å›¾ç‰‡ã€Œå°çŒ«ã€æ¥è¯´ï¼Œè®¾$\Delta$ä¸º1ï¼Œåˆ™ï¼š

    \[
        L_1 = max(0, 5.1-3.2+1) + max(0, -1.7-3.2+1) = 2.9+0 = 2.9    
    \]

    å³ï¼š```SVM```æŸå¤±å‡½æ•°ä¸ä»…å¸Œæœ›æ­£ç¡®åˆ†ç±»çš„åˆ†æ•°æ¯”å…¶ä½™åˆ†ç±»é«˜ï¼Œè€Œä¸”å¸Œæœ›ä¿æŒé«˜å‡ºä¸€ä¸ªè·ç¦»$\Delta$

    ```python
    
    # x:å¾—åˆ†çŸ©é˜µ [[3.2, 5.1, -1.7],
    #            [1.3, 4.9, 2,0],
    #            [2.2, 2.5, -3.1],
    #            ..]
    #y:çœŸå®æ ‡ç­¾ [0, 1, 2]
    
    class MulticlassSVMLoss:
    def __init__(self, x, y, delta):
        self.x_train = x
        self.y_train = y
        self.delta = delta
    def train(self):
        x_true = np.array([self.x_train[k][self.y_train[k]] for k in range(len(self.y_train))])
        margins = np.maximum(self.x_train - x_true.reshape(-1,1) + self.delta, 0)
        loss = np.sum(margins,axis=1) - 1
        
        return np.sum(loss) / len(self.y_train)
            
    ```

    è‹¥ä½¿ç”¨çš„æ˜¯å¹³æ–¹æŸå¤±SVMï¼š$max(0,(s_j - s_{y_j} + 1)^2)$ï¼Œåˆ™æŸå¤±å‡½æ•°ä¼šæ›´å¼ºçƒˆåœ°<B>æƒ©ç½š</B>è¿‡ç•Œçš„è¾¹ç•Œå€¼ã€‚

- Softmax classifier

    ![](./cs231-img/log.png)

    \[
        L = \frac{1}{N} \sum_i [-log(\frac{e^{s_{y_i}}}{\sum_j e^{s_j}})] + \lambda R(W)   
    \]

    å®é™…ä»£ç ç¼–å†™ä¸­ï¼Œç”±äºæŒ‡æ•°å½¢å¼çš„å­˜åœ¨ï¼Œå¦‚æœå¾—åˆ†å¾ˆé«˜ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªéå¸¸å¤§çš„æ•°ï¼ˆæŒ‡æ•°æ•°å€¼çˆ†ç‚¸ï¼‰ã€‚é™¤ä»¥å¤§æ•°å€¼å¯èƒ½å¯¼è‡´æ•°å€¼è®¡ç®—çš„ä¸ç¨³å®šï¼Œæ‰€ä»¥å­¦ä¼šä½¿ç”¨å½’ä¸€åŒ–æŠ€å·§éå¸¸é‡è¦ã€‚å¦‚æœåœ¨åˆ†å¼çš„åˆ†å­å’Œåˆ†æ¯éƒ½ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•°$C$ï¼Œå°±èƒ½å¾—åˆ°ä¸€ä¸ªä»æ•°å­¦ä¸Šç­‰ä»·çš„å…¬å¼ï¼š

    \[
        \frac{e^{s_{y_i}}}{\sum_j e^{s_j}} = \frac{Ce^{s_{y_i}}}{C\sum_j e^{s_j}} = \frac{e^{s_{y_i}+logC}}{\sum_j e^{s_j+logC}}
    \]

    é€šå¸¸å°†$C$è®¾ä¸ºï¼š$logC = -\max{s_j}$ï¼Œé€šè¿‡å°†æ•°å€¼è¿›è¡Œå¹³ç§»ï¼Œä½¿å¾—æœ€å¤§å€¼ä¸º0

    ```python
    s = np.array([123, 456, 789]) # ä¾‹å­ä¸­æœ‰3ä¸ªåˆ†ç±»ï¼Œæ¯ä¸ªè¯„åˆ†çš„æ•°å€¼éƒ½å¾ˆå¤§
    p = np.exp(s) / np.sum(np.exp(s)) # ä¸å¥½ï¼šæ•°å€¼é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´æ•°å€¼çˆ†ç‚¸
    # é‚£ä¹ˆå°†fä¸­çš„å€¼å¹³ç§»åˆ°æœ€å¤§å€¼ä¸º0ï¼š
    s -= np.max(s) # så˜æˆ [-666, -333, 0]
    p = np.exp(s) / np.sum(np.exp(s)) # ç°åœ¨å¯ä»¥äº†ï¼Œå°†ç»™å‡ºæ­£ç¡®ç»“æœ
    ```

    !!! info " ```Softmax``` å’Œ ```SVM``` æ¯”è¾ƒ"
        <font size = 3.5>
        - ç›¸å¯¹äº ```Softmax``` åˆ†ç±»å™¨ï¼Œ```SVM``` æ›´åŠ  ã€Œå±€éƒ¨ç›®æ ‡åŒ–ï¼ˆlocal objectiveï¼‰ã€ï¼Œåªè¦çœ‹åˆ°æ­£ç¡®åˆ†ç±»ç›¸è¾ƒäºä¸æ­£ç¡®åˆ†ç±»ï¼Œå·²ç»å¾—åˆ°äº†æ¯”è¾¹ç•Œå€¼è¿˜è¦é«˜çš„åˆ†æ•°ï¼Œå®ƒå°±ä¼šè®¤ä¸ºæŸå¤±å€¼æ˜¯ å…¬å¼ï¼Œå¯¹äºæ•°å­—ä¸ªä½“çš„ç»†èŠ‚æ˜¯ä¸å…³å¿ƒçš„ã€‚
        
        - ```Softmax``` åˆ†ç±»å™¨å¯¹äºåˆ†æ•°æ˜¯æ°¸ä¸æ»¡è¶³çš„ï¼šæ­£ç¡®åˆ†ç±»æ€»èƒ½å¾—åˆ°æ›´é«˜çš„æ¦‚ç‡ï¼Œé”™è¯¯åˆ†ç±»æ€»èƒ½å¾—åˆ°æ›´ä½çš„æ¦‚ç‡ï¼ŒæŸå¤±å€¼æ€»æ˜¯èƒ½å¤Ÿæ›´å°ã€‚
        </font>