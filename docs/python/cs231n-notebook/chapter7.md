# ğŸ›£Stanford CS231n:Deep Learning for Computer Vision  
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

!!! info "æƒ³è¯´çš„è¯ğŸ‡"
    
    <font size = 3.5>
    
    ğŸ”è¯¾ç¨‹ç½‘ç«™ï¼š[https://cs231n.stanford.edu/](https://cs231n.stanford.edu/)
    
    2024ç‰ˆPPT: [https://cs231n.stanford.edu/slides/2024/](https://cs231n.stanford.edu/slides/2024/)
    
    </font>

###  Sequence to Sequence with RNNs 
---

![](./cs231-img/seq2seq.png)

å·¦ä¾§çš„RNNï¼ˆEncoder:$h_t=f_w(x_t,h_{t-1})$ï¼‰å°†è¾“å…¥åºåˆ—ç¼–ç æ€»ç»“æˆ2ä¸ªå‘é‡ï¼ˆ$s_0$ï¼Œ$c$ï¼‰ï¼Œ$s_0$ä½œä¸ºè§£ç å™¨çš„åˆå§‹çŠ¶æ€(initial decoder state,æˆ–è€…è®¾ç½®ä¸º0)ï¼Œ$c$ä½œä¸ºè§£ç å™¨çš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆContext vectorï¼Œtansfer encoded sequence information to the decoderï¼‰ã€‚

å³ä¾§çš„RNN(decoder)å°†è¿™ä¸ªå‘é‡è§£ç æˆè¾“å‡ºåºåˆ—ã€‚

- During Training:
    åœ¨è®­ç»ƒç½‘ç»œè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡ä¸ä½¿ç”¨ä¸Šä¸€ä¸ªstateçš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªstateçš„è¾“å…¥ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æ ‡å‡†ç­”æ¡ˆ(ground truth)çš„å¯¹åº”ä¸Šä¸€é¡¹ä½œä¸ºä¸‹ä¸€ä¸ªstateçš„è¾“å…¥,ä¸ç®¡è¾“å‡ºæ˜¯å¦æ­£ç¡®ï¼ˆteacher-forcingï¼‰

    ![](./cs231-img/tea.png)

- During Test-time:

    æˆ‘ä»¬ä»è¾“å‡ºä¸­è¿›è¡ŒæŠ½æ ·ï¼Œç›´åˆ°æŠ½ä¸­```[STOP]```

æ˜¾ç„¶ï¼Œè¿™ä¸ªSeq2seqæ¨¡å‹å¹¶ä¸é€‚ç”¨äºé•¿æ–‡æœ¬ä»»åŠ¡
ï¼Œå› ä¸ºå¦‚æœè¾“å…¥åºåˆ—è¿‡é•¿ï¼ŒåŸºäºRNNçš„ç¼–ç å™¨æ²¡æœ‰èƒ½åŠ›å»æ•æ‰è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œå¯¼è‡´è§£ç å™¨æ— æ³•ç”Ÿæˆå‡†ç¡®çš„è¾“å‡ºã€‚å¹¶ä¸”ï¼Œå¸Œæœ›ç”¨å•ä¸€çš„ä¸Šä¸‹æ–‡å‘é‡$c$å»æ€»ç»“æ•´ä¸ªé•¿åºåˆ—ä¿¡æ¯ï¼Œæ˜¾ç„¶æ˜¯ä¸ç°å®çš„ã€‚

æˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸€ç§ç®—æ³•ï¼Œä¸æ˜¯ä½¿ç”¨å•ä¸ªçš„ä¸Šä¸‹æ–‡å‘é‡$c$ï¼Œ
è€Œæ˜¯åœ¨decoderçš„æ¯ä¸ªæ—¶é—´æ­¥ä¸­è®¡ç®—ä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡ï¼Œå³ç»™äºˆdecoderä¸“æ³¨äºè¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œé€‰æ‹©æˆ–è€…é‡å»ºä¸€ä¸ªæ–°çš„ä¸Šä¸‹æ–‡å‘é‡çš„èƒ½åŠ›ã€‚

![](./cs231-img/Bahdanau.png)

å¦‚ä¸Šï¼Œæˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå¯¹é½å‡½æ•°$f_{att}$ï¼ˆalignment functionï¼Œé€šå¸¸ä¸ºMLPsï¼‰ï¼Œå°†Encoderçš„éšè—çŠ¶æ€ä¸$s_0$è¾“å…¥å¾—åˆ°alignment scoresï¼ˆhow much should we attend to each hidden state of encoderï¼‰ï¼Œç„¶åä½¿ç”¨softmaxå‡½æ•°å½’ä¸€åŒ–å¾—åˆ°æƒé‡$a_{t,i}$ï¼ˆattention weightsï¼‰ã€‚

å¾—åˆ°æƒé‡åï¼Œæˆ‘ä»¬ä½¿ç”¨åŠ æƒæ±‚å’Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡$c_t$ï¼Œå³ï¼š
$$
c_t=\sum_{i=1}^{T_x}a_{t,i}h_i
$$
> å…¶ä¸­ï¼Œ$a_{t,i}$è¡¨ç¤ºç¬¬$t$ä¸ªdecoderçš„éšè—çŠ¶æ€å¯¹ç¬¬$i$ä¸ªencoderçš„éšè—çŠ¶æ€$h_i$çš„æ³¨æ„åŠ›æƒé‡ã€‚

![](./cs231-img/r.png)

æ¥ä¸‹æ¥é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œå°†ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€$s_1$ä¸Encoderçš„å„ä¸ª$h_t$è¾“å…¥$f_{att}$,å¾—åˆ°$c_2$ï¼Œä»¥æ­¤ç±»æ¨ã€‚

å› æ­¤ï¼š

-  è¾“å…¥åºåˆ—çš„ä¿¡æ¯ä¼ é€’ä¸ä¼šå—å•ä¸€ä¸Šä¸‹æ–‡å‘é‡çš„é˜»ç¢

- Decoderçš„æ¯ä¸ªæ—¶é—´æ­¥éƒ½èƒ½å¤Ÿâ€œæŸ¥çœ‹â€è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œä»è€Œèƒ½å¤Ÿç”Ÿæˆæ›´é«˜è´¨é‡çš„è¾“å‡ºåºåˆ—ã€‚

å¯¹äºè®¡ç®—å¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒ$a_{t,i}$è¿›è¡ŒçŸ©é˜µå¯è§†åŒ–ï¼Œå¯ä»¥çœ‹åˆ°decoderè¾“å‡ºçš„æ¯ä¸ªå•è¯å…³æ³¨äº†è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼š

![](./cs231-img/visu.png)

æˆ‘ä»¬å°†ä¸¤ç§è¯­è¨€çš„å•è¯è¿›è¡Œå¯¹åº”ï¼Œå¯ä»¥å‘ç°attentionæœºåˆ¶å¾ˆå¥½åœ°æ•æ‰åˆ°äº†ä¸¤ç§è¯­è¨€ä¸­åŒä¹‰å•è¯ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼š

![](./cs231-img/visua1.png)

äº‹å®ä¸Šï¼Œattentionæœºåˆ¶å¹¶ä¸å…³å¿ƒè¾“å…¥æ˜¯å¦æ˜¯ä¸€ä¸ªé¡ºåºåºåˆ—ï¼ˆordered sequence ï¼‰ï¼Œè€Œæ˜¯å¯¹æ•´ä¸ªè¾“å…¥åºåˆ—è¿›è¡Œâ€œæ³¨æ„â€ã€‚

### Image Captioning with RNNs and Attention
---

![](./cs231-img/sps.png)

![](./cs231-img/sssss.png)

![](./cs231-img/s3.png)

![](./cs231-img/seee.png)

### General Attention Layer
---


### The Self-attention Layer
---



### Positional encoding
---


### Masked self-attention layer
---


### Multi-head self-attention layer
---


### Image Captioning using Transformers
---


### The Transformer encoder block
---


### The Transformer decoder block
---


### ViTsâ€“ Vision Transformers
---