# ğŸ”—<B>Chapter 1ï¼šlinear regression and multilayer perceptron</B>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

## çº¿æ€§å›å½’
---

### çº¿æ€§å›å½’çš„è¡¨ç¤º

çº¿æ€§å›å½’(linear regression)åŸºäºå‡ ä¸ªç®€å•å‡è®¾ï¼š

- è‡ªå˜é‡$\mathbf{x}$ä¸å› å˜é‡$y$ä¹‹é—´çš„å…³ç³»æ˜¯çº¿æ€§çš„ï¼Œå³$y$å¯ä»¥è¡¨ç¤ºä¸º$\mathbf{x}$ä¸­å…ƒç´ çš„åŠ æƒå’Œ:

    \[
        \hat{y} = \mathbf{w}^\intercal \mathbf{x} + b         
    \]

    å¯¹äºå¤šç»´æ•°æ®$\mathbf{X} \in \mathbb{R}^{n \times d}$ï¼Œå³nä¸ªæ ·æœ¬ï¼Œdç§ç‰¹å¾ï¼š

    \[
        {\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b
    \]


- å…è®¸è§‚æµ‹å€¼ä¸­å­˜åœ¨å™ªå£°ï¼Œå‡è®¾ä»»ä½•å™ªå£°éƒ½æ¯”è¾ƒæ­£å¸¸ï¼Œå¦‚å™ªå£°éµå¾ªæ­£æ€åˆ†å¸ƒã€‚

### æŸå¤±å‡½æ•°

å¯¹äºçœŸå®æ ·æœ¬æ•°æ®é›†ï¼Œå…¶ä¸­çš„$\mathbf{x}$ä¸$y$ä¹‹é—´ä¸å¯èƒ½å…·æœ‰æ ‡å‡†çš„çº¿æ€§å…³ç³»ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ª<B>ç¡®å®šä¸€ä¸ªæ‹Ÿåˆç¨‹åº¦çš„åº¦é‡</B>,å³æŸå¤±å‡½æ•°ï¼ˆloss functionï¼‰ï¼Œæ¥<B>é‡åŒ–ç›®æ ‡çš„å®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®è·</B>ã€‚å¯¹äºå›å½’é—®é¢˜é€šå¸¸ä½¿ç”¨å¹³æ–¹è¯¯å·®å‡½æ•°ï¼ˆMSEï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼š

\[
    L(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \left(\hat{y}^{(i)} - y^{(i)}\right)^2 =\frac{1}{n} \sum_{i=1}^n \left(\mathbf{w}^\intercal \mathbf{x}^{(i)} + b - y^{(i)}\right)^2
\]

å³æˆ‘ä»¬å¸Œæœ›å¯»æ‰¾åˆ°æœ€ä¼˜çš„æƒé‡ä¸åç½®$(\mathbf{w}^*, b^*) = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b)$ï¼Œæ¥ä½¿å¾—æ€»æŸå¤±æœ€å°(ä¸çœŸå®å€¼çš„å·®è·å°)ã€‚

### éšæœºæ¢¯åº¦ä¸‹é™æ³•

æ¢¯åº¦ä¸‹é™æ³•ï¼ˆgradient descentï¼‰åœ¨æŸå¤±å‡½æ•°å‡å°çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°æ¥é™ä½è¯¯å·®ï¼š

![](./d2l-img/1.png)

\[
    (\mathbf{w},b) \leftarrow (\mathbf{w},b) - \eta \sum_{i =1}^n \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).   
\]

å› ä¸ºä¼ ç»Ÿçš„æ¢¯åº¦ä¸‹é™æ³•éœ€è¦éå†æ•´ä¸ªæ•°æ®é›†ï¼Œåœ¨å®é™…çš„æ‰§è¡Œä¸­å¯èƒ½ä¼šè¾ƒæ…¢ï¼Œå› æ­¤å¯ä»¥åœ¨æ¯ä¸€æ¬¡æ›´æ–°æƒé‡æ—¶éšæœºæŠ½å–ä¸€å°æ‰¹æ ·æœ¬æ¥è®¡ç®—æ›´æ–°ï¼Œè¿™ç§å˜ä½“ä¸ºå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ³•(minibatch stochastic gradient descent),å‡è®¾ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–ä¸€ä¸ªå°æ‰¹é‡$\mathcal{B}$ï¼š

\[
\begin{split}
\begin{aligned} 
\mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),    \\ 
b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).  \\
\end{aligned}
\end{split}
\]

å…¶ä¸­$|\mathcal{B}|$ä¸ºè¡¨ç¤ºæ¯ä¸ªå°æ‰¹é‡ä¸­çš„æ ·æœ¬æ•°ï¼Œå³æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰

> å³ä½¿æ•°æ®é›†æ˜¯å®Œç¾ç¬¦åˆçº¿æ€§ä¸”æ— å™ªå£°ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™æ³•å¾—åˆ°çš„ä¼°è®¡å€¼ä¹Ÿä¸ä¼šä½¿æŸå¤±å‡½æ•°çœŸæ­£åœ°è¾¾åˆ°æœ€å°å€¼ï¼šå› ä¸ºç®—æ³•ä¼šä½¿å¾—æŸå¤±å‘æœ€å°å€¼ç¼“æ…¢æ”¶æ•›ï¼Œä½†å´ä¸èƒ½åœ¨æœ‰é™çš„æ­¥æ•°å†…éå¸¸ç²¾ç¡®åœ°è¾¾åˆ°æœ€å°å€¼ã€‚

### çº¿æ€§å›å½’çš„è§£æè§£

å¯¹äºçº¿æ€§å›å½’ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å°åŒ–$\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$ï¼Œå¯¹äº${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b$ï¼š

\[
    \begin{aligned}
    \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 =& (\mathbf{y} - \mathbf{X}\mathbf{w})^\intercal (\mathbf{y} - \mathbf{X}\mathbf{w}) =  (\mathbf{y}^\intercal - \mathbf{w}^\intercal \mathbf{X}^\intercal) (\mathbf{y} - \mathbf{X}\mathbf{w}) \\
    =& \mathbf{y}^\intercal \mathbf{y} + \mathbf{w}^\intercal \mathbf{X}^\intercal \mathbf{X}\mathbf{w} - \mathbf{y}^\intercal \mathbf{X}\mathbf{w} - \mathbf{w}^\intercal \mathbf{X}^\intercal \mathbf{y} \\
    =& \mathbf{y}^\intercal \mathbf{y} + \mathbf{w}^\intercal \mathbf{X}^\intercal \mathbf{X}\mathbf{w} - 2 \mathbf{y}^\intercal \mathbf{X}\mathbf{w} \\
    \end{aligned}      
\]

ä¸ºæ±‚æå€¼ï¼Œä½¿$\nabla_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 = 0$,æœ‰ï¼š

\[
    \nabla_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 = 2 \mathbf{X}^\intercal \mathbf{X}\mathbf{w} - 2 \mathbf{X}^\intercal \mathbf{y}    
\]

å¯å¾—ï¼š

\[
    \mathbf{w}^* = (\mathbf X^\intercal \mathbf X)^{-1}\mathbf X^\intercal \mathbf{y}    
\]

### çº¿æ€§å›å½’çš„åŸºç¡€å®ç°

ä¸ºäº†å®ç°ä¸€ä¸ªå®Œæ•´çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆæ•°æ®ã€æ„å»ºæ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼Œæœ¬èŠ‚æ¥å®ç°ä¸€ä¸ªæœ€åŸºç¡€çš„çº¿æ€§å›å½’æ¨¡å‹ï¼š

- ç”Ÿæˆä¸€ä¸ªå¸¦å™ªå£°çš„æ•°æ®é›†

```python

def CreatData(features, num_examples, w, b):
    X = torch.normal(0, 1, size = (num_examples, features))
    y = torch.matmul(w, X.T) + b + torch.normal(0, 0.1, size=(1, num_examples))
    return X, y.reshape((-1,1))

```

- æ„å»ºçº¿æ€§å›å½’æ¨¡å‹ä»¥åŠæŸå¤±å‡½æ•°

```python

def LinearRegression(X, w, b):
    y_hat = torch.matmul(w, X.T) + b 
    return y_hat.reshape(-1, 1) 

def MSELossfunction(y_hat, y):
    return (y - y_hat) ** 2

```

- å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™

```python

def Dataiter_RandomBatch(batch_size, features, labels):   # è¿­ä»£å™¨
    num = len(features)
    numlist = [i for i in range(0, num)]
    random.shuffle(numlist)
    for k in range(0, num, batch_size):
        RandomBatch = numlist[k:min(k + batch_size, num)]
        yield features[RandomBatch], labels[RandomBatch]

def SGD(params, alpha, batch_size):
    with torch.no_grad():
        for param in params:
            param -= alpha * param.grad / batch_size
            param.grad.zero_()

```

- ç¨‹åºä¸»ä½“

```python

w_0 = torch.tensor([1, 3, 2, 4, 5, 6], dtype=torch.float)
b_0 = 5.5

num_examples = 100 
features = len(w_0)

X,y = CreatData(features, num_examples, w_0, b_0)

w = torch.normal(mean = 0, std = 1, size = w_0.shape, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

num_epochs = 50
alpha = 0.01
batch_size = 10

for epoch in range(num_epochs):    
    for X_batch, y_batch in Dataiter_RandomBatch(batch_size, X, y):
        loss = MSELossfunction(y_batch, LinearRegression(X_batch, w, b))
        loss.sum().backward()
        SGD([w, b], alpha, batch_size)  
    with torch.no_grad():
        train_loss = MSELossfunction(LinearRegression(X, w, b), y)
        print(f'epoch {epoch + 1}, loss {float(train_loss.mean()):f}')


print(f'wçš„ä¼°è®¡è¯¯å·®: {w_0 - w}')
print(f'bçš„ä¼°è®¡è¯¯å·®: {b_0 - b}')

```

!!! advice "å¯¹äºloss.sum().backward()çš„ç†è§£"
    
    <font size = 3>
    å‡ºå¤„ï¼š [https://zhuanlan.zhihu.com/p/427853673](https://zhuanlan.zhihu.com/p/427853673)

    ```python

    for epoch in range(num_epochs):
        for X, y in data_iter(batch_size, features, labels):
            l = loss(net(X, w, b), y)  # `X`å’Œ`y`çš„å°æ‰¹é‡æŸå¤±
            # å› ä¸º`l`å½¢çŠ¶æ˜¯(`batch_size`, 1)ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚`l`ä¸­çš„æ‰€æœ‰å…ƒç´ è¢«åŠ åˆ°ä¸€èµ·ï¼Œ
            # å¹¶ä»¥æ­¤è®¡ç®—å…³äº[`w`, `b`]çš„æ¢¯åº¦
    --------------------------------------------------------------
            l.sum().backward()
    --------------------------------------------------------------
            sgd([w, b], lr, batch_size)  # ä½¿ç”¨å‚æ•°çš„æ¢¯åº¦æ›´æ–°å‚æ•°
        with torch.no_grad():
            train_l = loss(net(features, w, b), labels)
            print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')

    ```
    å¦‚æœTensor æ˜¯ä¸€ä¸ªæ ‡é‡(å³å®ƒåŒ…å«ä¸€ä¸ªå…ƒç´ çš„æ•°æ®)ï¼Œåˆ™ä¸éœ€è¦ä¸º backward() æŒ‡å®šä»»ä½•å‚æ•°ï¼Œä½†æ˜¯å¦‚æœå®ƒæœ‰æ›´å¤šçš„å…ƒç´ ï¼Œåˆ™éœ€è¦æŒ‡å®šä¸€ä¸ª gradient å‚æ•°ï¼Œè¯¥å‚æ•°æ˜¯å½¢çŠ¶åŒ¹é…çš„å¼ é‡ã€‚æœ¬ä»£ç ä¸­lä¸ºçŸ©é˜µï¼Œéœ€è¦l.sum()è½¬åŒ–ä¸ºæ ‡é‡åå†.backward()ã€‚
    
    </font>

!!! info "pytorchä¸­è‡ªåŠ ï¼ˆ+=ï¼‰ä¸æ™®é€šåŠ çš„åŒºåˆ«" 
    
    <font size = 3>
    å‡ºå¤„ï¼š [https://blog.csdn.net/senbinyu/article/details/102634505](https://blog.csdn.net/senbinyu/article/details/102634505)
    
    è®¨è®ºpytorchä¸­x= x + a ä¸ x += aï¼ˆè‡ªåŠ ï¼‰çš„åŒºåˆ«ï¼Œåœ¨äºå†…å­˜åœ°å€

    ```python

    a = torch.tensor([1.0])
    id_a = id(a)
    a += 1

    id(a) == id_a  #---> True

    a = a + 1

    id(a) == id_a  #---> False

    ```

    ä¸ºäº†æ–¹ä¾¿è¿›è¡ŒåŸä½æ“ä½œï¼ŒPytorchä¸­çš„å‡½æ•°å¯ä»¥åœ¨è°ƒç”¨ä¹‹ååŠ ä¸‹åˆ’çº¿ ï¼Œå¼ºè°ƒè¿™æ˜¯è¿›è¡ŒåŸä½æ“ä½œ
    </font>

### çº¿æ€§å›å½’çš„ç®€æ´å®ç°

- å®šä¹‰æ¨¡å‹ä»¥åŠæ•°æ®é›†è¿­ä»£å™¨ï¼Œåˆå§‹åŒ–æ¨¡å‹å‚æ•°

```python

dataset = data.TensorDataset(features, labels)
dataLoader = data.DataLoader(dataset, batch_size, shuffle=True)

net = nn.Sequential(nn.Linear(2, 1))
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

loss = nn.MSELoss()
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

```

- è®­ç»ƒè¿‡ç¨‹

```python

num_epochs = 3
for epoch in range(num_epochs):
    for X, y in dataLoader:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

w = net[0].weight.data
print('wçš„ä¼°è®¡è¯¯å·®ï¼š', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('bçš„ä¼°è®¡è¯¯å·®ï¼š', true_b - b)

```