# ğŸ”—<B>Chapter 1ï¼šlinear regression and multilayer perceptron</B>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

## çº¿æ€§å›å½’
---

### çº¿æ€§å›å½’çš„è¡¨ç¤º

çº¿æ€§å›å½’(linear regression)åŸºäºå‡ ä¸ªç®€å•å‡è®¾ï¼š

- è‡ªå˜é‡$\mathbf{x}$ä¸å› å˜é‡$y$ä¹‹é—´çš„å…³ç³»æ˜¯çº¿æ€§çš„ï¼Œå³$y$å¯ä»¥è¡¨ç¤ºä¸º$\mathbf{x}$ä¸­å…ƒç´ çš„åŠ æƒå’Œ:

    \[
        \hat{y} = \mathbf{w}^\intercal \mathbf{x} + b         
    \]

    å¯¹äºå¤šç»´æ•°æ®$\mathbf{X} \in \mathbb{R}^{n \times d}$ï¼Œå³nä¸ªæ ·æœ¬ï¼Œdç§ç‰¹å¾ï¼š

    \[
        {\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b
    \]


- å…è®¸è§‚æµ‹å€¼ä¸­å­˜åœ¨å™ªå£°ï¼Œå‡è®¾ä»»ä½•å™ªå£°éƒ½æ¯”è¾ƒæ­£å¸¸ï¼Œå¦‚å™ªå£°éµå¾ªæ­£æ€åˆ†å¸ƒã€‚

### æŸå¤±å‡½æ•°

å¯¹äºçœŸå®æ ·æœ¬æ•°æ®é›†ï¼Œå…¶ä¸­çš„$\mathbf{x}$ä¸$y$ä¹‹é—´ä¸å¯èƒ½å…·æœ‰æ ‡å‡†çš„çº¿æ€§å…³ç³»ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ª<B>ç¡®å®šä¸€ä¸ªæ‹Ÿåˆç¨‹åº¦çš„åº¦é‡</B>,å³æŸå¤±å‡½æ•°ï¼ˆloss functionï¼‰ï¼Œæ¥<B>é‡åŒ–ç›®æ ‡çš„å®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®è·</B>ã€‚å¯¹äºå›å½’é—®é¢˜é€šå¸¸ä½¿ç”¨å¹³æ–¹è¯¯å·®å‡½æ•°ï¼ˆMSEï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼š

\[
    L(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \left(\hat{y}^{(i)} - y^{(i)}\right)^2 =\frac{1}{n} \sum_{i=1}^n \left(\mathbf{w}^\intercal \mathbf{x}^{(i)} + b - y^{(i)}\right)^2
\]

å³æˆ‘ä»¬å¸Œæœ›å¯»æ‰¾åˆ°æœ€ä¼˜çš„æƒé‡ä¸åç½®$(\mathbf{w}^*, b^*) = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b)$ï¼Œæ¥ä½¿å¾—æ€»æŸå¤±æœ€å°(ä¸çœŸå®å€¼çš„å·®è·å°)ã€‚

### éšæœºæ¢¯åº¦ä¸‹é™æ³•

æ¢¯åº¦ä¸‹é™æ³•ï¼ˆgradient descentï¼‰åœ¨æŸå¤±å‡½æ•°å‡å°çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°æ¥é™ä½è¯¯å·®ï¼š

![](./d2l-img/1.png)

\[
    (\mathbf{w},b) \leftarrow (\mathbf{w},b) - \eta \sum_{i =1}^n \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).   
\]

å› ä¸ºä¼ ç»Ÿçš„æ¢¯åº¦ä¸‹é™æ³•éœ€è¦éå†æ•´ä¸ªæ•°æ®é›†ï¼Œåœ¨å®é™…çš„æ‰§è¡Œä¸­å¯èƒ½ä¼šè¾ƒæ…¢ï¼Œå› æ­¤å¯ä»¥åœ¨æ¯ä¸€æ¬¡æ›´æ–°æƒé‡æ—¶éšæœºæŠ½å–ä¸€å°æ‰¹æ ·æœ¬æ¥è®¡ç®—æ›´æ–°ï¼Œè¿™ç§å˜ä½“ä¸ºå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ³•(minibatch stochastic gradient descent),å‡è®¾ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–ä¸€ä¸ªå°æ‰¹é‡$\mathcal{B}$ï¼š

\[
\begin{split}
\begin{aligned} 
\mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),    \\ 
b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).  \\
\end{aligned}
\end{split}
\]

å…¶ä¸­$|\mathcal{B}|$ä¸ºè¡¨ç¤ºæ¯ä¸ªå°æ‰¹é‡ä¸­çš„æ ·æœ¬æ•°ï¼Œå³æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰

> å³ä½¿æ•°æ®é›†æ˜¯å®Œç¾ç¬¦åˆçº¿æ€§ä¸”æ— å™ªå£°ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™æ³•å¾—åˆ°çš„ä¼°è®¡å€¼ä¹Ÿä¸ä¼šä½¿æŸå¤±å‡½æ•°çœŸæ­£åœ°è¾¾åˆ°æœ€å°å€¼ï¼šå› ä¸ºç®—æ³•ä¼šä½¿å¾—æŸå¤±å‘æœ€å°å€¼ç¼“æ…¢æ”¶æ•›ï¼Œä½†å´ä¸èƒ½åœ¨æœ‰é™çš„æ­¥æ•°å†…éå¸¸ç²¾ç¡®åœ°è¾¾åˆ°æœ€å°å€¼ã€‚

### çº¿æ€§å›å½’çš„è§£æè§£

å¯¹äºçº¿æ€§å›å½’ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å°åŒ–$\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$ï¼Œå¯¹äº${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b$ï¼š

\[
    \begin{aligned}
    \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 =& (\mathbf{y} - \mathbf{X}\mathbf{w})^\intercal (\mathbf{y} - \mathbf{X}\mathbf{w}) =  (\mathbf{y}^\intercal - \mathbf{w}^\intercal \mathbf{X}^\intercal) (\mathbf{y} - \mathbf{X}\mathbf{w}) \\
    =& \mathbf{y}^\intercal \mathbf{y} + \mathbf{w}^\intercal \mathbf{X}^\intercal \mathbf{X}\mathbf{w} - \mathbf{y}^\intercal \mathbf{X}\mathbf{w} - \mathbf{w}^\intercal \mathbf{X}^\intercal \mathbf{y} \\
    =& \mathbf{y}^\intercal \mathbf{y} + \mathbf{w}^\intercal \mathbf{X}^\intercal \mathbf{X}\mathbf{w} - 2 \mathbf{y}^\intercal \mathbf{X}\mathbf{w} \\
    \end{aligned}      
\]

ä¸ºæ±‚æå€¼ï¼Œä½¿$\nabla_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 = 0$,æœ‰ï¼š

\[
    \nabla_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 = 2 \mathbf{X}^\intercal \mathbf{X}\mathbf{w} - 2 \mathbf{X}^\intercal \mathbf{y}    
\]

å¯å¾—ï¼š

\[
    \mathbf{w}^* = (\mathbf X^\intercal \mathbf X)^{-1}\mathbf X^\intercal \mathbf{y}    
\]

### çº¿æ€§å›å½’çš„åŸºç¡€å®ç°

ä¸ºäº†å®ç°ä¸€ä¸ªå®Œæ•´çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆæ•°æ®ã€æ„å»ºæ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼Œæœ¬èŠ‚æ¥å®ç°ä¸€ä¸ªæœ€åŸºç¡€çš„çº¿æ€§å›å½’æ¨¡å‹ï¼š

- ç”Ÿæˆä¸€ä¸ªå¸¦å™ªå£°çš„æ•°æ®é›†

```python

def CreatData(features, num_examples, w, b):
    X = torch.normal(0, 1, size = (num_examples, features))
    y = torch.matmul(w, X.T) + b + torch.normal(0, 0.1, size=(1, num_examples))
    return X, y.reshape((-1,1))

```

- æ„å»ºçº¿æ€§å›å½’æ¨¡å‹ä»¥åŠæŸå¤±å‡½æ•°

```python

def LinearRegression(X, w, b):
    y_hat = torch.matmul(w, X.T) + b 
    return y_hat.reshape(-1, 1) 

def MSELossfunction(y_hat, y):
    return (y - y_hat) ** 2

```

- å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™

```python

def Dataiter_RandomBatch(batch_size, features, labels):   # è¿­ä»£å™¨
    num = len(features)
    numlist = [i for i in range(0, num)]
    random.shuffle(numlist)
    for k in range(0, num, batch_size):
        RandomBatch = numlist[k:min(k + batch_size, num)]
        yield features[RandomBatch], labels[RandomBatch]

def SGD(params, alpha, batch_size):
    with torch.no_grad():
        for param in params:
            param -= alpha * param.grad / batch_size
            param.grad.zero_()

```

- ç¨‹åºä¸»ä½“

```python

w_0 = torch.tensor([1, 3, 2, 4, 5, 6], dtype=torch.float)
b_0 = 5.5

num_examples = 100 
features = len(w_0)

X,y = CreatData(features, num_examples, w_0, b_0)

w = torch.normal(mean = 0, std = 1, size = w_0.shape, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

num_epochs = 50
alpha = 0.01
batch_size = 10

for epoch in range(num_epochs):    
    for X_batch, y_batch in Dataiter_RandomBatch(batch_size, X, y):
        loss = MSELossfunction(y_batch, LinearRegression(X_batch, w, b))
        loss.sum().backward()
        SGD([w, b], alpha, batch_size)  
    with torch.no_grad():
        train_loss = MSELossfunction(LinearRegression(X, w, b), y)
        print(f'epoch {epoch + 1}, loss {float(train_loss.mean()):f}')


print(f'wçš„ä¼°è®¡è¯¯å·®: {w_0 - w}')
print(f'bçš„ä¼°è®¡è¯¯å·®: {b_0 - b}')

```

!!! advice "å¯¹äºloss.sum().backward()çš„ç†è§£"
    
    <font size = 3>
    å‡ºå¤„ï¼š [https://zhuanlan.zhihu.com/p/427853673](https://zhuanlan.zhihu.com/p/427853673)

    ```python

    for epoch in range(num_epochs):
        for X, y in data_iter(batch_size, features, labels):
            l = loss(net(X, w, b), y)  # `X`å’Œ`y`çš„å°æ‰¹é‡æŸå¤±
            # å› ä¸º`l`å½¢çŠ¶æ˜¯(`batch_size`, 1)ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚`l`ä¸­çš„æ‰€æœ‰å…ƒç´ è¢«åŠ åˆ°ä¸€èµ·ï¼Œ
            # å¹¶ä»¥æ­¤è®¡ç®—å…³äº[`w`, `b`]çš„æ¢¯åº¦
    --------------------------------------------------------------
            l.sum().backward()
    --------------------------------------------------------------
            sgd([w, b], lr, batch_size)  # ä½¿ç”¨å‚æ•°çš„æ¢¯åº¦æ›´æ–°å‚æ•°
        with torch.no_grad():
            train_l = loss(net(features, w, b), labels)
            print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')

    ```
    å¦‚æœTensor æ˜¯ä¸€ä¸ªæ ‡é‡(å³å®ƒåŒ…å«ä¸€ä¸ªå…ƒç´ çš„æ•°æ®)ï¼Œåˆ™ä¸éœ€è¦ä¸º backward() æŒ‡å®šä»»ä½•å‚æ•°ï¼Œä½†æ˜¯å¦‚æœå®ƒæœ‰æ›´å¤šçš„å…ƒç´ ï¼Œåˆ™éœ€è¦æŒ‡å®šä¸€ä¸ª gradient å‚æ•°ï¼Œè¯¥å‚æ•°æ˜¯å½¢çŠ¶åŒ¹é…çš„å¼ é‡ã€‚æœ¬ä»£ç ä¸­lä¸ºçŸ©é˜µï¼Œéœ€è¦l.sum()è½¬åŒ–ä¸ºæ ‡é‡åå†.backward()ã€‚
    
    </font>

!!! info "pytorchä¸­è‡ªåŠ ï¼ˆ+=ï¼‰ä¸æ™®é€šåŠ çš„åŒºåˆ«" 
    
    <font size = 3>
    å‡ºå¤„ï¼š [https://blog.csdn.net/senbinyu/article/details/102634505](https://blog.csdn.net/senbinyu/article/details/102634505)
    
    è®¨è®ºpytorchä¸­x= x + a ä¸ x += aï¼ˆè‡ªåŠ ï¼‰çš„åŒºåˆ«ï¼Œåœ¨äºå†…å­˜åœ°å€

    ```python

    a = torch.tensor([1.0])
    id_a = id(a)
    a += 1

    id(a) == id_a  #---> True

    a = a + 1

    id(a) == id_a  #---> False

    ```

    ä¸ºäº†æ–¹ä¾¿è¿›è¡ŒåŸä½æ“ä½œï¼ŒPytorchä¸­çš„å‡½æ•°å¯ä»¥åœ¨è°ƒç”¨ä¹‹ååŠ ä¸‹åˆ’çº¿ ï¼Œå¼ºè°ƒè¿™æ˜¯è¿›è¡ŒåŸä½æ“ä½œ
    </font>

### çº¿æ€§å›å½’çš„ç®€æ´å®ç°

- å®šä¹‰æ¨¡å‹ä»¥åŠæ•°æ®é›†è¿­ä»£å™¨ï¼Œåˆå§‹åŒ–æ¨¡å‹å‚æ•°

```python

dataset = data.TensorDataset(features, labels)
dataLoader = data.DataLoader(dataset, batch_size, shuffle=True)

net = nn.Sequential(nn.Linear(2, 1))
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

loss = nn.MSELoss()
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

```

- è®­ç»ƒè¿‡ç¨‹

```python

num_epochs = 3
for epoch in range(num_epochs):
    for X, y in dataLoader:
        l = loss(net(X) ,y)
        trainer.zero_grad() #æ¸…é™¤ä¸Šä¸€æ¬¡çš„æ¢¯åº¦å€¼ 
        l.backward() #æŸå¤±å‡½æ•°è¿›è¡Œåå‘ä¼ æ’­ æ±‚å‚æ•°çš„æ¢¯åº¦
        trainer.step() #æ­¥è¿› æ ¹æ®æŒ‡å®šçš„ä¼˜åŒ–ç®—æ³•è¿›è¡Œå‚æ•°æ›´æ–°
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

w = net[0].weight.data
print('wçš„ä¼°è®¡è¯¯å·®ï¼š', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('bçš„ä¼°è®¡è¯¯å·®ï¼š', true_b - b)

```

!!! advice "å¦‚æœæˆ‘ä»¬ç”¨nn.MSELoss(reduction=â€˜sumâ€™)æ›¿æ¢nn.MSELoss()ä»£ç çš„è¡Œä¸ºç›¸åŒï¼Œéœ€è¦æ€ä¹ˆæ›´æ”¹å­¦ä¹ é€Ÿç‡ï¼Ÿ"
    
    <font size = 3>
    éœ€è¦å°†å­¦ä¹ ç‡lré™¤ä»¥batch_size(é»˜è®¤å‚æ•°æ˜¯'mean')
    
    åŸå› ï¼šè‹¥æŸå¤±å‡½æ•°é‡‡ç”¨'sum' ï¼Œç­‰æ•ˆäºæŸå¤±å‡½æ•°ç›¸å¯¹äº'mean'æ”¾å¤§ï¼Œå°†ä¼šä½¿å¾—è®¡ç®—æ‰€å¾—çš„æ¢¯åº¦å¢å¤§,è¿™æ—¶å€™åŸæœ‰çš„å­¦ä¹ ç‡æ˜¾å¾—è¿‡å¤§ï¼Œæ— æ³•é«˜æ•ˆé€¼è¿‘æœ€ä¼˜ç‚¹ã€‚

    </font>

!!! advice "å¦‚æœæˆ‘ä»¬å°†æƒé‡åˆå§‹åŒ–ä¸ºé›¶ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆã€‚ç®—æ³•ä»ç„¶æœ‰æ•ˆå—ï¼Ÿ"
    
    <font size = 3>
    å‚è€ƒæ–‡ç« ï¼š[è°ˆè°ˆç¥ç»ç½‘ç»œæƒé‡ä¸ºä»€ä¹ˆä¸èƒ½åˆå§‹åŒ–ä¸º0](https://zhuanlan.zhihu.com/p/75879624)

    åœ¨å•å±‚ç½‘ç»œä¸­(ä¸€å±‚çº¿æ€§å›å½’å±‚)ï¼Œå¯ä»¥æŠŠæƒé‡åˆå§‹åŒ–ä¸º0ï¼Œä½†æ˜¯å½“ç½‘ç»œåŠ æ·±åï¼Œåœ¨å…¨è¿æ¥çš„æƒ…å†µä¸‹ï¼Œåœ¨åå‘ä¼ æ’­çš„æ—¶å€™ï¼Œç”±äº<B>æƒé‡çš„å¯¹ç§°æ€§</B>ä¼šå¯¼è‡´å‡ºç°éšè—ç¥ç»å…ƒçš„å¯¹ç§°æ€§ï¼Œä½¿å¾—å¤šä¸ªéšè—ç¥ç»å…ƒçš„ä½œç”¨å°±å¦‚åŒ1ä¸ªç¥ç»å…ƒï¼Œç®—æ³•è¿˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†æ˜¯æ•ˆæœä¸å¤§å¥½ã€‚

    </font>

### Softmaxå›å½’

åœ¨åˆ†ç±»é—®é¢˜å½“ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¾—åˆ°æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼ˆå› æ­¤æœ‰å¤šä¸ªè¾“å‡ºï¼‰,æˆ‘ä»¬å¸Œæœ›æ¨¡å‹çš„è¾“å‡º$\hat{y}_j$å¯ä»¥è§†ä¸ºå±äºç±»$j$çš„æ¦‚ç‡ï¼Œç„¶åé€‰æ‹©å…·æœ‰æœ€å¤§è¾“å‡ºå€¼çš„ç±»åˆ«$\operatorname*{argmax} y_j$ä½œä¸ºæˆ‘ä»¬çš„é¢„æµ‹ã€‚ 

ä¾‹å¦‚ï¼Œå¦‚æœ$\hat{y}_1$ã€$\hat{y}_2$å’Œ$\hat{y}_3$åˆ†åˆ«ä¸º0.1ã€0.8å’Œ0.1ï¼Œ é‚£ä¹ˆæˆ‘ä»¬é¢„æµ‹çš„ç±»åˆ«å°±æ˜¯ç¬¬2ç±»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¦å°†å¤šä¸ªè¾“å‡ºæ•°å­—çš„æ€»å’Œé™åˆ¶ä¸º1ï¼Œå¹¶ä¸”æ¯ä¸ªè¾“å‡ºæ°¸è¿œå¤§äº0ï¼Œ

<B>softmaxå‡½æ•°</B>èƒ½å¤Ÿå°†æœªè§„èŒƒåŒ–çš„é¢„æµ‹å˜æ¢ä¸ºéè´Ÿæ•°å¹¶ä¸”æ€»å’Œä¸º1ï¼ŒåŒæ—¶è®©æ¨¡å‹ä¿æŒå¯å¯¼çš„æ€§è´¨(è§„èŒƒåŒ–)ã€‚

\[
    \hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{å…¶ä¸­} \quad 0 \leq \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}  \leq 1  
\]

åœ¨å°æ‰¹é‡æ ·æœ¬å¤„ç†ä¸­ï¼Œå…¶ä¸­ç‰¹å¾ç»´åº¦ï¼ˆè¾“å…¥æ•°é‡ï¼‰ä¸ºdï¼Œæ‰¹é‡å¤§å°ä¸ºnï¼Œè¾“å‡ºä¸ºqï¼Œå‡è®¾å°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾ä¸º$\mathbf{X} \in \mathbb{R}^{n \times d}$ï¼Œæƒé‡çŸ©é˜µä¸º$\mathbf{W} \in \mathbb{R}^{d \times q}$ï¼Œåç½®ä¸º$\mathbf{b} \in \mathbb{R}^{1\times q}$ï¼Œåˆ™softmaxå›å½’å¯è¡¨ç¤ºä¸ºï¼š

\[
   \begin{split}
   \begin{aligned} 
   \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ 
   \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). 
   \end{aligned}
   \end{split} 
\]

ç”±ä¸Šè¿°æ˜“çŸ¥ï¼Œ$\hat{\mathbf{y}}$å¯ç†è§£ä¸º<B>å¯¹ç»™å®šä»»æ„è¾“å…¥
$\mathbf{x}$çš„å±äºæ¯ä¸ªç±»çš„æ¡ä»¶æ¦‚ç‡</B>ã€‚å¯¹äºæ•°æ®é›†$\{\mathbf{X}, \mathbf{Y}\}$å…·æœ‰nä¸ªæ ·æœ¬ï¼Œæœ‰ï¼š

\[
   P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}). 
\]

è¦æœ€å¤§åŒ–$P(\mathbf{Y} \mid \mathbf{X})$,å‘ä¸Šå¼åšè´Ÿå¯¹æ•°åŒ–:

\[
    -\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)})    
\]

å³ä¸ºï¼š

\[
    l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. 
\]

è¿™ä¸ªæŸå¤±å‡½æ•°è¢«ç§°ä¸ºäº¤å‰ç†µæŸå¤±ï¼ˆcross-entropy lossï¼‰