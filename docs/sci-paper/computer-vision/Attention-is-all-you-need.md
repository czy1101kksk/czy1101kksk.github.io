# Attention is all you need

!!! info "相关信息"
    <font size = 3.5>
    
    论文地址：[Attention is all you need](https://arxiv.org/abs/1706.03762)

    代码（Pytorch版）:[https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/master?tab=readme-ov-file](https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/master?tab=readme-ov-file)

    本页内容是对```Transformers```的文章总结/代码阅读(侧重代码学习)

    必读论文，懂的都懂，不懂的快去看。

    </font>


