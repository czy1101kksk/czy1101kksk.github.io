{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Aloha ! Here is \u201cBRIGHT_Czy\u2019s Notebook","text":"<p>Quote\ud83c\udf0f</p> <p>  \u6625\u6c34\u5954\u817e\u8fc7\u7684\u5730\u65b9\uff0c\u5982\u4eca\u5230\u5904\u662f\u9c9c\u82b1\u7684\u6d2a\u6d41...  <p>\u2014\u2014 \u666e\u91cc\u4ec0\u6587\u300a\u82b1\u6eaa\u300b \u00a0 \u00a0 </p></p> <p>Who Am I\ud83e\uddd0</p> <p> BRIGHT\u5ddd  \ud83c\udf93ZJU\u5927\u4e09 \u80fd\u73af\u672c\u79d1\u751f  \u5934\u50cf\u4f5c\u8005:@kine_tougen </p> \u270d\u6446\u70c2\u9ad8\u624b\ud83e\udd23,\u60f3\u7684\u592a\u591a\u505a\u7684\u592a\u5c11 <p>\ud83d\udd2e\u5f53\u524d\u5b66\u4e60\u65b9\u5411/\u5174\u8da3:Multi-energy Load Forecasting/Graph Neural Network/ AI4Science  \ud83d\udc30\u7231\u597d:\u7fbd\u6bdb\u7403\ud83c\udff8,Mr.Quin\u5fe0\u5b9e\u89c2\u4f17,\u5b66\u81ea\u5df1\u611f\u5174\u8da3\u7684\u77e5\u8bc6 </p> Contact Me <p>  Github : BRIGHT_CZY <p> Gmail : brightyoungchuan@gmail.com</p> <p></p> &gt;&gt;&gt;\u65e0\u6cd5\u6539\u53d8\u7684\u540e\u6094\u6b63\u5728\u79ef\u7d2f\u6210\u5c71\ud83c\udfd4\ud83d\ude34.....\u201d"},{"location":"DataAnalysis/","title":"Data Analysis","text":""},{"location":"DataAnalysis/#dataset","title":"Dataset","text":"<p>\u538b\u7f29\u62d0\u89d2\u957f\\(1m\\)\uff0c\u538b\u7f29\u89d2\\(20\u00b0~42\u00b0\\)\uff0c\u9a6c\u8d6b\u6570\\(0.4~5\\)\uff0c\u9ad8\u5ea6\\(0~40km\\)\u3002\u8ba1\u7b97\u7f51\u683c\u5728\u6d41\u5411\u6709200\u4e2a\u70b9\uff0c\u6cd5\u541196\u4e2a\u70b9\u3002 \u8fd1\u58c1\u7b2c\u4e00\u5c42\u7f51\u683c\u9ad8\u5ea6\\(1\u00d710-5m\\)\uff0c\u6cbf\u5c55\u5411\u9010\u6e10\u52a0\u7c97\u3002</p>"},{"location":"DataAnalysis/#compressible-navier-stokes-equation","title":"Compressible Navier-Stokes Equation","text":""},{"location":"wait-for-me/","title":"\u65bd\u5de5\u4e2d.....","text":"<p>Info</p> <p> <p>\u672a\u6765\u53ef\u80fd\u63a8\u51fa:</p> <ul> <li> <p>\u80fd\u73af\u90e8\u5206\u8bfe\u7a0b\u7b14\u8bb0(\u770b\u5fc3\u60c5)</p> </li> <li> <p>\u4e00\u4e9b\u968f\u7b14</p> </li> </ul> <p></p>"},{"location":"cpp/","title":"\ud83d\udd2dC++\u5b66\u4e60","text":"<p>Chapter</p> <p> <ul> <li>Chapter 1\uff1a </li> </ul> <p></p>"},{"location":"cpp/file/chapter1/","title":"\ud83d\udd2dC++\u5b66\u4e60:\u57fa\u7840\u8bed\u6cd5","text":""},{"location":"energy/Engineering_Thermodynamics/","title":"\u5de5\u7a0b\u70ed\u529b\u5b66(\u7532)","text":"\ud83c\udf07Information <p> <ul> <li> <p>\u8bfe\u7a0b:\u5de5\u7a0b\u70ed\u529b\u5b66(\u7532) Engineering\u2002Thermodynamics(A)</p> </li> <li> <p>\u5b66\u5206:4.0\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u8bfe\u7a0b\u4ee3\u7801: 59120030</p> </li> <li> <p>\u6559\u5e08:\u4fde\u81ea\u6d9b\u8001\u5e08\uff08\u4e3b\u8bb2\uff09\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u6559\u6750:\u300a\u5de5\u7a0b\u70ed\u529b\u5b66(\u7b2c\u4e09\u7248)\u300b(\u66fe\u4e39\u82d3\u3001\u6556\u8d8a\u7b49) </p> </li> <li> <p>\u63a8\u8350\u9605\u8bfb\uff1a\u2605\u2605\u2605\u300a\u5de5\u7a0b\u70ed\u529b\u5b66\u7cbe\u8981\u5206\u6790\u53ca\u7ecf\u5178\u9898\u7cbe\u89e3\u300b\uff08\u4f55\u96c5\u73b2\uff09</p> </li> <li> <p>\u6210\u7ee9\uff1a5.0\u2002\u671f\u672b\u5377\u976294\u5206</p> </li> </ul> <p> <p>\u2002\u2002\u2002\u2002\u65e0\u610f\u4e2d\u9009\u5230\u4e86\u6167\u80fd\u73ed\u7684\u8bfe,\u81ea\u5df1\u548c\u4fde\u8001\u5e08\u8fd8\u7b97\u6bd4\u8f83\u719f\u6089,\u73ed\u4e0a\u4e00\u5171\u53ea\u670920\u591a\u4eba\u6240\u4ee5\u542c\u8bfe\u4f53\u9a8c\u8fd8\u4e0d\u9519.\u671f\u672b\u8003\u8bd5\u5360\u603b\u8bc460%\uff0c\u67098\u4e2a\u7b80\u7b54\u9898+3\u4e2a\u8ba1\u7b97\u5927\u9898\uff0c\u603b\u4f53\u96be\u5ea6\u4e00\u822c\uff0c\u8ba1\u7b97\u9898\u4fa7\u91cd\u4e8e\u6700\u540e11-14\u7ae0\u7684\u51e0\u4e2a\u7ecf\u5178\u5faa\u73af\u548c6\u30017\u7ae0\u7684\u6c34\u84b8\u6c14\u4e0e\u6e7f\u7a7a\u6c14\u70ed\u529b\u8fc7\u7a0b\uff08\u770b98\u4e0a\u7684\u56de\u5fc6\u4e5f\u6709\u53ef\u80fd\u4f1a\u8003\u6bd4\u8f83\u57fa\u7840\u7684\u7b2c\u4e5d\u7ae0\u55b7\u7ba1\uff09\uff0c\u5982\u679c\u5728\u8003\u524d\u80fd\u5c06\u524d\u9762\u51e0\u7ae0\u7684\u6982\u5ff5\u8fc7\u4e00\u904d\u5e76\u4e14\u719f\u6089\u5e38\u8003\u8ba1\u7b97\u9898\u578b\uff0c\u5e94\u4ed8\u8003\u8bd5\u5c31\u5b8c\u5168\u6ca1\u95ee\u9898\u4e86\u3002</p> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u4e00\u7ae0-\u57fa\u672c\u6982\u5ff5\u53ca\u5b9a\u4e49","title":"\u7b2c\u4e00\u7ae0 \u57fa\u672c\u6982\u5ff5\u53ca\u5b9a\u4e49","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u70ed\u529b\u7cfb\u7edf\u4e0e\u5916\u754c,\u5f00\u53e3\u7cfb\u4e0e\u95ed\u53e3\u7cfb,\u7edd\u70ed\u7cfb,\u5b64\u7acb\u7cfb,\u5e73\u8861\u72b6\u6001,\u72b6\u6001\u53c2\u6570(T\u3001p),\u8868\u538b\u529b\u4e0e\u771f\u7a7a\u5ea6,\u53ef\u9006\u8fc7\u7a0b\u4e0e\u51c6\u9759\u6001\u8fc7\u7a0b,\u6e29\u6807</p> <p>\ud83d\udd27\u8ba1\u7b97:\u538b\u529b\u8ba1(\u8868\u538b\\(p_e\\))\u4e0e\u771f\u7a7a\u8ba1(\u771f\u7a7a\u5ea6\\(p_v\\)),\u4e0d\u540c\u6e29\u6807\u4e4b\u95f4\u7684\u76f8\u5173\u8ba1\u7b97</p> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u70ed\u529b\u7cfb\u7edf","title":"\u70ed\u529b\u7cfb\u7edf","text":"<p>\u70ed\u529b\u7cfb\u7edf\u662f\u4eba\u4e3a\u5206\u5272\u51fa\u6765\u7684\u4f5c\u4e3a\u70ed\u529b\u5b66\u5206\u6790\u5bf9\u8c61\u7684\u6709\u9650\u7269\u8d28\u7cfb\u7edf,\u800c\u4e0e\u7cfb\u7edf\u8fdb\u884c\u8d28\u80fd\u4ea4\u6362\u7684\u7269\u4f53\u4e3a\u5916\u754c:</p> <ul> <li> <p>\u95ed\u53e3\u7cfb:\u4e0e\u5916\u754c\u53ea\u6709\u80fd\u91cf\u4ea4\u6362\u800c\u6ca1\u6709\u7269\u8d28\u4ea4\u6362(\u7269\u8d28\u4e0d\u900f\u8fc7\u8fb9\u754c),\u5373\u63a7\u5236\u8d28\u91cf\\(\\frac{dm}{dt}=0\\)(\\(C.M.\\))</p> </li> <li> <p>\u5f00\u53e3\u7cfb:\u4e0d\u4ec5\u6709\u80fd\u91cf\u4ea4\u6362\u4e5f\u6709\u7269\u8d28\u4ea4\u6362,\u5373\u63a7\u5236\u4f53\u79ef/\u63a7\u5236\u4f53(\\(C.V.\\)).</p> <p>\u5f00\u53e3\u7cfb\u4e0e\u95ed\u53e3\u7cfb\u7684\u533a\u5206\u5728\u4e8e\u6709\u6ca1\u6709\u8d28\u91cf\u8d8a\u8fc7\u8fb9\u754c</p> </li> <li> <p>\u7edd\u70ed\u7cfb:\u7cfb\u7edf\u4e0e\u5916\u754c\u6ca1\u6709\u70ed\u91cf\u4ea4\u6362</p> </li> <li> <p>\u5b64\u7acb\u7cfb:\u65e2\u6ca1\u6709\u80fd\u91cf\u4ea4\u6362\u4e5f\u6ca1\u6709\u7269\u8d28\u4ea4\u6362(\u663e\u7136,\u5b64\u7acb\u7cfb\u5fc5\u7136\u4e3a\u7edd\u70ed\u7cfb)</p> <p>\u5b64\u7acb\u7cfb\u4e0d\u4ee3\u8868\u4e0d\u53d1\u751f\u4f5c\u7528,\u800c\u662f\u4e00\u5207\u76f8\u4e92\u4f5c\u7528\u90fd\u53d1\u751f\u5728\u7cfb\u7edf\u5185\u90e8,\u5982:\u5c06\u67d0\u4e00\u7cfb\u7edf\u4ee5\u53ca\u4e0e\u4e4b\u53d1\u751f\u8d28\u80fd\u4ea4\u6362\u7684\u5916\u754c\u770b\u4f5c\u4e00\u4e2a\u5927\u7684\u7cfb\u7edf,\u8be5\u8054\u5408\u7cfb\u7edf\u5373\u4e3a\u5b64\u7acb\u7cfb\u7edf</p> </li> <li> <p>\u53ef\u538b\u7f29\u7cfb\u7edf:\u7531\u53ef\u538b\u7f29\u6d41\u4f53\u6784\u6210\u7684\u70ed\u529b\u7cfb\u7edf</p> </li> <li> <p>\u7b80\u5355\u53ef\u538b\u7f29\u7cfb\u7edf:\u4ec5\u6709\u51c6\u9759\u6001\u4f53\u79ef\u53d8\u5316\u529f(\u81a8\u80c0\u529f\u3001\u538b\u7f29\u529f)\u7684\u53ef\u538b\u7f29\u7cfb\u7edf</p> </li> </ul> <p>\u72b6\u6001\u53c2\u6570\u2192\u63cf\u8ff0\u5de5\u8d28\u6240\u5904\u5e73\u8861\u72b6\u6001\u7684\u5b8f\u89c2\u7269\u7406\u91cf,\u53cd\u6620\u7684\u662f\u5927\u91cf\u5206\u5b50\u8fd0\u52a8\u7684\u5b8f\u89c2\u5e73\u5747\u6548\u679c(\u7531\u6b64\u53ef\u77e5,\u72b6\u6001\u65b9\u7a0b\u662f\u5728\u5e73\u8861\u72b6\u6001\u4e0b,\u72b6\u6001\u53c2\u6570\u7684\u5173\u7cfb\u5f0f)</p> <p>\u5e73\u8861\u72b6\u6001</p> <p> \u5728\u4e0d\u53d7\u5916\u754c\u5f71\u54cd\u7684\u6761\u4ef6\u4e0b(\u7cfb\u7edf\u4e0e\u5916\u754c\u7684\u4e0d\u5e73\u8861\u52bf\u6d88\u5931)\u7cfb\u7edf\u7684\u5185\u90e8\u72b6\u6001\u80fd\u59cb\u7ec8\u4fdd\u6301\u4e0d\u53d8(\u5b8f\u89c2\u53d8\u5316\u5168\u90e8\u505c\u6b62,\u5b8f\u89c2\u6027\u8d28\u4e0d\u53d8)\u5373\u4e3a\u5e73\u8861\u72b6\u6001 <p>\u53ea\u6709\u5e73\u8861\u72b6\u6001\u7684\u7cfb\u7edf\u624d\u80fd\u7528\u72b6\u6001\u53c2\u6570\u6765\u63cf\u8ff0,\u53ea\u8981\u6709\u4e24\u4e2a\u72ec\u7acb\u72b6\u6001\u53c2\u6570\u5373\u53ef\u786e\u5b9a\u4e00\u4e2a\u72b6\u6001,\u6240\u6709\u5176\u4ed6\u72b6\u6001\u53c2\u6570\u5747\u53ef\u8868\u793a\u4e3a\u8fd9\u4e24\u4e2a\u72b6\u6001\u53c2\u6570\u7684\u51fd\u6570</p> <p></p> <p>\u5f53\u70ed\u529b\u7cfb\u7ecf\u5386\u4e00\u5c01\u95ed\u72b6\u6001\u53d8\u5316\u8fc7\u7a0b\u800c\u53c8\u6062\u590d\u5230\u539f\u59cb\u72b6\u6001\u65f6,\u5176\u72b6\u6001\u53c2\u6570\u7684\u53d8\u5316\u4e3a0,\u5373:\\(\\oint d \\xi= 0\\)</p> <ul> <li> <p>\u57fa\u672c\u72b6\u6001\u53c2\u6570:p, V, T</p> </li> <li> <p>\u5f3a\u5ea6\u91cf(p, T):\u4e0e\u7269\u8d28\u7684\u6570\u91cf\u65e0\u5173,\u4e0d\u5177\u6709\u53ef\u52a0\u6027,\u5bf9\u5904\u4e8e\u5e73\u8861\u72b6\u6001\u7684\u7cfb\u7edf\u624d\u5177\u6709\u786e\u5b9a\u7684\u6570\u503c</p> </li> <li> <p>\u5e7f\u5ef6\u91cf/\u5c3a\u5ea6\u91cf(V, U, H, S):\u4e0e\u5de5\u8d28\u8d28\u91cf\u6210\u6b63\u6bd4,\u5177\u6709\u53ef\u52a0\u6027</p> </li> </ul> \u6e29\u5ea6 <p> \u6e29\u5ea6\u662f\u63cf\u8ff0\u548c\u5224\u65ad\u7cfb\u7edf\u4e0e\u5176\u4ed6\u7cfb\u7edf/\u5916\u754c\u5904\u4e8e\u70ed\u5e73\u8861\u72b6\u6001\u7684\u53c2\u6570 <p>\u70ed\u529b\u5b66\u6e29\u6807(\u7edd\u5bf9\u6e29\u6807)</p> <p>\u70ed\u529b\u7cfb\u6e29\u6807(K)\u5c06\u6c34\u7684\u4e09\u76f8\u70b9\u6e29\u5ea6\u5b9a\u4e3a\u57fa\u51c6\u70b9,\u89c4\u5b9a\u4e3a273.16K,\u800c\u7edd\u5bf9\u96f6\u5ea6\u4e3a0K,\u800c\u5176\u4ed6\u6e29\u6807\u4e0e\u70ed\u529b\u7cfb\u6e29\u6807\u4ec5\u662f\u96f6\u70b9\u53d6\u503c\u7684\u4e0d\u540c.</p> <p>\u6e29\u6807\u8f6c\u6362:\u901a\u8fc7\u5c06\u65b0\u6e29\u6807(\u7ebf\u6027)\u4e0e\u5df2\u77e5\u6e29\u6807(\u5982\u70ed\u529b\u5b66\u7edd\u5bf9\u6e29\u6807)\u7684\u5c3a\u5ea6\u4f5c\u5bf9\u5e94,\u6c42\u5f97\u4e24\u79cd\u6e29\u6807\u7684\u7ebf\u6027\u5173\u7cfb,\u8bbe\u67d0\u4e00\u6e29\u6807(\\(^oN\\))\u57281\u4e2a\u6807\u51c6\u5927\u6c14\u538b\u4e0b\u7684\u51b0\u70b9\u4e0e\u6c7d\u70b9\u4e3a\\(T_1^oN\u4e0eT_2^oN\\),\u5df2\u77e5\u70ed\u529b\u5b66\u7edd\u5bf9\u6e29\u6807(K),\u5219</p> \\[       \\frac{T_2 - T_1}{373.15 - 273.15} = \\frac{ \\{T_{N}\\}_{^oN} - T_1 }{ \\{T_{N}\\}_{K} - 273.15 }           \\] <p>\u7531\u6b64,\u5373\u53ef\u6c42\u5f97\u4e24\u4e2a\u6e29\u6807\u4e4b\u95f4\u7684\u7ebf\u6027\u5173\u7cfb: \\(\\{T_{N}\\}_{^oN} = \\frac{T_2 - T_1}{373.15 - 273.15} (\\{T_{N}\\}_{K} - 273.15)\\)</p> <p></p> <p>\u538b\u529b</p> <p> <p>\u5355\u4f4d\u9762\u79ef\u4e0a\u6240\u53d7\u7684\u5782\u76f4\u4f5c\u7528\u529b\u4e3a\u538b\u529b(\u538b\u5f3a)[\\(Pa\\)],\u6d4b\u91cf\u5de5\u8d28\u538b\u529b\u7684\u4eea\u5668\u4e3a\u538b\u529b\u8ba1</p> <p>\u538b\u529b\u8ba1\u7684\u5de5\u4f5c\u539f\u7406</p> <p> <p>\u538b\u529b\u8ba1\u6d4b\u91cf\u7684\u662f\u5de5\u8d28\u7edd\u5bf9\u538b\u529b\\(p\\)\u4e0e\u5916\u754c\u73af\u5883\u538b\u529b\\(p_b\\)\u4e4b\u5dee,\u5373\u76f8\u5bf9\u538b\u529b/\u8868\u538b(\\(p_e, p_v\\)),\u800c\u7edd\u5bf9\u538b\u529b\u4e0e\u5927\u6c14\u538b\u529b\u65e0\u5173,\u56e0\u6b64\u5927\u6c14\u538b\u529b\u53d8\u5316\u5e76\u4e0d\u4f1a\u5f71\u54cd\u7edd\u5bf9\u538b\u529b:</p> \\[     \\begin{equation}     p=     \\begin{cases}     p_b + p_e &amp;, \\text{ $ p &gt; p_b $ } (p_e\u4e3a\u8868\u538b\u529b)   \\\\     p_b - p_v &amp;, \\text{ $ p &lt; p_b $ } (p_v\u4e3a\u771f\u7a7a\u5ea6)   \\\\     \\end{cases}     \\end{equation} \\] <p></p> <p></p> <p> </p>"},{"location":"energy/Engineering_Thermodynamics/#\u51c6\u9759\u6001\u8fc7\u7a0b","title":"\u51c6\u9759\u6001\u8fc7\u7a0b","text":"<p>\u72b6\u6001\u53d8\u5316\u65e0\u9650\u7f13\u6162,\u5f1b\u8c6b\u65f6\u95f4\u5f88\u77ed\u800c\u65e0\u9650\u63a5\u8fd1\u5e73\u8861\u72b6\u6001\u7684\u8fc7\u7a0b</p> <p>\u51c6\u9759\u6001\u6761\u4ef6:\u6c14\u4f53\u5de5\u8d28\u4e0e\u5916\u754c\u4e4b\u95f4\u7684\u6e29\u5dee/\u538b\u529b\u5dee\u4e3a\u65e0\u9650\u5c0f,\u82e5\u8fd8\u5b58\u5728\u5176\u4ed6\u4e0d\u5e73\u8861\u52bf\u5219\u5fc5\u987b\u52a0\u4e0a\u76f8\u5e94\u7684\u65e0\u9650\u5c0f\u6761\u4ef6,\u53ea\u6709\u51c6\u9759\u6001\u8fc7\u7a0b\u5728\u5750\u6807\u56fe\u4e2d\u53ef\u7528\u8fde\u7eed\u66f2\u7ebf\u6765\u8868\u793a</p> \\[     \u5373: p \\rightarrow p_{out} + \\frac{F}{A}, T \\rightarrow T_{out} \\]"},{"location":"energy/Engineering_Thermodynamics/#\u53ef\u9006\u8fc7\u7a0b-\u51c6\u9759\u6001--\u65e0\u8017\u6563","title":"\u53ef\u9006\u8fc7\u7a0b: \u51c6\u9759\u6001 + \u65e0\u8017\u6563:","text":"<p>\u51c6\u9759\u6001\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4f1a\u53d1\u751f\u80fd\u91cf\u7684\u8017\u6563(\u6bd4\u5982\u5916\u90e8\u673a\u68b0\u5668\u4ef6\u4e4b\u95f4\u6469\u64e6\u529b\u7684\u4f5c\u7528),\u800c\u53ef\u9006\u8fc7\u7a0b\u7740\u773c\u4e8e\u5de5\u8d28\u4e0e\u5916\u754c\u4f5c\u7528\u4ea7\u751f\u7684\u603b\u6548\u679c,\u4e0d\u4ec5\u8981\u6c42\u5de5\u8d28\u5185\u90e8\u662f\u5e73\u8861\u7684,\u800c\u4e14\u5de5\u8d28\u4e0e\u5916\u754c\u53ef\u4ee5\u65e0\u6761\u4ef6\u7684\u9006\u590d,\u8be5\u7406\u60f3\u8fc7\u7a0b\u4e2d\u4e0d\u5b58\u5728\u4efb\u4f55\u8017\u6563(\u5373\u8fd4\u56de\u539f\u6765\u72b6\u6001\u5e76\u4e14\u5728\u5916\u754c\u4e0d\u7559\u4e0b\u4efb\u4f55\u53d8\u5316)</p> <p>\u8fc7\u7a0b\u70ed\u91cf\u7684\u6982\u5ff5</p> <p>\"\u70ed\u91cf\"[\\(J\\)]\u4e00\u8bcd\u4e3a\u70ed\u529b\u7cfb\u4e0e\u5916\u754c\u4e4b\u95f4\u4ec5\u4ec5\u7531\u4e8e\u6e29\u5ea6\u4e0d\u540c\u800c\u901a\u8fc7\u8fb9\u754c\u4f20\u9012\u7684\u80fd\u91cf,\u662f\u7528\u6765\u5ea6\u91cf\u4f20\u9012\u80fd\u91cf\u591a\u5c11\u7684\u8fc7\u7a0b\u91cf,\u4e0e\u72b6\u6001\u53c2\u6570(\u53ea\u53d6\u51b3\u4e8e\u521d\u3001\u7ec8\u6001)\u4e0d\u540c,\u8fc7\u7a0b\u91cf\u4e0e\u8fdb\u884c\u7684\u8def\u5f84\u6709\u5173,\u4e0d\u80fd\u8868\u793a\u4e3a\u72b6\u6001\u53c2\u6570\u7684\u51fd\u6570</p> <p>\u5728\u53ef\u9006\u8fc7\u7a0b\u4e2d,\u70ed\u91cf\u53ef\u8868\u793a\u4e3a:</p> \\[     \\Delta q = Tds, q_{1-2} = \\int _{1}^{2} Tds \\] <p></p> <p>\u53ef\u9006\u72b6\u6001\u4e0b\u7684\u529f</p> <p> <p>\u529f[\\(J\\)]\u4e0e\u70ed\u91cf\u4e00\u6837\u662f\u80fd\u91cf\u4f20\u9012\u7684\u5ea6\u91cf,\u662f\u8fc7\u7a0b\u91cf,,\u4e0d\u80fd\u8868\u793a\u4e3a\u72b6\u6001\u53c2\u6570\u7684\u51fd\u6570(\\(w \\neq f(p,v)\\)),\u70ed\u529b\u5b66\u89c4\u5b9a\u5bf9\u5916\u505a\u529f\u4e3a\"+\",\u5916\u754c\u5bf9\u7cfb\u7edf\u505a\u529f\u4e3a\"-\"</p> <p>\u5bf9\u4e8e\u53ef\u9006\u8fc7\u7a0b\u4e2d\u7684\u4f53\u79ef\u53d8\u5316\u529f:</p> \\[     \\Delta W = Fdx = pAdx = pdV, W_{1-2} = \\int _{1}^{2} pdV \\] <p> </p> <p>\u6709\u7528\u529f</p> <p> \u95ed\u53e3\u7cfb\u5de5\u8d28\u81a8\u80c0\u6240\u505a\u7684\u529f\u5e76\u4e0d\u5168\u90e8\u6709\u7528\u4e8e\u81a8\u80c0,\u6709\u4e00\u90e8\u5206\u7528\u6765\u6392\u65a5\u5927\u6c14\u548c\u6469\u64e6\u8017\u6563,\u4f59\u4e0b\u7684\u624d\u662f\u6709\u7528\u529f\\(W_{u}\\),\u5373 \\[     W_u = W - p_{0} \\Delta V - W_f \\] <p>\u82e5\u4e3a\u53ef\u9006\u8fc7\u7a0b,\u5219\\(W_f=0\\),\u53ef\u5f97:</p> \\[     W_u = \\int _{1}^{2} pdV - p_{0} \\Delta V  \\] <p></p> <p>1.\u4e3a\u4ec0\u4e48\u81ea\u7531\u81a8\u80c0\u662f\u4e0d\u53ef\u9006\u8fc7\u7a0b\uff1f</p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u4e8c\u7ae0-\u80fd\u91cf\u4e0e\u70ed\u529b\u5b66\u7b2c\u4e00\u5b9a\u5f8b","title":"\u7b2c\u4e8c\u7ae0 \u80fd\u91cf\u4e0e\u70ed\u529b\u5b66\u7b2c\u4e00\u5b9a\u5f8b","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u70ed\u529b\u7cfb\u7b2c\u4e00\u5b9a\u5f8b,\u70ed\u529b\u5b66\u80fd\\(U\\),\u7113\\(H\\)\u7684\u5b9a\u4e49,\u63a8\u8fdb\u529f\u548c\u6d41\u52a8\u529f,\u5f00\u53e3\u7cfb\u80fd\u91cf\u65b9\u7a0b,\u7a33\u5b9a\u6d41\u52a8\u80fd\u91cf\u65b9\u7a0b</p> <p>\ud83d\udd27\u8ba1\u7b97:\u5f00\u53e3\u7cfb\u80fd\u91cf\u65b9\u7a0b,\u7a33\u5b9a\u6d41\u52a8\u80fd\u91cf\u65b9\u7a0b \uff08\u4e0d\u6d89\u53ca\u4e00\u822c\u5f00\u53e3\u7cfb\u516c\u5f0f\uff09</p> <p></p> <p>\u63a8\u8fdb\u529f\u548c\u6d41\u52a8\u529f</p> <p> \u63a8\u8fdb\u529f\u5dee\\(p_2 v_2 - p_1 v_1 = \\Delta (pv)\\)\u662f\u7ef4\u7cfb\u5de5\u8d28\u6d41\u52a8\u6240\u9700\u7684\u529f,\u79f0\u4e3a\u6d41\u52a8\u529f <p>\u6d41\u52a8\u529f\u53ef\u89c6\u4e3a\u6d41\u52a8\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u4e0e\u5916\u754c\u7531\u4e8e\u7269\u8d28\u8fdb\u51fa\u800c\u4f20\u9012\u7684\u673a\u68b0\u529f </p> <p></p> <p>\u7113\u7684\u5b9a\u4e49</p> <p> \u7113[\\(J\\)]\u662f\u7cfb\u7edf\u4e2d\u56e0\u5f15\u5165/\u6392\u9664\u5de5\u8d28\u8fd8\u6539\u53d8\u7684\u603b\u80fd\u91cf(\u70ed\u529b\u5b66\u80fd+\u63a8\u8fdb\u529f),\u5373\\(H=U+pV\\) <p>\u663e\u7136,\u7113\u662f\u4e00\u4e2a\u72b6\u6001\u53c2\u6570,\u6709\\(\\Delta h_{1-2} = \\int_{1}^{2} dh = h_2 - h_1\\), \\(\\oint dh = 0\\) </p>"},{"location":"energy/Engineering_Thermodynamics/#\u70ed\u529b\u5b66\u7b2c\u4e00\u5b9a\u5f8b\u7684\u8868\u8fbe\u5f0f","title":"\u70ed\u529b\u5b66\u7b2c\u4e00\u5b9a\u5f8b\u7684\u8868\u8fbe\u5f0f","text":"<p>\u6839\u636e\u70ed\u529b\u5b66\u7b2c\u4e00\u5b9a\u5f8b\u7684\u539f\u5219,\u7cfb\u7edf\u4e2d\u50a8\u5b58\u80fd\u91cf\u7684\u589e\u52a0 = \u8fdb\u5165\u7cfb\u7edf\u7684\u80fd\u91cf - \u79bb\u5f00\u7cfb\u7edf\u7684\u80fd\u91cf,\u5bf9\u4e00\u4e2a\u5fae\u5143\u8fc7\u7a0b,\u6709:</p> \\[     \\delta Q = dU + \\delta W \\] <p>\u5bf9\u4e8e\u53ef\u9006\u8fc7\u7a0b,\u6709:</p> \\[     \\delta Q = dU + pdV, Q = \\Delta U + \\int_{1}^{2}pdV \\] <p>\u5bf9\u4e8e\u5faa\u73af,\\(\\oint dU = 0\\),\u6709:</p> \\[     \\oint \\delta Q = \\oint dU + \\oint \\delta W = \\oint \\delta W , \u5373Q_{net} = W_{net} \\] <p>\u53ef\u77e5\u5728\u5faa\u73af\u4e2d,\u4ea4\u6362\u7684\u51c0\u70ed\u91cf\\(Q_{net}\\)\u7b49\u4e8e\u51c0\u529f\u91cf\\(W_{net}\\)</p>"},{"location":"energy/Engineering_Thermodynamics/#\u5f00\u53e3\u7cfb\u80fd\u91cf\u65b9\u7a0b","title":"\u5f00\u53e3\u7cfb\u80fd\u91cf\u65b9\u7a0b","text":"<p>\u5982\u4e0b\u56fe\u7684\u5f00\u53e3\u7cfb\u573a\u666f,\u5728\\(d \\tau\\)\u65f6\u95f4\u5185\u7684\u5fae\u5143\u8fc7\u7a0b,\\(\\delta m_1,dV_1 \\rightarrow \\delta m_2,dV_2\\),\u7cfb\u7edf\u4ece\u5916\u754c\u63a5\u6536\u70ed\u91cf\\(\\delta Q\\),\u5de5\u8d28\u5bf9\u673a\u5668\u8bbe\u5907\u505a\u529f\\(\\delta W_i\\)(\u5185\u90e8\u529f),\u7cfb\u7edf\u603b\u80fd\u91cf\u589e\u52a0\\(dE_{cv}\\) </p> \\[     \\begin{aligned}     \\delta Q &amp;= dE_{cv} + (dE_2 + p_2 d V_2) - (dE_1 + p_1 d V_1) + \\delta W_i   \\\\     &amp;\\Rightarrow  dE_{cv} + (\\frac{1}{2}c_{2}^{2} + gz_2) \\delta m_2 - (\\frac{1}{2}c_{1}^{2} + gz_1)\\delta m_1 + \\delta W_i    \\;   (*)  \\\\     \\end{aligned} \\]"},{"location":"energy/Engineering_Thermodynamics/#\u7a33\u5b9a\u6d41\u52a8\u80fd\u91cf\u65b9\u7a0b","title":"\u7a33\u5b9a\u6d41\u52a8\u80fd\u91cf\u65b9\u7a0b","text":"<p>\u5bf9\u4e8e\u7a33\u5b9a\u6d41\u52a8,\u70ed\u529b\u7cfb\u4efb\u4f55\u622a\u9762\u4e0a\u5de5\u8d28\u7684\u6240\u6709\u53c2\u6570\u90fd\u4e0d\u968f\u65f6\u95f4\u6539\u53d8,\u56e0\u6b64\u5fc5\u8981\u6761\u4ef6\u4e3a\\(\\frac{dE_{cv}}{d \\tau} = 0\\), \u56e0\u6b64\u4e0a\u8ff0(*)\u5f0f\u53d8\u4e3a:</p> \\[     \\begin{aligned}     \\delta Q &amp;= dH + \\frac{1}{2} m dc^2 + mgdz + \\delta W_i  \\\\     \\delta q &amp;= dh + \\frac{1}{2} dc^2 + gdz + \\delta w_i  \\\\     \\end{aligned} \\] <p>\u5176\u4e2d\\(\\delta w_i\\)\u4ee3\u88681kg\u5de5\u8d28\u8fdb\u5165\u7cfb\u7edf\u540e\u5728\u673a\u5668\u5185\u90e8\u505a\u7684\u529f</p> <p>\u5b9e\u9645\u4e0a\u5c06\u7b49\u5f0f\u540e\u4e09\u9879\u7684\u673a\u68b0\u529f\u79f0\u4e3a\u6280\u672f\u529f\\(W_t = \\frac{1}{2} m dc^2 + mgdz + \\delta W_i\\),\u56e0\u6b64:</p> \\[     \\begin{aligned}     Q &amp;= \\Delta H + W_t  \\\\     \\delta Q &amp;= dH + \\delta W_t  \\\\     \\delta q &amp;= dh + \\delta w_t  \\\\     \\end{aligned} \\] <p>\u5bf9\u4e8e\u4e0a\u8ff0\u516c\u5f0f\u8fd8\u53ef\u4ee5\u7ee7\u7eed\u53d8\u5f62,\u5f15\u5165\u5bb9\u79ef\u53d8\u5316\u529f:</p> \\[     \\begin{aligned}     W_{\\text{\u5bb9\u79ef\u53d8\u5316\u529f}} &amp;= q - \\Delta u =  \\frac{1}{2} \\Delta c^2 + g \\Delta z + \\Delta (pv)  \\\\     &amp;\\Rightarrow W_t + \\Delta (pv)         \\\\      W_t &amp;= W_{\\text{\u5bb9\u79ef\u53d8\u5316\u529f}} - \\Delta (pv)  \\\\     \\text{\u5bf9\u4e8e\u53ef\u9006\u8fc7\u7a0b:} W_t &amp;= \\int_{1}^{2}pdV - \\Delta (pv)  \\\\     &amp;= \\int_{1}^{2}pdV - \\int_{1}^{2}d(pv) = -\\int_{1}^{2}vdp  \\\\      \u5373\u4e3a \\delta w_t &amp;= -vdp  \\\\      \\delta q &amp;= dh + \\delta w_t = dh -vdp  \\\\      \\delta Q &amp;= dH -Vdp  \\\\     \\end{aligned} \\] <p>1.\u4f53\u79ef\u53d8\u5316\u529f\u3001\u6280\u672f\u529f\u3001\u8f74\u529f\u7684\u5173\u7cfb\uff1f</p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u4e09\u7ae0-\u71b5\u4e0e\u70ed\u529b\u5b66\u7b2c\u4e8c\u5b9a\u5f8b","title":"\u7b2c\u4e09\u7ae0 \u71b5\u4e0e\u70ed\u529b\u5b66\u7b2c\u4e8c\u5b9a\u5f8b","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u70ed\u529b\u5b66\u7b2c\u4e8c\u5b9a\u5f8b\u7684\u4e24\u79cd\u8868\u8ff0,\u5361\u8bfa\u5faa\u73af,\u72b6\u6001\u53c2\u6570\u71b5</p> <p>\ud83d\udd27\u8ba1\u7b97:\u5361\u8bfa\u5b9a\u7406,\u71b5\u65b9\u7a0b</p> <p></p> <p>\u70ed\u529b\u5b66\u7b2c\u4e8c\u5b9a\u5f8b</p> <p> <ul> <li>\u514b\u52b3\u4fee\u65af\u8868\u8ff0(\u70ed\u91cf\u4f20\u9012\u89d2\u5ea6):\u4e0d\u53ef\u80fd\u5c06\u70ed\u4ece\u4f4e\u6e29\u7269\u4f53\u4f20\u81f3\u9ad8\u6e29\u7269\u4f53\u800c\u4e0d\u5f15\u8d77\u5176\u5b83\u53d8\u5316\u3002</li> </ul> <p>\u5f3a\u8c03\"\u81ea\u53d1\u5730,\u4e0d\u4ed8\u4ee3\u4ef7\u5730\"</p> <ul> <li>\u5f00\u5c14\u6587\u8868\u8ff0(\u70ed\u529f\u8f6c\u6362\u89d2\u5ea6):\u4e0d\u53ef\u80fd\u4ece\u5355\u4e00\u70ed\u6e90\u53d6\u70ed\uff0c\u5e76\u4f7f\u4e4b\u5b8c\u5168\u8f6c\u53d8\u4e3a\u6709\u7528\u529f(\u5168\u90e8\u5bf9\u5916\u505a\u529f)\u800c\u4e0d\u4ea7\u751f\u5176\u5b83\u5f71\u54cd\u3002(\u4e0d\u53ef\u80fd\u5236\u9020\u4e00\u53f0\u673a\u5668,\u5728\u5faa\u73af\u52a8\u4f5c\u4e2d\u628a\u4e00\u91cd\u7269\u5347\u9ad8\u7684\u540c\u65f6\u4f7f\u70ed\u6e90\u51b7\u5374),\\(Q_{\u70ed\u6e90\u653e\u70ed} &gt; \\Delta W\\)</li> </ul> <p>\u4e0d\u53ef\u80fd\u5c06\u4ece\u70ed\u6e90\u53d6\u5f97\u7684\u70ed\u5168\u90e8\u8f6c\u6362\u4e3a\u529f,\u4e0d\u53ef\u907f\u514d\u5730\u5c06\u4e00\u90e8\u5206\u4f20\u9012\u7ed9\u6e29\u5ea6\u66f4\u4f4e\u7684\u4f4e\u6e29\u70ed\u6e90</p> <p>\u975e\u81ea\u53d1\u8fc7\u7a0b(\u70ed\u8f6c\u4e3a\u529f/\u4f4e\u6e29\u5411\u9ad8\u6e29\u4f20\u70ed)\u7684\u5b9e\u73b0\u5fc5\u987b\u6709\u4e00\u4e2a\u81ea\u53d1\u53cd\u5e94(\u9ad8\u6e29\u5411\u4f4e\u6e29\u4f20\u70ed/\u673a\u68b0\u80fd\u8f6c\u53d8\u4e3a\u70ed\u80fd)\u4f5c\u4e3a\u8865\u5145\u6761\u4ef6</p> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u5361\u8bfa\u5b9a\u7406","title":"\u5361\u8bfa\u5b9a\u7406","text":"<p>\u5361\u8bfa\u53d1\u73b0\u70ed\u673a\u5faa\u73af\u4e2d\u7684\u4e0d\u53ef\u9006\u56e0\u7d20(\u8017\u6563)\u90fd\u4f1a\u5f15\u8d77\u529f\u635f\u5931,\u56e0\u6b64\u5047\u8bbe\u4e00\u4e2a\u7406\u60f3\u7684\u53ef\u9006\u70ed\u673a,\u5de5\u8d28\u5728\u70ed\u6e90\u76f8\u540c\u6e29\u5ea6\u4e0b\u5b9a\u6e29\u5438\u70ed,\u51b7\u6e90\u76f8\u540c\u6e29\u5ea6\u4e0b\u5b9a\u6e29\u653e\u70ed.</p> <p>\u5361\u8bfa\u5faa\u73af(Carnot Cycle)</p> <p> \u5361\u8bfa\u5faa\u73af\u5728\\(T_1\u4e0eT_2\\)\u4e24\u4e2a\u70ed\u6e90\u4e4b\u95f4\u7684\u6b63\u5411\u5faa\u73af,\u7531\u4e24\u4e2a\u53ef\u9006\u5b9a\u6e29\u8fc7\u7a0b\u4e0e\u4e24\u4e2a\u53ef\u9006\u7edd\u70ed\u8fc7\u7a0b\u7ec4\u6210. <p></p> <p>\u6839\u636e\u5faa\u73af\u7279\u6027,\u5361\u8bfa\u5faa\u73af\u6548\u7387\u4e3a:</p> \\[     \\eta_t = \\frac{w_{net}}{q_1} = \\frac{q_1 - q_2}{q_1} = 1 - \\frac{q_2}{q_1} = 1 - \\frac{T_2 \\left| \\Delta s_{c-d} \\right| }{T_1 \\left| \\Delta s_{a-b} \\right|} = 1 - \\frac{T_2}{T_1} \\] <p>\u5361\u8bfa\u5faa\u73af\u7684\u7279\u70b9</p> <p> <ul> <li> <p>\u5361\u8bfa\u5faa\u73af\u7684\u70ed\u6548\u7387\u4ec5\u4ec5\u53d6\u51b3\u4e8e\u4e24\u4e2a\u70ed\u6e90\u7684\u6e29\u5ea6,\u82e5\u63d0\u9ad8\u9ad8\u6e29\u70ed\u6e90\\(T_1\\),\u964d\u4f4e\u4f4e\u6e29\u70ed\u6e90\\(T_2\\)(\u63d0\u9ad8\u5de5\u4f5c\u70ed\u6e90\u6e29\u5dee),\u5219\u80fd\u591f\u63d0\u9ad8\u5faa\u73af\u70ed\u6548\u7387</p> </li> <li> <p>\u4e0d\u80fd\u5236\u9020\u51fa\u5728\u4e24\u4e2a\u6e29\u5ea6\u4e0d\u540c\u7684\u70ed\u6e90\u95f4\u5de5\u4f5c\u7684\u70ed\u673a\uff0c\u800c\u4f7f\u5176\u6548\u7387\u8d85\u8fc7\u540c\u6837\u70ed\u6e90\u89c1\u5de5\u4f5c\u7684\u53ef\u9006\u70ed\u673a.(\u53ef\u9006\u70ed\u673a\u6548\u7387\u6700\u9ad8)</p> </li> <li> <p>\u5728\u4e24\u4e2a\u56fa\u5b9a\u70ed\u6e90\u4e4b\u95f4\u5de5\u4f5c\u7684\u4e00\u5207\u53ef\u9006\u70ed\u673a\u5177\u6709\u76f8\u540c\u7684\u6548\u7387\\(\\eta_t =1 - \\frac{T_2}{T_1}\\)\u3002</p> </li> <li> <p>\u82e5\\(T_1 = T_2\\),\u5219\\(\\eta_t = 0\\),\u8bf4\u660e\u70ed\u80fd\u4ea7\u751f\u52a8\u529b\u4e00\u5b9a\u8981\u6e29\u5dee\u4f5c\u4e3a\u70ed\u529b\u5b66\u6761\u4ef6,\u6240\u4ee5\u5355\u4e00\u70ed\u6e90\u8fde\u7eed\u505a\u529f\u7684\u673a\u5668(\u7b2c\u4e8c\u7c7b\u6c38\u52a8\u673a)\u4e0d\u5b58\u5728 </p> </li> </ul> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u5236\u51b7\u70ed\u6cf5\u5faa\u73af","title":"\u5236\u51b7\u70ed\u6cf5\u5faa\u73af","text":"<p>\u9006\u5361\u8bfa\u5faa\u73af</p> <p> \u5bf9\u4e8e\u5236\u51b7\u673a/\u70ed\u6cf5:\u4ece\u51b7\u6e90\u5438\u70ed\\(Q_2\\),\u5411\u70ed\u6e90\u653e\u70ed\\(Q_1\\),\u6027\u80fd\u7cfb\u6570\\([COP]\\)(\u5236\u51b7\u7cfb\u6570/\u4f9b\u6696\u7cfb\u6570) \\[     \\begin{aligned}     \\text{\u8017\u529f\u91cf:} W = Q_1 - Q_2 \\\\     \\end{aligned} \\] <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u71b5","title":"\u71b5","text":"<p>\u514b\u52b3\u4fee\u65af\u4e0d\u7b49\u5f0f</p> <p> \u5047\u8bbe\u4e00\u4e2a\u60c5\u666f,\u67d0\u95ed\u53e3\u7cfb\u7edf\u5728\u67d0\u8fc7\u7a0b\u4e2d\u6709\u70ed\\(\\delta Q\\)\u4e0e\u529f\\(\\delta W\\)\u7a7f\u8fc7\u8fb9\u754c,\u53ef\u9006\u673a\u4ece\u6e29\u5ea6\u4e3a\\(T_0\\)\u7684\u6052\u6e29\u70ed\u6e90\u5f97\u5230\\(\\delta Q_R\\),\u540c\u65f6\u5b8c\u6210\u529f\u91cf\\(\\delta W_R\\),\u6700\u7ec8\u5c06\u70ed\\(\\delta Q\\)\u4f20\u9012\u7ed9\u4efb\u610f\u6e29\u5ea6\u4e3a\\(T\\)\u7684\u67d0\u4e00\u7cfb\u7edf.\u5b8c\u6210\u7684\u603b\u529f\u91cf\u4e3a\\(\\delta W_T = \\delta W_R + \\delta W\\)\u53ef\u77e5: \\[     \\begin{aligned}      \\delta W_R &amp;= \\delta Q_R - \\delta Q   \\\\     \\delta W &amp;= \\delta Q - dU       \\\\     \\text{\u7531\u4e8e\u53ef\u9006\u673a,}\\frac{T_0}{T} &amp;=\\frac{\\delta Q_R}{\\delta Q}  \\\\     \\Rightarrow  \\delta W_T &amp;= \\frac{T_0}{T} \\delta Q - dU  \\\\     \\text{\u5bf9\u5c01\u95ed\u5faa\u73af,}  \\oint \\delta W_T &amp;= T_0 \\oint \\frac{\\delta Q}{T} - \\oint dU  \\\\     \\text{\u5728\u5faa\u73af\u4e2d,\u5355\u4e00\u70ed\u6e90\u4e0d\u53ef\u80fd\u8f93\u51fa\u6709\u7528\u529f:} \\oint \\delta W_T &amp;= T_0 \\oint \\frac{\\delta Q}{T} \\le 0  \\\\     \\text{\u53ef\u5f97}\\oint \\frac{\\delta Q}{T} &amp;\\le 0     \\end{aligned}    \\] <p></p> <p>\u6b64\u4e0d\u7b49\u5f0f\u8868\u660e\uff1a\u6240\u6709\u53ef\u9006\u5faa\u73af\u7684\u514b\u52b3\u4fee\u65af\u79ef\u5206\u503c\\(\\oint \\frac{\\delta Q}{T} = 0\\)\uff0c\u6240\u6709\u4e0d\u53ef\u9006\u5faa\u73af\u7684\u514b\u52b3\u4fee\u65af\u79ef\u5206\u503c\\(\\oint \\frac{\\delta Q}{T} &lt; 0\\)\u3002\u6545\u672c\u4e0d\u7b49\u5f0f\u53ef\u4f5c\u4e3a\u5224\u65ad\u4e00\u5207\u4efb\u610f\u5faa\u73af\u662f\u5426\u53ef\u9006\u7684\u4f9d\u636e\u3002\u5e94\u7528\u514b\u52b3\u4fee\u65af\u4e0d\u7b49\u5f0f\u8fd8\u53ef\u63a8\u51fa\u5982\u4e0b\u7684\u91cd\u8981\u7ed3\u8bba:\u4efb\u4f55\u7cfb\u7edf\u6216\u5de5\u8d28\u7ecf\u5386\u4e00\u4e2a\u4e0d\u53ef\u9006\u7684\u7edd\u70ed\u8fc7\u7a0b\u4e4b\u540e\uff0c\u5176\u71b5\u503c\u5fc5\u5c06\u6709\u6240\u589e\u5927\u3002</p> <p></p> <p>\u6839\u636e\u514b\u52b3\u4fee\u65af\u4e0d\u7b49\u5f0f,\u8bbe\\(dS = \\frac{\\delta Q}{T}\\),\u79f0S\u4e3a\u71b5(Entropy)[\\(kJ/K\\)],\u662f\u4e00\u79cd\u5c3a\u5ea6\u91cf,\u5177\u6709\u53ef\u52a0\u6027</p> <p>\u5bf9\u4e8e\u71b5\u7684\u8ba1\u7b97,\u53ea\u80fd\u6309\u7167\u53ef\u9006\u8def\u5f84\u6765\u8fdb\u884c,\u6709:</p> \\[     \\begin{aligned}     \\oint dS = \\oint \\frac{\\delta Q}{T} &amp;= 0,        \\\\     \\Delta S = S_2 - S_1 &amp;= \\int_{1}^{2} \\frac{\\delta Q}{T} \\\\     \\end{aligned}    \\] <p>\u5bf9\u4e8e\u4e0d\u53ef\u9006\u8fc7\u7a0b\u4e2d\u7684\u71b5,\u5219</p> \\[     \\begin{aligned}     dS &amp;&gt; \\frac{\\delta Q}{T},                           \\\\     \\int_{1}^{2}dS &amp;= S_2 - S_1 &gt; \\int_{1}^{2} \\frac{\\delta Q}{T}        \\\\     \\end{aligned}    \\] <p>\u4e0d\u53ef\u9006\u8fc7\u7a0b\u71b5\u7684\u53d8\u5316\u53ef\u4ee5\u9009\u62e9\u76f8\u540c\u7684\u521d\u7ec8\u6001\u95f4\u7684\u4efb\u610f\u7684\u53ef\u9006\u8fc7\u7a0b\u6765\u8ba1\u7b97</p>"},{"location":"energy/Engineering_Thermodynamics/#\u5b64\u7acb\u4f53\u7cfbisolated-system\u71b5\u589e\u539f\u7406","title":"\u5b64\u7acb\u4f53\u7cfb(isolated system)\u71b5\u589e\u539f\u7406","text":"<p>\u7531\u4e8e\u5728\u4e0d\u53ef\u9006\u8fc7\u7a0b\u4e2d\\(dS &gt; \\frac{\\delta Q}{T}\\),\u5f15\u5165\\(\\delta S_g = dS - \\frac{\\delta Q}{T}\\)\u6765\u8868\u793a\u4e24\u8005\u7684\u5dee\u503c,\u5219:</p> \\[     \\begin{aligned}     dS = \\delta S_g + \\frac{\\delta Q}{T},                                \\begin{equation}             \\delta S_g =             \\begin{cases}             = 0 (\u53ef\u9006\u8fc7\u7a0b)  \\\\             &gt; 0 (\u4e0d\u53ef\u9006\u8fc7\u7a0b)   \\\\             \\end{cases}     \\end{equation}     \\end{aligned}    \\] <p>\u5b9a\u4e49\\(\\delta S_g\\)\u4e3a\u71b5\u4ea7\u6765\u5ea6\u91cf\u4e0d\u53ef\u9006\u56e0\u7d20\u7684\u5b58\u5728\u800c\u5f15\u8d77\u7684\u71b5\u7684\u589e\u52a0,\\(d S_f = \\frac{\\delta Q}{T}\\)\u4e3a\u7531\u4e8e\u4e0e\u5916\u754c\u53d1\u751f\u70ed\u4ea4\u6362\uff0c\u7531\u70ed\u6d41\u5f15\u8d77\u7684\u71b5\u6d41</p> <p>\u5bf9\u4e8e\u5b64\u7acb\u7cfb,\\(\\delta Q = 0, \\delta m = 0, \\Rightarrow dS_f = 0\\),\u56e0\u6b64:</p> \\[     \\begin{aligned}     \\begin{equation}             dS_{iso} = \\delta S_g =             \\begin{cases}             = 0 (\u53ef\u9006\u8fc7\u7a0b)  \\\\             &gt; 0 (\u4e0d\u53ef\u9006\u8fc7\u7a0b)   \\\\             \\end{cases}     \\end{equation}     \\end{aligned} \\] <p>\u4e0a\u8ff0\u63a8\u5bfc\u8bf4\u660e,\u5728\u5b64\u7acb\u7cfb\u5185\uff0c\u4e00\u5207\u5b9e\u9645\u8fc7\u7a0b(\u4e0d\u53ef\u9006\u8fc7\u7a0b)\u90fd\u671d\u7740\u4f7f\u7cfb\u7edf\u71b5\u589e\u52a0\u7684\u65b9\u5411\u8fdb\u884c\uff0c\u6216\u5728\u6781\u9650\u60c5\u51b5\u4e0b(\u53ef\u9006\u8fc7\u7a0b)\u7ef4\u6301\u7cfb\u7edf\u7684\u71b5\u4e0d\u53d8\uff0c\u800c\u4efb\u4f55\u4f7f\u7cfb\u7edf\u71b5\u51cf\u5c11\u7684\u8fc7\u7a0b\u662f\u4e0d\u53ef\u80fd\u53d1\u751f\u7684.</p> <p>\u5b64\u7acb\u4f53\u7cfb\u71b5\u589e\u539f\u7406</p> <p> <ul> <li>\u5982\u679c\u67d0\u8fc7\u7a0b\u8fdb\u884c\u7684\u5168\u90e8\u7ed3\u679c\u662f\u4f7f\u5b64\u7acb\u7cfb\u7684\u603b\u71b5\u589e\u52a0\uff0c\u5b83\u5c31\u53ef\u4ee5\u5355\u72ec\u8fdb\u884c\u800c\u4e0d\u9700\u8981\u8865\u5145\u6761\u4ef6\uff0c\u4e5f\u5c31\u662f\u8bf4\u53ef\u4ee5\u81ea\u53d1\u5730\u8fdb\u884c.</li> <li>\u5982\u679c\u67d0\u8fc7\u7a0b\u8fdb\u884c\u7684\u7ed3\u679c\u5c06\u4f7f\u5b64\u7acb\u7cfb\u603b\u71b5\u51cf\u5c11\uff0c\u5b83\u5fc5\u4e0d\u53ef\u80fd\u5355\u72ec\u8fdb\u884c\u3002\u8981\u4f7f\u8fd9\u79cd\u8fc7\u7a0b\u6210\u4e3a\u53ef\u80fd\uff0c\u5fc5\u987b\u4f34\u968f\u8fdb\u884c\u4e00\u79cd\u71b5\u589e\u52a0\u7684\u8fc7\u7a0b\uff0c\u4f7f\u5f97\u4e24\u8fc7\u7a0b\u76f8\u4f34\u8fdb\u884c\u7684\u7ed3\u679c\uff0c\u5b64\u7acb\u7cfb\u7684\u603b\u71b5\u589e\u5927\uff0c\u6216\u81f3\u5c11\u7ef4\u6301\u4e0d\u53d8.</li> <li>\u4e0d\u53ef\u9006\u8fc7\u7a0b\u8fdb\u884c\u7684\u7ed3\u679c\u4f7f\u7cfb\u7edf\u7684\u71b5\u589e\u52a0\uff0c\u540c\u65f6\u4f7f\u5176\u4f5c\u529f\u80fd\u529b\u4e0b\u964d\uff0c\u800c\u4f7f\u80fd\u91cf\u8f6c\u53d8\u4e3a\u8f83\u4e3a\u65e0\u7528\u7684\u5f62\u5f0f\u3002</li> <li>\u80fd\u91cf\u5728\u6570\u91cf\u4e0a\u5e76\u672a\u53d8\u5316\uff0c\u800c\u4f5c\u529f\u80fd\u529b\u51cf\u5c11\u7684\u73b0\u8c61\u79f0\u4e3a\u80fd\u91cf\u8d2c\u503c\u3002\u5b64\u7acb\u4f53\u7cfb\u7684\u71b5\u589e\u610f\u5473\u7740\u80fd\u91cf\u7684  \u8d2c\u503c\uff0c\u6240\u4ee5\u5b64\u7acb\u4f53\u7cfb\u71b5\u589e\u539f\u7406\u53c8\u79f0\u4e3a\u80fd\u91cf\u8d2c\u503c\u539f\u7406 </li> </ul>"},{"location":"energy/Engineering_Thermodynamics/#\u71b5\u65b9\u7a0b","title":"\u71b5\u65b9\u7a0b","text":""},{"location":"energy/Engineering_Thermodynamics/#\u95ed\u53e3\u7cfb\u7edf\u71b5\u65b9\u7a0b","title":"\u95ed\u53e3\u7cfb\u7edf\u71b5\u65b9\u7a0b","text":"<p>\u5728\u95ed\u53e3\u7cfb\u4e2d,\\(dS_f\\)\u4e3a\u71b5\u6d41, \u662f\u6362\u70ed\u91cf\u4e0e\u70ed\u6e90\u6e29\u5ea6\u7684\u6bd4\u503c,\u8868\u660e\u5916\u754c\u6362\u70ed\u5f15\u8d77\u7684\u7cfb\u7edf\u71b5\u53d8(\u5438\u70ed\u4e3a\"+\",\u653e\u70ed\u4e3a\"-\",\u7edd\u70ed\u4e3a0).</p> <p>\\(\\delta S_g\\)\u4e3a\u71b5\u4ea7,\u662f\u4e0d\u53ef\u9006\u56e0\u7d20\u9020\u6210\u7684\u7cfb\u7edf\u71b5\u589e\u52a0,\u4ec5\u53ef\u80fd\u5927\u4e8e\u7b49\u4e8e0</p> \\[     \\begin{aligned}     dS = \\delta S_g + \\frac{\\delta Q}{T} = \\delta S_g + dS_f ,                                \\end{aligned}    \\] <p>\u5728\u95ed\u53e3\u7edd\u70ed\u7cfb\u4e2d,\u5219\u6709\\(dS_f = 0\\):</p> \\[     \\begin{aligned}     dS = \\delta S_g \\geq 0                          \\end{aligned}    \\] <p>\u53ef\u77e5,\u5728\u95ed\u53e3\u4e0d\u53ef\u9006\u7edd\u70ed\u7cfb\u4e2d,\u7531\u4e8e\u8fc7\u7a0b\u4e2d\u4ecd\u7136\u5b58\u5728\u4e0d\u53ef\u9006\u56e0\u7d20\u800c\u4ea7\u751f\u4e0d\u53ef\u907f\u514d\u7684\u8017\u6563,\u4f7f\u673a\u68b0\u529f\u8f6c\u5316\u4e3a\u70ed\u80fd\u88ab\u5de5\u8d28\u5438\u6536,\u5bf9\u71b5\u53d8\u505a\u51fa\u8d21\u732e.</p>"},{"location":"energy/Engineering_Thermodynamics/#\u5f00\u53e3\u7cfb\u7edf\u71b5\u65b9\u7a0b","title":"\u5f00\u53e3\u7cfb\u7edf\u71b5\u65b9\u7a0b","text":"<p>\u5728\u5f00\u53e3\u7cfb\u4e2d,\u7cfb\u7edf\u4e0e\u5916\u754c\u4ea4\u6362\u8d28\u91cf\u5c06\u5f15\u8d77\u7cfb\u7edf\u71b5\u7684\u6539\u53d8,\u56e0\u4e3a\u7269\u8d28\u8fc1\u79fb\u800c\u5f15\u8d77\u71b5\u53d8\u7684\u71b5\u6d41\u4e3a\u8d28\u71b5\u6d41,\u5b9a\u4e49\u4e3a\\(\\delta S_m = d(ms)\\)</p> \\[     \\begin{aligned}     \\begin{equation}     dS_{C.V.} = \\delta S_{g.C.V} + \\delta S_f + \\delta S_m = \\delta S_{g} + \\frac{\\delta Q}{T} + d(ms) =          \\begin{cases}             = \\delta S_{g} + \\frac{\\delta Q}{T} + mds =0 (\u7a33\u5b9a\u6d41\u52a8\u8fc7\u7a0b)\\\\             = \\frac{\\delta Q}{T} + mds =0 (\u53ef\u9006\u7a33\u5b9a\u6d41\u52a8\u8fc7\u7a0b)\\\\             \\Rightarrow S_1 = S_2 (\u53ef\u9006\u7edd\u70ed\u7a33\u5b9a\u6d41\u52a8\u8fc7\u7a0b) \\\\         \\end{cases}       \\end{equation}     \\end{aligned}    \\] <p>1.\u514b\u52b3\u4fee\u65af\u79ef\u5206\u5f0f\u7684\u8fd0\u7528\uff1f 2.\u5b64\u7acb\u4f53\u7cfb\u71b5\u589e\u7684\u8fd0\u7528\uff1f 3.\u5361\u8bfa\u5faa\u73af\u2192\u6709\u7528\u529f</p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u56db\u7ae0-\u70ed\u529b\u5b66\u4e00\u822c\u5173\u7cfb","title":"\u7b2c\u56db\u7ae0 \u70ed\u529b\u5b66\u4e00\u822c\u5173\u7cfb","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u7b80\u5355\u53ef\u538b\u7f29\u7cfb\u7edf\u7684\u4e94\u4e2a\u57fa\u672c\u72b6\u6001\u53c2\u6570(p,v,T,u,s), \u4e09\u4e2a\u53ef\u6d4b\u53c2\u6570\u7684\u72b6\u6001\u65b9\u7a0b\\(F(p, v, T)=0\\), \u70ed\u529b\u5b66\u4e00\u822c\u5173\u7cfb,\u7ec4\u5408\u72b6\u6001\u53c2\u6570(h, f, g)</p> <p>\ud83d\udd27\u8ba1\u7b97:</p> <p></p> <p>\u7b80\u5355\u53ef\u538b\u7f29\u7cfb\u7edf\u7684\u7279\u70b9:\u5b58\u5728\u4e24\u4e2a\u72ec\u7acb\u7684\u72b6\u6001\u53c2\u6570,\u5176\u72b6\u6001\u51fd\u6570\u4e3a\u4e8c\u5143\u51fd\u6570</p> <p></p> <p>\u6839\u636e\u70ed\u529b\u5b66\u7b2c\u4e00\u5b9a\u5f8b\u4e0e\u7b2c\u4e8c\u5b9a\u5f8b, \u7b80\u5355\u53ef\u538b\u7f29\u5de5\u8d28\u5728\u53ef\u9006\u53d8\u5316\u4e2d\u7684\u80fd\u91cf\u5e73\u8861(\\(F(u, v, s)=0\\)\u7684\u5168\u5fae\u5206\u5f62\u5f0f)\u6709:</p> \\[         \\begin{aligned}         &amp;du = Tds - pdv                   \\\\         \\small \u5373:&amp; \\small\u70ed\u529b\u5b66\u80fd = \u5438\u70ed\u91cf - \u505a\u529f\u91cf         \\\\                                      &amp;dh = Tds + vdp (\u7113h = u + pv) \\\\         &amp;df = -Tds - vdp (\u5f15\u5165\u81ea\u7531\u80fdf = u - Ts)                                    \\\\         &amp;dg = -sdT +vdp  (\u5f15\u5165\u81ea\u7531\u7113g = h - Ts)                                    \\\\         \\end{aligned} \\] <p>\u5373\u901a\u8fc7Legendre\u53d8\u6362,\u53ef\u5f97\\(F(h,s,p)=0\\), \\(F(f,T,v)=0\\), \\(F(g,T,p)=0\\)\u7684\u5168\u5fae\u5206\u8868\u8fbe\u5f0f. \u518d\u5bf9\u4e0a\u8ff0\u7684\u7b49\u5f0f\u505a\u4e00\u9636\u504f\u5fae\u5546,\u6709:</p> \\[         \\begin{aligned}         &amp;( \\frac{\\partial u}{\\partial s} )_v = (\\frac{\\partial h}{\\partial s})_p = T                 \\\\         -&amp;(\\frac{\\partial u}{\\partial v} )_s = -(\\frac{\\partial f}{\\partial v})_T = p                                   \\\\         &amp;( \\frac{\\partial h}{\\partial p} )_s = (\\frac{\\partial g}{\\partial p})_T = v                 \\\\         -&amp;(\\frac{\\partial f}{\\partial T} )_v = -(\\frac{\\partial g}{\\partial T})_p = s                                   \\\\         \\end{aligned} \\] <p>\u7531\u6b64\u53ef\u77e5, \u5bf9\u4e8e\\(F(h,s,p)=0\\), \\(F(f,T,v)=0\\), \\(F(g,T,p)=0\\),\u53ea\u9700\u8981\u77e5\u9053\u4efb\u610f\u4e00\u4e2a\u5173\u7cfb\u5f0f\u5c31\u80fd\u5f97\u5230\u6240\u6709\u7684\u72b6\u6001\u51fd\u6570</p> <p>\u4f8b\u5982:\u5df2\u77e5\\(F(g,T,p) = 0\\),\u4ee5(T,p)\u4e3a\u72ec\u7acb\u53d8\u91cf,\u5c06\\(g(p,T)\\)\u5bf9p\u6c42\u504f\u5bfc\u5f97\u5230\\(v(T,p)\\),\u5bf9T\u6c42\u504f\u5bfc\u5f97\u5230\\(s(T,p)\\),\u800c\\(h(p,T) = g(p,T) + Ts(p,T), u(T,p) = h(p,T) - pv(p,T)\\)\u5373\u53ef\u5f97\u51fa\u5de5\u8d28\u70ed\u529b\u5e73\u8861\u6027\u8d28\u7684\u6240\u6709\u4fe1\u606f,\u8fd9\u4e9b\u70ed\u529b\u72b6\u6001\u51fd\u6570\u79f0\u4e3a\u7279\u6027\u51fd\u6570.</p> <p>\u56e0\u4e3a\u4e8c\u5143\u51fd\u6570\u7684\u4e8c\u9636\u6df7\u5408\u504f\u5fae\u5546\u4e0e\u6c42\u5bfc\u987a\u5e8f\u65e0\u5173,\u5373\\( \\frac{\\partial}{\\partial y} (\\frac{\\partial z}{\\partial x})_{y} = \\frac{\\partial}{\\partial x} (\\frac{\\partial z}{\\partial y})_{x}  \\),\u5219\u53ef\u5f97\u4e0a\u8ff0\u7279\u6027\u51fd\u6570\u7684\u4e8c\u9636\u6df7\u5408\u504f\u5fae\u5546\u5173\u7cfb\u5f0f(maxwell\u5173\u7cfb\u5f0f),\u5982:</p> \\[     \\begin{aligned}     \\frac{\\partial}{\\partial T} (\\frac{\\partial g}{\\partial p})_T &amp;= \\frac{\\partial}{\\partial p} (\\frac{\\partial g}{\\partial T})_p \\\\     (\\frac{\\partial v}{\\partial T})_{p} &amp;= - (\\frac{\\partial s}{\\partial p})_{T}   \\\\     \\frac{\\partial}{\\partial T} (\\frac{\\partial f}{\\partial v})_T &amp;= \\frac{\\partial}{\\partial v} (\\frac{\\partial f}{\\partial T})_v \\\\     (\\frac{\\partial p}{\\partial T})_{v} &amp;= (\\frac{\\partial s}{\\partial v})_{T}   \\\\     \\end{aligned} \\] <p>\u7279\u6027\u51fd\u6570\u7684\u4e8c\u9636\u6df7\u5408\u504f\u5fae\u5546\u5173\u7cfb\u5c06\u4e0d\u53ef\u6d4b\u7684\u71b5\u4e0e\u53ef\u6d4b\u53c2\u6570\u8054\u7cfb\u5728\u4e00\u8d77</p> <p>\u5982\u4e0a\u8ff0\u7684\u67d0\u4e9b\u504f\u5fae\u5546,\u5177\u6709\u660e\u786e\u7684\u7269\u7406\u610f\u4e49,\u5c06\u8fd9\u4e9b\u7279\u6b8a\u7684\u504f\u5fae\u5546\u5b9a\u4e49\u4e3a\u70ed\u7cfb\u6570:</p> <ul> <li>\u5de5\u8d28\u5728\u5b9a\u538b\u6761\u4ef6\u4e0b(\u56e0\u4e3ap\u540c\u6837\u4f1a\u5f71\u54cd\u4f53\u79ef,\u901a\u8fc7\u5b9a\u538b\u6761\u4ef6\u6392\u9664)\u7684\u70ed\u81a8\u80c0/\u4f53\u81a8\u80c0\u7cfb\u6570\u7cfb\u6570\\([K^{-1}]\\): </li> </ul> \\[         \\alpha_{v} = \\frac{1}{v} (\\frac{\\partial v}{\\partial T})_{p} \\] <ul> <li>\u5de5\u8d28\u5728\u7b49\u6e29\u6761\u4ef6\u4e0b\u7684\u7b49\u6e29\u538b\u7f29\u7387\\([Pa^{-1}]\\):</li> </ul> \\[         \\kappa_T  = - \\frac{1}{v} (\\frac{\\partial v}{\\partial p})_T \\] <ul> <li>\u538b\u529b\u7684\u6e29\u5ea6\u7cfb\u6570\\(\\beta\\)</li> </ul> \\[     \\beta = \\frac{1}{p} (\\frac{\\partial p}{\\partial T})_{v}      \\] <p>\u53ef\u5f97:</p> \\[     (\\frac{\\partial p}{\\partial T})_{v} (\\frac{\\partial T}{\\partial v})_p (\\frac{\\partial v}{\\partial p})_T = -1     \\] <p>\u5373\u5f97\u4e09\u4e2a\u70ed\u7cfb\u6570\u4e4b\u95f4\u7684\u5173\u7cfb\u5f0f:</p> \\[     \\frac{\\alpha_{v}}{\\kappa_T \\beta} = p \\] <ul> <li>\u5de5\u8d28\u5728\u53ef\u9006\u7edd\u70ed\u8fc7\u7a0b\u4e2d\u7684\u538b\u7f29\u6027\u8d28---\u7edd\u70ed\u538b\u7f29\u7cfb\u6570\\(\\kappa_s\\)[\\(Pa^{-1}\\)]:</li> </ul> \\[     \\kappa_s = - \\frac{1}{v} (\\frac{\\partial v}{\\partial p})_{s}     \\]"},{"location":"energy/Engineering_Thermodynamics/#\u6bd4\u70ed\u5bb9","title":"\u6bd4\u70ed\u5bb9","text":"<p>\u4ee5\\((T,v)\\)\u4e3a\u72ec\u7acb\u53d8\u91cf\u7684\u70ed\u529b\u5b66\u80fd\u51fd\u6570\u5bf9\u6e29\u5ea6\u7684\u504f\u5fae\u5546\\((\\frac{\\partial u}{\\partial T})_{v}\\)\u5177\u6709\u91cd\u8981\u610f\u4e49,\u5b9a\u4e49\u4e3a\u5de5\u8d28\u7684\u6bd4\u5b9a\u5bb9\u70ed\u5bb9\\(c_V\\)[\\(J/(kg\u00b7K)\\)],\u5373\u5728\u4f53\u79ef\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b,\u70ed\u529b\u5b66\u80fd\u5bf9\u6e29\u5ea6\u7684\u504f\u5fae\u5546:</p> \\[     c_V = (\\frac{\\partial u}{\\partial T})_{v} \\] <p>\u5bf9\u4e8e\u51c6\u9759\u6001\u5e73\u8861\u5b9a\u5bb9\u8fc7\u7a0b\u4e2d,\u6709\\(\\delta q = d u\\),\u56e0\u6b64\\(c_V\\)\u8868\u793a\u5355\u4f4d\u8d28\u91cf\u7684\u5de5\u8d28\u6e29\u5ea6\u5347\u9ad81K\u6240\u5438\u6536\u7684\u70ed\u91cf\u5219\u6709:</p> \\[     c_V = (\\frac{\\delta q}{d T})_{v} \\] <p>\u5b9a\u4e49\u5728\u5b9a\u538b\u6761\u4ef6\u4e0b,\u7113\u5bf9\u6e29\u5ea6\u7684\u504f\u5fae\u5546\u5fae\u6bd4\u5b9a\u538b\u70ed\u5bb9\\(c_p\\):</p> \\[     c_p = (\\frac{\\partial h}{\\partial T})_{p} \\] <p>\u540c\u6837,\u5728\u51c6\u9759\u6001\u5e73\u8861\u5b9a\u538b\u8fc7\u7a0b\u4e2d,\\(c_p\\)\u6307\u5355\u4f4d\u8d28\u91cf\u7684\u5de5\u8d28\u6e29\u5ea6\u5347\u9ad81K\u6240\u5438\u6536\u7684\u70ed\u91cf,\u53ef\u8868\u793a\u4e3a:</p> \\[     c_p = (\\frac{\\delta q}{d T})_{p} \\] <p>\u7edd\u70ed\u8282\u6d41\u7cfb\u6570\\(\\mu_{J}\\):\u5728\u7113\u503c\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u5de5\u8d28\u6e29\u5ea6\u968f\u538b\u529b\u7684\u53d8\u5316\u7387[\\(K/Pa\\)]</p> \\[     \\mu_{J} = (\\frac{\\partial T}{\\partial p})_{h} \\]"},{"location":"energy/Engineering_Thermodynamics/#\u70ed\u529b\u5b66\u80fd\u7113\u71b5\u7684\u5fae\u5206\u5f0f","title":"\u70ed\u529b\u5b66\u80fd\u3001\u7113\u71b5\u7684\u5fae\u5206\u5f0f","text":"<ul> <li>\u70ed\u529b\u5b66\u80fd\\(u(T,v)\\)\u7684\u5168\u5fae\u5206\u8868\u8fbe\u5f0f:</li> </ul> \\[     \\begin{aligned}     du &amp;= (\\frac{\\partial u}{\\partial T})_{v} dT + (\\frac{\\partial u}{\\partial v})_{T} dv \\\\     &amp;= c_V dT + (T(\\frac{\\partial s}{\\partial v})_{T} - p(\\frac{\\partial v}{\\partial v})_{T})dv \\\\     &amp;= c_V dT + [T(\\frac{\\partial p}{\\partial T})_{v} - p] dv \\\\     \\end{aligned} \\] <ul> <li>\u7113\\(h(T,p)\\)\u7684\u5168\u5fae\u5206\u8868\u8fbe\u5f0f:</li> </ul> \\[     \\begin{aligned}     dh &amp;= (\\frac{\\partial h}{\\partial T})_{p} dT + (\\frac{\\partial h}{\\partial p})_{T} dp \\\\     &amp;= c_p dT + (T(\\frac{\\partial s}{\\partial p})_{T} + v(\\frac{\\partial p}{\\partial p})_{T})dp \\\\     &amp;= c_p dT - [T(\\frac{\\partial v}{\\partial T})_{p} - v] dp \\\\     \\end{aligned} \\] <ul> <li>\u5bf9\u4e8e\u71b5\\(s(T,v)\\)\u548c\\(s(T,p)\\)\u7684\u5168\u5fae\u5206\u5f62\u5f0f:</li> </ul> \\[     \\begin{aligned}     ds &amp;= (\\frac{\\partial s}{\\partial T})_{v} dT + (\\frac{\\partial s}{\\partial v})_{T} dv  \\\\     &amp;= \\frac{c_V}{T} dT + (\\frac{\\partial p}{\\partial T})_{v} dv \\\\     ds &amp;= (\\frac{\\partial s}{\\partial T})_{p} dT + (\\frac{\\partial s}{\\partial p})_{T} dp  \\\\     &amp;= \\frac{c_p}{T} dT - (\\frac{\\partial v}{\\partial T})_{p} dp \\\\     \\end{aligned}    \\]"},{"location":"energy/Engineering_Thermodynamics/#\u70ed\u7cfb\u6570\u4e4b\u95f4\u7684\u4e00\u822c\u5173\u7cfb","title":"\u70ed\u7cfb\u6570\u4e4b\u95f4\u7684\u4e00\u822c\u5173\u7cfb","text":"<ul> <li>(\u4e00) \\((\\frac{\\partial c_V}{\\partial v})_{T} \u3001(\\frac{\\partial c_p}{\\partial p})_{T}\\)\u4e0e\u72b6\u6001\u65b9\u7a0b\u95f4\u7684\u5173\u7cfb</li> </ul> <p>\u7531\u4e0a\u8ff0\u70ed\u529b\u5b66\u80fd\u4e0e\u7113\u7684\u5fae\u5206\u5f0f\u53ef\u5f97:</p> \\[     \\begin{aligned}     (\\frac{\\partial c_V}{\\partial v})_{T} &amp;= T (\\frac{\\partial^2 p}{\\partial T^2})_{v} \\\\     (\\frac{\\partial c_p}{\\partial p})_{T} &amp;= - T (\\frac{\\partial^2 v}{\\partial T^2})_{p} \\\\     \\end{aligned}    \\] <p>\u5f53\u7ed9\u51fa\u8f83\u51c6\u786e\u7684\u72b6\u6001\u65b9\u7a0b\u4ee5\u53ca\u67d0\u4e00\u538b\u529b\\(p_0\\)\u4e0b\u6d4b\u5f97\u7684\u6bd4\u5b9a\u538b\u70ed\u5bb9\u6570\u636e\\(c_{p0}(T)\\),\u53ef\u4ee5\u901a\u8fc7\u79ef\u5206\u7b97\u5f97\u51fd\u6570\u5173\u7cfb\\(c_p(T,p)\\):</p> \\[     c_p(T,p) = c_{p0}(T) - T \\int_{p_0}^{p} (\\frac{\\partial^2 v}{\\partial T^2})_{p} dp           \\] <ul> <li>(\u4e8c)\u6bd4\u70ed\u5bb9\u5dee(\\(c_p - c_v\\))\u4e0e\u72b6\u6001\u65b9\u7a0b\u5173\u7cfb</li> </ul> \\[     c_v = T(\\frac{\\partial s}{\\partial T})_{v}            \\] \\[     c_p - c_v = T (\\frac{\\partial v}{\\partial T })_{p} (\\frac{\\partial p}{\\partial T})_{v}  = T v \\frac{\\alpha_V ^2}{\\kappa_T}  &gt; 0 \\] <p>\u7531\u4e0a\u5f0f\u53ef\u77e5,\\(c_p &gt; c_v\\) \u6052\u6210\u7acb.</p> <ul> <li>\u7edd\u70ed\u8282\u6d41\u7cfb\u6570\u7684\u4e00\u822c\u5173\u7cfb\u5f0f</li> </ul> <p>\\(\\mu_j\\)\u662f\u5728\u7113\u503c\u4e0d\u53d8\u65f6(\\(dh = 0\\))\u6e29\u5ea6\u5bf9\u538b\u529b\u7684\u504f\u5fae\u5546:</p> \\[     \\mu_j = (\\frac{\\partial T}{\\partial p})_{h}= \\frac{1}{c_p} [T (\\frac{\\partial v}{\\partial T })_{p} - v] =\\frac{v}{c_p} (T \\alpha_V - 1) \\] <p>1.maxwell\u5173\u7cfb\u5f0f\u7684\u8bb0\u5fc6 2.\u7126\u8033-\u6c64\u59c6\u5b59\u7cfb\u6570\u7684\u6545\u4e8b\u3001\u771f\u5b9e\u6c14\u4f53</p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u4e94\u7ae0-\u6c14\u4f53\u7684\u70ed\u529b\u6027\u8d28","title":"\u7b2c\u4e94\u7ae0 \u6c14\u4f53\u7684\u70ed\u529b\u6027\u8d28","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u7406\u60f3\u6c14\u4f53,\u7406\u60f3\u6c14\u4f53\u72b6\u6001\u65b9\u7a0b,</p> <p>\ud83d\udd27\u8ba1\u7b97:\u7406\u60f3\u6c14\u4f53\u72b6\u6001\u65b9\u7a0b \uff08\u4e0d\u6d89\u53ca\u5b9e\u9645\u6c14\u4f53\u7684\u6027\u8d28\uff09</p> <p></p> <p>\u7406\u60f3\u6c14\u4f53</p> <p> \u7406\u60f3\u6c14\u4f53\u6027\u8d28\u6307\u7684\u662f\u5ffd\u7565\u5206\u5b50\u81ea\u8eab\u5360\u6709\u7684\u4f53\u79ef\u548c\u5206\u5b50\u95f4\u76f8\u4e92\u4f5c\u7528\u529b\u5bf9\u5176\u4ed6\u5b8f\u89c2\u70ed\u529b\u6027\u8d28\u7684\u5f71\u54cd,\u5728\u5de5\u7a0b\u4e0a(\u901a\u5e38\u7684\u5de5\u4f5c\u53c2\u6570\u8303\u56f4\u4e2d),\u5c06\u5b9e\u9645\u6c14\u4f53\u5de5\u8d28\u5f53\u4f5c\u7406\u60f3\u6c14\u4f53\u5904\u7406\u6709\u8db3\u591f\u7684\u8ba1\u7b97\u7cbe\u5ea6. <p>\u7406\u60f3\u6c14\u4f53\u72b6\u6001\u65b9\u7a0b(3\u4e2a\u53ef\u6d4b\u72b6\u6001\u53c2\u6570\\(p,v,T\\)\u7684\u51fd\u6570\u5173\u7cfb\\(F(p,v,T)=0\\)):Clapeyron\u65b9\u7a0b</p> \\[     pV = nRT = m R_g T \\] <p>\\(R_g = \\frac{R}{M}\\):\u6c14\u4f53\u5e38\u6570[\\(J/(kg\u00b7K)\\)],\\(R\\)\u4e3a\u6469\u5c14\u6c14\u4f53\u5e38\u6570\\(8.314 J/(mol\u00b7K)\\)</p> <p>\u53d8\u5f0f:</p> \\[     \\begin{aligned}     pv = R_g T,&amp;(\\text{\u5355\u4f4d\u8d28\u91cf(1kg)\u5f62\u5f0f})                   \\\\     pV_m = RT,&amp;(\\text{\u5355\u4f4dmol\u5f62\u5f0f,} V_m=V/n)                    \\\\     p M = \\rho R T,&amp;(\\text{\u5bc6\u5ea6\u5f0f })     \\end{aligned} \\] <p></p> <p>\u6bd4\u70ed\u5bb9\u6362\u7b97</p> <p> 1kg\u7269\u8d28\u7684\u70ed\u5bb9\u91cf\u4e3a\u6bd4\u70ed\u5bb9\\(c\\),1mol\u7684\u70ed\u5bb9\u91cf\u4e3a\u6469\u5c14\u70ed\u5bb9\\(C_m\\),1\\(m^3\\)\u6807\u51c6\u72b6\u6001\u4e0b\u6c14\u4f53\u7684\u70ed\u5bb9\u91cf\u4e3a\u4f53\u79ef\u70ed\u5bb9. \\[      \\begin{aligned}     C_m = Mc &amp;= MvC`  \\\\     C_{p,m} - C_{v,m} &amp;= R \\\\     \\end{aligned}    \\] <p></p> <p>\u6839\u636e\u7406\u60f3\u6c14\u4f53\u65b9\u7a0b,\u53ef\u5f97\u5bf9\u4e8e\u7406\u60f3\u6c14\u4f53:</p> \\[     \\begin{aligned}     \\alpha_V = \\beta = \\frac{1}{T} \\\\     \\kappa_T = \\frac{1}{p}  \\\\       (\\frac{\\partial c_v}{\\partial v})_T = T (\\frac{\\partial^2 p}{\\partial T^2})_v = 0 \\\\     (\\frac{\\partial c_p}{\\partial v})_T = -T (\\frac{\\partial^2 v}{\\partial T^2})_p = 0 \\\\     c_p - c_v = \\frac{Tv \\alpha_v^2}{\\kappa_T} = \\frac{pv}{T} = R_g \\\\     \\mu_j = \\frac{v}{c_p}(T \\alpha_T - 1) = 0 \\\\     \\end{aligned} \\] <p>\u7406\u60f3\u6c14\u4f53\u6bd4\u70ed\u5bb9\u4e3a\u6e29\u5ea6\u7684\u5355\u503c\u51fd\u6570,\u5176\u5dee\u503c\u4e3a\u6052\u5b9a\u503c</p> <p>\u5bf9\u4e8e\u70ed\u529b\u5b66\u80fdu\u4e0e\u7113h\u7684\u7279\u6027,\u5747\u4e3a\u6e29\u5ea6T\u7684\u5355\u503c\u51fd\u6570:</p> \\[     \\begin{aligned}     (\\frac{\\partial u}{\\partial v})_T = T (\\frac{\\partial p}{\\partial T})_v - p = \\frac{R_g T}{v} - p = 0 \\Rightarrow du = c_v dT\\\\     (\\frac{\\partial h}{\\partial p})_T = -T (\\frac{\\partial v} {\\partial T})_p + v = 0  \\Rightarrow  dh = c_p dT\\\\     \\end{aligned} \\] <p>\u5bf9\u4e8e\u7406\u60f3\u6c14\u4f53\u71b5\u65b9\u7a0b,\u4ee5\u5206\u522b\u4ee5(T,v),(T,p)\u4e3a\u72ec\u7acb\u53d8\u91cf:</p> \\[     ds =c_v \\frac{dT}{T} + R_g \\frac{dv}{v}=c_p \\frac{dT}{T} - R_g \\frac{dp}{p} \\] <p>\u653e\u5165\u8fc7\u7a0b\u4e2d:</p> \\[     \\begin{aligned}     \\Delta u_{1-2} =&amp; c_v \\Delta T_{1-2}      \\\\     \\Delta h_{1-2} =&amp; c_p \\Delta T_{1-2}       \\\\     \\Delta s_{1-2} = s_2 - s_1 = c_v \\ln{\\frac{T_2}{T_1}} +&amp; R_g \\ln{\\frac{v_2}{v_1}} = c_p \\ln{\\frac{T_2}{T_1}} - R_g \\ln{\\frac{p_2}{p_1}}     \\end{aligned} \\]"},{"location":"energy/Engineering_Thermodynamics/#\u5e73\u5747\u6bd4\u70ed\u5bb9","title":"\u5e73\u5747\u6bd4\u70ed\u5bb9","text":"<p>\u6839\u636e\u5b9a\u4e49:'</p> \\[     \\begin{aligned}     c_v \\lvert_{t_1}^{t_2} = \\frac{\\int_{t_2}^{t_1} c_v dt}{t_2 -t_1} = \\frac{\\Delta u_{1-2}}{t_2 -t_1} = \\frac{c_v \\lvert_0^{t_2} t_2 - c_v \\lvert_0^{t_1} t_1 }{t_2 -t_1} \\\\     c_p \\lvert_{t_1}^{t_2} = \\frac{\\int_{t_2}^{t_1} c_p dt}{t_2 -t_1} = \\frac{\\Delta h_{1-2}}{t_2 -t_1} = \\frac{c_p \\lvert_0^{t_2} t_2 - c_p \\lvert_0^{t_1} t_1 }{t_2 -t_1} \\\\     \\end{aligned} \\] <p>\u6839\u636e\u5b9a\u4e49\u5f0f,\u5e73\u5747\u6bd4\u70ed\u5bb9\u4e0e\u521d\u6001\u6e29\u5ea6\\(t_1\\)\u4e0e\u7ec8\u6001\u6e29\u5ea6\\(t_2\\)\u6709\u5173</p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u516d\u7ae0-\u84b8\u6c7d\u7684\u70ed\u529b\u6027\u8d28","title":"\u7b2c\u516d\u7ae0 \u84b8\u6c7d\u7684\u70ed\u529b\u6027\u8d28","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u70ed\u529b\u5b66\u7684\u5e73\u8861\u5224\u636e\u3001\u5409\u5e03\u65af\u76f8\u7387\u3001\u514b\u52b3\u4fee\u65af-\u514b\u62c9\u8d1d\u9f99\u65b9\u7a0b\u3001\u89e3\u91ca\u6c34\u7684\u53cd\u5e38\u9971\u548c\u56fa-\u6db2\u7ebf\u3001\u84b8\u6c7d\u70ed\u529b\u6027\u8d28\uff0c\u84b8\u6c7d\u7684\u5e38\u89c1\u70ed\u529b\u8fc7\u7a0b\uff08\u91cd\u70b9:\u84b8\u6c7d\u5b9a\u538b\u53d1\u751f\u8fc7\u7a0b\u7684T-s\u56fe\uff09</p> <p>\ud83d\udd27\u8ba1\u7b97:\u84b8\u6c7d\u5e72\u5ea6x,\u5e38\u7528\u7684\u84b8\u6c7d\u70ed\u529b\u8fc7\u7a0b\uff08\u5b9a\u538b\u3001\u7edd\u70ed...\uff09\uff08\u4e00\u822c\u6765\u8bf4\u4e0d\u8003\u5bdf\u5316\u5b66\u52bf\u7684\u76f8\u5173\u5185\u5bb9\uff09</p> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u70ed\u529b\u5b66\u5224\u636e","title":"\u70ed\u529b\u5b66\u5224\u636e","text":"<p>\u5728\u4e00\u5b9a\u7684\u7ea6\u675f\u6761\u4ef6\u4e0b\uff0c\u7cfb\u7edf\u56e0\u4e3a\u5404\u79cd\u4e0d\u5e73\u8861\u52bf\u5dee\u800c\u671d\u7740\u4e00\u5b9a\u65b9\u5411\u53d8\u5316\uff0c\u5f53\u5404\u79cd\u52bf\u5dee\u51cf\u5c0f\u4e3a\u96f6\u65f6\u7cfb\u7edf\u72b6\u6001\u4e0d\u518d\u53d8\u5316\uff08\u5373\u8fbe\u5230\u5e73\u8861\uff09\uff1a</p> <p>\u5b64\u7acb\u4f53\u7cfb\u71b5\u589e\u539f\u7406</p> <p> <p>\u5373\u5e73\u8861\u7684\u71b5\u5224\u636e\uff0c\u8868\u8ff0\u4e3a\u201c\u5b64\u7acb\u7cfb\u5904\u4e8e\u5e73\u8861\u72b6\u6001\u65f6\uff0c\u71b5\u5177\u6709\u6700\u5927\u503c\u201d\uff1a</p> \\[      dS_{iso} \\geq 0 \\] <p>\u5f53\u4e0a\u8ff0\u5f0f\u5b50\u53d6\u5230\u7b49\u53f7\u65f6\uff0c\u8868\u660e\u7cfb\u7edf\u7684\u71b5\u589e\u5230\u8fbe\u6700\u5927\u503c\uff0c\u5373\u53ef\u5224\u65ad\u7cfb\u7edf\u4e0d\u518d\u53d8\u5316\uff08\u5904\u4e8e\u5e73\u8861\u6001\uff09</p> <p></p> <p>\u4efb\u610f\u5c01\u95ed\u4f53\u7cfb\u7684\u5e73\u8861\u5224\u636e</p> <p> \u5bf9\u4e8e\u5904\u4e8e\\(T_0,p_0\\)\u7684\u73af\u5883\u4e0b\u7684\u4efb\u610f\u5c01\u95ed\u4f53\u7cfb\uff0c\u8bbe\u529f\u52bf\u51fd\u6570\u4e3a\\(\\Psi = U + p_0 V - T_0 S\\)\uff0c\u6709: \\[      \\begin{aligned}     -(du + p_0 v - T_0 ds) &amp;\\geq \\delta w_u \\\\     - \\psi &amp;\\geq \\delta w_u \\\\     \\end{aligned} \\] <p>\u5373\u7cfb\u7edf\u5b8c\u6210\u7684\u6709\u7528\u529f\u603b\u5c0f\u4e8e\u7b49\u4e8e\u5176\u529f\u52bf\u51fd\u6570\u7684\u51cf\u5c0f\u503c\uff0c\u5047\u8bbe\u5916\u754c\u4e0d\u5bf9\u7cfb\u7edf\u5b8c\u6210\u6709\u7528\u529f\uff08\\(\\delta w_u \\geq 0\\)\uff09,\u5219\uff1a</p> \\[      \\begin{aligned}     -(du + p_0 v - T_0 ds) &amp;\\geq \\delta 0 \\\\     \\psi &amp;\\leq \\delta w_u \\\\     \\end{aligned} \\] <p>\u5f53\u4e0a\u8ff0\u5f0f\u5b50\u8868\u660e:\u7cfb\u7edf\u72b6\u6001\u603b\u662f\u671d\u7740\u529f\u52bf\u51fd\u6570\\(\\Psi\\)\u51cf\u5c0f\u7684\u65b9\u5411\u8fdb\u884c\u53d8\u5316\uff0c\u5f53\u53d6\u5230\u7b49\u53f7\u65f6\uff0c\u7cfb\u7edf\u4e0d\u5e73\u8861\u52bf\u6d88\u5931\uff0c\u5373\u4e3a\u5904\u4e8e\u5e73\u8861\u6001</p> <p>\u5b9a\u6e29\u5b9a\u5bb9\u4e0b\u7684\u81ea\u7531\u80fd\u5224\u636e\uff08F\uff09</p> <p> \u5728\\(dT = 0,dv = 0\\)\u6761\u4ef6\u4e0b\uff0c\u6709\uff1a \\[      \\begin{aligned}     (du - d(Ts))_{T,v} &amp;\\leq 0 \\\\     (df)_{T,v} &amp;\\leq \\\\     \\end{aligned} \\] <p>\u5373\uff1a\u5728\u5b9a\u6e29\u5b9a\u5bb9\u6761\u4ef6\u4e0b\uff0c\u5c01\u95ed\u7cfb\u7edf\u671d\u81ea\u7531\u80fd\\(F\\)\u51cf\u5c0f\u7684\u65b9\u5411\u81ea\u53d1\u8fdb\u884c\uff0c\u5e73\u8861\u65f6\u81ea\u7531\u80fd\u6700\u5c0f\uff08\u81ea\u7531\u80fd\u5224\u636e\uff09 </p> <p>\u5b9a\u6e29\u5b9a\u538b\u4e0b\u7684\u81ea\u7531\u80fd\u7113\u5224\u636e\uff08G\uff09</p> <p> \u5728\\(dT = 0,dp = 0\\)\u6761\u4ef6\u4e0b\uff0c\u6709\uff1a \\[      \\begin{aligned}     (du + d(pv)- d(Ts))_{T,p} &amp;\\leq 0 \\\\     (dg)_{T,p} &amp;\\leq \\\\     \\end{aligned} \\] <p>\u5373\uff1a\u5728\u5b9a\u6e29\u5b9a\u538b\u6761\u4ef6\u4e0b\uff0c\u5c01\u95ed\u7cfb\u7edf\u671d\u81ea\u7531\u7113\\(G\\)\u51cf\u5c0f\u7684\u65b9\u5411\u81ea\u53d1\u8fdb\u884c\uff0c\u5e73\u8861\u65f6\u81ea\u7531\u7113\u6700\u5c0f\uff08\u81ea\u7531\u7113\u5224\u636e\uff09 </p> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u5409\u5e03\u65af\u76f8\u7387","title":"\u5409\u5e03\u65af\u76f8\u7387","text":"<p>\u5bf9\u4e8e\u65e0\u5316\u5b66\u53cd\u5e94\u7684\u7cfb\u7edf\uff0c\u6709\u5409\u5e03\u65af\u76f8\u7387\u6765\u5f97\u51fa\u7cfb\u7edf\u4e2d\u72ec\u7acb\u5f3a\u5ea6\u53c2\u6570\u7684\u6570\u76ee\uff08\u81ea\u7531\u5ea6\uff09\uff1a</p> \\[     I = C - P + 2 \\] <p>\u5176\u4e2d\uff0cI\u4e3a\u7cfb\u7edf\u4e2d\u72ec\u7acb\u5f3a\u5ea6\u53c2\u6570\u7684\u6570\u76ee\uff0cC\u4e3a\u7cfb\u7edf\u5305\u542b\u7684\u7ec4\u5143\u6570\uff0cP\u4e3a\u7cfb\u7edf\u5305\u542b\u7684\u76f8\u6570\u3002</p> <p>\u4e00\u822c\u6765\u8bf4\uff0c\u672c\u8bfe\u7a0b\u8003\u5bdf\u5355\u4e00\u7269\u8d28\uff08\u5355\u5143\uff0c\\(C = 1\\)\uff09\u7684\u4e09\u79cd\u5e38\u89c1\u60c5\u5f62\uff1a <ul> <li> <p>\u5bf9\u4e8e\u5355\u5143\u5355\u76f8\u7cfb\uff08\u5982\u8fc7\u70ed\u84b8\u6c7d\u3001\u672a\u9971\u548c\u6c34\uff09\uff1a\u7cfb\u7edf\u7684\u5404\u79cd\u5f3a\u5ea6\u53c2\u6570\u786e\u5b9a\u4e8e\u4e24\u4e2a\u72ec\u7acb\u53c2\u6570\uff0c\u5982\\(p\\)\u4e0e\\(T\\)\uff0c\u5373\\(I = 2\\)\u3002</p> </li> <li> <p>\u5bf9\u4e8e\u5355\u5143\u4e24\u76f8\u7cfb\uff08\u6e7f\u84b8\u6c7d\uff09\uff1a\u5171\u5b58\u7684\u4e24\u76f8\u5904\u4e8e\u9971\u548c\u6e29\u5ea6\u4e0e\u9971\u548c\u538b\u529b\uff0c\u7531\u4e8e\\(p_s\\)\u4e0e\\(T_s\\)\u5e76\u4e0d\u76f8\u4e92\u72ec\u7acb\uff0c\u56e0\u6b64\u8be5\u4e24\u76f8\u7cfb\u7edf\u7684\u5404\u79cd\u5f3a\u5ea6\u53c2\u6570\u786e\u5b9a\u4e8e\u4e00\u4e2a\u72ec\u7acb\u53c2\u6570\uff0c\u5373\\(I = 1\\)\u3002</p> </li> <li> <p>\u5bf9\u4e8e\u5355\u5143\u4e09\u76f8\u7cfb\uff1a\u6709\u4e0e\u5355\u4e00\u7269\u8d28\u7684\u4e09\u76f8\u70b9\u5b8c\u5168\u786e\u5b9a\uff0c\u56e0\u6b64\u4e09\u76f8\u5e73\u8861\u7684\u5355\u5143\u7cfb\u6ca1\u6709\u53ef\u53d8\u7684\u72ec\u7acb\u5f3a\u5ea6\u53c2\u6570\uff0c\u5373\\(I = 0\\)\u3002</p> </li> <li> <p>\u7279\u6b8a\uff1a\u5bf9\u4e8e\u6e7f\u9971\u548c\u84b8\u6c7d\uff0c\u5904\u4e8e\u4e24\u76f8\u533a\uff0c\u9700\u8981\u989d\u5916\u5f15\u5165\u5e72\u5ea6\\(x = \\frac{m_g}{m_g + m_f}\\)\u6765\u63cf\u8ff0\u6e7f\u84b8\u6c7d\u4e2d\u6db2\u4f53\u4e0e\u6c14\u7684\u6bd4\u4f8b\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e24\u4e2a\u72ec\u7acb\u53d8\u91cf\uff0c\uff0c\u5373\\(I = 2\\)\u3002</p> </li> </ul> <p>\uff08\u5355\u5143\u7cfb\u4e0d\u5b58\u5728\u56db\u79cd\u4ee5\u4e0a\u7684\u4e0d\u540c\u76f8\u5e73\u8861\u7684\u72b6\u6001\uff09</p>"},{"location":"energy/Engineering_Thermodynamics/#\u514b\u52b3\u4fee\u65af-\u514b\u62c9\u8d1d\u9f99\u65b9\u7a0b","title":"\u514b\u52b3\u4fee\u65af-\u514b\u62c9\u8d1d\u9f99\u65b9\u7a0b","text":"<p>\u5047\u8bbe\\(p_s\\)\u4e3a\u9971\u548c\u538b\u529b\uff0c\\(T_s\\)\u4e3a\u9971\u548c\u6e29\u5ea6\uff0c\\(v_{\\alpha}, v_{\\beta}\\), \\(s_{\\alpha}, s_{\\beta}\\)\u5206\u522b\u4e3a\u4e24\u76f8\u7684\u6bd4\u4f53\u79ef\u4e0e\u6bd4\u71b5\uff0c\\(r\\)\u4e3a\u76f8\u53d8\u6f5c\u70ed,\u6709\uff1a</p> \\[     \\begin{aligned}     \\frac{dp_s}{dT_s} &amp;= \\frac{s_{\\beta} - s_{\\alpha}}{v_{\\beta} - v_{\\alpha}}  \\\\     &amp;= \\frac{r}{T_s (v_{\\beta} - v_{\\alpha})} \\\\     \\end{aligned} \\] <p>\u8be5\u5f0f\u8868\u793a\u7684\u659c\u7387\u5373\u4e3ap-T\u56fe\u4e0a\u5bf9\u5e94\u4e24\u76f8\u7684\u9971\u548c\u66f2\u7ebf\u659c\u7387</p> <p>p-T\u56fe\u4e0a\u6c34\u7684\u53cd\u5e38\u659c\u7387</p> <p> <p>\u5bf9\u4e8e\u4e00\u822c\u7684\u5438\u70ed\u76f8\u53d8\u8fc7\u7a0b\u662f\u7531\u6bd4\u4f53\u79ef\u5c0f\u7684\u76f8\u8f6c\u53d8\u4e3a\u6bd4\u4f53\u79ef\u5927\u7684\u76f8(\\(v_{\\alpha} &lt; v_{\\beta}\\)),\u5219\u4e0a\u8ff0\u659c\u7387\u4e00\u822c\u4e3a\u6b63\uff0c\u4f46\u5bf9\u4e8e\u67d0\u4e9b\u7279\u6b8a\u7269\u8d28\uff08\u5982 \u6c34 \uff09,\u56fa\u6001\u6c34\uff08\u51b0\uff09\u5728\u7194\u878d\u8fc7\u7a0b\u4e2d\u6bd4\u4f53\u79ef\u53d8\u5c0f\uff0c\u56e0\u6b64\u6c34\u7684\u7194\u89e3\u7ebf\u659c\u7387\u4e3a\u8d1f\u503c"},{"location":"energy/Engineering_Thermodynamics/#\u84b8\u6c7d\u5b9a\u538b\u53d1\u751f\u8fc7\u7a0b","title":"\u84b8\u6c7d\u5b9a\u538b\u53d1\u751f\u8fc7\u7a0b","text":"<p>\u84b8\u6c7d\u5728\u5b9a\u538b\u52a0\u70ed\u8bbe\u5907\u4e2d\uff0c\u7ecf\u5386\u672a\u9971\u548c\u6db2\u4f53 \u2014\u2014\u2192 \u9971\u548c\u6db2\u4f53 \u2014\u2014\u2192 \u6e7f\u9971\u548c\u84b8\u6c7d \u2014\u2014\u2192 \u5e72\u9971\u548c\u84b8\u6c7d \u2014\u2014\u2192 \u8fc7\u70ed\u84b8\u6c7d\u7684\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u7531\u5982\u4ee5\u4e0bT-s\u56fe\u8868\u793a\uff1a</p> <p></p> <p>\u8003\u8bd5\u4e2d\uff0c\u91cd\u8981\u7684\u53c2\u6570\u6765\u6e90\u4e8e\u5377\u9762\u7ed9\u51fa\u7684\u8868\u683c\uff0c\u56e0\u6b64\u5f53\u4e0d\u6e05\u695a\u5de5\u8d28\u72b6\u6001\u65f6\uff0c\u5148\u67e5\u9971\u548c\u8868\u4e2d\u5bf9\u5e94\u7684\\(p_s,T_s\\)\u6765\u5224\u65ad\u5de5\u8d28\u7684\u72b6\u6001\uff0c\u5728\u4ece\u8fc7\u70ed\u8868/\u9971\u548c\u8868\u4e2d\u67e5\u8be2\u5bf9\u5e94\u7684\u72b6\u6001\u3002 <p>\u5bf9\u4e8e\u5904\u4e8e\u4e24\u76f8\u533a\u7684\u6e7f\u9971\u548c\u84b8\u6c7d\uff0c\u9700\u8981\u5148\u6839\u636e\u7ed9\u51fa\u7684\u53c2\u6570\u6765\u786e\u5b9a\u5176\u5e72\u5ea6(y\u4e3a\u67d0\u4e00\u6bd4\u53c2\u6570)\uff1a</p> \\[     x = \\frac{m_g}{m_g + m_f} = \\frac{y - y'}{y\" - y'} \\] <p>\u901a\u8fc7\u6240\u5f97\u7684\u5e72\u5ea6\\(x\\)\uff0c\u53ef\u5f97\u6e7f\u84b8\u6c7d\u7684\u5176\u4f59\u53c2\u6570:</p> \\[     \\begin{aligned}     h &amp;= h' + x(h\" - h') \\\\     v &amp;= v' + x(v\" - v') \\\\     s &amp;= s' + x(s\" - s') \\\\     \\end{aligned} \\] <p>\u6e7f\u84b8\u6c7d\u7684\u7edd\u70ed\u8fc7\u7a0b</p> <p> \u82e5\u4e0d\u8003\u8651\u635f\u8017\uff0c\u6c34\u84b8\u6c14\u5728\u6c7d\u8f6e\u673a\u3001\u6c34\u6cf5\u4e2d\u7684\u81a8\u80c0\u3001\u538b\u7f29\u8fc7\u7a0b\u53ef\u4ee5\u770b\u4f5c\u53ef\u9006\u7edd\u70ed\u8fc7\u7a0b\uff08\u5b9a\u71b5\u8fc7\u7a0b\uff09\uff0c\u5982\u4e0b\u56fe\uff0c\u56e0\u4e3a\\(q = 0 = \\Delta h + w_t\\): \\[     w_t = - \\Delta h = h_1 - h_2     \\] <p></p> <p>\u82e5\u8be5\u8fc7\u7a0b\u5b58\u5728\u635f\u8017\uff0c\u5373\u4e0d\u53ef\u9006\u8fc7\u7a0b\u5b58\u5728\u71b5\u589e\uff08\u5982\u56fe1-2'\u8fc7\u7a0b\u7ebf\uff09\u5219\u5b9e\u9645\u6280\u672f\u529f\u4e3a\uff1a</p> \\[     w_t ' = - \\Delta h = h_1 - h_{2'} &lt; w_t  \\] <p>\u4e3a\u4e86\u53cd\u6620\u7edd\u70ed\u81a8\u80c0\u7684\u4e0d\u53ef\u9006\u7a0b\u5ea6(\u5bf9\u5916\u505a\u529f\u6709\u635f\u5931\uff0c\u663e\u7136\\(w_t'&lt; w_t\\))\uff0c\u6211\u4eec\u7528\u6c7d\u8f6e\u673a\u76f8\u5bf9\u5185\u6548\u7387\\(\\eta_T\\)\u6765\u8868\u793a\uff1a</p> \\[     \\eta_T = \\frac{w_t '}{w_t} = \\frac{h_1 - h_{2'}}{h_1 - h_2} \\] <p>\u7531\u6b64\u53ef\u53cd\u8fc7\u6765\u6c42\u8fc7\u7a0b\u6700\u540e\u7684\\(h_{2'}\\)\uff0c\u4e5f\u53ef\u5f97\u8fc7\u7a0b\u7684\u71b5\u4ea7\\(\\delta s_g\\)\u4e0e\u6709\u6548\u80fd\u635f\u5931\\(i\\),\u8bbe\u73af\u5883\u6e29\u5ea6\u4e3a\\(T_0\\):</p> \\[     \\begin{aligned}     h_{2'} =&amp; h_1 + \\eta_T (h_1 - h_2) \\\\      \\Delta s_g &amp;= s_{2'} - s_2 \\\\     i =&amp; T_0 \\Delta s_g = T_0 (s_{2'} - s_2) \\\\     \\end{aligned} \\] <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u4e03\u7ae0-\u7406\u60f3\u6c14\u4f53\u6df7\u5408\u7269\u4e0e\u6e7f\u7a7a\u6c14","title":"\u7b2c\u4e03\u7ae0 \u7406\u60f3\u6c14\u4f53\u6df7\u5408\u7269\u4e0e\u6e7f\u7a7a\u6c14","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u6e7f\u7a7a\u6c14\u6e29\u5ea6\uff08\u5e72\u7403\uff0c\u6e7f\u7403\uff0c\u9732\u70b9\uff0c\u7edd\u70ed\u9971\u548c\uff09\uff0c\u7113-\u6e7f\u56fe\uff08h-s\uff09</p> <p>\ud83d\udd27\u8ba1\u7b97:\u542b\u6e7f\u91cfd\u3001\u76f8\u5bf9\u6e7f\u5ea6\\(\\phi\\)\u3001\u6bd4\u7113\u3001\u5e38\u7528\u7684\u6e7f\u7a7a\u6c14\u8fc7\u7a0b\uff08\u52a0\u70ed/\u51b7\u5374\u3001\u7edd\u70ed\u52a0\u6e7f\u3001\u51b7\u5374\u53bb\u6e7f...\uff09</p> <p></p> <p>\u6e7f\u7a7a\u6c14\u662f\u5e72\u7a7a\u6c14\u4e0e\u6c34\u84b8\u6c14\u7684\u6df7\u5408\u7269\uff0c\u53ef\u4ee5\u5c06\u5176\u770b\u4f5c\u7406\u60f3\u6c14\u4f53\u6df7\u5408\u7269,\u9002\u7528\u7406\u60f3\u6c14\u4f53\u7684\u72b6\u6001\u65b9\u7a0b\u7b49\uff0c\u533a\u522b\u662f\u6e7f\u7a7a\u6c14\u53ef\u80fd\u5728\u70ed\u529b\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u51dd\u7ed3\u6c34"},{"location":"energy/Engineering_Thermodynamics/#\u5e72\u7403\u6e7f\u7403\u6e29\u5ea6\u7edd\u70ed\u9971\u548c\u6e29\u5ea6t_w\u548c\u9732\u70b9\u6e29\u5ea6t_d","title":"\u5e72\u7403\uff0c\u6e7f\u7403\u6e29\u5ea6\u3001\u7edd\u70ed\u9971\u548c\u6e29\u5ea6\\(T_w\\)\u548c\u9732\u70b9\u6e29\u5ea6\\(T_d\\)","text":"<p>\u5bf9\u4e8e\u6e7f\u84b8\u6c7d\u4e2d\u7684\u6c34\u84b8\u6c14\u5206\u538b\\(p_v\\)\uff0c\u5b58\u5728\u4e00\u4e2a\u4e34\u754c\u503c\uff0c\u5373\u9971\u548c\u538b\u529b\\(p_s(p_v \\leq p_s)\\),\u5bf9\u5e94\u7684\u72b6\u6001\u4e3a\u9971\u548c\u6e7f\u7a7a\u6c14\uff0c\u5f53\\(p_v\\)\u8fbe\u5230\\(p_s\\)\u65f6\uff0c\u6c34\u84b8\u6c14\u4fbf\u4f1a\u51fa\u73b0\u51dd\u7ed3\u73b0\u8c61\uff0c\u4f7f\u5f97\u6e7f\u7a7a\u6c14\u4e2d\u7684\u6c34\u542b\u91cf\u51cf\u5c11\u3002</p> <ul> <li> <p>\u5e72\u7403\u6e29\u5ea6\\(T\\)\uff0c\u6307\u7684\u662f\u4f7f\u7528\u666e\u901a\u6e29\u5ea6\u8ba1\u6d4b\u5f97\u7684\u6e7f\u7a7a\u6c14\u7684\u6e29\u5ea6\\(T\\);</p> </li> <li> <p>\u6e7f\u7403\u6e29\u5ea6\uff0c\u6307\u7684\u662f\u4f7f\u7528\u6e7f\u7eb1\u5e03\u5305\u88f9\u7684\u6e7f\u7403\u6e29\u5ea6\u8ba1\u6d4b\u5f97\u7684\u6e7f\u7eb1\u5e03\u4e2d\u6c34\u7684\u6e29\u5ea6\uff1a\u82e5\u73af\u5883\u6e7f\u7a7a\u6c14\u672a\u9971\u548c\uff0c\u5219\u6e7f\u7eb1\u5e03\u4e0a\u7684\u6c34\u5c06\u4e0d\u65ad\u84b8\u53d1\uff0c\u5438\u6536\u6c7d\u5316\u6f5c\u70ed\uff0c\u4f7f\u5f97\u6e7f\u7403\u6e29\u5ea6\u4e0b\u964d\u4e0e\u5468\u56f4\u7a7a\u6c14\u5f62\u6210\u6e29\u5dee\uff0c\u5bfc\u81f4\u73af\u5883\u5411\u6e7f\u7403\u4f20\u70ed\uff0c\u8fdb\u800c\u8fbe\u5230\u5438\u653e\u70ed\u7684\u4e00\u4e2a\u7a33\u5b9a\u72b6\u6001\uff08\u4e0d\u662f\u5e73\u8861\u72b6\u6001\uff09\uff0c\u5b9e\u9a8c\u53ef\u77e5\u6e7f\u7403\u6e29\u5ea6=\u7edd\u70ed\u9971\u548c\u6e29\u5ea6\\(T_w\\) <li> <p>\u9732\u70b9\u6e29\u5ea6\\(T_d\\)\u662f\u6307\u5f53\u524d\u6e7f\u7a7a\u6c14\u4e2d\u6c34\u84b8\u6c14\u5206\u538b\\(p_v\\)\u6240\u5bf9\u5e94\u7684\u9971\u548c\u6e29\u5ea6(\\(p_v = p_s\\)),\u5bf9\u4e8e\u672a\u9971\u548c\u7a7a\u6c14\uff0c\u9700\u8981\u5728\\(p_v\\)\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u964d\u6e29(\\(T\\)\u2b07\uff0c\\(p_s\\)\u2b07)\u624d\u80fd\u8fbe\u5230\uff0c\u663e\u7136\\(T_d &lt; T\\)\u3002\u53cd\u4e4b\uff0c\u5f53\u6c34\u84b8\u6c14\u5206\u538b\\(p_v\\)\u589e\u5927\u65f6\uff0c\u5bf9\u5e94\u7684\u9732\u70b9\u6e29\u5ea6\\(T_d\\)\u968f\u4e4b\u589e\u5927\u3002</p> </li> <li> <p>\u7edd\u70ed\u9971\u548c\u6e29\u5ea6\\(T_w\\),\u5b9e\u5728\u7edd\u70ed\u6761\u4ef6\u4e0b\u5411\u6e7f\u7a7a\u6c14\u4e2d\u52a0\u5165\u6c34\u5206\uff0c\u5e76\u4e14\u4f7f\u4e4b\u84b8\u53d1\u800c\u5230\u8fbe\u9971\u548c\u72b6\u6001\u65f6\u5bf9\u5e94\u7684\u6e29\u5ea6\u3002\u56e0\u4e3a\u7edd\u70ed\u6761\u4ef6\u4e0b\u84b8\u53d1\u5438\u70ed\uff0c\u5bfc\u81f4\\(T_w &lt; T\\)\u3002</p> </li> <p></p> <p>\u7efc\u4e0a\u53ef\u77e5\uff0c\u5bf9\u4e8e\u672a\u9971\u548c\u7a7a\u6c14\uff0c\u6709\\(T_d &lt; T_w &lt; T\\)\uff1b\u5bf9\u4e8e\u9971\u548c\u7a7a\u6c14\uff0c\u5219\\(T_d = T_w = T\\)\u3002</p>"},{"location":"energy/Engineering_Thermodynamics/#\u6e7f\u7a7a\u6c14\u7684\u91cd\u8981\u53c2\u6570\u4ee5\u5e72\u7a7a\u6c14\u4e3a\u5355\u4f4d","title":"\u6e7f\u7a7a\u6c14\u7684\u91cd\u8981\u53c2\u6570\uff08\u4ee5\u5e72\u7a7a\u6c14\u4e3a\u5355\u4f4d","text":"<p>\u76f8\u5bf9\u6e7f\u5ea6\\(\\phi\\)\u662f\u6e7f\u7a7a\u6c14\u4e2d\u6c34\u84b8\u6c14\u5206\u538b\u529b\\(p_v\\)\u4e0e\u8be5\u6e29\u5ea6\u5bf9\u5e94\u7684\u9971\u548c\u538b\u529b\\(p_s\\)\u4e4b\u6bd4,\u80fd\u591f\u53cd\u6620\u6e7f\u7a7a\u6c14\u7684\u5e72\u6e7f\u7a0b\u5ea6\uff1a</p> \\[     \\phi = \\frac{p_v}{p_s} \\] <p>\u542b\u6e7f\u91cf\\(d(kg/kg(A))\\)\u662f\u5355\u4f4d\u8d28\u91cf\u5e72\u7a7a\u6c14\u4e2d\u6240\u542b\u6709\u7684\u6c34\u84b8\u6c14\u8d28\u91cf\uff1a</p> \\[     d = \\frac{m_v}{m_a} = 0.622\\frac{p_v}{p - p_v} = 0.622\\frac{\\phi p_s}{p - \\phi p_s}  \\] <p>\u6e7f\u7a7a\u6c14\u7684\u6bd4\u7113\\(h=\\)\u5e72\u7a7a\u6c14\u7113\u4e0e\u6e7f\u7a7a\u6c14\u7113\u4e4b\u548c,\u662f\u76f8\u5bf9\u4e8e\u5355\u4f4d\u8d28\u91cf\u7684\u5e72\u7a7a\u6c14\u7684\u6bd4\u53c2\u6570,\u56e0\u6b64\u8981\u5728\u5f0f\u5b50\u4e2d\u5206\u522b\u4f53\u73b0\u5e72\u7a7a\u6c14\u7684\u7113(\\(h_a\\))\u4e0e\u6e7f\u7a7a\u6c14\u7113(\\(dh_v\\)):</p> \\[     h = h_a + d h_v  \\] <p>\u8003\u8bd5\u5e38\u7528\\({h}_{kJ/kg} = 1.005{t}_{\u2103} + d (2501 + 1.86{t}_{\u2103}) kJ/kg(A)\\),\u6ce8\u610f\u4ee3\u5165\u8be5\u5f0f\u7684\u6e29\u5ea6\\(t\\)\u5355\u4f4d\u4e3a\u6444\u6c0f\u5ea6</p>"},{"location":"energy/Engineering_Thermodynamics/#\u7113\u6e7f\u56feh-s","title":"\u7113\u6e7f\u56fe\uff08h-s\uff09","text":"<p>\u6ce8\u610f:h-s\u56fe\u9700\u8981\u5728\u786e\u5b9a\u603b\u538b\u529b\\(p\\)\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u624d\u80fd\u753b\u51fa</p> <p>\u5728\u5206\u6790\u89e3\u9898\u601d\u8def\u65f6\uff0c\u501f\u52a9\u7113\u6e7f\u56fe\u53ef\u4ee5\u6e05\u695a\u5730\u8868\u793a\u6e7f\u7a7a\u6c14\u7684\u72b6\u6001\u53d8\u5316\uff0c\u662f\u7edd\u5bf9\u9700\u8981\u6e05\u695a\u638c\u63e1\u7684\u5185\u5bb9\uff08\u5bf9\u6e7f\u70ed\u6bd4\\(\\epsilon\\)\u7684\u8003\u5bdf\u76f8\u5bf9\u8f83\u5c11\uff09</p> <p></p> <p>\u4ece\u4e0a\u56fe\u53ef\u77e5\uff1a</p> <ul> <li> <p>\u5b9ad\u7ebf\u4e0e\u4e00\u4e2a\\(p_v\\)\u503c\u5bf9\u5e94\uff0c\u800c\\(t_d\\)\u4ec5\u786e\u5b9a\u4e0e\\(p_v\\),\u56e0\u6b64\u5b9a\u542b\u6e7f\u91cf\u7ebf\u4e0e\u5b9a\\(p_v\\)\u7ebf\u3001\u5b9a\u9732\u70b9\u6e29\u5ea6\\(t_d\\)\u7ebf\u76f8\u4e92\u5bf9\u5e94\uff1b</p> </li> <li> <p>\u5b9ah\u7ebf\u4e0e\\(\\phi=100%\\)\u7684\u4ea4\u70b9\u80fd\u786e\u5b9a\u8be5\u7113\u503c\u4e0b\u7684\u7edd\u70ed\u9971\u548c\u6e29\u5ea6\\(t_w\\)\uff0c\u56e0\u6b64\u5b9a\u7113\u7ebf\u4e0e\u5b9a\u7edd\u70ed\u9971\u548c\u6e29\u5ea6\\(t_w\\)\u7ebf\u4e00\u4e00\u5bf9\u5e94\uff1b</p> </li> </ul> <p>\u52a0\u70ed/\u51b7\u5374\u8fc7\u7a0b</p> <p> \u52a0\u70ed/\u51b7\u5374\u8fc7\u7a0b\u4e3a\u5b9a\u542b\u6e7f\u91cf\u8fc7\u7a0b\uff0c\u6cbf\u5b9ad\u7ebf\u8fdb\u884c\uff0c\u52a0\u70ed\u65f6\u671d\u7113\u589e\u65b9\u5411\u8fdb\u884c\uff08\u5411\u4e0a\uff09\uff0c\u53cd\u4e4b\u5411\u4e0b\uff1a <p></p> <p>\u5bf9\u8fd9\u4e2a\u8fc7\u7a0b\u52a0\u5165/\u653e\u51fa\u7684\u70ed\u91cf\u6709\uff1a</p> \\[     q = \\Delta h = h_2 - h_1 \\] <p></p> <p>\u7edd\u70ed\u52a0\u6e7f\u8fc7\u7a0b</p> <p> \u5728\u7edd\u70ed\u6761\u4ef6\u4e0b\u5411\u7a7a\u6c14\u52a0\u5165\u6c34\u5e76\u4e14\u84b8\u53d1\uff0c\u5438\u6536\u6c7d\u5316\u6f5c\u70ed\u4f7f\u5f97\u6e29\u5ea6\u964d\u4f4e\uff0c\u8be5\u8fc7\u7a0b\u53ef\u4ee5\u8fd1\u4f3c\u6cbf\u7740\u5b9ah\u7ebf\u8fdb\u884c\uff08\u5411\\(\\phi=100%\\)\u7ebf\u9760\u8fd1\uff09\uff1a <p></p> <p>\u5bf9\u8fd9\u4e2a\u8fc7\u7a0b\u7684\u80fd\u91cf\u5b88\u6052\u5f0f\u6709\uff1a</p> \\[     h_1 + (d_2 - d_1) h_w = h_2 \\] <p>\u5176\u4e2d\\(h_w\\)\u4e3a\u52a0\u5165\u65f6\u6e29\u5ea6\u7684\u6c34\u7113 </p> <p>\u51b7\u5374\u53bb\u6e7f\u8fc7\u7a0b</p> <p> \u6e7f\u7a7a\u6c14\u7ebf\u51b7\u5374\u5230\u9732\u70b9\\(T_d\\)\u540e\u7ee7\u7eed\u51b7\u5374\u6790\u51fa\u6c34\uff0c\u8be5\u8fc7\u7a0b\u5148\u6cbf\u7740\u5b9ad\u7ebf\u518d\u6cbf\u7740\\(\\phi=100%\\)\u7ebf\u5411\u4e0b\uff1a <p></p> <p>\u5bf9\u8fd9\u4e2a\u8fc7\u7a0b\u7684\u80fd\u91cf\u5b88\u6052\u5f0f\u6709\uff1a</p> \\[     q = h_1 - (h_2 + (d_1 - d_2)h_w) \\] <p>\u5176\u4e2d\\(h_w\\)\u4e3a\u8fc7\u7a0b\u7ec8\u70b9\u6e29\u5ea6\u7684\u6c34\u7113 </p> <p>\u5e72\u6e7f\u7403\u6e29\u5ea6\u8ba1\u6d4b\u76f8\u5bf9\u6e7f\u5ea6\\(\\phi\\)\u7684\u5de5\u4f5c\u539f\u7406</p> <p> \u5047\u8bbe\u4e00\u4e2a\u7edd\u70ed\u9971\u548c\u8fc7\u7a0b\uff0c\u901a\u8fc7\u52a0\u6e7f\u7531\u4e0d\u9971\u548c\u72b6\u60011\u2192\u9971\u548c\u72b6\u60012\uff0c\u6709\uff1a \\[     h_{a,1} + d_1 h_{v,1} + (d_2 - d_1)h_w = h_{a,2} + d_1 h_{v,2} \\] <p>\u6574\u7406\u53ef\u5f97\uff1a</p> \\[     d_1 = \\frac{(h_{a,2} - h_{a,1}) + d_2 (h_{v,2} - h_w)}{h_{v,1} - h_w} \\] <p>\u82e5\u52a0\u5165\u7684\u6c34\u5206\u6e29\u5ea6\u4e3a\\(T_2\\)\uff0ch_{v,2}\u4e3a\u9971\u548c\u6e29\u5ea6\\(T_2\\)\u4e0b\u9971\u548c\u84b8\u6c7d\u7684\u6bd4\u7113\uff0c\u5219\u7531\\(h_{v,2} - h_w = r\\)\u4e3a$T_2\u6e29\u5ea6\u4e0b\u7684\u6c7d\u5316\u6f5c\u70ed\uff0c\u901a\u8fc7\u4ee3\u6362\u53ef\u5f97\uff1a</p> \\[     d_1 = \\frac{c_{p,a}(T_{2} - T_{1}) + d_2 r}{h_{v,1} - h_w} \\] <p>\u7531\u4e8e\u6e7f\u7403\u6e29\u5ea6\u8ba1\u53ef\u6d4b\u51fa\u7edd\u70ed\u9971\u548c\u6e29\u5ea6\\(T_2\\)\uff0c\u53ef\u5f97\u51fa\u5bf9\u5e94\u7684\\(p_s{T_2}\\)\u4e0e\\(d_2\\)\u4ee5\u53ca\u9971\u548c\u6c34\u7113\\(h_w\\)\uff0c\u7531\\(T_1\\)\u53ef\u786e\u5b9a\\(h_{v,1}=2501+1.86t_1\\),\u56e0\u6b64\u786e\u5b9a\u4e86\u4e0a\u5f0f\u53f3\u4fa7\u7684\u5404\u4e2a\u91cf\uff0c\u6709\uff1a</p> \\[     0.622\\frac{\\phi p_s(T_1)}{p - \\phi p_s(T_1)} = \\frac{c_{p,a}(T_{2} - T_{1}) + d_2 r}{h_{v,1} - h_w} = f(T_1, T_2) \\] <p>\u6709\u4e0a\u5f0f\u6574\u7406\u53ef\u77e5\u76f8\u5bf9\u6e7f\u5ea6\\(\\phi = f(T_1, T_2)\\)\u53ef\u4ee5\u7531\u5e72\u7403\u6e29\u5ea6\\(T_1\\)\u4e0e\u6e7f\u7403\u6e29\u5ea6\\(T_2\\)\u786e\u5b9a\uff0c\u8fd9\u4fbf\u662f\u5e72\u6e7f\u7403\u6e29\u5ea6\u8ba1\u7684\u5de5\u4f5c\u539f\u7406\u3002</p> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u516b\u7ae0-\u7406\u60f3\u6c14\u4f53\u7684\u70ed\u529b\u8fc7\u7a0b\u5f85\u8865","title":"\u7b2c\u516b\u7ae0 \u7406\u60f3\u6c14\u4f53\u7684\u70ed\u529b\u8fc7\u7a0b(\u5f85\u8865)","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u56db\u4e2a\u7ecf\u5178\u70ed\u529b\u8fc7\u7a0b\u4ee5\u53ca\u5bf9\u5e94\u56fe\u7ebf\uff0c\u7406\u60f3\u6c14\u4f53\u591a\u53d8\u8fc7\u7a0b</p> <p>\ud83d\udd27\u8ba1\u7b97:\u70ed\u529b\u8fc7\u7a0b\u7684\u7113\u503c\u3001\u71b5\u3001\u70ed\u529b\u5b66\u80fd\u7684\u53d8\u5316\u91cf</p> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u4e5d\u7ae0-\u6c14\u4f53\u4e0e\u84b8\u6c7d\u7684\u6d41\u52a8\u5f85\u8865","title":"\u7b2c\u4e5d\u7ae0 \u6c14\u4f53\u4e0e\u84b8\u6c7d\u7684\u6d41\u52a8\uff08\u5f85\u8865\uff09","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:</p> <p>\ud83d\udd27\u8ba1\u7b97:</p> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u5341\u7ae0-\u6c14\u4f53\u7684\u538b\u7f29","title":"\u7b2c\u5341\u7ae0 \u6c14\u4f53\u7684\u538b\u7f29","text":"<p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u538b\u6c14\u673a\u7684\u5de5\u4f5c\u539f\u7406\u3001\u589e\u538b\u6bd4\\(\\pi\\)\u3001\u7edd\u70ed\u538b\u7f29\u3001\u591a\u53d8\u538b\u7f29\u3001\u5b9a\u6e29\u538b\u7f29\u7684\u7279\u70b9\uff0c\u4f59\u9699\u5bb9\u79ef</p> <p>\ud83d\udd27\u8ba1\u7b97:\u7406\u60f3\u6c14\u4f53\u538b\u7f29\u8fc7\u7a0b\u7684\u70ed\u529b\u5b66\u53c2\u6570\u5206\u6790\uff08\u7279\u70b9\u4e0e\u4f18\u52bf\uff1f\uff09\uff0c\u6700\u4f73\u4e2d\u95f4\u538b\u529b</p> <p></p>"},{"location":"energy/Engineering_Thermodynamics/#\u591a\u7ea7\u538b\u7f29\u4e2d\u95f4\u51b7\u5374","title":"\u591a\u7ea7\u538b\u7f29\uff0c\u4e2d\u95f4\u51b7\u5374","text":"<p>\u591a\u7ea7\u538b\u7f29\uff0c\u4e2d\u95f4\u51b7\u5374\u901a\u8fc7\u4e2d\u95f4\u51b7\u5374\u5668\u8fdb\u884c\u5b9a\u538b\u51b7\u5374\uff0c\u51b7\u5374\u5230\u8fdb\u6c14\u6e29\u5ea6\u540e\u518d\u9001\u5165\u9ad8\u538b\u6c14\u7f38\uff0c\u5230\u7ec8\u538b\u540e\u6392\u51fa\u6c14\u7f38\u3002</p> <p></p> <p>\u4ece\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\uff0c\u5f53\u521d\u7ec8\u6001\u538b\u529b\u76f8\u540c\u65f6\uff08\u5373\u589e\u538b\u6bd4\\(\\pi = \\frac{p_4}{p_1}\\)\u76f8\u540c\uff09,\u4e24\u7ea7\u538b\u6c14\u673a\u8017\u529f\u91cf\u66f4\u5c11\uff0c\u6240\u8282\u7701\u7684\u529f\u91cf\u7b49\u4e8e\u9762\u79ef2-3-4-5-2\uff0c\u5e76\u4e14\u4eceT-s\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u56e0\u4e3a\u591a\u4e86\u4e2d\u95f4\u51b7\u5374\u7684\u6b65\u9aa4\uff0c\u591a\u7ea7\u538b\u7f29\u7684\u7ec8\u6001\u6392\u6c14\u6e29\u5ea6\u5c06\u66f4\u5c0f\u3002\u8fdb\u4e00\u6b65\u589e\u52a0\u538b\u7f29\u7ea7\u6570\uff0c\u53ef\u4ee5\u4f7f\u8017\u529f\u91cf\u66f4\u52a0\u5c0f\uff0c\u5373\u4f7f\u5f97\u538b\u7f29\u8fc7\u7a0b\u66f4\u52a0\u63a5\u8fd1\u4e8e\u5b9a\u6e29\u538b\u7f29\u3002\u4f46\u82e5\u538b\u7f29\u7ea7\u6570\u8fc7\u591a\uff0c\u4f1a\u4f7f\u5f97\u8bbe\u5907\u8fc7\u4e8e\u590d\u6742\uff0c\u5e76\u4e14\u589e\u52a0\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u53ef\u9006\u635f\u8017\uff0c\u56e0\u6b64\u901a\u5e38\u4f7f\u75282-4\u7ea7\u538b\u7f29\u3002</p> <p>\u5982\u4e0a\u56fe\u4e8c\u7ea7\u538b\u7f29\u673a\uff0c\u6709\\(T_1=T_3, p_2=p_3\\),\u5373\\(p_1 v_1 = p_3 v_3\\)\uff0c\u603b\u8017\u529f\u91cf\u4e3a\uff1a</p> \\[     \\begin{aligned}     w_{t} =&amp; w_t^1 + w_t^2 = \\frac{\\gamma}{\\gamma - 1}p_1 v_1 [1- (\\frac{p_2}{p_1})^{\\frac{\\gamma-1}{\\gamma}}] + \\frac{\\gamma}{\\gamma - 1}p_3 v_3 [1- (\\frac{p_4}{p_3})^{\\frac{\\gamma-1}{\\gamma}}]   \\\\     =&amp; \\frac{\\gamma}{\\gamma - 1}p_1 v_1 [2- (\\frac{p_2}{p_1})^{\\frac{\\gamma-1}{\\gamma}} - (\\frac{p_4}{p_2})^{\\frac{\\gamma-1}{\\gamma}}]  \\\\     \\end{aligned} \\] <p>\u5728\u521d\u6001\\(p_1,v_1\\)\u4e00\u5b9a\uff0c\u7ec8\u6001\\(p_4\\)\u786e\u5b9a\u65f6\uff0c\u538b\u7f29\u8fc7\u7a0b\u7684\u8017\u529f\u91cf\u53d6\u51b3\u4e8e\u4e2d\u95f4\u538b\u529b\\(p_2\\)\uff0c\u56e0\u6b64\u9700\u8981\u5bfb\u627e\u4e00\u4e2a\u6700\u4f73\u4e2d\u95f4\u538b\u529b\u6765\u4f7f\u5f97\u8017\u529f\u91cf\u8fbe\u5230\u6700\u5c0f\uff0c\u53ef\u5f97\uff1a</p> \\[     \\begin{aligned}     \\frac{p_2}{p_1} =&amp; \\frac{p_4}{p_2} \\\\     \\pi_1 =&amp; \\pi_2 \\\\     p_2 =&amp; \\sqrt{p_1 p_4} \\\\     \\end{aligned} \\] <p>\u5373\u5f53\u5404\u7ea7\u589e\u538b\u6bd4\\(\\pi\\)\u76f8\u540c\u65f6\uff0c\u603b\u8017\u529f\u91cf\u8fbe\u5230\u6700\u5c0f,\\(w_t = 2 \\frac{\\gamma}{\\gamma - 1}p_1 v_1 [1- (\\frac{p_2}{p_1})^{\\frac{\\gamma-1}{\\gamma}}]\\)</p> <p>\u5bf9\u4e8e\u4e2d\u95f4\u538b\u7f29\u8fc7\u7a0b\u4e3a\u591a\u53d8\u8fc7\u7a0b\u7684\uff0c\u5219\u6709\uff1a</p> \\[     \\begin{aligned}     w_{t\uff0cn} =&amp; 2 \\frac{n}{n - 1}p_1 v_1 [1- (\\frac{p_2}{p_1})^{\\frac{n-1}{n}}] \\\\     q_{n} =&amp; 2 \\frac{n-\\gamma}{n-1} c_v (T_2 - T_1) \\\\     \\end{aligned} \\] <p>\u540c\u7406\u5bf9\u4e8ez\u7ea7\u538b\u7f29\u8fc7\u7a0b\uff0c\u6700\u4f73\u589e\u538b\u6bd4\\(\\pi_z = \\sqrt[z]{\\pi_{\\text{\u603b\u589e\u538b\u6bd4}}}\\)</p>"},{"location":"energy/Engineering_Thermodynamics/#\u538b\u6c14\u673a\u7edd\u70ed\u6548\u7387eta","title":"\u538b\u6c14\u673a\u7edd\u70ed\u6548\u7387\\(\\eta\\)  <p>\u5f85\u8865</p>","text":""},{"location":"energy/Engineering_Thermodynamics/#\u4f59\u9699\u5bb9\u79ef","title":"\u4f59\u9699\u5bb9\u79ef  <p>\u5f85\u8865</p>","text":""},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u5341\u4e00\u7ae0-\u84b8\u6c7d\u52a8\u529b\u5faa\u73af","title":"\u7b2c\u5341\u4e00\u7ae0 \u84b8\u6c7d\u52a8\u529b\u5faa\u73af   <p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u84b8\u6c7d\u5361\u8bfa\u5faa\u73af\u8fc7\u7a0b\uff0c\u6717\u80af\u5faa\u73af\uff08T-s\u56fe\uff09\uff0c\u518d\u70ed\u5faa\u73af\u3001\u56de\u70ed\u5faa\u73af\uff08\u4e24\u79cd\u4e0d\u540c\u7684\u56de\u70ed\u88c5\u7f6e\uff09\uff0c\u6df7\u5408\u5f0f\u5faa\u73af</p> <p>\ud83d\udd27\u8ba1\u7b97:\u6717\u80af\u5faa\u73af\uff0c\u518d\u70ed\u5faa\u73af\u3001\u56de\u70ed\u5faa\u73af\u8fc7\u7a0b\u5206\u6790</p> <p></p>","text":""},{"location":"energy/Engineering_Thermodynamics/#\u84b8\u6c7d\u5361\u8bfa\u5faa\u73af","title":"\u84b8\u6c7d\u5361\u8bfa\u5faa\u73af","text":"<p>\u84b8\u6c7d\u5361\u8bfa\u5faa\u73af\u5229\u7528\u4e86\u6e7f\u9971\u548c\u84b8\u6c7d\u5b9a\u6e29\u8fc7\u7a0b\u4e5f\u662f\u5b9a\u538b\u8fc7\u7a0b\u7684\u6027\u8d28\uff0c\u6765\u5b9e\u73b0\u5361\u8bfa\u5faa\u73af\u4e2d\u7684\u5b9a\u6e29\u5438\u3001\u653e\u70ed\u8fc7\u7a0b\uff1a</p> <p></p> <p>\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c3-4\u8fc7\u7a0b\u5bf9\u4f4e\u5e72\u5ea6\u7684\u6e7f\u84b8\u6c7d\u8fdb\u884c\u538b\u7f29\u7684\u8fc7\u7a0b\u96be\u4ee5\u5b9e\u73b0\uff0c\u5bf9\u6c34\u5206\u542b\u91cf\u8f83\u9ad8\u7684\u6df7\u5408\u7269\u8fdb\u884c\u538b\u7f29\u8017\u529f\u5de8\u5927\uff0c\u5e76\u4e14\u9020\u6210\u7684\u6db2\u51fb\u4f1a\u635f\u8017\u538b\u7f29\u673a\uff0c\u9020\u6210\u4e0d\u5229\u5f71\u54cd\u3002\u5e76\u4e14\u7531\u4e8e\u6c34\u7684\u4e34\u754c\u6e29\u5ea6\u8f83\u4f4e\uff0c\u4f7f\u5f97\u8fc7\u7a0b\u4e2d\u7684\u5e73\u5747\u5438\u70ed\u6e29\u5ea6\u8f83\u4f4e\uff0c\u56e0\u6b64\u84b8\u6c7d\u5361\u8bfa\u5faa\u73af\u7684\u70ed\u6548\u7387\u6709\u5de8\u5927\u7684\u9650\u5236</p>"},{"location":"energy/Engineering_Thermodynamics/#\u6717\u80af\u5faa\u73af","title":"\u6717\u80af\u5faa\u73af","text":"<p>\u6717\u80af\u5faa\u73af\u7531\u9505\u7089\uff08\u5b9a\u538b\u52a0\u70ed\uff09\u3001\u6c7d\u8f6e\u673a\uff08\u7edd\u70ed\u81a8\u80c0\uff09\u3001\u51b7\u51dd\u5668\uff08\u5b9a\u538b\u5438\u70ed\uff09\u3001\u6c34\u6cf5\uff08\u7edd\u70ed\u538b\u7f29\uff09\u56db\u4e2a\u90e8\u5206\u7ec4\u6210</p> <p></p> <p>\u5bf9\u6717\u80af\u5faa\u73af\u8fdb\u884c\u5206\u6790\uff1a</p> <p>\u5faa\u73af\u5438\u70ed\uff084 \u2192 1\uff09\uff1a\\(q_1 = h_1 - h_4\\)</p> <p>\u5faa\u73af\u653e\u70ed\uff082 \u2192 3\uff09\uff1a\\(q_2 = h_2 - h_3\\)</p> <p>\u5bf9\u5916\u505a\u529f\uff08\u6c7d\u8f6e\u673a\uff0c1 \u2192 2\uff09\uff1a \\(w_T = h_1 - h_2\\)</p> <p>\u8017\u529f\uff08\u6cf5\u529f\uff0c4 \u2192 3\uff09\uff1a \\(w_p = h_4 - h_3\\)</p> <p>\u7531\u4e0a\u53ef\u5f97\u6717\u80af\u5faa\u73af\u7684\u6548\u7387\uff1a</p> \\[     \\eta_t = \\frac{w_T - w_p}{q_1} = \\frac{h_1 - h_2 + h_3 - h_4}{h_1 - h_4} \\stackrel{\\text{\u5ffd\u7565\u6cf5\u529f}}{=} \\frac{h_1 - h_2}{h_1 - h_4} \\] <p>\u901a\u8fc7\u5bf9\u6717\u80af\u5faa\u73af\u7684\u5206\u6790\u53ef\u77e5\uff0c\u63d0\u9ad8\u521d\u6001\u538b\u529b\\(p_1\\),\u521d\u6e29\u5ea6\\(T_1\\)\uff0c\u80fd\u591f\u63d0\u9ad8\u5faa\u73af\u5e73\u5747\u5438\u70ed\u6e29\u5ea6\uff1b\u964d\u4f4e\u7ec8\u6001\u538b\u529b\\(p_2\\)\uff0c\u5219\u53ef\u4ee5\u964d\u4f4e\u5faa\u73af\u5e73\u5747\u653e\u70ed\u6e29\u5ea6\uff0c\u8fdb\u800c\u4f7f\u5f97\u5faa\u73af\u70ed\u6548\u7387\u63d0\u5347\u3002</p> <p>\u521d\u6e29\\(T_1\\)\u7684\u63d0\u9ad8\u53d7\u9650\u4e8e\u8bbe\u5907\u6750\u6599\u7684\u5141\u8bb8\u5de5\u4f5c\u6700\u9ad8\u6e29\u5ea6\uff0c\u800c\u521d\u6001\u538b\u529b\\(p_1\\)\u7684\u63d0\u9ad8\u4f1a\u964d\u4f4e\u6392\u6c7d\u5e72\u5ea6\uff0c\u9020\u6210\u6db2\u51fb\u6548\u5e94\u635f\u4f24\u6c7d\u8f6e\u673a\uff0c\u751a\u81f3\u5f71\u54cd\u5b89\u5168\uff0c\u56e0\u6b64\u53ef\u4ee5\u91c7\u7528'\u518d\u70ed'\u4e0e'\u56de\u70ed'\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5faa\u73af\u7684\u6548\u7387\u3002</p>"},{"location":"energy/Engineering_Thermodynamics/#\u518d\u70ed\u5faa\u73af","title":"\u518d\u70ed\u5faa\u73af","text":"<p>\u518d\u70ed\u5faa\u73af\u5728\u6717\u80af\u5faa\u574f\u7684\u57fa\u7840\u4e0a\uff0c\u5c06\u6c7d\u8f6e\u673a\u81a8\u80c0\u8fc7\u7a0b\u8fdb\u884c\u5230\u67d0\u4e00\u70b9\u65f6\uff0c\u5c06\u6c14\u4f53\u5bfc\u5165\u518d\u70ed\u5668\u8fdb\u884c\u518d\u6b21\u52a0\u70ed\uff08\u5b9a\u538b\u5347\u6e29\uff09\uff0c\u800c\u540e\u518d\u56de\u5230\u6c7d\u8f6e\u673a\u4e2d\u8fdb\u884c\u7edd\u70ed\u538b\u7f29\uff1a</p> <p></p> <p>\u5bf9\u518d\u70ed\u5faa\u73af\u7684\u5206\u6790\u4e0e\u6717\u80af\u5faa\u73af\u4e00\u81f4\uff0c\u4ec5\u5206\u522b\u591a\u4e86\u4e00\u6bb5\u52a0\u70ed\u4e0e\u4f5c\u529f\u8fc7\u7a0b\uff1a</p> <p>\u5faa\u73af\u5438\u70ed\uff1a\\(q_1 = (h_1 - h_4) + (h_A - h_B)\\)</p> <p>\u5faa\u73af\u653e\u70ed\uff1a\\(q_2 = h_2 - h_3\\)</p> <p>\u5bf9\u5916\u505a\u529f\uff1a \\(w_T = (h_1 - h_B) + (h_A + h_2)\\)</p> <p>\u53ef\u5f97\u518d\u70ed\u5faa\u73af\u7684\u6548\u7387\uff1a</p> \\[     \\eta_t = \\frac{w_T - w_p}{q_1} = \\frac{(h_1 - h_B) + (h_A - h_2)}{(h_1 - h_4) + (h_A - h_B)} \\] <p>\u901a\u8fc7\u518d\u70ed\u5668\uff0c\u5faa\u73af\u63d0\u9ad8\u4e86\u4e4f\u6c7d\u5e72\u5ea6\uff082\u72b6\u6001\u70b9\uff09\uff0c\u5e76\u4e14\u63d0\u9ad8\u4e86\u5e73\u5747\u52a0\u70ed\u6e29\u5ea6\uff0c\u4f7f\u5f97\u5faa\u73af\u70ed\u6548\u5e94\u63d0\u5347\u3002</p> <p>\u4f46\u518d\u70ed\u7684\u6b21\u6570\u4e0d\u5b9c\u8fc7\u591a\uff0c\u867d\u7136\u80fd\u591f\u63d0\u5347\u70ed\u6548\u7387\uff0c\u4f46\u540c\u65f6\u5927\u5927\u589e\u52a0\u4e86\u8bbe\u5907\u590d\u6742\u5ea6\uff0c\u8fd0\u884c\u7ba1\u7406\u96be\u5ea6\u589e\u52a0\uff0c\u4e5f\u589e\u52a0\u4e86\u6469\u64e6\u635f\u8017\uff0c\u53cd\u800c\u5e26\u6765\u8bf8\u591a\u4e0d\u5229\u3002\u56e0\u6b64\u5b9e\u9645\u8fc7\u7a0b\u4e2d\u4e00\u822c\u4e0d\u8d85\u8fc7\u4e24\u6b21\u518d\u70ed\u3002</p>"},{"location":"energy/Engineering_Thermodynamics/#\u56de\u70ed\u5faa\u73af","title":"\u56de\u70ed\u5faa\u73af","text":"<p>\u56de\u70ed\u5faa\u73af\u662f\u5728\u6717\u80af\u5faa\u574f\u7684\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u62bd\u6c7d\u56de\u70ed\u7684\u65b9\u6cd5\u52a0\u70ed\u7ed9\u6c34\uff0c\u589e\u5927\u5e73\u5747\u52a0\u70ed\u6e29\u5ea6\u6765\u63d0\u5347\u70ed\u6548\u7387\u3002</p> <p></p> <p>\u56de\u70ed\u52a0\u70ed\u88c5\u7f6e\u5206\u4e3a\u6df7\u5408\u5f0f\u4e0e\u8868\u9762\u5f0f\uff0c\u4e0a\u56fe\u4e3a\u6df7\u5408\u5f0f\u56de\u70ed\u5faa\u73af\u88c5\u7f6e\u793a\u610f\u56fe\uff0c\u6839\u636e\u80fd\u91cf\u5b88\u6052\uff1a</p> \\[     \\alpha h_{o1} + (1 - \\alpha)h_4 = h_{o1'} \\] <p>\u53ef\u5f97\u62bd\u6c7d\u7cfb\u6570\uff1a</p> \\[     \\alpha = \\frac{h_{o1'} - h_4}{h_{o1} - h_4} \\] <p>\u8fdb\u800c\u53ef\u5f97\u5faa\u73af\u4f5c\u529f\u4ee5\u53ca\u70ed\u6548\u7387\uff1a</p> \\[     \\begin{aligned}     w_t = h_1 - \\alpha h_{o1} - (1 - \\alpha) h_4 \\\\     \\eta_T = \\frac{h_1 - \\alpha h_{o1} - (1 - \\alpha) h_4}{h_1 - h_5} \\\\     \\end{aligned} \\] <p>\u53e6\u4e00\u79cd\u56de\u70ed\u65b9\u5f0f\u4e3a\u8868\u9762\u5f0f\uff0c\u5373\u62bd\u6c7d\u4e0d\u4e0e\u51b7\u51dd\u6c34\u76f4\u63a5\u63a5\u89e6\uff0c\u800c\u4ec5\u901a\u8fc7\u6362\u70ed\u5668\u4ea4\u6362\u70ed\u91cf,\u82e5\u5c06\u4e0a\u56fe\u88c5\u7f6e\u4e2d\u7684\u56de\u70ed\u5668\u6362\u6210\u8868\u9762\u5f0f\uff0c\u5219\uff1a</p> \\[     \\alpha h_{o1} + (1 - \\alpha)h_4 = \\alpha h_{o1'} + (1 - \\alpha)h_{o1'} \\] <p>\u770b\u4f3c\u4e0e\u4e0a\u5f0f\u6ca1\u6709\u533a\u522b\uff0c\u4f46\u56e0\u4e3a\u662f\u8868\u9762\u5f0f\uff0c\u5b9e\u9645\u4e0a\u5411\u540e\u8f93\u51fa\u7684\u53ea\u6709\\((1 - \\alpha)h_{o1'}\\)\u8fd9\u4e00\u90e8\u5206\uff0c\u53e6\u4e00\u90e8\u5206\\(\\alpha h_{o1'}\\)\u6d41\u56de\u51b7\u51dd\u5668\u3002</p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u5341\u4e8c\u7ae0-\u6c14\u4f53\u52a8\u529b\u5faa\u73af","title":"\u7b2c\u5341\u4e8c\u7ae0 \u6c14\u4f53\u52a8\u529b\u5faa\u73af   <p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u71c3\u6c14\u8f6e\u673a\u7684\u5e03\u83b1\u987f\u5faa\u73af\u8fc7\u7a0b\u5206\u6790</p> <p>\ud83d\udd27\u8ba1\u7b97:\u5b9a\u538b\u52a0\u70ed\u5faa\u73af\u3001\u5b9a\u5bb9\u52a0\u70ed\u5faa\u73af\u3001\u6df7\u5408\u52a0\u70ed\u5faa\u73af\u7684\u70ed\u6548\u7387\u4ee5\u53ca\u5bf9\u6bd4\u5206\u6790</p> <p></p>","text":""},{"location":"energy/Engineering_Thermodynamics/#\u5b9a\u5bb9\u52a0\u70ed\u5faa\u73afotto\u5faa\u73af","title":"\u5b9a\u5bb9\u52a0\u70ed\u5faa\u73af\uff08otto\u5faa\u73af\uff09","text":"<p>\u5f15\u5165\u538b\u7f29\u6bd4\\(\\epsilon\\): \u538b\u7f29\u524d\u7684\u6c14\u7f38\u5bb9\u79ef\u4e0e\u538b\u7f29\u540e\u7684\u6c14\u7f38\u5bb9\u79ef\u4e4b\u6bd4,\u5373\\(\\epsilon = \\frac{v_1}{v_2}\\)</p> <p>\u5faa\u73af\u5438\u70ed\u91cf\uff1a\\(q_1 = c_v(T_3 - T_2)\\)</p> <p>\u5faa\u73af\u653e\u70ed\u91cf\uff1a\\(q_2 = c_v(T_4 - T_1)\\)</p> <p>\u51c0\u529f: \\(w_net = q_1 - q_2\\)</p> <p>\u70ed\u6548\u7387\\(\\eta_t = 1 - \\frac{q_2}{q_1} = 1 - \\frac{T_4 - T_1}{T_3 - T_2} = 1 - \\frac{T_1}{T_2} = 1 - (\\frac{v_2}{v_1})^{\\gamma - 1} = 1 - \\frac{1}{\\epsilon^{\\gamma - 1}}\\)</p> <p>\u7531\u6b64\\(\\eta_t = 1 - \\frac{1}{\\epsilon^{\\gamma - 1}}\\)\u53ef\u77e5\u5b9a\u5bb9\u52a0\u70ed\u5faa\u73af\u7684\u6548\u7387\u53d6\u51b3\u4e8e\u538b\u7f29\u6bd4\\(\\epsilon\\)\uff0c\u968f\u7740\u538b\u7f29\u6bd4\u7684\u589e\u5927\u800c\u5347\u9ad8\u3002</p>"},{"location":"energy/Engineering_Thermodynamics/#\u5b9a\u538b\u52a0\u70ed\u5faa\u73af\u5e03\u96f7\u987f\u5faa\u73af","title":"\u5b9a\u538b\u52a0\u70ed\u5faa\u73af\uff08\u5e03\u96f7\u987f\u5faa\u73af\uff09","text":"<p>\u5f15\u5165\u589e\u538b\u6bd4\\(\\pi = \\frac{p_2}{p_1}\\)</p> <p>\u5faa\u73af\u5438\u70ed\u91cf\uff1a\\(q_1 = c_p(T_3 - T_2)\\)</p> <p>\u5faa\u73af\u653e\u70ed\u91cf\uff1a\\(q_2 = c_p(T_4 - T_1)\\)</p> <p>\u51c0\u529f: \\(w_net = q_1 - q_2\\)</p> <p>\u70ed\u6548\u7387\\(\\eta_t = 1 - \\frac{q_2}{q_1} = 1 - \\frac{T_4 - T_1}{T_3 - T_2} = 1 - \\frac{T_1}{T_2} = 1 - (\\frac{p_1}{p_2})^{\\frac{\\gamma - 1}{\\gamma}} = 1 - \\frac{1}{\\pi^{\\frac{\\gamma - 1}{\\gamma}}}\\)</p> <p>\u7531\u6b64\\(\\eta_t = 1 - \\frac{1}{\\pi^{\\frac{\\gamma - 1}{\\gamma}}}\\)\u53ef\u77e5\u5b9a\u538b\u52a0\u70ed\u5faa\u73af\u7684\u6548\u7387\u53d6\u51b3\u4e8e\u589e\u538b\u6bd4\\(\\pi\\)\uff0c\u968f\u7740\u589e\u538b\u6bd4\u7684\u589e\u5927\u800c\u5347\u9ad8\u3002</p>"},{"location":"energy/Engineering_Thermodynamics/#\u6df7\u5408\u52a0\u70ed\u7406\u60f3\u5faa\u73af","title":"\u6df7\u5408\u52a0\u70ed\u7406\u60f3\u5faa\u73af","text":"<p>\u5faa\u73af\u5438\u70ed\u91cf\uff1a\\(q_1 = c_v(T_3 - T_2) + c_p(T_4 - T_3)\\)</p> <p>\u5faa\u73af\u653e\u70ed\u91cf\uff1a\\(q_2 = c_v(T_5 - T_1)\\)</p> <p>\u6df7\u5408\u5faa\u73af\u70ed\u6548\u7387\uff1a\\(\\eta_t = 1 - \\frac{q_2}{q_1} = 1 - \\frac{T_5-T_1}{(T_3 - T_2) + \\gamma (T_4 - T_3)}  = 1 - \\frac{\\lambda \\rho^\\gamma - 1}{\\epsilon^{\\gamma-1} [(\\lambda-1) + \\gamma \\lambda (\\rho - 1)] }\\)</p> <p>\u53ef\u77e5\uff0c\u6df7\u5408\u52a0\u70ed\u7406\u60f3\u5faa\u73af\u70ed\u6548\u7387\u968f\u7740\u538b\u7f29\u6bd4\\(\\epsilon=\\frac{v_1}{v_2}\\)\u3001\u5b9a\u5bb9\u5347\u538b\u6bd4\\(\\lambda=\\frac{p_3}{p_2}\\)\u7684\u589e\u5927\u800c\u589e\u5927\uff0c\u968f\u7740\u9884\u80c0\u6bd4\\(\\rho=\\frac{v_4}{v_3}\\)\u7684\u589e\u5927\u800c\u51cf\u5c0f\u3002</p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u5341\u4e09\u7ae0-\u5236\u51b7\u5faa\u73af","title":"\u7b2c\u5341\u4e09\u7ae0 \u5236\u51b7\u5faa\u73af   <p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u5236\u51b7\u6548\u7387COP\u3001\u70ed\u6cf5\u6548\u7387\u3001\u5236\u51b7\u5faa\u73af\u5206\u6790</p> <p>\ud83d\udd27\u8ba1\u7b97:\u9006\u5361\u8bfa\u5faa\u73af\u3001\u7a7a\u6c14\u538b\u7f29\u5faa\u73af\u3001\u84b8\u6c7d\u538b\u7f29\u5faa\u73af\u7684\u8fc7\u7a0b\u5206\u6790\u4e0e\u6548\u7387\u6bd4\u8f83</p> <p></p>  <p>\u5236\u51b7\u5faa\u73af\u662f\u4e00\u79cd\u9006\u5411\u5faa\u73af\uff0c\u5176\u4f5c\u7528\u5728\u4e8e\u5c06\u4f4e\u6e29\u70ed\u6e90\u7684\u70ed\u91cf\u8f6c\u79fb\u5230\u9ad8\u6e29\u70ed\u6e90\u4e0a\u53bb\uff0c\u6839\u636e\u70ed\u529b\u5b66\u7b2c\u4e8c\u5b9a\u5f8b\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u9700\u8981\u989d\u5916\u8017\u8d39\u80fd\u91cf\u4f5c\u4e3a\u4ee3\u4ef7\u624d\u80fd\u8fdb\u884c\u3002</p> <ul> <li> <p>\u82e5\u5faa\u73af\u7684\u76ee\u7684\u5728\u4e8e\u4ece\u4f4e\u6e29\u7269\u4f53\uff08\u51b7\u5e93\uff09\u53d6\u51fa\u70ed\u91cf\uff0c\u5219\u4e3a\u5236\u51b7\u5faa\u73af  <li> <p>\u82e5\u5faa\u73af\u7684\u76ee\u7684\u5728\u4e8e\u5411\u9ad8\u6e29\u7269\u4f53\u63d0\u4f9b\u70ed\u91cf\uff08\u4f9b\u6696\uff09\uff0c\u5219\u4e3a\u70ed\u6cf5\u5faa\u73af   <p>\u4e3a\u4e86\u8861\u91cf\u5236\u51b7\u5faa\u73af/\u70ed\u6cf5\u5faa\u73af\u7684\u6027\u80fd\uff0c\u6211\u4eec\u4f7f\u7528COP\uff08Coefficient Of Performance\uff09\u6765\u8868\u793a\u3002\uff08\u8003\u8bd5\u4e3b\u8981\u8003\u5bdf\u5236\u51b7\u5faa\u73af\uff09</p>","text":""},{"location":"energy/Engineering_Thermodynamics/#\u9006\u5361\u8bfa\u5faa\u73af","title":"\u9006\u5361\u8bfa\u5faa\u73af","text":"<p>\u9006\u5361\u8bfa\u5faa\u73af\u662f\u540c\u6e29\u9650\u95f4\uff08\u73af\u5883\u6e29\u5ea6-\u51b7\u5e93\u6e29\u5ea6\uff09\u7684\u5236\u51b7\u7cfb\u6570\u6700\u9ad8\u7684\u5faa\u73af\uff0c\u901a\u5e38\u60c5\u51b5\u4e0b\u5236\u51b7\u5faa\u73af\u662f\u4ee5\u73af\u5883\u4f5c\u4e3a\u9ad8\u6e29\u70ed\u6e90(\\(T_1 = T_{\u2160}\\))\uff0c\u51b7\u5e93\u6e29\u5ea6\u4e3a\\(T_2=T_{\u2161}\\)\uff0c\u6613\u5f97\u9006\u5361\u8bfa\u5faa\u73af\u7684\u5236\u51b7\u7cfb\u6570\uff1a</p> \\[     \\epsilon = \\frac{T_2}{T_1 - T_2} = \\frac{T_{\u2161}}{T_{\u2160} - T_{\u2161}}   \\] <p>\u5373\u5728\u4e00\u5b9a\u73af\u5883\u6e29\u5ea6\u4e0b\uff0c\u51b7\u5e93\u6e29\u5ea6\\(T_{\u2161}\\)\u8d8a\u5927\uff0c\\(\\epsilon\\)\u8d8a\u5927;\u51b7\u5e93\u6e29\u5ea6\\(T_{\u2161}\\)\u8d8a\u5c0f\uff0c\u5219\\(\\epsilon\\)\u5c31\u8d8a\u5c0f\u3002</p>"},{"location":"energy/Engineering_Thermodynamics/#\u7a7a\u6c14\u538b\u7f29\u5236\u51b7\u5faa\u73af","title":"\u7a7a\u6c14\u538b\u7f29\u5236\u51b7\u5faa\u73af","text":"<p>\u7a7a\u6c14\u538b\u7f29\u5236\u51b7\u5faa\u73af\u53ef\u89c6\u4f5c\u5e03\u96f7\u987f\u5faa\u73af\uff08\u7b2c12\u7ae0\u5b9a\u538b\u52a0\u70ed\u5faa\u73af\uff09\u7684\u9006\u5faa\u73af</p> <p></p> <p>\u5faa\u73af\u4ece\u51b7\u5e93\u4e2d\u7684\u5438\u70ed\u91cf\uff1a\\(q_1 = c_p(T_1 - T_4)\\)</p> <p>\u5faa\u73af\u5411\u9ad8\u6e29\u70ed\u6e90\u7684\u653e\u70ed\u91cf\uff1a\\(q_2 = c_p(T_2 - T_3)\\)</p> <p>\u5faa\u73af\u6240\u8017\u7684\u51c0\u529f\uff1a \\(w_{net} = q_2 - q_1 = c_p(T_2 - T_3) - c_p(T_1 - T_4)\\)</p> <p>\u5219\u7a7a\u6c14\u538b\u7f29\u5faa\u73af\u7684\u5236\u51b7\u7cfb\u6570\uff1a</p> \\[     \\begin{aligned}     \\epsilon =&amp; \\frac{q_1}{w_{net}} = \\frac{T_1 - T_4}{(T_2 - T_3) - (T_1 - T_4)} \\\\     =&amp; \\frac{1}{\\frac{T_2 - T_3}{T_1 - T_4} - 1} = \\frac{1}{\\frac{T_2}{T_1} - 1} = \\frac{T_1}{T_2-T_1} = \\frac{1}{\\pi^{\\frac{\\gamma-1}{\\gamma}} - 1}\\\\     \\end{aligned} \\] <p>\u4e0a\u5f0f\u53ef\u77e5\uff0c\u589e\u538b\u6bd4\\(\\pi\\)\u8d8a\u5c0f\uff0c\u7a7a\u6c14\u538b\u7f29\u5faa\u73af\u5236\u51b7\u7cfb\u6570\u8d8a\u5927\uff0c\u4f46\u540c\u65f6\u5355\u4f4d\u8d28\u91cf\u7684\u5de5\u8d28\u5236\u51b7\u91cf\u4e5f\u51cf\u5c0f\uff0c\u56e0\u6b64\\(\\pi\\)\u4e0d\u80fd\u4e00\u5473\u51cf\u5c0f\u3002</p> <p>\u4e0e\u9006\u5361\u8bfa\u5faa\u73af\u5bf9\u6bd4\uff0c\u7a7a\u6c14\u538b\u7f29\u5236\u51b7\u5faa\u73af\u7684\\(\\epsilon = \\frac{T_1}{T_2-T_1}\\),\u800c\u8bbe\u5907\u51b7\u5e93\u80fd\u8fbe\u5230\u7684\u4f4e\u6e29\\(T_{\u2161} = T_1\\),\u73af\u5883\u6e29\u5ea6\u4e3a\\(T_{\u2160}=T_3\\),\u6709\\(\\epsilon_c = \\frac{T_{\u2161}}{T_{\u2160}-T_{\u2161}}\\)\uff0c</p> <p>\u540c\u6761\u4ef6\u7684\u9006\u5361\u8bfa\u5faa\u73af\u6548\u7387\u66f4\u9ad8\uff0c\\(\\epsilon_c &gt; \\epsilon\\)\uff08\u663e\u7136\uff09</p>"},{"location":"energy/Engineering_Thermodynamics/#\u84b8\u6c7d\u538b\u7f29\u5236\u51b7\u5faa\u73af","title":"\u84b8\u6c7d\u538b\u7f29\u5236\u51b7\u5faa\u73af","text":"<p>\u7a7a\u6c14\u5236\u51b7\u5faa\u73af\u7684\u7f3a\u9677</p> <p> <ul> <li> <p>\u5b9a\u538b\u5438\u653e\u70ed\u8fc7\u7a0b\u4e0e\u9006\u5361\u8bfa\u5faa\u73af\u7684\u5b9a\u6e29\u8fc7\u7a0b\u76f8\u5dee\u8f83\u5927\uff0c\u56e0\u800c\u964d\u4f4e\u4e86\u7ecf\u6d4e\u6027</p> </li> <li> <p>\u7a7a\u6c14\u4f5c\u4e3a\u5de5\u8d28\u7684\u6bd4\u5b9a\u538b\u70ed\u5bb9\u8f83\u5c0f\uff0c\u5faa\u73af\u7684\u5236\u51b7\u91cf\u4e5f\u8f83\u5c0f</p> </li> </ul> <p>\u56e0\u6b64\u4e3a\u4e86\u5b9e\u73b0\u5b9a\u6e29\u5438\u653e\u70ed\u8fc7\u7a0b\uff0c\u6211\u4eec\u5229\u7528\u6e7f\u84b8\u6c7d\u7684\u6027\u8d28\u6765\u5b8c\u6210\u5faa\u73af</p> <p></p> <p></p> <p>\u84b8\u6c7d\u538b\u7f29\u5236\u51b7\u88c5\u7f6e\u7531\u84b8\u53d1\u5668\u3001\u538b\u7f29\u673a\u3001\u51b7\u51dd\u5668\u4e0e\u8282\u6d41\u9600\uff08\u4ee3\u66ff\u81a8\u80c0\u673a\uff09\u7ec4\u6210\uff0c\u7531\u51b7\u51dd\u5668\u51fa\u6765\u7684\u9971\u548c\u6db2\u4f53\uff08\u72b6\u60011\uff09\u901a\u8fc7\u8282\u6d41\u9600\u8282\u6d41\u51cf\u538b\uff0c\u7531\u4e8e\u5728\u4e24\u76f8\u5171\u5b58\u533a\u7684\u7edd\u70ed\u8282\u6d41\u7cfb\u6570\\(\\mu_J &gt; 0\\)\uff0c\u4f7f\u5f97\u8282\u6d41\u540e\u7684\u6e29\u5ea6\u4e0b\u964d\u5e76\u4e14\u71b5\u589e\uff0c\u51fa\u6765\u7684\u4f4e\u5e72\u5ea6\u6e7f\u84b8\u6c7d\u8fdb\u5165\u84b8\u53d1\u5668\u5b9a\u538b\u5438\u70ed\uff0c\u5e72\u5ea6\u589e\u52a0\u6210\u4e3a\u9ad8\u5e72\u5ea6\u6e7f\u84b8\u6c7d\uff0c\u8fdb\u5165\u538b\u7f29\u673a\u4e2d\u8fdb\u884c\u7edd\u70ed\u538b\u7f29\uff0c\u6700\u540e\u5728\u51b7\u51dd\u5668\u4e2d\u653e\u70ed\u51dd\u7ed3\u4e3a\u9971\u548c\u6db2\u4f53\uff0c\u5b8c\u6210\u95ed\u73af\u3002</p> <p>\u84b8\u6c7d\u538b\u7f29\u5236\u51b7\u8fc7\u7a0b\u6bd4\u8f83\u7b80\u5355\uff0c\u538b\u7f29\u673a\u8017\u529f\u91cf\uff1a\\(w = h_4 - h_3\\)</p> <p>\u4ece\u51b7\u85cf\u5e93\u5438\u70ed\uff1a\\(q_2 = h_3 - h_2\\)</p> <p>\u53ef\u5f97\u84b8\u6c7d\u538b\u7f29\u5faa\u73af\u7684\u5236\u51b7\u7cfb\u6570:</p> \\[     \\epsilon = \\frac{q_2}{w} = \\frac{h_3 - h_2}{h_4 - h_3} \\] <p>\u8ba1\u7b97\u53ef\u77e5\uff0c\u84b8\u6c7d\u538b\u7f29\u5faa\u73af\u7684\u5236\u51b7\u7cfb\u6570\u6bd4\u7a7a\u6c14\u538b\u7f29\u5faa\u73af\u9ad8\u5f97\u591a\uff0c\u66f4\u52a0\u63a5\u8fd1\u9006\u5361\u8bfa\u5faa\u73af\u7684\u5236\u51b7\u7cfb\u6570</p> <p>\u4e3a\u4ec0\u4e48\u4f7f\u7528\u8282\u6d41\u9600\uff1f</p> <p> \u867d\u7136\u4f7f\u7528\u8282\u6d41\u9600\u4f1a\u635f\u5931\u4e00\u90e8\u5206\u7684\u5236\u51b7\u91cf\uff0c\u4f46\u8282\u7701\u4e86\u4e00\u53f0\u81a8\u80c0\u673a\u7684\u4f7f\u7528\uff0c\u66f4\u52a0\u7ecf\u6d4e\uff0c\u5e76\u4e14\u8282\u6d41\u9600\u8bbe\u5907\u7b80\u5355\u53ef\u9760\uff0c\u901a\u8fc7\u63a7\u5236\u5176\u5f00\u5ea6\u5c31\u80fd\u65b9\u4fbf\u5730\u8c03\u8282\u8282\u6d41\u540e\u7684\u6e29\u5ea6\u4e0e\u538b\u529b\uff0c\u4ee5\u5b9e\u73b0\u51b7\u85cf\u5e93\u6e29\u5ea6\u7684\u8fde\u7eed\u8c03\u8282 </p>"},{"location":"energy/Engineering_Thermodynamics/#\u7b2c\u5341\u56db\u7ae0-\u5316\u5b66\u53cd\u5e94\u7cfb\u7edf\u7684\u70ed\u529b\u5b66\u539f\u7406\u5f85\u8865","title":"\u7b2c\u5341\u56db\u7ae0 \u5316\u5b66\u53cd\u5e94\u7cfb\u7edf\u7684\u70ed\u529b\u5b66\u539f\u7406\uff08\u5f85\u8865\uff09   <p>\u77e5\u8bc6\u68b3\u7406</p> <p> <p>\ud83c\udf1f\u6982\u5ff5:\u5316\u5b66\u7ffb\u8bd1\u8fc7\u7a0b\u7684\u53cd\u5e94\u70ed\u3001\u70ed\u6548\u5e94\u3001\u70ed\u503c\u3001\u751f\u6210\u70ed\u7684\u5b9a\u4e49\u3002\u8d6b\u65af\u5b9a\u5f8b\u548c\u57fa\u5c14\u970d\u592b\u5b9a\u5f8b\u7684\u8868\u8ff0</p> <p>\ud83d\udd27\u8ba1\u7b97:\u6807\u51c6\u70ed\u6548\u5e94\u3001\u6807\u51c6\u71c3\u70e7\u7113\u3001\u6807\u51c6\u751f\u6210\u7113\u3001\u8d6b\u65af\u5b9a\u5f8b\u7684\u8fd0\u7528\u3001\u57fa\u5c14\u970d\u592b\u5b9a\u5f8b\u7684\u8fd0\u7528\uff0c\u7406\u8bba\u71c3\u70e7\u6e29\u5ea6</p> <p></p>  <p>\u5f85\u7eed\u672a\u5b8c.......\ud83d\ude1e</p>","text":""},{"location":"energy/guolu/","title":"\u80fd\u6e90\u8f6c\u5316\uff08\u9505\u7089\u539f\u7406\uff09","text":"\ud83c\udf07Information <p> <p></p>"},{"location":"energy/guolu/#\u7b2c\u4e8c\u7ae0-\u7164\u7c89\u5236\u5907","title":"\u7b2c\u4e8c\u7ae0-\u7164\u7c89\u5236\u5907","text":""},{"location":"energy/guolu/#\u7164\u7c89\u7684\u6027\u8d28","title":"\u7164\u7c89\u7684\u6027\u8d28","text":"<p>\u7164\u78e8\u6210\u7c89\u540e\u7684\u6027\u8d28\u4f1a\u4e0e\u539f\u7164\u4ea7\u751f\u8f83\u591a\u4e0d\u540c</p> <p>\u7164\u7c89\u7684\u5c3a\u5bf8\uff1a\u7164\u7c89\u662f\u5fae\u5c0f\u9897\u7c92\uff0c\u6700\u5927\u7c92\u5f84\u5f88\u5c11\u8d85\u8fc7\\(250 - 500 \\mu m\\)\uff0c\u5e38\u89c1\u5c3a\u5bf8\u5728\\(20-60 \\mu m\\)\u3002\u56e0\u4e3a\u9897\u7c92\u5c0f\uff0c\u6bd4\u8868\u9762\u79ef\u5927\u3002</p> <p>\u5e72\u6e7f\u7164\u7c89\uff1a</p> <ul> <li> <p>\u5e72\u7164\u7c89\u53ef\u4ee5\u5438\u9644\u5927\u91cf\u7a7a\u6c14\uff0c\u4e14\u50cf\u6d41\u4f53\u4e00\u6837\u5177\u6709\u826f\u597d\u7684\u6d41\u52a8\u6027\u4e0e\u5f88\u5c0f\u7684\u5806\u79ef\u89d2\\(\\rightarrow\\)\u65b9\u4fbf\u7ba1\u5185\u8fd0\u8f93\uff0c\u4f46\u4e5f\u5bb9\u6613\u6cc4\u9732\uff08\u6c61\u67d3\uff09</p> </li> <li> <p>\u6e7f\u7164\u7c89\u6d41\u52a8\u6027\u5dee\uff0c\u5806\u79ef\u89d2\u5927\\(\\rightarrow\\)\u8f93\u9001\u56f0\u96be\uff0c\u4ea7\u751f\u7164\u7c89\u642d\u6865\u73b0\u8c61</p> </li> </ul> <p>\u7164\u7c89\u7206\u70b8\u6027\uff1a</p> <p>\u5438\u9644\u5927\u91cf\u7a7a\u6c14\u7684\u7164\u7c89\u4f1a\u7f13\u6162\u6c27\u5316\u800c\u79ef\u84c4\u70ed\u91cf\uff0c\u5347\u9ad8\u6e29\u5ea6\u5230\u4e00\u5b9a\u7a0b\u5ea6\u540e\u5f15\u8d77\u7164\u7c89\u7684\u81ea\u71c3\uff0c\u4e14\u4e0e\u7a7a\u6c14\u6df7\u5408\u7269\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u4f1a\u7206\u71c3/\u7206\u70b8\uff0c\u4e3b\u8981\u6709\u4ee5\u4e0b\u5f71\u54cd\u56e0\u7d20\uff1a</p> <ol> <li>\u7164\u7684\u6325\u53d1\u5206\u542b\u91cf\uff0c\u5f71\u54cd\u5f88\u5927\uff0c\u6325\u53d1\u5206\u8d8a\u9ad8\u8d8a\u6613\u7206\u70b8</li> </ol> <p>10% ~ 14%\u65f6\uff0c\u4e0d\u4f1a\u7206\u70b8\uff1b</p> <ol> <li> <p>\u7164\u7c89\u6d53\u5ea6\u8d8a\u9ad8\uff0c\u7206\u70b8\u6027\u8d8a\u5927</p> </li> <li> <p>\u7164\u7c89\u9897\u7c92\u8d8a\u7c97\uff0c\u6bd4\u8868\u9762\u79ef\u8d8a\u5c0f\uff0c\u7206\u70b8\u6027\u8d8a\u5c0f</p> </li> </ol> <p>\\(d_p \\geq 200 \\mu m\\)\u7684\u9897\u7c92\u4e0d\u4f1a\u7206\u70b8</p> <ol> <li> <p>\u7164\u7c89\u4e2d\u7070\u5206\u542b\u91cf\u8d8a\u5927\uff0c\u7206\u70b8\u6027\u8d8a\u5c0f\uff08\u4e0e\u6325\u53d1\u5206\u76f8\u53cd\uff09</p> </li> <li> <p>\u6c27\u6c14\u542b\u91cf\u8d8a\u5927\uff0c\u7164\u7c89\u7684\u7206\u70b8\u6027\u8d8a\u5927</p> </li> <li> <p>\u7164\u7c89\u5904\u5728\u7684\u73af\u5883\u6e29\u5ea6\u8d8a\u9ad8\uff0c\u7206\u70b8\u6027\u8d8a\u5927</p> </li> </ol> <p>\u7164\u7c89\u7684\u9644\u7740\u6027\u4e0e\u56e2\u805a\u6027\uff1a</p> <p>\u5bf9\u4e8e\u9644\u7740\u6027\uff1a\u629b\u5149 &lt; \u7c97\u7cd9\uff0c\u9632\u6c34 &lt; \u4e0d\u9632\u6c34\uff0c\u7eaf\u91d1\u5c5e &lt; \u5851\u6599/\u6d82\u5c42\uff0c\u5bfc\u7535\u6027\u9ad8 &lt; \u5bfc\u7535\u6027\u4f4e\u3002</p> <p>\u56e2\u805a\u6027\uff1a\u7164\u7c89\u9897\u7c92\u7684\u4f4e\u632f\u5e45\u632f\u8361\u4f7f\u5f97\u5fae\u5c0f\u9897\u7c92\u6e17\u900f\u5230\u5927\u9897\u7c92\u7684\u7f1d\u9699\u4e2d\uff0c\u9020\u6210\u56e2\u805a\u7ed3\u5757\uff0c\u589e\u52a0\u9644\u7740\u6027</p>"},{"location":"energy/guolu/#\u7164\u7c89\u7ec6\u5ea6\u4ee5\u53ca\u9897\u7c92\u6027\u8d28","title":"\u7164\u7c89\u7ec6\u5ea6\u4ee5\u53ca\u9897\u7c92\u6027\u8d28","text":"<p>\u7164\u7c89\u9897\u7c92\u5c3a\u5bf8\uff1a\u9897\u7c92\u80fd\u591f\u901a\u8fc7\u7684\u6700\u5c0f\u7b5b\u5b54\u7684\u5c3a\u5bf8\uff0c\u79f0\u4e3a\u7164\u7c89\u7c92\u5b50\u7684\u76f4\u5f84</p> <p>\u7164\u7c89\u7ec6\u5ea6\uff1a\u4e00\u5b9a\u8d28\u91cf\u7684\u7164\u7c89\u901a\u8fc7\u4e00\u5b9a\u5c3a\u5bf8\u7684\u7b5b\u5b54\u540e\uff0c\u7b5b\u5b50\u4e0a\u7684\u5269\u4f59\u91cf\u5360\u7b5b\u5206\u7164\u7c89\u603b\u91cf\u7684\u767e\u5206\u6bd4\uff1a$$ R_x = \\frac{a}{a+b}\\(\\(\u5176\u4e2d\\)x\\)\u4e3a\u7b5b\u5b50\u89c4\u683c\uff08\u7b5b\u53f7\uff1a\u6bcf\\(cm\\)\u7684\u5b54\u6570/\u5b54\u7684\u5185\u8fb9\u957f(\\(\\mu m\\))\uff0c\u5982\\(R_{750}\\)\u5373\u4e3a\\(750 \\mu m\\)\u5b54\u5f84\uff08\u5185\u8fb9\u957f\uff09\u7684\u7b5b\u5b50\uff09</p> <p>\u6b27\u7f8e\u56fd\u5bb6\u4f7f\u7528\\(D_x = 1 - R_x = \\frac{b}{a+b}\\)\u4f5c\u4e3a\u7164\u7c89\u7ec6\u5ea6</p> <p>\u901a\u5e38\u5bf9\u7164\u7c89\u4f7f\u75284-5\u4e2a\u7b5b\u5b50\u8fdb\u884c\u7b5b\u5206\uff0c\u82e5\u7b5b\u5b54\u4e0a\u5269\u4f59\u7164\u7c89\u8d8a\u591a\\(R_x\\)\u8d8a\u5927\uff08\\(D_x\\)\u8d8a\u5c0f\uff09\uff0c\u5219\u8868\u793a\u7164\u7c89\u8d8a\u7c97\uff0c</p> <p>\u7535\u5382\u5bf9\u70df\u7164\uff0c\u65e0\u70df\u7164\u7684\u7164\u7c89\u5e38\u752830\u53f7\\(R_{200}\\)\uff0c70\u53f7\\(R_{90}\\)\u7b5b\u5b50\u3002</p> <p>\u5168\u7b5b\u5206\u66f2\u7ebf\\(R_x = f(xx)\\)\u53ef\u76f4\u89c2\u8868\u793a\u7164\u7c89\u7684\u7c97\u7ec6\uff0c\u66f2\u7ebf\u5728\u4e0a\u65b9\u5219\u7164\u7c89\u8f83\u7c97\uff0c\u5728\u4e0b\u65b9\u5219\u7164\u7c89\u8f83\u7ec6</p> <p>Rosin-Rammler\u7834\u788e\u516c\u5f0f:</p> \\[ R_x = 100 e^{-b x^n} \\] <p>\u5176\u4e2d\uff0c\\(n\\)\u4e3a\u7164\u7c89\u5747\u5300\u6027\u7684\u6307\u6570\uff0c\\(b\\)\u662f\u8868\u793a\u7164\u7c89\u7ec6\u5ea6\u7684\u7cfb\u6570\u3002</p> <ul> <li> <p>\u5728\u4e00\u5b9a\u7684n\u503c\u4e0b\uff0cb\u503c\u8d8a\u5927\uff0c\\(R_x\\)\u8d8a\u5c0f\uff0c\u7164\u7c89\u8d8a\u7ec6\uff1b</p> </li> <li> <p>\u5747\u5300\u6307\u6570n\u503c\u4e0e\u78e8\u7164\u673a\u4e0e\u7c97\u7ec6\u5206\u79bb\u673a\u7684\u578b\u5f0f\u6709\u5173\uff0cn\u8d8a\u5927\uff0c\u7164\u7c89\u9897\u7c92\u7684\u7ec4\u6210\u8d8a\u5747\u5300\uff0c\u5373\u8fc7\u7c97\uff0c\u8fc7\u7ec6\u7684\u9897\u7c92\u8d8a\u5c11\uff1b</p> </li> </ul> <p>\u79bb\u5fc3\u5f0f\u5206\u79bb\u5668\uff0cn=1.0 ~ 1.1; \u65cb\u8f6c\u5f0f\u5206\u79bb\u5668\uff0cn=1.1 ~ 1.2</p> <ul> <li>\u8fc7\u7c97\u7684\u7164\u7c89\u5927\u9897\u7c92\u4f1a\u589e\u52a0\u56fa\u4f53\u672a\u71c3\u70e7\u635f\u5931\uff0c\u867d\u7136\u8bbe\u5907\u80fd\u8017\\(E_m\\)\u5c0f\uff0c\u4f46\u4e0d\u5b8c\u5168\u70ed\u635f\u5931\\(q_4\\)\u8f83\u5927\uff1b\u8fc7\u7ec6\u5219\u4f1a\u589e\u52a0\u7535\u8017\u548c\u8bbe\u5907\u78e8\u635f\uff0c\u7164\u7c89\u8d8a\u7ec6\u5219\u8017\u80fd\\(E_m\\)\u8d8a\u5927\uff0c\u4f46\u7ec6\u7164\u7c89\u7740\u706b\u5feb\uff0c\u5bb9\u6613\u71c3\u5c3d\uff0c\u71c3\u70e7\u4e0d\u5b8c\u5168\u70ed\u635f\u5931\\(q_4\\)\u8f83\u5c0f\u3002</li> </ul> <p>\u56e0\u6b64\uff0c\u9700\u8981\u4f7f\u5f97\u7ec6\u5ea6\u9002\u5f53\uff0c\u8ba9\\((E_m + q_4)_{min}\\)\u8fbe\u5230\u6700\u5c0f\u503c\uff0c\u79f0\u4e3a\u6700\u4f73\u7ec6\u5ea6/\u7ecf\u6d4e\u7ec6\u5ea6\\(R^{opt}_x\\)\u3002</p> <p>\u6839\u636e\u7164\u7c89\u7684\u7c92\u5f84\uff1a</p> \\[ y= \\frac{-d R_x}{dx} = R_x b n x^{n-1} \\] <p>\u5f53\\(n &gt; 1\\)\u65f6\uff0c\u5728\\(x:15-25 \\mu m\\)\u5b58\u5728\u6700\u5927\u503c\uff0c\u56e0\u6b64\u5176\u4e2d\u7684\u7ec6\u7c89\u4e0e\u7c97\u7c89\u6240\u5360\u8f83\u5c0f\u3002\\(n \\leq 1\\)\u65f6\uff0c\u7ec6\u7164\u7c89\u542b\u91cf\u8f83\u591a\uff0c\u4e14\u968f\u7740n\u7684\u51cf\u5c0f\u800c\u8d8a\u6765\u8d8a\u591a\uff0c\u4e5f\u542b\u6709\u8f83\u591a\u7684\u7c97\u7c89\uff0c\u4f7f\u5f97\u5206\u5e03\u4e25\u91cd\u4e0d\u5747\u5300\u3002</p>"},{"location":"energy/guolu/#\u7164\u7c89\u7684\u53ef\u78e8\u6027","title":"\u7164\u7c89\u7684\u53ef\u78e8\u6027","text":"<p>\u7164\u7c89\u7684\u53ef\u78e8\u6027\u6307\u7684\u662f\u7164\u88ab\u7c89\u788e\u7814\u78e8\u7684\u96be\u6613\u7a0b\u5ea6\u7684\u7279\u5f81\u3002\u96be\u78e8\u7684\u7164\u76f8\u5bf9\u8017\u80fd\u5927</p> <p>\u4fc4\u7f57\u65af\uff1a\u53ef\u78e8\u6027\u7cfb\u6570\\(K_{gr}\\)\uff1a</p> \\[ K_{gr} = \\frac{E_s}{E} \\] <p>\u4f7f\u7528\u76f8\u540c\u8d28\u91cf\uff0c\u76f8\u540c\u521d\u59cb\u7c92\u5ea6\u7684\u6807\u51c6\u7164\u6837\uff08\u96be\u78e8\u7684\u65e0\u70df\u7164\uff09\u4e0e\u6d4b\u8bd5\u7164\u6837\u5728\u7403\u78e8\u673a\u4e2d\u78e8\u5236\u6210\u7ec6\u5ea6\u76f8\u540c\u7684\u7164\u7c89\uff0c\u4e24\u8005\u6240\u8017\u7684\u80fd\u91cf\\(E_s, \\ E\\)\u5373\u4e3a\u53ef\u78e8\u6027\u7cfb\u6570\u3002\uff08\u4e00\u822c\\(K_{gr} &gt; 1\\)\uff09</p> <p>\u6b27\u7f8e\uff1a\u54c8\u6c0f\u53ef\u78e8\u6027\u7cfb\u6570\\(\\text{HGI}\\):</p> \\[ \\text{HGI} = 13 + 6.93D_{75} \\] <p>\u91c7\u752850g\u89c4\u5b9a\u7c92\u5ea6\u7684\u7164\u6837\u5728\u4e2d\u901f\u78e8\u7164\u673a\u4e2d\u78e8\u52363min\u540e\u53d6\u51fa\uff0c\u7528\\(75\\mu m\\)\u5b54\u5f84\u7684\u7b5b\u5b50\u6765\u7b5b\u5206\uff0c\u901a\u8fc7\u5f97\u5230\u7684\\(D_{75}\\)\u6765\u8ba1\u7b97\u3002</p> <p>\\(HGI &lt; 60 (K_{gr} &lt; 1.2)\\)\u5c5e\u4e8e\u96be\u78e8\u7164\uff1b\\(60 \\leq HGI &lt; 80 (1.2 \\leq K_{gr} &lt; 1.5)\\)\u5c5e\u4e8e\u4e2d\u7b49\u53ef\u78e8\u7164\uff1b\\(HGI \\geq 80 (K_{gr} \\geq 1.5)\\)\u5c5e\u4e8e\u6613\u78e8\u7164\u3002</p> <p>\u7531\u4e8e\u7164\u4e2d\u542b\u6709\u7684\u786c\u8d28\u6210\u5206\uff0c\u4f1a\u5bf9\u8bbe\u5907\u9020\u6210\u78e8\u635f\uff0c\uff0c\u5206\u4e3a\u7814\u78e8\u5f0f\\(AI\\)\u4e0e\u51b2\u5237\u5f0f\u6307\u6570\\(K_e\\)\uff0c\u6211\u56fd\u91c7\u7528\u51b2\u5237\u5f0f\u78e8\u635f\u6307\u6570\\(K_e\\)\u8868\u793a\uff08\\(K_e\\)\u8d8a\u5927\u5219\u78e8\u635f\u8d8a\u5927\uff09\u3002</p> <p>\u51b2\u5237\u5f0f\u78e8\u635f\u6307\u6570\\(K_e\\)\uff1a\u5728\u9ad8\u901f\u55b7\u5c04\u7164\u7c89\u6d41\u5bf9\u91d1\u5c5e\u8bd5\u7247\u78e8\u635f\u6d4b\u8bd5\u4eea\u4e2d\uff0c\u6d4b\u8bd5\u7164\u6837\u5728\u4e00\u5b9a\u65f6\u95f4\u5185\u7684\u91d1\u5c5e\u78e8\u635f\u91cf\u4e0e\u76f8\u540c\u65f6\u95f4\u5185\u6d4b\u8bd5\u7164\u6837\uff08\u78e8\u635f\u91cf10mg/min\uff09\u7684\u78e8\u635f\u91cf\u4e4b\u6bd4\\(\\frac{m}{m_s}\\)</p>"},{"location":"energy/guolu/#\u7164\u7c89\u7684\u6c34\u5206","title":"\u7164\u7c89\u7684\u6c34\u5206","text":"<p>\u7164\u7c89\u6c34\u5206\u5bf9\u7164\u7c89\u7684\u4f9b\u5e94\u8fde\u7eed\u6027\uff0c\u5747\u5300\u6027\uff0c\u71c3\u70e7\u7ecf\u6d4e\u6027\uff0c\u8bbe\u5907\u8017\u80fd\u4ee5\u53ca\u5de5\u4f5c\u5b89\u5168\u7b49\u90fd\u6709\u5f88\u5927\u5f71\u54cd\uff1a</p> <ul> <li> <p>\u6c34\u5206\u8fc7\u9ad8\uff0c\u6709\u5bb3\u4e8e\u7164\u7c89\u7684\u8f93\u9001\uff0c\u63a8\u8fdf\u7164\u7c89\u7684\u7740\u706b\u4e0e\u71c3\u70e7\u3002\uff08+\u5e72\u71e5\u5242\uff09</p> </li> <li> <p>\u6c34\u5206\u8fc7\u4f4e\uff08\u7164\u7c89\u8fc7\u5e72\uff09\uff0c\u5bf9\u4f1a\u81ea\u71c3\u4e0e\u7206\u70b8\u6027\u7684\u70df\u7164\uff0c\u8910\u7164\u5bb9\u6613\u5f15\u8d77\u7206\u70b8</p> </li> </ul> <p>\u7164\u7c89\u78e8\u5236\u540e\u7684\u6700\u7ec8\u6c34\u5206\\(M_{pc}\\)\u6709\u8981\u6c42\uff0c\u4e0e\u7164\u7684\u5168\u6c34\u5206\u548c\u5de5\u4f5c\u6e29\u5ea6\u6709\u5173\uff1a</p> \\[ M_{pc} = (0.5-1.0) M_{ad} \\] <p>\u5176\u4e2d\\(M_{ad}\\)\u4e3a\u7a7a\u6c14\u5e72\u71e5\u57fa\u6c34\u5206</p>"},{"location":"energy/guolu/#\u7b52\u5f0f\u78e8\u7164\u673a","title":"\u7b52\u5f0f\u78e8\u7164\u673a","text":"<p>\u78e8\u7164\u673a\u662f\u5236\u7c89\u7cfb\u7edf\u7684\u4e3b\u8981\u8bbe\u5907\uff0c\u4f5c\u7528\u662f\u5c06\u7164\u5757\u78e8\u5236\u6210\u7164\u7c89\uff0c\u5e76\u4e14\u8fdb\u884c\u5e72\u71e5\u3002</p> <ul> <li> <p>\u4f4e\u901f\u673a\uff1a15~25r/min\uff0c\u5982\u7403\u78e8\u673a</p> </li> <li> <p>\u4e2d\u901f\u673a\uff1a25~100r/min\uff0c\u5982\u8f6e\u5f0f\u78e8\u7164\u673a\uff0c\u7897\u5f0f\u78e8\u7164\u673a\uff0c\u73af\u7403\u5f0f\u7403\u78e8\u673a</p> </li> <li> <p>\u9ad8\u901f\u673a\uff1a425~1000r/min\uff0c\u5982\u98ce\u6247\u78e8\u7164\u673a</p> </li> </ul> <p>\u56fd\u5185\u7535\u5382\u591a\u7528\u7b52\u5f0f\u94a2\u7403\u78e8\u7164\u673a\uff08\u7403\u78e8\u673a\uff09\u548c\u4e2d\u901f\u78e8\u7164\u673a\u3002</p> <p>\u7b52\u5f0f\u94a2\u7403\u78e8\u7164\u673a\u5206\u4e3a\u5355\u8fdb\u5355\u51fa\u4e0e\u53cc\u8fdb\u53cc\u51fa\u4e24\u79cd\u5f62\u5f0f\u3002</p>"},{"location":"energy/guolu/#\u5355\u8fdb\u5355\u51fa\u7403\u78e8\u673a","title":"\u5355\u8fdb\u5355\u51fa\u7403\u78e8\u673a","text":"<p>\u5185\u90e8\u88c5\u6709\u5927\u91cf\u76f4\u5f8425~60mm\u94a2\u7403\uff0c\u5185\u58c1\u4e3a\u6ce2\u6d6a\u5f62\u9530\u94a2\u88c5\u7532\uff0c\u4e00\u7aef\u8f93\u5165\u539f\u7164\u548c\u70ed\u7a7a\u6c14\uff0c\u53e6\u4e00\u7aef\u8f93\u51fa\u7164\u7c89\u4e0e\u5e72\u71e5\u8f93\u9001\u4ecb\u8d28\uff08\u6c14\u7c89\u6df7\u5408\u7269\uff09\u3002</p> <p>\u5355\u8fdb\u5355\u51fa\u7403\u78e8\u673a\u5de5\u4f5c\u539f\u7406\uff1a</p> <p>\u5728\u7535\u673a\u9a71\u52a8\u4e0b\uff0c\u62a4\u7532\u5c06\u94a2\u7403\u4e0e\u7164\u7c92\u63d0\u5347\u5230\u4e00\u5b9a\u9ad8\u5ea6\uff0c\u800c\u540e\u81ea\u7531\u4e0b\u843d\uff0c\u7164\u53d7\u5230\u94a2\u7403\u649e\u51fb\uff0c\u94a2\u7403\u4e0e\u94a2\u7403\u3001\u62a4\u7532\u4e4b\u95f4\u7684\u6324\u538b\u4e0e\u7814\u78e8\u800c\u7c89\u788e\u78e8\u5236\u6210\u7164\u7c89\u3002\u800c\u8f93\u5165\u7684\u70ed\u7a7a\u6c14\u8d77\u5230\u4e86\u5e72\u71e5\u5242\u4ee5\u53ca\u8f93\u9001\u4ecb\u8d28\u7684\u4f5c\u7528\uff0c\u5c06\u7164\u7c89\u5e26\u51fa\u7b52\u4f53\u3002\u6c14\u6d41\u901f\u5ea6\u5728\u7b52\u5185\u4e00\u822c\u4e3a1~3m/s\uff0c\u901f\u5ea6\u8d8a\u5feb\uff0c\u5e26\u51fa\u7684\u7164\u7c89\u91cf\u8d8a\u5927\uff08\u4f46\u5e26\u51fa\u7684\u7164\u7c89\u8d8a\u7c97\uff09\uff0c\u78e8\u7164\u673a\u51fa\u529b\u8d8a\u5927\u3002</p> <p>\u5355\u8fdb\u5355\u51fa\u7403\u78e8\u673a\u7684\u4f18\u70b9\uff1a</p> <ul> <li> <p>\u7164\u79cd\u9002\u5e94\u6027\u5e7f\uff0c\u53ef\u9760\u6027\u9ad8\uff0c\u7ef4\u62a4\u65b9\u4fbf\uff0c\u5bf9\u6742\u8d28\u4e0d\u654f\u611f\uff0c\u80fd\u78e8\u5236\u7684\u7ec6\u5ea6\u597d\u3002</p> </li> <li> <p>\u7535\u8017\u8fc7\u9ad8\uff0c\u4f46\u4f4e\u8d1f\u8377\u8fd0\u884c\u66f4\u4e0d\u7ecf\u6d4e\uff0c\u566a\u58f0\u5927\uff08\u78b0\u649e\uff09\uff0c\u8bbe\u5907\u5927\uff0c\u521d\u59cb\u6295\u8d44\u9ad8\uff0c\u7164\u7c89\u5747\u5300\u6027\u5dee\uff08n\u503c\u5c0f\uff09</p> </li> </ul> <p>\u5de5\u4f5c\u8f6c\u901f\\(n\\)</p> <ul> <li> <p>\u8f6c\u901f\u8f83\u4f4e\u65f6\uff0c\u94a2\u7403\u4e0d\u80fd\u88ab\u63d0\u5347\u5230\u5e94\u6709\u7684\u9ad8\u5ea6\uff0c\u7834\u788e\u529f\u80fd\u5f88\u5c0f\u3002\u4e14\u7164\u7c89\u88ab\u94a2\u7403\u538b\u4f4f\uff0c\u4e0d\u6613\u88ab\u8f93\u9001\u4ecb\u8d28\u5e26\u8d70\u3002\u4f7f\u5f97\u78e8\u7164\u673a\u51fa\u529b\u5c0f\uff0c\u4e14\u7814\u78e8\u8fc7\u7ec6</p> </li> <li> <p>\u8f6c\u901f\u8fc7\u5feb\u65f6\uff0c\u94a2\u7403\u5728\u79bb\u5fc3\u529b\u4f5c\u7528\u4e0b\uff0c\u8d34\u7740\u7b52\u58c1\u65cb\u8f6c\u800c\u4e0d\u4e0b\u843d\uff0c\u5931\u53bb\u770b\u7834\u788e\u529f\u80fd\u3002</p> </li> </ul> <p>\u94a2\u7403\u968f\u7740\u7b52\u58c1\u65cb\u8f6c\u7684\u7b52\u4f53\u8f6c\u901f\u79f0\u4e3a\u4e34\u754c\u8f6c\u901f\\(n_{cr} = 42.3/ \\sqrt{D_{\\text{\u5185\u5f84}}}\\)\u3002\u800c\u5de5\u4f5c\u8f6c\u901f\\(n\\)\u5e94\u5f53\u7565\u5c0f\u4e8e\u4e34\u754c\u8f6c\u901f\\(n_{cr}\\)\uff0c\u4e00\u822c\uff1a</p> \\[ \\frac{n}{n_{cr}} = 0.74-0.8 \\] <p>\u62a4\u7532</p> <p>\u5728\u5b9e\u9645\u8fc7\u7a0b\u4e2d\uff0c\u94a2\u7403\u65cb\u8f6c\u901f\u5ea6\u4e0e\u7b52\u4f53\u8f6c\u901f\u7684\u5dee\u5f02\uff0c\u53d6\u51b3\u4e8e\u94a2\u7403\u4e0e\u62a4\u7532\u4e4b\u95f4\u7684\u6469\u64e6\u7cfb\u6570\u3002\u62a4\u7532\u7684\u6469\u64e6\u7cfb\u6570\u8d8a\u5927\uff0c\u589e\u52a0\u4e86\u94a2\u7403\u6536\u5230\u7684\u6469\u64e6\u529b\uff0c\u53ef\u4ee5\u5728\u8f83\u5c0f\u7684\u80fd\u8017\u4e0b\u5c06\u94a2\u7403\u63d0\u5347\u5230\u4e00\u5b9a\u9ad8\u5ea6\uff08\u56e0\u4e3a\u94a2\u7403\u4e0e\u62a4\u7532\u5b58\u5728\u76f8\u5bf9\u6ed1\u52a8\uff09\u3002</p> <p>\u5bf9\u4e8e\u62a4\u7532\u7684\u7ed3\u6784\uff0c\u9f7f\u5f62\u62a4\u7532\u6700\u597d</p> <p>\u94a2\u7403\u5145\u6ee1\u7cfb\u6570</p> <p>\u94a2\u7403\u5145\u6ee1\u7cfb\u6570/\u5145\u7403\u7cfb\u6570\\(\\phi\\)\u6307\u7684\u662f\u7b52\u4f53\u5185\u88c5\u6709\u7684\u94a2\u7403\u91cf\u7ad9\u7b52\u4f53\u4f53\u79ef\\(V\\)\u7684\u4efd\u989d\uff1a</p> \\[ \\phi = \\frac{G_b}{\\rho_b V}, \\quad V=\\frac{\\pi}{4} D^2 L \\] <p>\u5176\u4e2d\\(G_b\\)\u4e3a\u94a2\u7403\u88c5\u8f7d\u91cf(t)\uff0c\\(\\rho_b\\)\u4e3a\u94a2\u7403\u5bc6\u5ea6(=4.9t/m\u00b3)\u3002</p> <p>\u4e0d\u540c\u5c42\u7684\u94a2\u7403\u5177\u6709\u7684\u78e8\u7c89\u80fd\u529b\u4e0d\u540c\uff0c\u7d27\u8d34\u7b52\u58c1\u7684\u6700\u5916\u5c42\u94a2\u7403\u78e8\u7c89\u80fd\u529b\u6700\u5f3a\uff08\u4e0d\u662f\u7834\u788e\u80fd\u529b\uff09\uff0c\u5404\u5185\u5c42\u94a2\u7403\u6570\u91cf\u4e0e\u6700\u5916\u5c42\u94a2\u7403\u6570\u91cf\u4e4b\u6bd4\u968f\u7740\\(\\phi\\)\u7684\u589e\u5927\u800c\u589e\u5927\u3002</p> <p>\u78e8\u7164\u51fa\u529b\\(B_m\\)\u4e0e\\(\\phi^{0.6}\\)\u6210\u6b63\u6bd4\uff0c\u78e8\u7164\u529f\u7387\u6d88\u8017\\(P_m\\)\u57fa\u672c\u53d6\u51b3\u4e8e\u94a2\u7403\u8d28\u91cf\uff0c\u6b63\u6bd4\u4e8e\\(\\phi^{0.9}\\)</p> <p>\u56e0\u4e3a\u968f\u7740\\(\\phi\\)\u7684\u589e\u5927\uff0c\u94a2\u7403\u7684\u8f7d\u8377\u91cd\u5fc3\u63a5\u8fd1\u4e8e\u7b52\u4f53\u65cb\u8f6c\u4e2d\u5fc3\uff0c\u94a2\u7403\u65cb\u8f6c\u60ef\u6027\u77e9\u7684\u589e\u52a0\u5c0f\u4e8e\u94a2\u7403\u91cf\u7684\u589e\u52a0\u3002</p> <p>\u78e8\u7164\u5355\u4f4d\u7535\u8017\\(E_m = P_m/B_m\\)\u4e0e\\(\\phi^{0.3}\\)\u6210\u6b63\u6bd4\u3002\u5373\uff0c\u589e\u52a0\u88c5\u8f7d\u91cf\u53ef\u4ee5\u589e\u5927\u78e8\u7164\u51fa\u529b\uff0c\u4f46\u4e5f\u4f1a\u589e\u52a0\u78e8\u7164\u5355\u4f4d\u7535\u8017\u3002\u5728\u4e00\u5b9a\u7ec6\u5ea6\u4e0b\uff0c\u78e8\u7164\u5355\u4f4d\u7535\u8017\u6700\u5c0f\u7684\u5145\u7403\u7cfb\u6570\u4e3a\u6700\u4f73\u51b2\u7403\u7cfb\u6570\\(\\phi_{opt}\\)\u3002</p> <p>\u94a2\u7403\u76f4\u5f84</p> <ul> <li> <p>\u94a2\u7403\u76f4\u5f84\u592a\u5c0f\uff0c\u5219\u649e\u51fb\u529b\u592a\u5f31\uff0c\u4e0d\u9002\u5e94\u786c\u8d28\u7164\u548c\u5927\u7164\u5757</p> </li> <li> <p>\u94a2\u7403\u76f4\u5f84\u5e94\u8be5\u9002\u5f53\u5c0f\uff0c\u63d0\u9ad8\u7814\u78e8\u8868\u9762\u79ef\u548c\u649e\u51fb\u6b21\u6570\uff0c\u63d0\u9ad8\u78e8\u7164\u51fa\u529b\uff0c\u4f46\u78e8\u635f\u52a0\u5267</p> </li> </ul> <p>\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u91c7\u752830<sub>40mm\u94a2\u7403\uff1b\u5bf9\u4e8e\u8f83\u786c\uff0c\u8f83\u5927\u7684\u7164\uff0c\u9009\u62e950</sub>60mm\u94a2\u7403\u3002\uff08\u7535\u5382\u642d\u914d\u4e24\u79cd\u76f4\u5f84\u8303\u56f4\u94a2\u7403\uff09</p> <p>\u5b58\u7164\u91cf</p> <ul> <li> <p>\u5f53\u7b52\u5185\u5b58\u7164\u91cf\u592a\u5c11\uff0c\u6d88\u8017\u4e8e\u94a2\u7403\u78b0\u649e\u7684\u65e0\u7528\u529f\u7387\u8f83\u5927\uff0c\u78e8\u7164\u6709\u6548\u529f\u7387\u4f4e\u3002\u56e0\u6b64\u968f\u7740\u5b58\u7164\u91cf\u589e\u591a\uff0c\u78e8\u7164\u673a\u51fa\u529b\u589e\u5927\uff0c\u5176\u4e2d\u7684\u6709\u6548\u529f\u7387\u4e5f\u589e\u52a0</p> </li> <li> <p>\u82e5\u5b58\u7164\u91cf\u8fc7\u591a\uff0c\u5219\u94a2\u7403\u5bf9\u7164\u7684\u649e\u51fb\u4f5c\u7528\u51cf\u5f31\uff0c\u78e8\u7164\u51fa\u529b\u53cd\u800c\u4e0b\u964d\uff0c\u751a\u81f3\u53ef\u80fd\u5bfc\u81f4\u5165\u53e3\u5835\u585e</p> </li> </ul> <p>\u901a\u98ce\u91cf</p> <p>\u901a\u98ce\u91cf\u76f4\u63a5\u5f71\u54cd\u71c3\u6599\u6cbf\u7740\u7b52\u957f\u65b9\u5411\u4e0a\u7684\u5747\u5300\u5206\u5e03\u548c\u78e8\u7164\u51fa\u529b</p> <ul> <li> <p>\u5f53\u901a\u98ce\u91cf\u5f88\u5c0f\u65f6\uff0c\u5927\u90e8\u5206\u7164\u96c6\u4e2d\u4e8e\u8fdb\u53e3\u5904\uff0c\u8fd9\u5bfc\u81f4\u51fa\u53e3\u7aef\u7684\u94a2\u7403\u80fd\u91cf\u6ca1\u6709\u88ab\u5145\u5206\u5229\u7528\uff0c\u4ec5\u5e26\u51fa\u5c11\u91cf\u7684\u7ec6\u7164\u7c89\uff0c\u78e8\u7164\u51fa\u529b\u4f4e</p> </li> <li> <p>\u5f53\u901a\u98ce\u91cf\u8fc7\u5927\u65f6\uff0c\u4e0d\u5408\u683c\u7684\u7c97\u7164\u7c89\u4e5f\u88ab\u5e26\u51fa\uff0c\u7ecf\u8fc7\u7c97\u7ec6\u5206\u79bb\u5668\u540e\u56de\u7c89\u91cf\u589e\u52a0\uff0c\u9020\u6210\u65e0\u6548\u5faa\u73af\u3002</p> </li> </ul> <p>\u968f\u7740\u901a\u98ce\u91cf\u589e\u5927\uff0c\u78e8\u7164\u51fa\u529b\u589e\u52a0\uff0c\u5355\u4f4d\u78e8\u7164\u7535\u8017\u964d\u4f4e\u3002\u5b58\u5728\u4e00\u4e2a\u78e8\u7164\u4e8e\u901a\u98ce\u7535\u8017\u7684\u6700\u4f73\u901a\u98ce\u91cf\\(Q_{V,opt}\\)\uff0c\u5176\u4e0e\u7164\u79cd\u7c7b\uff0c\u53ef\u78e8\u6027\uff0c\u7b52\u4f53\u4f53\u79ef\\(V\\)\uff0c\u5145\u7403\u7cfb\u6570\\(\\phi\\)\uff0c\u5206\u79bb\u5668\u7684\u51fa\u53e3\u7ec6\u5ea6\\(R_{90}''\\)\u6709\u5173\u3002</p>"},{"location":"energy/guolu/#\u53cc\u8fdb\u53cc\u51fa\u7403\u78e8\u673a","title":"\u53cc\u8fdb\u53cc\u51fa\u7403\u78e8\u673a","text":"<p>\u53cc\u8fdb\u53cc\u51fa\u7403\u78e8\u673a\u7684\u7ed3\u6784\u4e0e\u5de5\u4f5c\u539f\u7406\u4e0e\u5355\u8fdb\u5355\u51fa\u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u5176\u5728\u7b52\u4f53\u4e24\u7aef\u7684\u7a7a\u5fc3\u8f74\u65e2\u662f\u70ed\u98ce\u548c\u539f\u7164\u7684\u8fdb\u53e3\uff0c\u4e5f\u662f\u7164\u7c89\u4e0e\u8f93\u9001\u4ecb\u8d28\u7684\u51fa\u53e3\u3002</p> <p>\u8fdb\u5165\u4e24\u7aef\u7684\u6c14\u6d41\u5728\u7b52\u4f53\u4e2d\u90e8\u5bf9\u51b2\u800c\u53cd\u5411\u6d41\u52a8\uff0c\u4f7f\u5f97\u4e24\u4e2a\u5bf9\u79f0\u800c\u72ec\u7acb\u7684\u78e8\u7164\u56de\u8def\u540c\u65f6\u5de5\u4f5c\u3002</p> <p>\u72ec\u7279\u8bbe\u8ba1\uff1a</p> <p>\u5728\u4e2d\u7a7a\u8f74\u5185\u6709\u4e00\u4e2d\u5fc3\u7ba1\uff0c\u5b58\u5728\u4e00\u5b9a\u95f4\u9699\u7684\u73af\u5f62\u7a7a\u95f4\uff0c\u5728\u8be5\u7ba1\u5916\u5f39\u6027\u56fa\u5b9a\u87ba\u65cb\u8f93\u9001\u5668\u968f\u7740\u7b52\u4f53\u8f6c\u52a8\uff0c\u4f7f\u5f97\u539f\u7164\u7531\u7aef\u90e8\u73af\u72b6\u95f4\u9699\u4e0b\u534a\u90e8\u5206\u4e0d\u65ad\u88ab\u522e\u5411\u7b52\u5185\u3002\u800c\u4f5c\u4e3a\u5e72\u71e5\u5242\u7684\u70ed\u98ce\u4ece\u4e2d\u5fc3\u7ba1\u8fdb\u5165\u7b52\u5185\uff0c\u518d\u4ece\u53cd\u65b9\u5411\u7ecf\u8fc7\u73af\u5f62\u901a\u9053\u4e0a\u534a\u90e8\u5206\uff0c\u5c06\u7164\u7c89\u5e26\u51fa\u7b52\u4f53\uff0c\u8fdb\u5165\u7164\u7c89\u5206\u79bb\u5668\u95f4\u9699\u5206\u79bb\u3002\u5206\u79bb\u7684\u7c97\u7c89\u518d\u901a\u8fc7\u56de\u7c89\u56de\u8def\u843d\u5230\u4e2d\u7a7a\u8f74\u4e0e\u539f\u7164\u6df7\u5408\uff0c\u6700\u540e\u8fd4\u56de\u7b52\u5185\u3002</p> <p>\u4f18\u52bf\uff1a</p> <ul> <li> <p>\u4fdd\u6301\u4e86\u5355\u8fdb\u5355\u51fa\u7403\u78e8\u673a\u7684\u5404\u79cd\u4f18\u70b9</p> </li> <li> <p>\u53ef\u589e\u5927\u901a\u98ce\u91cf\uff0c\u9002\u5f53\u964d\u4f4e\u5355\u4f4d\u7535\u8017</p> </li> </ul>"},{"location":"energy/guolu/#\u4e2d\u901f\u78e8\u7164\u673a","title":"\u4e2d\u901f\u78e8\u7164\u673a","text":""},{"location":"energy/heat_transfer/","title":"\u4f20\u70ed\u5b66","text":"\ud83c\udf07Information <p> <ul> <li> <p>\u8bfe\u7a0b:\u4f20\u70ed\u5b66(\u7532)Heat Transfer(A)</p> </li> <li> <p>\u5b66\u5206:4.0\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u8bfe\u7a0b\u4ee3\u7801: 59120040</p> </li> </ul>"},{"location":"energy/heat_transfer/#\u5bfc\u70ed\u90e8\u5206","title":"\u5bfc\u70ed\u90e8\u5206","text":"<p>Heat-Conduction.pdf</p>"},{"location":"energy/heat_transfer/#\u5bf9\u6d41\u90e8\u5206","title":"\u5bf9\u6d41\u90e8\u5206","text":"<p>Convection.pdf</p>"},{"location":"energy/heat_transfer/#\u8f90\u5c04\u90e8\u5206","title":"\u8f90\u5c04\u90e8\u5206","text":"<p>Radiation.pdf</p>"},{"location":"math/FFT/","title":"\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09","text":""},{"location":"math/FFT/#\u5085\u91cc\u53f6\u7ea7\u6570","title":"\u5085\u91cc\u53f6\u7ea7\u6570","text":"<p>\u5bf9\u4e8e\u5468\u671f\u4e3a\\(T\\)\u7684\u51fd\u6570\\(f(t)\\)\uff0c\u5085\u91cc\u53f6\u7ea7\u6570\u7684\u76ee\u6807\u662f\u5c06\u5b83\u8868\u793a\u4e3a\u4e00\u7cfb\u5217\u590d\u6307\u6570\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\uff1a</p> \\[ f(t) = \\sum_{n=-\\infty}^{\\infty} c_n e^{j n \\omega_0 t}\uff0cw_0 = \\frac{2\\pi}{T} \\] <p>\u5176\u4e2d\uff0c\\(e^{j n \\omega_0 t}= \\cos{(n \\omega_0 t)} + j \\sin{(n \\omega_0 t)}\\)\u662f\u57fa\u51fd\u6570\uff0c\u6ee1\u8db3\u6b63\u4ea4\u6027\uff08\u4e0d\u540c\u9891\u7387\u7684\u590d\u6307\u6570\u51fd\u6570\u5728\u7279\u5b9a\u533a\u95f4\u4e0a\u7684\u79ef\u5206\u7ed3\u679c\u4e3a\u96f6\uff0c\u800c\u540c\u4e00\u9891\u7387\u7684\u79ef\u5206\u7ed3\u679c\u4e3a\u975e\u96f6\u503c\uff08\u901a\u5e38\u4e3a\u5468\u671f\u957f\u5ea6\uff09\uff1a</p> \\[ \\int_{-T/2}^{T/2} e^{j n \\omega_0 t} \\cdot e^{-j m \\omega_0 t} dt = \\delta(n-m) \\cdot T =  \\begin{cases} T, &amp;  n = m \\\\ 0, &amp;  n \\neq m \\end{cases} \\] <p>\u5bf9\\(f(t) = \\sum_{n=-\\infty}^{\\infty} c_n e^{j n \\omega_0 t}\uff0cw_0 = \\frac{2\\pi}{T}\\)\uff0c\u5728\u7b49\u5f0f\u4e24\u8fb9\u4e58\u4ee5\\(e^{j -m \\omega_0 t}\\)\u5e76\u4e14\u79ef\u5206\uff1a</p> \\[ \\int_{-T/2}^{T/2} f(t) e^{-j m \\omega_0 t} dt = \\sum_{n=-\\infty}^{\\infty} c_n \\int_{-T/2}^{T/2} e^{j n \\omega_0 t} \\cdot e^{-j m \\omega_0 t} dt  \\] <p>\u6839\u636e\u6b63\u4ea4\u6027\uff0c\u53f3\u8fb9\u4ec5\u5f53\\(n=m\\)\u65f6\u79ef\u5206\u503c\u4e3a\\(T\\)\uff0c\u5176\u4f59\u4e3a0\uff1a</p> \\[ \\int_{-T/2}^{T/2} f(t) e^{-j m \\omega_0 t} dt = c_m \\cdot T \\] \\[ c_m = \\frac{1}{T} \\int_{-T/2}^{T/2} f(t) e^{-j m \\omega_0 t} dt \\] <p>\u7cfb\u6570\\(c_n\\)\u8868\u793a\u4fe1\u53f7\\(f(t)\\)\u4e2d\u9891\u7387\u4e3a\\(n \\omega_0\\)\u7684\u5206\u91cf\u7684\u5f3a\u5ea6\uff0c\u56e0\u6b64\u5085\u91cc\u53f6\u7ea7\u6570\u662f\u5c06\u5c06\u4fe1\u53f7\u5206\u89e3\u4e3a\u57fa\u9891\\(\\omega_0\\)\u4ee5\u53ca\u8c10\u6ce2\uff08\\(2\\omega_0,3\\omega,...\\)\uff09\u7684\u7ebf\u6027\u7ec4\u5408\u3002</p> <p>\u800c\u5bf9\u4e8e\u975e\u5468\u671f\u51fd\u6570\uff0c\u6211\u4eec\u628a\u5b83\u7684\u5468\u671f\u770b\u4f5c\u65e0\u7a77\u5927\\(T \\rightarrow \\infty\\)\uff0c\u5219\u57fa\u9891\\(\\omega_0=2\\pi/T \\rightarrow 0\\)\uff0c\u79bb\u6563\u9891\u7387\\(n\u03c9_0\\)\u53d8\u4e3a\u8fde\u7eed\u53d8\u91cf\\(\u03c9\\)\u3002</p> \\[ c_n = \\frac{\\omega_0}{2\\pi} \\int_{-T/2}^{T/2} f(t) e^{-j n \\omega_0 t} dt \\] <p>\u5f53\\(T \\rightarrow \\infty\\)\uff0c\u4ee4\\(\\omega=n\\omega_0\uff0c\\omega_0 = d\\omega\\):</p> \\[ \\begin{aligned} f(t) =&amp; \\lim_{T \\rightarrow \\infty} \\sum_{n=-\\infty}^{\\infty} [\\frac{\\omega_0}{2\\pi} \\int_{-T/2}^{T/2} f(\\tau) e^{-j n \\omega_0 \\tau} d\\tau] e^{j n \\omega_0 t} \\\\ =&amp; \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} [\\int_{-\\infty}^{\\infty} f(\\tau) e^{-j \\omega \\tau} d\\tau]e^{j \\omega t} d\\omega \\end{aligned} \\] <p>\u56e0\u6b64\u6211\u4eec\u5f97\u5230\u975e\u5468\u671f\u51fd\u6570\u7684\u5085\u91cc\u53f6\u53d8\u6362\u53ca\u5176\u9006\u53d8\u6362\uff1a</p> \\[ \\begin{aligned} F(\\omega) =&amp; \\int_{-\\infty}^{\\infty} f(t) e^{-j \\omega t} dt \\\\ f(t) =&amp; \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} F(\\omega) e^{j \\omega t} d\\omega \\end{aligned} \\]"},{"location":"math/FFT/#\u5085\u91cc\u53f6\u53d8\u6362","title":"\u5085\u91cc\u53f6\u53d8\u6362","text":"<p>\u5085\u91cc\u53f6\u53d8\u6362\u7684\u516c\u5f0f\u4e3a\uff1a</p> \\[ F(\\omega) = \\int_{-\\infty}^{\\infty} f(t) e^{-j\\omega t} dt \\] <ul> <li> <p>\u8f93\u5165\uff1a\u65f6\u57df\u4fe1\u53f7\\(f(t)\\)\uff08\u4f8b\u5982\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7535\u538b\u3001\u58f0\u6ce2\uff09</p> </li> <li> <p>\u8f93\u51fa\uff1a\u9891\u8c31\u5bc6\u5ea6\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u7684\u81ea\u53d8\u91cf\u662f\\(\\omega=\\frac{2\\pi}{T}\\)\uff0c\u56e0\u53d8\u91cf\u662f\u4fe1\u53f7\u5e45\u503c\u5728\u9891\u57df\u4e2d\u7684\u5206\u5e03\u5bc6\u5ea6\uff0c\u5373\u5355\u4f4d\u9891\u7387\u4fe1\u53f7\u7684\u5f3a\u5ea6\u3002</p> </li> </ul>"},{"location":"math/FFT/#dft","title":"DFT","text":"<p>\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08Discrete Fourier Transform, DFT\uff09\u662f\u5085\u91cc\u53f6\u53d8\u6362\u7684\u79bb\u6563\u5316\u7248\u672c\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u6570\u5b57\u4fe1\u53f7\uff08\u5982\u8ba1\u7b97\u673a\u91c7\u96c6\u7684\u65f6\u57df\u6570\u636e\uff09\u3002\u5b83\u5c06\u6709\u9650\u957f\u5ea6\u7684\u79bb\u6563\u65f6\u57df\u4fe1\u53f7\u8f6c\u6362\u4e3a\u79bb\u6563\u9891\u57df\u8868\u793a\uff0c\u63ed\u793a\u4fe1\u53f7\u4e2d\u9690\u542b\u7684\u9891\u7387\u6210\u5206\u3002</p> <p>\u5f15\u5165\u68b3\u72b6\u51fd\u6570\uff0c\u5176\u4e2d\\(T_s\\)\u4e3a\u91c7\u6837\u5468\u671f\uff1a</p> \\[ \\delta_s (t) = \\sum_{n=-\\infty}^{\\infty} \\delta(t-nT_s) \\] <p>\u5c06\u65f6\u57df\u4e0a\u7684\u8fde\u7eed\u4fe1\u53f7\u4e0e\u5b83\u76f8\u4e58\uff0c\u5373\u53ef\u5f97\u5230\uff1a</p> \\[ x_s(t) = x(t) \\cdot \\delta_s(t) = \\sum_{n=-\\infty}^{\\infty} x(t) \\delta(t-nT_s) \\] <p>\u6839\u636e\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u5b8c\u6210\u65f6\u57df\u79bb\u6563\u5316\uff1a</p> \\[ X(\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} dt \\] \\[ \\begin{aligned} X_s(\\omega) &amp;= \\int_{-\\infty}^{\\infty} [\\sum_{n=-\\infty}^{\\infty} x(t) \\delta(t-nT_s) e^{-j\\omega t}] dt \\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x(t) \\delta(t-nT_s) e^{-j\\omega t} dt\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} x(nT_s) e^{-j\\omega nT_s}\\\\ \\end{aligned} \\] <p>\u663e\u7136\u8ba1\u7b97\u673a\u53ea\u80fd\u5bf9\u4e8e\u8fde\u7eed\u4fe1\u53f7\\(x(t)\\)\u8fdb\u884c\u6709\u9650\u7684N\u6b21\u7684\u91c7\u6837\uff0c\u91c7\u6837\u5468\u671f\u4e3a\\(T_s\\)\uff0c\u6211\u4eec\u5bf9\u91c7\u6837\u5f97\u5230\u7684\u4fe1\u53f7\u8fdb\u884c\u65f6\u57df\u4e0a\u7684\u5468\u671f\u5ef6\u62d3\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u5f97\u5230\u4e86\u4e00\u4e2a\u5468\u671f\u4e3a\\(T_0=NT_s\\)  \u7684\u51fd\u6570\u3002\u5bf9\u4e8e\u5468\u671f\u51fd\u6570\u800c\u8a00\uff0c\u5176\u9891\u8c31\u5bc6\u5ea6\u51fd\u6570\u662f\u79bb\u6563\u5316\u7684\uff0c\u8fd9\u6837\u5c31\u628a\u9891\u57df\u4e5f\u8fdb\u884c\u4e86\u79bb\u6563\u5316\u3002</p> <p>\u5728\u4e00\u4e2a\u5468\u671f\\(T_0=NT_s\\)\u5185\uff1a</p> \\[ x_s (t) = \\sum_{n=0}^{N-1} x(t) \\delta(t-nT_s) \\] <p>\u56e0\u6b64\u79bb\u6563\u4fe1\u53f7\u7684\u5085\u91cc\u53f6\u7ea7\u6570(\\(\\omega_0=2\\pi/T_0\\)):</p> \\[ \\begin{aligned} X(k\\omega_0) &amp;= \\frac{1}{T_0} \\int_{0}^{T} (\\sum_{n=0}^{N-1} x(t) \\delta(t-nT_s)) e^{-jkw_0t} dt \\\\ &amp;= \\frac{1}{T_0} \\sum_{n=0}^{N-1} \\int_{0}^{T} x(t) \\delta(t-nT_s) e^{-jkw_0t} dt\\\\ &amp;= \\frac{1}{T_0} \\sum_{n=0}^{N-1} x(nT_s) e^{-jkw_0nT_s}\\\\ &amp;= \\frac{1}{NT_s} \\sum_{n=0}^{N-1} x(nT_s) e^{-j \\frac{2\\pi}{NT_s} knT_s }\\\\ &amp;= \\frac{1}{NT_s} \\sum_{n=0}^{N-1} x(nT_s) e^{-j \\frac{2\\pi}{N} kn } \\end{aligned}  \\] <p>\u4ee4\\(X[k] = X(k\\omega_0) T_0\uff0cx[n] = x(nT_s)\\):</p> \\[ X[k] = \\sum_{n=0}^{N-1} x[n] e^{-j \\frac{2\\pi}{N} kn }\uff0c(k=0,1,...,N-1) \\] <p>\u8981\u8ba1\u7b97\u7279\u5b9a\u7684k\u503c\u7684\\(X[k]\\)\uff0c\u8981\u8fdb\u884cN\u6b21\\(x[n]\\)\u4e0e\\(e^{-j \\frac{2\\pi}{N} kn }\\)\u7684\u4e58\u6cd5\u8fd0\u7b97\uff0c\u518d\u8fdb\u884cN-1\u6b21\u52a0\u6cd5\u8fd0\u7b97\u3002\u56e0\u6b64\uff0cDFT\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\\(O(N^2)\\)\u3002</p>"},{"location":"math/FFT/#fft","title":"FFT","text":"<p>\u5229\u7528\u590d\u6570\u5355\u4f4d\u6839\\(e^{-j \\frac{2\\pi}{N} kn }\\)\u7684\u5468\u671f\u6027\u8d28\uff0cFFT\u53ef\u4ee5\u5c06DFT\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u5230\\(O(N \\log N)\\)\u3002</p>"},{"location":"math/GumbelSoftmax/","title":"Gumbel-Softmax","text":""},{"location":"math/GumbelSoftmax/#\u524d\u7f6e\u79bb\u6563\u6982\u7387\u5206\u5e03\u7684\u91c7\u6837","title":"\u524d\u7f6e\uff1a\u79bb\u6563\u6982\u7387\u5206\u5e03\u7684\u91c7\u6837","text":"<p>\u4e00\u822c\u7a0b\u5e8f\u4e2d\u5bf9\u4e8e\u79bb\u6563\u6982\u7387\u5206\u5e03\u91c7\u6837\u65b9\u6cd5\u4e3a:</p> \\[ z = \\{ i \\ | \\ \\sum_{k=1}^{i-1} p_k \\leq u \\}, \\quad i=1,2,...,n \\] <p>\u5176\u4e2d\\(i=1,...,n\\)\u4e3a\u7c7b\u522b\u7684\u4e0b\u6807\uff0c\u968f\u673a\u53d8\u91cf\\(u \\sim U(0,1)\\)\uff0c\u5176\u4e2d\\(\\sum_{k=1}^{i-1} p_k\\)\u662f\u7d2f\u8ba1\u6982\u7387\u7684\u8fc7\u7a0b\uff0c\u7c7b\u4f3c\u4e8e\u7d2f\u8ba1\u5206\u5e03\u51fd\u6570\u3002</p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u9996\u5148\u5728\\([0,1]\\)\u4e4b\u95f4\u5747\u5300\u91c7\u6837\u51fa\\([0.1, 0.2,...,0.9]\\)\uff0c\u518d\u5c06\u8fd99\u4e2a\u503c\u4f5c\u4e3ay\u503c\u4ee3\u5165\u67d0\u4e2a\u5206\u5e03\u7684CDF\u51fd\u6570\uff0c\u6c42\u51fax\uff0c\u5373\u4e3a\u91c7\u6837\u5f97\u5230\u7684\u503c\u3002</p>"},{"location":"math/GumbelSoftmax/#\u4ecb\u7ecd","title":"\u4ecb\u7ecd","text":"<p>\u8bba\u6587\uff1aCategorical Reparameterization with Gumbel-Softmax</p> <p>\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e3b\u8981\u4f9d\u9760\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u53c2\u6570\u7684\u66f4\u65b0\uff0c\u6fc0\u6d3b\u51fd\u6570\u9700\u8981\u6ee1\u8db3\u5355\u8c03\u3001\u5904\u5904\u53ef\u5bfc\u7b49\u6761\u4ef6\u3002\u5bf9\u4e8eReLU\u51fd\u6570\uff0c\u663e\u7136\u5728\\(x=0\\)\u5904\u4e0d\u53ef\u5bfc\uff0c\u7406\u8bba\u4e0a\u65e0\u6cd5\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u4f18\u5316\u3002\u6b64\u65f6\u6211\u4eec\u4f7f\u7528\u7684\u662f\u6b21\u68af\u5ea6\uff1a\u5f53\\(x&gt;0\\)\u65f6\u5176\u5bfc\u6570\u4e3a1\uff0c\u5f53\\(x&lt;0\\)\u65f6\u4e3a0\uff0c\u800c\u5f53\u5728\\(x=0\\)\u5904\u65f6\u7684\u6b21\u68af\u5ea6\u4e3a\\(c \\in [0,1]\\)\uff0c\u4e00\u822c\u76f4\u63a5\u53d6\\(c=0\\) \u5728\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u9700\u8981\u4ece\u4e00\u4e2a\u79bb\u6563\u7684\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u4f46\u76f4\u63a5\u4ece\u79bb\u6563\u7684\u5206\u5e03\u4e2d\u91c7\u6837\u662f\u4e0d\u8fde\u7eed\u7684\uff0c\u65e0\u6cd5\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u6765\u66f4\u65b0\u53c2\u6570(\u6838\u5fc3\u95ee\u9898\uff1a\u79bb\u6563\u91c7\u6837\u4e0d\u53ef\u5bfc)\u3002</p> <p><code>Gumbel-Softmax</code>\u662f\u4e00\u79cd\u7ed3\u5408<code>Gumbel</code>\u5206\u5e03\u548c<code>Softmax</code>\u51fd\u6570\u7684\u6280\u5de7\uff0c\u7528\u4e8e\u5728\u79bb\u6563\u53d8\u91cf\u7684\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u3002</p> <p>Gumbel distribution</p> <p>\u5176\u5206\u5e03\u51fd\u6570\u548c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5206\u522b\u4e3a:</p> \\[ F(x;\\mu, \\beta) = e^{-e^{-(x-\\mu)/\\beta}} ,\\quad f(x;\\mu, \\beta) = \\frac{1}{\\beta} e^{[e^{-(x-\\mu)/\\beta} + (x-\\mu)/\\beta]} \\] <p>\u6807\u51c6Gumbel\u5206\u5e03(\u5373\\(\\mu=0,\\beta=1\\))\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u4e3a:</p> \\[ F(x) = e^{-e^{-x}} ,\\quad f(x) = e^{-(e^{-x} + x)} \\] <p>Gumbel-Max Trick</p> <p>\u56e0\u4e3a\uff1a</p> \\[ P(F^{-1}(u) \\leq x) = P(u \\leq F(x)) = F(x), \\quad u \\sim U(0,1). \\] <p>\u4eceGumbel\u5206\u5e03\u4e2d\u91c7\u6837, \u53ea\u9700:</p> \\[ x = F^{-1}(u) = \\mu - \\beta \\log(-\\log(u)) \\] <p>\u5373\\(F^{-1}(u)\\)\u7684\u5206\u5e03\u51fd\u6570\u4e3a\\(F(x)\\).</p> <p>\u5047\u8bbe\u6709\u4e00\u4e2a\u79bb\u6563\u7684\u5206\u5e03\\([\\pi_1, \\pi_2,...,\\pi_k]\\)\u5171k\u7c7b\uff0c\\(\\pi_i\\)\u4ee3\u8868\u7b2ci\u7c7b\u7684\u6982\u7387\uff0c\u6709\\(P(z=i)=\\pi_i\\)\uff0c\u6211\u4eec\u5e0c\u671b\u4ece\u8be5\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u5219\u53ef\u4ee5\u91c7\u7528\u4ee5\u4e0b\u65b9\u6cd5:</p> \\[ \\begin{aligned} P(\\log{\\pi_i} + G_i \\geq \\max_{j \\neq i} \\{ \\log{\\pi_j} + G_j \\}) = \\int_{- \\infty}^{+ \\infty} f(x) P(x + \\log{\\pi_i} \\geq \\{ \\log{\\pi_j} + G_j \\}_{j \\neq i}) dx \\\\ \\end{aligned} \\] <p>\u53ef\u77e5\u5bf9\u4e8e\u6240\u6709\\(j \\neq i\\)\uff0c\u4e0a\u5f0f\u7684\u6982\u7387\u6761\u4ef6\u53ef\u5199\u4e3a\\(G_j \\leq x + \\log{\\pi_i} - \\log{\\pi_j}\\) </p> \\[ P(G_j \\leq x + \\log{\\pi_i} - \\log{\\pi_j}) = e^{-e^{-(x + \\log{\\pi_i} - \\log{\\pi_j})}} = e^{e^{-x} \\cdot \\frac{\\pi_j}{\\pi_i}} \\] <p>\u7531\u4e8e\\(G_j\\)\u662f\u72ec\u7acb\u7684<code>Gumbel</code>\u5206\u5e03\u968f\u673a\u53d8\u91cf\uff0c\u6211\u4eec\u5c06\u8054\u5408\u6982\u7387\u5206\u89e3\u4e3a\u5404\u4e2a\u72ec\u7acb\u4e8b\u4ef6\u7684\u4e58\u79ef\uff1a</p> \\[ \\begin{aligned} P(x + \\log{\\pi_i} \\geq \\{ \\log{\\pi_j} + G_j \\}_{j \\neq i} ) &amp;= \\prod_{j \\neq i} P(G_j \\leq x + \\log{\\pi_i} - \\log{\\pi_j})  \\\\ &amp; = \\prod_{j \\neq i} e^{e^{-x} \\cdot \\frac{\\pi_j}{\\pi_i}} = e^{- \\sum_{j \\neq i} (e^{-x} \\cdot \\frac{\\pi_j}{\\pi_i})} \\\\ &amp;= e^{-e^{-x} \\cdot \\frac{1-\\pi_i}{\\pi_i}} \\\\ \\end{aligned} \\] <p>\u4ee3\u5165\u4e0a\u8ff0\u516c\u5f0f\u4e2d\uff0c\u53ef\u5f97\uff1a</p> \\[ P(\\log{\\pi_i} + G_i \\geq \\max_{j \\neq i} \\{ \\log{\\pi_j} + G_j \\})=\\int_{- \\infty}^{+ \\infty} e^{-(e^{-x} + x)} \\cdot e^{-e^{-x} \\cdot \\frac{1-\\pi_i}{\\pi_i}} dx \\\\ =\\int_{- \\infty}^{+ \\infty} e^{-(x + e^{-x} \\cdot \\frac{1}{\\pi_i} )} dx =\\int_{- \\infty}^{+ \\infty} \\pi_i e^{-[(x + \\log{\\pi_i}) + e^{-(x+\\log{\\pi_i})} ]} dx= \\pi_i = P(z=i)\\\\ \\] <p>\u56e0\u6b64\u5c06\u8be5\u5206\u5e03\u7684\u91c7\u6837\\(z\\)\u7b49\u4ef7\u4e8e:</p> \\[ z = \\argmax_i{[G_i + \\log{\\pi_i}]}, \\quad G_i \\sim \\text{Gumbel}(0,1) \\] <p>\u5176\u4e2d\\(G_i\\)\u4e5f\u53ef\u4ee5\u5199\u6210\u6807\u51c6Gumbel\u5206\u5e03\u7684\u91c7\u6837\u5f62\u5f0f:</p> \\[ G_i = -\\log(-\\log(u_i)), \\quad u_i \\sim U(0,1) \\] <p>\u5b9e\u9645\u4e0a\uff0c<code>Gumbel-Max Trick</code>\u4f7f\u7528\u4e86\u8fd9\u6837\u7684\u91cd\u53c2\u6570\u6280\u5de7\u628a\u91c7\u6837\u8fc7\u7a0b\u5206\u6210\u4e86\u786e\u5b9a\u6027\u7684\u90e8\u5206\u548c\u968f\u673a\u6027\u7684\u90e8\u5206\uff08\u5bf9\u6570\u6982\u7387+\u566a\u58f0\\(G_i\\)\uff09.\u3002\u6839\u636e\u4e0a\u8ff0\u63a8\u5bfc\uff0c\u9009\u62e9<code>Gumbel</code>\u5206\u5e03\u4f5c\u4e3a\u566a\u58f0\uff0c\u53ef\u4ee5\u4fdd\u8bc1\u91c7\u6837\u7ed3\u679c\u670d\u4ece\u79bb\u6563\u5206\u5e03\\([\\pi_1, \\pi_2,...,\\pi_k]\\)\u3002</p> <p><code>[0.1, 0.7, 0.2] -&gt; [log(0.1) + gumbel_noise, log(0.7) + gumbel_noise, log(0.2) + gumbel_noise]</code></p>"},{"location":"math/GumbelSoftmax/#gumbel-softmax_1","title":"Gumbel Softmax","text":"<p>\u663e\u7136\uff0c<code>Gumbel-Max Trick</code>\u4e2d\u542b\u6709\u4e0d\u53ef\u5bfc\u7684<code>argmax</code>\uff0c\u800c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u53ef\u5bfc\u7684<code>softmax</code>\u51fd\u6570\u6765\u66ff\u6362\uff0c\u5f97\u5230\u516c\u5f0f\uff1a</p> \\[ \\pi_i' = \\frac{e^{{(G_i + \\log{\\pi_i})}/\\tau} }{\\sum_{j=1}^{k} e^{(G_j + \\log{\\pi_j})/\\tau}}, \\quad G_i \\sim \\text{Gumbel}(0,1) \\] <p>\u5176\u4e2d\\(\\tau\\)\u662f\u6e29\u5ea6\u7cfb\u6570\uff0c\\(\\tau\\)\u8d8a\u5c0f\uff0c<code>softmax</code>\u7684\u7ed3\u679c\u8d8a\u903c\u8fd1<code>argmax</code>\uff0c\u5206\u5e03\u8d8a\u5c16\u9510\uff1b\\(\\tau\\)\u8d8a\u5927\uff0c\u5206\u5e03\u8d8a\u5e73\u6ed1\uff0c\u63a5\u8fd1\u4e8e\u5747\u5300\u5206\u5e03\u3002</p> <p></p>"},{"location":"math/RBF/","title":"\u5f84\u5411\u57fa\u51fd\u6570(Radial basis function)","text":""},{"location":"math/CFD/chapter/chapter1/","title":"\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\uff08CFD\uff09","text":""},{"location":"math/CFD/chapter/chapter1/#\u63a7\u5236\u65b9\u7a0b\u7ec4","title":"\u63a7\u5236\u65b9\u7a0b\u7ec4","text":"<p>\u5bf9\u4e8e\u968f\u6d41\u4f53\u8fd0\u52a8\u7684\u6d41\u4f53\u5fae\u56e2\uff0c\u5176\u5bc6\u5ea6\u8868\u793a\u4e3a\\(\\rho = \\rho(x, y, z, t)\\)\uff0c\uff0c\u5047\u8bbe\\(t_2\\)\u548c\\(t_1\\)\u4e24\u4e2a\u65f6\u523b\uff0c\u6709\uff1a</p> \\[ \\rho_2 = \\rho_1 + (\\frac{\\partial \\rho}{\\partial t})_1 (t_2 - t_1) + (\\frac{\\partial \\rho}{\\partial x})_1 (x_2 - x_1) + (\\frac{\\partial \\rho}{\\partial y} )_1 (y_2 - y_1) + (\\frac{\\partial \\rho}{\\partial z})_1 (z_2 - z_1) + (\u9ad8\u9636\u9879) \\] <p>\u5f53\\(t_2\\)\u8d8b\u8fd1\u4e8e\\(t_1\\):</p> \\[ \\lim_{t_2 \\rightarrow t_1} \\frac{\\rho_2 - \\rho_1}{t_2 -t_1} = \\frac{D \\rho}{D t} \\] <p>\u5176\u4e2d\\(\\frac{D}{D t}\\)\u4e3a\u7269\u8d28\u5bfc\u6570\uff0c\u8868\u793a\u6d41\u4f53\u5fae\u56e2\u968f\u6d41\u4f53\u8fd0\u52a8\u65f6\u5176\u4efb\u4f55\u5c5e\u6027\u7684\u65f6\u95f4\u53d8\u5316\u7387\uff0c\u800c\\(\\frac{\\partial}{\\partial t}\\)\u4e3a\u5f53\u5730\u5bfc\u6570\uff0c\u662f\u7269\u7406\u4e0a\u56fa\u5b9a\u70b9\u5904\u7684\u65f6\u95f4\u53d8\u5316\u7387\uff0c\u7531\u4e0a\u8ff0\u516c\u5f0f\u6613\u5f97\uff1a</p> \\[ \\frac{D \\rho}{Dt} = \\frac{\\partial \\rho}{\\partial t} + u \\frac{\\partial \\rho}{\\partial x} + v \\frac{\\partial \\rho}{\\partial y} + w \\frac{\\partial \\rho}{\\partial z} \\] \\[ \\frac{D }{Dt} = \\frac{\\partial }{\\partial t} + u \\frac{\\partial }{\\partial x} + v \\frac{\\partial }{\\partial y} + w \\frac{\\partial }{\\partial z} \\] <p>\u4e5f\u53ef\u5199\u4f5c\uff1a\\(\\frac{D }{Dt} = \\frac{\\partial }{\\partial t} + (\\nabla \\cdot \\vec{V}) \\(\u3002\\)\\nabla \\cdot \\vec{V}\\)\u4e3a\u8fc1\u79fb\u5bfc\u6570\uff0c\u8868\u793a\u7269\u7406\u4e0a\u7531\u4e8e\u6d41\u4f53\u5fae\u56e2\u4ece\u6d41\u573a\u79cd\u7684\u4e00\u70b9\u8fd0\u52a8\u5230\u53e6\u4e00\u70b9\uff0c\u56e0\u4e3a\u6d41\u7545\u7a7a\u95f4\u4e0d\u5747\u5300\u6027\u800c\u5f15\u8d77\u7684\u65f6\u95f4\u53d8\u5316\u7387\u3002</p> <p>\u901f\u5ea6\u6563\u5ea6\\(\\nabla \\cdot \\vec{V}\\)</p> <p>\u7531\u4e8e\u6d41\u4f53\u63a7\u5236\u4f53\u8fd0\u52a8\u5230\u4e0d\u540c\u533a\u57df\u65f6\u5bc6\u5ea6\u4e0d\u540c\uff0c\u56e0\u800c\u4f53\u79ef\\(\\mathscr{V}\\)\u4e0e\u63a7\u5236\u9762\\(S\\)\u4f1a\u968f\u7740\u65f6\u95f4\u6539\u53d8\uff1a</p> \\[ \\Delta \\mathscr{V} =[(\\vec{V} \\Delta t) \\cdot \\vec{n}] d S = (\\vec{V} \\Delta t) \\cdot d\\vec{S} \\] \\[ \\frac{D \\mathscr{V}}{Dt} = \\frac{1}{\\Delta t} \\iint_S (\\vec{V} \\Delta t) \\cdot d\\vec{S} = \\iint_S \\vec{V} \\cdot d\\vec{S} \\] <p>\u6839\u636e\u6563\u5ea6\u5b9a\u5f8b\uff1a</p> \\[ \\frac{D \\mathscr{V}}{Dt} = \\iiint_{\\mathscr{V}} (\\nabla \\cdot \\vec{V}) d\\mathscr{V} \\] \\[ \\frac{D (\\delta \\mathscr{V})}{Dt} = (\\nabla \\cdot \\vec{V}) \\delta \\mathscr{V} \\] <p>\u56e0\u6b64\u6211\u4eec\u5f97\u5230\\(\\(\\nabla \\cdot \\vec{V} = \\frac{1}{\\delta \\mathscr{V}} \\frac{D (\\delta \\mathscr{V})}{Dt}\\)\\)</p> <p>\u5373\\(\\nabla \\cdot \\vec{V}\\)\u662f\u5355\u4f4d\u4f53\u79ef\u7684\u6d41\u4f53\u5fae\u56e2\u4f53\u79ef\u7684\u65f6\u95f4\u53d8\u5316\u7387</p> <p>\u8fde\u7eed\u6027\u65b9\u7a0b</p> <p>\u5bf9\u4e8e\u7a7a\u95f4\u4f4d\u7f6e\u56fa\u5b9a\u7684\u6709\u9650\u63a7\u5236\u4f53\uff1a</p> \\[ \\frac{\\partial}{\\partial t}\\iiint_{\\mathscr{V}} \\rho d\\mathscr{V} + \\iint_S \\rho \\vec{V} \\cdot d\\vec{S} = 0 \\] <p>\u5176\u4e2d\uff0c\\(\\frac{\\partial}{\\partial t}\\iiint_{\\mathscr{V}} \\rho d\\mathscr{V}\\)\u662f\u4f53\u79ef\\(\\mathscr{V}\\)\u5185\u8d28\u91cf\u968f\u65f6\u95f4\u7684\u53d8\u5316\u7387\uff0c\\(\\iint_S \\rho \\vec{V} \\cdot d\\vec{S}\\)\u662f\u4f53\u79ef\\(\\mathscr{V}\\)\u662f\u5355\u4f4d\u65f6\u95f4\u5185\u6d41\u51fa(+)/\u6d41\u5165(-)\u63a7\u5236\u4f53\u7684\u51c0\u8d28\u91cf\u3002  </p> <p>\u5bf9\u4e8e\u968f\u7740\u6d41\u4f53\u8fd0\u52a8\u7684\u6709\u9650\u63a7\u5236\u4f53\uff1a</p> \\[ \\frac{D}{Dt} \\iiint_{\\mathscr{V}} \\rho d\\mathscr{V} = 0 \\] <p>\u6839\u636e\u7269\u8d28\u5bfc\u6570\uff0c\u6709\u9650\u63a7\u5236\u4f53\u5177\u6709\u56fa\u5b9a\u4e0d\u53d8\u7684\u603b\u8d28\u91cf\\(m=\\iiint_{\\mathscr{V}} \\rho d\\mathscr{V}\\)\uff0c\u56e0\u6b64\u5f97\u5230\u4e0a\u5f0f\u3002</p> <p>\u5bf9\u4e8e\u7a7a\u95f4\u4f4d\u7f6e\u56fa\u5b9a\u7684\u65e0\u7a77\u5c0f\u5fae\u56e2\uff1a</p> \\[ \\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\vec{V}) = 0 \\] <p>\u5bf9\u4e8e\u968f\u7740\u6d41\u4f53\u8fd0\u52a8\u7684\u7684\u65e0\u7a77\u5c0f\u5fae\u56e2\uff1a</p> \\[ \\frac{D(\\delta m)}{Dt} = \\frac{D(\\rho \\delta \\mathscr{V})}{Dt} =\\delta \\mathscr{V} \\frac{D(\\rho)}{Dt} + \\rho \\frac{D(\\delta \\mathscr{V})}{Dt} = 0 \\] \\[ \\frac{D \\rho}{D t} + \\rho \\nabla \\cdot  \\vec{V}  = 0 \\]"},{"location":"math/CFD/chapter/chapter1/#\u52a8\u91cf\u65b9\u7a0b","title":"\u52a8\u91cf\u65b9\u7a0b","text":"<p>\u7efc\u4e0a\uff0c\u5047\u8bbe\\(\\vec{f}\\)\u4e3a\u4f53\u79ef\u529b\uff0c\u5bf9\u4e8e\u8fd0\u52a8\u7684\u6d41\u4f53\u5fae\u56e2\uff1a</p> \\[ F_x = ma_x = [p - (p + \\frac{\\partial p}{\\partial x} dx)]dydz + [(\\tau_{xx} + \\frac{\\partial \\tau_{xx}}{\\partial x} dx) - \\tau_{xx}]dydz \\\\ + [(\\tau_{yx} + \\frac{\\partial \\tau_{yx}}{\\partial y} dy) - \\tau_{yx}]dxdz + [(\\tau_{zx} + \\frac{\\partial \\tau_{zx}}{\\partial z} dz) - \\tau_{zx}]dxdy + \\rho f_x dxdydz\\\\ =  (-\\frac{\\partial p}{\\partial x} + \\frac{\\partial \\tau_{xx}}{\\partial x}  + \\frac{\\partial \\tau_{yx}}{\\partial y} + \\frac{\\partial \\tau_{zx}}{\\partial z} )dxdydz + \\rho f_x dxdydz \\] <p></p> <p>\u56e0\u4e3a\u8fd0\u52a8\u7684\u6d41\u4f53\u5fae\u56e2\u8d28\u91cf\u56fa\u5b9a\u4e0d\u53d8\\(m = \\rho dxdydz\\)\uff0c\u4e14\\(a_x = \\frac{Du}{Dt}\\)\uff0c\u56e0\u6b64\u5f97\u5230\u7c98\u6027\u6d41\u7684\u52a8\u91cf\u65b9\u7a0b\uff1a</p> \\[ \\begin{aligned} \\rho \\frac{Du}{Dt} = (-\\frac{\\partial p}{\\partial x} + \\frac{\\partial \\tau_{xx}}{\\partial x}  + \\frac{\\partial \\tau_{yx}}{\\partial y} + \\frac{\\partial \\tau_{zx}}{\\partial z} ) + \\rho f_x \\\\ \\rho \\frac{Dv}{Dt} = (-\\frac{\\partial p}{\\partial y} + \\frac{\\partial \\tau_{xy}}{\\partial x}  + \\frac{\\partial \\tau_{yy}}{\\partial y} + \\frac{\\partial \\tau_{zy}}{\\partial z} ) + \\rho f_y \\\\ \\rho \\frac{Dw}{Dt} = (-\\frac{\\partial p}{\\partial z} + \\frac{\\partial \\tau_{xz}}{\\partial x}  + \\frac{\\partial \\tau_{yz}}{\\partial y} + \\frac{\\partial \\tau_{zz}}{\\partial z} ) + \\rho f_z \\\\ \\end{aligned} \\] <p>\u7531\u4e8e:</p> \\[ \\begin{aligned} \\rho \\frac{Du}{Dt} &amp;= \\rho \\frac{\\partial u}{\\partial t} + \\rho \\vec{V} \\cdot \\nabla u  \\\\ \\rho \\frac{\\partial u}{\\partial t} &amp;= \\frac{\\partial (\\rho u)}{\\partial t} - u \\frac{\\partial \\rho}{\\partial t} \\\\ \\rho \\vec{V} \\cdot \\nabla u &amp;= \\nabla \\cdot (\\rho u \\vec{V}) - u \\nabla \\cdot (\\rho \\vec{V}) \\\\ \\end{aligned} \\] <p>\u6574\u5408\u4e0a\u8ff0\u5f0f\u5b50\uff0c\u5f97\u5230\uff1a</p> \\[ \\begin{aligned} \\rho \\frac{Du}{Dt} &amp;= \\rho \\frac{\\partial u}{\\partial t} + \\rho \\vec{V} \\cdot \\nabla u  \\\\ &amp;= \\frac{\\partial (\\rho u)}{\\partial t} - u \\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho u \\vec{V}) - u \\nabla \\cdot (\\rho \\vec{V}) \\\\ &amp;= \\frac{\\partial (\\rho u)}{\\partial t} + \\nabla \\cdot (\\rho u \\vec{V}) - u[\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\vec{V})] \\\\ &amp;= \\frac{\\partial (\\rho u)}{\\partial t} + \\nabla \\cdot (\\rho u \\vec{V})\\\\ \\end{aligned} \\] <p>\u4ee3\u5165\u4e0a\u8ff0\u5f0f\u5b50\u53ef\u5f97\\(Naive-Stokes\\)\u65b9\u7a0b\u7684\u5b88\u6052\u5f62\u5f0f\uff1a</p> \\[ \\begin{aligned} \\frac{\\partial (\\rho u)}{\\partial t} + \\nabla \\cdot (\\rho u \\vec{V}) = (-\\frac{\\partial p}{\\partial x} + \\frac{\\partial \\tau_{xx}}{\\partial x}  + \\frac{\\partial \\tau_{yx}}{\\partial y} + \\frac{\\partial \\tau_{zx}}{\\partial z} ) + \\rho f_x \\\\ \\frac{\\partial (\\rho v)}{\\partial t} + \\nabla \\cdot (\\rho v \\vec{V}) = (-\\frac{\\partial p}{\\partial y} + \\frac{\\partial \\tau_{xy}}{\\partial x}  + \\frac{\\partial \\tau_{yy}}{\\partial y} + \\frac{\\partial \\tau_{zy}}{\\partial z} ) + \\rho f_y \\\\ \\frac{\\partial (\\rho w)}{\\partial t} + \\nabla \\cdot (\\rho w \\vec{V}) = (-\\frac{\\partial p}{\\partial z} + \\frac{\\partial \\tau_{xz}}{\\partial x}  + \\frac{\\partial \\tau_{yz}}{\\partial y} + \\frac{\\partial \\tau_{zz}}{\\partial z} ) + \\rho f_z \\\\ \\end{aligned} \\]"},{"location":"math/CFD/chapter/chapter1/#\u80fd\u91cf\u65b9\u7a0b","title":"\u80fd\u91cf\u65b9\u7a0b","text":"<p>\u663e\u7136\uff0c\u6d41\u4f53\u5fae\u56e2\u5185\u80fd\u91cf\u7684\u53d8\u5316\u7387 = \u6d41\u5165\u5fae\u56e2\u7684\u51c0\u70ed\u6d41\u91cf + \u4f53\u79ef\u529b\u4e0e\u8868\u9762\u529b\u5bf9\u5fae\u56e2\u505a\u529f\u7684\u529f\u7387\u3002</p> <p>\u4f53\u79ef\u529b\u5bf9\u5fae\u56e2\u505a\u529f\u7684\u529f\u7387\uff1a</p> \\[ \\vec{f} \\cdot \\rho \\vec{V} dxdydz \\] <p>\u5bf9\u4e8e\u8868\u9762\u529b\uff08\u538b\u529b+\u5207\u5e94\u529b+\u6b63\u5e94\u529b\uff09\u5728x\u8f74\u4e0a\u7684\u505a\u529f\u529f\u7387:</p> \\[ [-\\frac{\\partial (up)}{\\partial x} + \\frac{\\partial (u \\tau_{xx})}{\\partial x}  + \\frac{\\partial (u \\tau_{yx})}{\\partial y} + \\frac{\\partial (u \\tau_{zx})}{\\partial z} ]dxdydz \\] <p>\u56e0\u6b64x,y,z\u65b9\u5411\u4e0a\u8868\u9762\u529b\u8d21\u732e\u7684\u529f\u7387\u603b\u548c\u4e3a\uff1a</p> \\[ [-(\\frac{\\partial (up)}{\\partial x}+\\frac{\\partial (vp)}{\\partial y}+\\frac{\\partial (wp)}{\\partial z}) + \\frac{\\partial (u \\tau_{xx})}{\\partial x}  + \\frac{\\partial (u \\tau_{yx})}{\\partial y} + \\frac{\\partial (u \\tau_{zx})}{\\partial z} +  \\frac{\\partial (v \\tau_{xy})}{\\partial x}  + \\\\ \\frac{\\partial (v \\tau_{yy})}{\\partial y} + \\frac{\\partial (v \\tau_{zy})}{\\partial z} + \\frac{\\partial (w \\tau_{xz})}{\\partial x}  + \\frac{\\partial (w \\tau_{yz})}{\\partial y} + \\frac{\\partial (w \\tau_{zz})}{\\partial z}]dxdydz + \\rho f \\cdot \\vec{V} dxdydz \\] <p></p> <p>\u5047\u8bbe\\(\\dot{q}\\)\u4e3a\u5355\u4f4d\u8d28\u91cf\u7684\u4f53\u79ef\u52a0\u70ed\u7387\uff0c\u5219\u5fae\u56e2\u7684\u4f53\u79ef\u52a0\u70ed\\(=\\rho \\dot{q} dxdydz\\)\u3002\u8fd8\u9700\u8981\u8003\u8651\u5468\u56f4\u901a\u8fc7\u5fae\u56e2\u8868\u9762\u7684\u70ed\u4f20\u5bfc\uff1a</p> \\[ \\Big[\\dot{q}_x - (\\dot{q}_x + \\frac{\\partial \\dot{q}_x}{\\partial x} dx) \\Big] dydz = - \\frac{\\partial \\dot{q}_x}{\\partial x} dxdydz \\] <p>\u70ed\u4f20\u5bfc\u5bf9\u6d41\u4f53\u5fae\u56e2\u7684\u52a0\u70ed$$ - (\\frac{\\partial \\dot{q}_x}{\\partial x} + \\frac{\\partial \\dot{q}_y}{\\partial y} + \\frac{\\partial \\dot{q}_z}{\\partial z} )dxdydz$$</p> <p>\u6839\u636e\u5085\u91cc\u53f6\u5bfc\u70ed\u5b9a\u5f8b\uff1a\\(\\dot{q}_x = - k \\frac{\\partial T}{\\partial x},\\dot{q}_y = - k \\frac{\\partial T}{\\partial y},\\dot{q}_z = - k \\frac{\\partial T}{\\partial z}\\)</p> <p>\u56e0\u6b64\u6d41\u5165\u5fae\u56e2\u7684\u51c0\u70ed\u6d41\u91cf\uff1a</p> \\[ \\Big[ \\rho \\dot{q} + \\frac{\\partial}{\\partial x} (k \\frac{\\partial T}{\\partial x}) + \\frac{\\partial}{\\partial y} (k \\frac{\\partial T}{\\partial y}) + \\frac{\\partial}{\\partial z} (k \\frac{\\partial T}{\\partial z}) \\Big] dxdydz \\] <p>\u800c\u8fd0\u52a8\u7740\u7684\u6d41\u4f53\u5fae\u56e2\u7684\u603b\u80fd\u91cf=\u52a8\u80fd+\u5185\u80fd\uff08\\(e + V^2/2\\)\uff09\uff0c\u5219\u6d41\u4f53\u5fae\u56e2\u5185\u80fd\u91cf\u7684\u53d8\u5316\u7387\uff1a</p> \\[ \\rho \\frac{D}{Dt} (e + \\frac{V^2}{2})dxdydz \\] <p>\u56e0\u800c\u5f97\u5230\u975e\u5b88\u6052\u7684\u80fd\u91cf\u65b9\u7a0b:</p> \\[ \\begin{aligned} \\rho \\frac{D}{Dt} (e + \\frac{V^2}{2}) =&amp; \\Big[ \\rho \\dot{q} + \\frac{\\partial}{\\partial x} (k \\frac{\\partial T}{\\partial x}) + \\frac{\\partial}{\\partial y} (k \\frac{\\partial T}{\\partial y}) + \\frac{\\partial}{\\partial z} (k \\frac{\\partial T}{\\partial z}) \\Big]  \\\\ -&amp; (\\frac{\\partial (up)}{\\partial x}+\\frac{\\partial (vp)}{\\partial y}+\\frac{\\partial (wp)}{\\partial z}) + \\frac{\\partial (u \\tau_{xx})}{\\partial x}  + \\frac{\\partial (u \\tau_{yx})}{\\partial y} +  \\frac{\\partial (u \\tau_{zx})}{\\partial z} + \\frac{\\partial (v \\tau_{xy})}{\\partial x}  \\\\ +&amp; \\frac{\\partial (v \\tau_{yy})}{\\partial y} + \\frac{\\partial (v \\tau_{zy})}{\\partial z} + \\frac{\\partial (w \\tau_{xz})}{\\partial x}  + \\frac{\\partial (w \\tau_{yz})}{\\partial y} + \\frac{\\partial (w \\tau_{zz})}{\\partial z} + \\rho f \\cdot \\vec{V} \\end{aligned} \\] <p>\u6839\u636e\u7269\u8d28\u5bfc\u6570\u7684\u5b9a\u4e49\uff1a</p> \\[ \\begin{aligned} \\rho \\frac{De}{Dt} &amp;= \\rho \\frac{\\partial e}{\\partial t} + \\rho \\vec{V} \\cdot \\nabla e \\\\ &amp;= \\frac{\\partial (\\rho e)}{\\partial t} - e [\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\vec{V})] + \\nabla (\\rho e \\vec{V} ) \\\\ &amp;= \\frac{\\partial (\\rho e)}{\\partial t} + \\nabla \\cdot (\\rho e \\vec{V} ) \\\\ \\end{aligned} \\] <p>\u663e\u7136\uff1a</p> \\[ \\rho \\frac{D}{Dt}(e + \\frac{V^2}{2}) = \\frac{\\partial}{\\partial t}[\\rho (e + \\frac{V^2}{2})] + \\nabla \\cdot (\\rho (e + \\frac{V^2}{2}) \\vec{V} ) \\] <p>\u56e0\u6b64\uff0c\u5f97\u5230\u5b88\u6052\u7684\u80fd\u91cf\u65b9\u7a0b\u53ef\u4ee5\u5199\u4e3a\uff1a</p> \\[ \\begin{aligned} &amp;\\frac{\\partial}{\\partial t}[\\rho (e + \\frac{V^2}{2})] + \\nabla \\cdot (\\rho (e + \\frac{V^2}{2}) \\vec{V} ) \\\\ =&amp; \\Big[ \\rho \\dot{q} + \\frac{\\partial}{\\partial x} (k \\frac{\\partial T}{\\partial x}) + \\frac{\\partial}{\\partial y} (k \\frac{\\partial T}{\\partial y}) + \\frac{\\partial}{\\partial z} (k \\frac{\\partial T}{\\partial z}) \\Big]  \\\\ -&amp; (\\frac{\\partial (up)}{\\partial x}+\\frac{\\partial (vp)}{\\partial y}+\\frac{\\partial (wp)}{\\partial z}) + \\frac{\\partial (u \\tau_{xx})}{\\partial x}  + \\frac{\\partial (u \\tau_{yx})}{\\partial y} +  \\frac{\\partial (u \\tau_{zx})}{\\partial z} + \\frac{\\partial (v \\tau_{xy})}{\\partial x}  \\\\ +&amp; \\frac{\\partial (v \\tau_{yy})}{\\partial y} + \\frac{\\partial (v \\tau_{zy})}{\\partial z} + \\frac{\\partial (w \\tau_{xz})}{\\partial x}  + \\frac{\\partial (w \\tau_{yz})}{\\partial y} + \\frac{\\partial (w \\tau_{zz})}{\\partial z} + \\rho f \\cdot \\vec{V} \\end{aligned} \\] <p>\u4ee5\u4e0a\u662f\u975e\u5b9a\u5e38\u4e09\u7ef4\u7c98\u6027\u6d41\u52a8\u7684Navier-Stokes\u65b9\u7a0b\u7684\u5168\u90e8\u5f62\u5f0f</p>"},{"location":"math/CFD/chapter/chapter1/#\u65e0\u7c98\u6d41\u6b27\u62c9eulaer\u65b9\u7a0b","title":"\u65e0\u7c98\u6d41\u6b27\u62c9\uff08Eulaer\uff09\u65b9\u7a0b","text":"<p>\u65e0\u7c98\u6d41\uff1a\u5ffd\u7565\u8017\u6563\u3001\u7c98\u6027\u8fd0\u8f93\u3001\u8d28\u91cf\u6269\u6563\u4ee5\u53ca\u70ed\u4f20\u5bfc\u7684\u6d41\u52a8\uff08\u53bb\u6389\u6469\u64e6\u9879\u548c\u70ed\u4f20\u5bfc\u9879\uff09</p> <p>\u8fde\u7eed\u6027\u65b9\u7a0b</p> \\[ \\text{\u975e\u5b88\u6052\u5f62\u5f0f\uff1a} \\ \\frac{D \\rho}{D t} + \\rho \\nabla \\cdot \\vec{V} = 0 \\] \\[ \\text{\u5b88\u6052\u5f62\u5f0f\uff1a} \\ \\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\vec{V}) = 0 \\] <p>\u52a8\u91cf\u65b9\u7a0b</p> <p>\u975e\u5b88\u6052\u5f62\u5f0f\uff1a</p> \\[ \\begin{cases} \\rho \\frac{D u}{D t} = - \\frac{\\partial p}{\\partial x} + \\rho f_x \\\\ \\rho \\frac{D v}{D t} = - \\frac{\\partial p}{\\partial y} + \\rho f_y \\\\ \\rho \\frac{D w}{D t} = - \\frac{\\partial p}{\\partial z} + \\rho f_z \\\\ \\end{cases} \\] <p>\u5b88\u6052\u5f62\u5f0f\uff1a</p> \\[ \\begin{cases} \\frac{\\partial (\\rho u)}{\\partial t} + \\nabla \\cdot (\\rho u \\vec{V}) = - \\frac{\\partial p}{\\partial x} + \\rho f_x \\\\ \\frac{\\partial (\\rho v)}{\\partial t} + \\nabla \\cdot (\\rho v \\vec{V}) = - \\frac{\\partial p}{\\partial y} + \\rho f_y \\\\ \\frac{\\partial (\\rho w)}{\\partial t} + \\nabla \\cdot (\\rho w \\vec{V}) = - \\frac{\\partial p}{\\partial z} + \\rho f_z \\\\ \\end{cases} \\] <p>\u80fd\u91cf\u65b9\u7a0b</p> <p>\u975e\u5b88\u6052\u5f62\u5f0f\uff1a</p> \\[ \\rho \\frac{D}{Dt} (e + \\frac{V^2}{2}) = \\rho \\dot{q} - \\frac{\\partial (u p)}{\\partial x} - \\frac{\\partial (v p)}{\\partial y} - \\frac{\\partial (w p)}{\\partial z} + \\rho f \\cdot \\vec{V} \\] <p>\u5b88\u6052\u5f62\u5f0f\uff1a</p> \\[  \\frac{\\partial}{\\partial t} \\Big[\\rho (e + \\frac{V^2}{2}) \\Big] + \\nabla \\cdot  \\Big[\\rho (e + \\frac{V^2}{2}) \\Big]= \\rho \\dot{q} - \\frac{\\partial (u p)}{\\partial x} - \\frac{\\partial (v p)}{\\partial y} - \\frac{\\partial (w p)}{\\partial z} + \\rho f \\cdot \\vec{V} \\]"},{"location":"math/CFD/chapter/chapter1/#cfd\u4e2d\u7684\u63a7\u5236\u65b9\u7a0b","title":"CFD\u4e2d\u7684\u63a7\u5236\u65b9\u7a0b","text":"<p>\u8054\u5408\u6240\u6709\u63a7\u5236\u65b9\u7a0b\u7684\u5b88\u6052\u5f62\u5f0f\uff1a\u8fde\u7eed\u6027\u65b9\u7a0b\uff0c\u52a8\u91cf\u65b9\u7a0b\uff0c\u80fd\u91cf\u65b9\u7a0b</p> \\[ \\frac{\\partial U}{\\partial t} + \\frac{\\partial F-F_v}{\\partial x} + \\frac{\\partial G-G_v}{\\partial y} + \\frac{\\partial H-H_v}{\\partial z}= J \\] <p>\u5176\u4e2d\\(U,F,G,H,J\\)\u4e3a\u5217\u5411\u91cf\uff1a</p> \\[ U = \\begin{pmatrix} \\rho \\\\ \\rho u \\\\ \\rho v \\\\ \\rho w \\\\ \\rho (e+\\frac{V^2}{2}) \\end{pmatrix} \\] \\[ F = \\begin{pmatrix} \\rho u \\\\ \\rho u^2 + p \\\\ \\rho uv \\\\ \\rho uw \\\\ \\rho u(e + \\frac{u^2}{2}) +pu \\end{pmatrix}, F_v = \\begin{pmatrix} 0 \\\\ \\tau_{xx} \\\\ \\tau_{xy} \\\\ \\tau_{xz} \\\\ k \\frac{\\partial T}{\\partial x} + u \\tau_{xx} + v \\tau_{xy} + w \\tau_{xz} \\end{pmatrix} \\] \\[ G = \\begin{pmatrix} \\rho v \\\\ \\rho uv  \\\\ \\rho v^2 + p \\\\ \\rho wv \\\\ \\rho v(e + \\frac{u^2}{2}) + pv \\end{pmatrix}, G_v = \\begin{pmatrix} 0 \\\\ \\tau_{yx} \\\\ \\tau_{yy} \\\\ \\tau_{yz} \\\\ k \\frac{\\partial T}{\\partial y} + u \\tau_{yx} + v \\tau_{yy} + w \\tau_{yz} \\end{pmatrix} \\] \\[ H = \\begin{pmatrix} \\rho w \\\\ \\rho wu  \\\\ \\rho vw \\\\ \\rho w^2 + p \\\\ \\rho w(e + \\frac{u^2}{2}) + pw \\end{pmatrix}, H_v = \\begin{pmatrix} 0 \\\\ \\tau_{zx} \\\\ \\tau_{zy} \\\\ \\tau_{zz} \\\\ k \\frac{\\partial T}{\\partial z} + u \\tau_{zx} + v \\tau_{zy} + w \\tau_{zz} \\end{pmatrix} \\] \\[ J = \\begin{pmatrix} 0  \\\\ \\rho f_x  \\\\ \\rho f_y \\\\ \\rho f_z \\\\ \\rho \\dot{q} + \\rho(uf_x + vf_y + wf_z)  \\end{pmatrix} \\] <p>\u5176\u4e2d\u5217\u5411\u91cf\\(F,G,H\\)\u4e3a\u901a\u91cf\u9879\uff0c\\(J\\)\u4ee3\u8868\u6e90\u9879\uff0c\\(U\\)\u5219\u79f0\u4e3a\u89e3\u5411\u91cf\u3002</p>"},{"location":"math/HigherAlgebra/","title":"\u9ad8\u7b49\u4ee3\u6570\uff1aHigher Algebra \u4e2a\u4eba\u7b14\u8bb0","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599: </p> <p>\u7ae0\u8282</p> <p> <ul> <li> <p> 1\uff1a\u77e9\u9635\u57fa\u7840 </p> </li> <li> <p> 2\uff1a\u591a\u9879\u5f0f\u73af </p> </li> <li> <p> 3\uff1a\u7ebf\u6027\u7a7a\u95f4 </p> </li> <li> <p> 4\uff1a\u7ebf\u6027\u6620\u5c04 </p> </li> <li> <p> 5\uff1a\u5177\u6709\u5ea6\u91cf\u7684\u7ebf\u6027\u7a7a\u95f4 </p> </li> <li> <p> 6\uff1a\u591a\u91cd\u7ebf\u6027\u4ee3\u6570 </p> </li> </ul> <p></p>"},{"location":"math/HigherAlgebra/chapter/1/","title":"Higher Algebra: \u77e9\u9635","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599:</p> <p></p>"},{"location":"math/HigherAlgebra/chapter/1/#\u77e9\u9635\u7684\u6027\u8d28","title":"\u77e9\u9635\u7684\u6027\u8d28","text":"<p>\u77e9\u9635\u7684\u9006\uff1a\u8bbe\\(A\\)\u4e3an\u9636\u65b9\u9635\uff0c\u82e5\u5b58\u5728n\u9636\u65b9\u9635\\(B\\)\uff0c\u4f7f\u5f97\\(AB=BA=I_n\\)\uff0c\u5219\u79f0\\(A\\)\u53ef\u9006\uff0c\\(B\\)\u4e3a\\(A\\)\u7684\u9006\u77e9\u9635\uff0c\u8bb0\u4e3a\\(A^{-1}\\)\u3002</p> <p>\u82e5\u77e9\u9635\u6ca1\u6709\u9006\uff0c\u5219\u79f0\u77e9\u9635\u4e3a\u5947\u5f02\u9635\uff0c\u53cd\u4e4b\u5219\u79f0\u77e9\u9635\u4e3a\u975e\u5f02\u9635\u6216\u53ef\u9006\u9635\u3002\u53ef\u9006\u77e9\u9635\u7ecf\u521d\u7b49\u53d8\u6362\u540e\u4ecd\u662f\u53ef\u9006\u77e9\u9635\uff0c\u5947\u5f02\u9635\u7ecf\u521d\u7b49\u53d8\u6362\u540e\u4ecd\u662f\u5947\u5f02\u9635\u3002</p> <p>\u8bbe\\(A\\)\u662f\u4e00\u4e2an\u9636\u53ef\u9006\u77e9\u9635\uff0c\u5219\u901a\u8fc7\u6709\u9650\u6b21\u7684\u521d\u7b49\u53d8\u6362\\(P_m...P_1 A\\)\u5c31\u53ef\u4ee5\u8f6c\u5316\u4e3a\u5355\u4f4d\u77e9\u9635\\(I_n\\)\u3002\uff08\\(A = P_1^{-1}...P_m^{-1}\\)\uff09</p> <p>\u77e9\u9635\u7684\u76f8\u62b5\uff1a\u82e5\u4e00\u77e9\u9635\\(A\\)\u7ecf\u8fc7\u6709\u9650\u6b21\u521d\u7b49\u53d8\u6362\u540e\u53d8\u6210\\(B\\),\u5219\u79f0\\(A\\)\u4e0e\\(B\\)\u662f\u7b49\u4ef7\u7684\uff0c\u6216\\(A\\)\u4e0e\\(B\\)\u76f8\u62b5\uff0c\u8bb0\u4e3a\\(A\\sim B\\).</p> <p>\u5bf9\u4e8e\u4efb\u4e00\u77e9\u9635\\(A_{m \\times n} = (a_{ij})_{m \\times n}\\)\u5fc5\u76f8\u62b5\u4e8e\u4e0b\u5217\\(m \\times n\\)\u77e9\u9635\uff1a</p> \\[ B = \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} \\] <p>\u5373\\(A \\sim B\\)\uff0c\u4efb\u4e00\\(m \\times n\\)\u77e9\u9635\u5747\u4e0e\u4e00\u4e2a\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u5143\u7d20\u7b49\u4e8e1\u62160\u800c\u800c\u5176\u4f59\u5143\u7d20\u5747\u4e3a0\u7684\\(m \\times n\\)\u77e9\u9635\u76f8\u62b5\u3002</p> <p>\u6613\u5f97\uff1a\u5bf9\u4efb\u610f\u4e00\u4e2a\u79e9\u4e3a\\(r\\)\u7684\\(m \\times n\\)\u77e9\u9635\\(A\\)\uff0c\u603b\u5b58\u5728\\(m\\)\u9636\u53ef\u9006\u9635\\(P\\)\u548c\\(n\\)\u9636\u53ef\u9006\u9635\\(Q\\)\uff0c\u4f7f\u5f97\uff1a</p> \\[ PAQ = \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} \\] <p>\u5173\u4e8e\u77e9\u9635\u7684\u79e9\uff1a</p> <ul> <li> <p>\u4efb\u610f\u77e9\u9635\\(A\\)\u7684\u8f6c\u7f6e\\(A^T\\)\u4e0e\\(A\\)\u5177\u6709\u76f8\u540c\u7684\u79e9\uff0c\u5373\\(rank(A) = rank(A^T)\\)</p> </li> <li> <p>\u4efb\u610f\u77e9\u9635\u4e0e\u53ef\u9006\u9635\u76f8\u4e58\uff0c\u5176\u79e9\u4e0d\u53d8</p> </li> <li> <p>\\(n\\)\u9636\u65b9\u9635\\(A\\)\u4e3a\u53ef\u9006\u9635\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u4e3a\\(A\\)\u4e3a\u6ee1\u79e9\u9635\uff08\\(rank(A) = n\\)\uff09</p> </li> <li> <p>\u4e24\u4e2a\\(m \\times n\\)\u77e9\u9635\u7b49\u4ef7\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u4e3a\u5b83\u4eec\u5177\u6709\u76f8\u540c\u7684\u79e9\uff0c\u5373\\(rank(A_{m \\times n}) = rank(B_{m \\times n})\\)</p> </li> <li> <p>\u5bf9\u4e8e\u7ed9\\(A_{m \\times n}\\)\u4e0e\\(B_{n \\times s}\\)\uff0c\u6709\\(\\(r(A) + r(B) -n \\leq rank(AB) \\leq min \\{ r(A), r(B) \\}\\)\\) \u8bc1\u660eSylvester\u4e0d\u7b49\u5f0f\uff1a\\(\\(\\begin{pmatrix} I_n &amp; 0 \\\\ 0 &amp; AB \\end{pmatrix} \\sim \\begin{pmatrix} I_n &amp; 0 \\\\ A &amp; AB \\end{pmatrix} \\sim \\begin{pmatrix} I_n &amp; A \\\\ -B &amp; 0 \\end{pmatrix} \\sim \\begin{pmatrix} -B &amp; I_n \\\\ 0 &amp; A \\end{pmatrix}\\)\\) \u56e0\u6b64\\(r(A) + r(B) -n \\leq rank(AB)\\)\u663e\u7136\u6210\u7acb\u3002</p> </li> </ul>"},{"location":"math/HigherAlgebra/chapter/1/#\u77e9\u9635\u5206\u5757","title":"\u77e9\u9635\u5206\u5757","text":"<p>\u8bbe\u77e9\u9635\\(A_{s \\times} n\\)\u4e0e\\(B_{n \\times m}\\)\uff0c\\(C_{s \\times m}\\)\uff0c\u5219\\(B\\)\u7684\u5217\u5411\u91cf\u7ec4\u4e3a\\(\\beta_1,\\beta_2,...,\\beta_m\\)\uff0c\u5219\\(AB=A(\\beta_1,\\beta_2,...,\\beta_m)=(A\\beta_1,A\\beta_2,...,A\\beta_m)\\)\u3002\\(C\\)\u7684\u5217\u5411\u91cf\u7ec4\\(\\delta_1,\\delta_2,...,\\delta_m\\)\u3002</p> <p>\u5bf9\u4e8e\\(AB=C\\)\u53ef\u4ee5\u770b\u4f5c\\((A\\beta_1,A\\beta_2,...,A\\beta_m)=(\\delta_1,\\delta_2,...,\\delta_m) \\ \\rightarrow \\ A \\beta_j = \\delta_j \\ (j=1,2,...,m)\\)\uff0c\u5373\\(\\beta_j\\)\u662f\\(Ax=\\delta_j\\)\u7684\u4e00\u4e2a\u89e3\u3002</p> <p>$Exercise.  $ \u8bc1\u660e\uff1a\u8bbe\\(A_{s \\times n},B_{n \\times m}\\)\uff0c\u82e5\\(AB=0\\)\uff0c\u8bc1\u660e\uff1a\\(rank(A)+rank(B) \\leq n\\)\u3002</p> <p>\\(B\\)\u7684\u5217\u5411\u91cf\u7ec4\u4e3a\\(\\beta_1,\\beta_2,...,\\beta_m\\)\u3002\u663e\u7136\\(\\beta_j\\)\u5c5e\u4e8e\\(Ax=0\\)\u7684\u89e3\u7a7a\u95f4\\(W\\)\uff0c\u5219\\(rank(B) = dim&lt;\\beta_1,\\beta_2,...,\\beta_m&gt; \\leq dim \\ W = n - rank(A)\\)\u3002\u5373\u4e3a\\(rank(A)+rank(B) \\leq n\\)</p>"},{"location":"math/HigherAlgebra/chapter/1/#n\u7ef4\u5411\u91cf\u7a7a\u95f4","title":"n\u7ef4\u5411\u91cf\u7a7a\u95f4","text":"<p>$Def.1  $ \u5b9a\u4e49\u6570\u57df\\(K\\)\u4e0a\u6240\u6709\\(n\\)\u5143\u6709\u5e8f\u6570\u7ec4\u7ec4\u6210\u7684\u51e0\u4f55\\(K^n = \\{ (a_1,a_2,...,a_n) | a_i \\in K, \\ i=1,2,...,n \\}\\)\uff0c\u8fde\u540c\u5b9a\u4e49\u5728\u5176\u4e0a\u9762\u7684\u52a0\u6cd5\u8fd0\u7b97\u548c\u6570\u4e58\u8fd0\u7b97\uff0c\u53ca\u5176\u6ee1\u8db3\u7684\u8fd0\u7b97\u6cd5\u5219\uff0c\u79f0\u4e3a\u6570\u57df\\(K\\)\u4e0a\u7684\u4e00\u4e2a\\(n\\)\u7ef4\u5411\u91cf\u7a7a\u95f4\u3002</p> <p>$Def.2  $ \u82e5\\(K^n\\)\u7684\u4e00\u4e2a\u975e\u7a7a\u5b50\u96c6\\(U\\)\u6ee1\u8db3\uff1a\uff081\uff09\\(\\mathbf{\\alpha,\\gamma} \\in U \\ \\rightarrow \\ \\mathbf{\\alpha + \\gamma} \\in U\\) \uff1b\uff082\uff09\\(\\mathbf{\\alpha} \\in U, \\ k \\in K \\ \\rightarrow \\ k\\mathbf{\\alpha} \\in U\\)\uff0c\u5219\u79f0\\(U\\)\u4e3a\\(K^n\\)\u7684\u4e00\u4e2a\u7ebf\u6027\u5b50\u7a7a\u95f4\uff08\u52a0\u6cd5\u5c01\u95ed\u3001\u6570\u4e58\u5c01\u95ed\uff09\u3002</p> <p>\\(Exercise. \\ \\(\u5047\u8bbe\\)\\mathbf{a_1,...,a_s}\\)\u7ebf\u6027\u65e0\u5173\uff0c\u6709\uff1a</p> \\[ \\begin{aligned} \\mathbf{b_1} =&amp; a_{11} \\mathbf{a_1} + ... + a_{1s} \\mathbf{a_s} \\\\ ...&amp; \\\\ \\mathbf{b_s} =&amp; a_{s1} \\mathbf{a_1} + ... + a_{1s} \\mathbf{a_s} \\\\ \\end{aligned} \\] <p>\u8bc1\u660e\uff1a\\(\\mathbf{b_1},...,\\mathbf{b_s}\\)\u7ebf\u6027\u65e0\u5173\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u662f\uff1a\\(\\(\\begin{vmatrix} a_{11} &amp; ... &amp; a_{s1}  \\\\ ... &amp; &amp; ... \\\\ a_{1s} &amp; ... &amp; a_{ss} \\end{vmatrix} \\neq 0\\)\\)</p> <p>\u8bbe\\(k_1 \\mathbf{b_1} + ... + k_s \\mathbf{b_s} = 0\\)\uff0c\u5373\u4e3a\\((k_1 a_{11}+...+k_s a_{s1}) \\mathbf{a_1} + ...+ (k_1 a_{1s}+...+k_s a_{ss}) \\mathbf{a_s} = 0\\)\u3002\u5df2\u77e5\\(\\mathbf{a_1,...,a_s}\\)\u7ebf\u6027\u65e0\u5173\uff1a</p> \\[ \\begin{cases} k_1 a_{11}+...+k_s a_{s1} =0\\\\ ... \\\\ k_1 a_{1s}+...+k_s a_{ss} =0\\\\ \\end{cases}  \\ \\rightarrow \\ |A| =   \\begin{vmatrix} a_{11} &amp; ... &amp; a_{s1}  \\\\ ... &amp; &amp; ... \\\\ a_{1s} &amp; ... &amp; a_{ss} \\end{vmatrix}  \\] <p>\u82e5\\(|A| \\neq 0\\)\uff0c\u5219\u4ee5\u4e0a\u9f50\u6b21\u7ebf\u6027\u65b9\u7a0b\u53ea\u6709\u96f6\u89e3\uff0c\\(k_1=0,...k_s=0\\)\uff0c\\(\\mathbf{b_1},...,\\mathbf{b_s}\\)\u7ebf\u6027\u65e0\u5173</p> <p>$Def.3  $\u5411\u91cf\u7ec4\u7684\u6781\u5927\u7ebf\u6027\u65e0\u5173\u7ec4\u6240\u542b\u7684\u5411\u91cf\u4e2a\u6570\u79f0\u4e3a\u8fd9\u4e2a\u5411\u91cf\u7ec4\u7684\u79e9\u3002</p> <p>\u82e5\u5411\u91cf\u7ec4\\(\\mathbf{a_1,...,a_s}\\)\u53ef\u4ee5\u7531\u5411\u91cf\u7ec4\\(\\mathbf{b_1,...,b_s}\\)\u7ebf\u6027\u8868\u51fa\uff0c\u5219\\(rank(\\mathbf{a_1,...,a_s}) \\leq rank(\\mathbf{b_1,...,b_s})\\)\u3002\u7b49\u4ef7\u7684\u5411\u91cf\u7ec4\u6709\u76f8\u540c\u7684\u79e9\uff0c\u4f46\u79e9\u76f8\u540c\u7684\u5411\u91cf\u7ec4\u4e0d\u4e00\u5b9a\u7b49\u4ef7\uff08\u9664\u975e\u6ee1\u79e9\uff09\u3002\u56e0\u6b64\uff0c\u4e24\u4e2a\u5411\u91cf\u7ec4\u7b49\u4ef7\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u662f\uff1a\u79e9\u76f8\u540c\u4e14\u5176\u4e2d\u4e00\u4e2a\u5411\u91cf\u7ec4\u80fd\u7531\u53e6\u4e00\u4e2a\u5411\u91cf\u7ec4\u7ebf\u6027\u6807\u51fa\u3002</p> <p>\\(Def.4 \\ \\(\u8bbe\\)U\\)\u662f\\(K^n\\)\u7684\u4e00\u4e2a\u5b50\u7a7a\u95f4\uff0c\u82e5\u7ebf\u6027\u65e0\u5173\u7684\u5411\u91cf\u7ec4\\(\\mathbf{a_1,...,a_s} \\in U\\)\uff0c\u4e14\\(U\\)\u7684\u6bcf\u4e2a\u5411\u91cf\u90fd\u53ef\u7531\\(\\mathbf{a_1,...,a_s}\\)\u7ebf\u6027\u8868\u51fa\uff0c\u5219\u79f0\\(\\mathbf{a_1,...,a_s}\\)\u4e3a\\(U\\)\u7684\u4e00\u4e2a\u57fa\u3002\u57fa\u6240\u542b\u7684\u5411\u91cf\u4e2a\u6570\u4e3a\\(U\\)\u7684\u7ef4\u6570\uff0c\u8bb0\u4f5c\\(dim_K(U) = s\\)\u3002\uff08\u663e\u7136\\(dimK^N = n\\)\uff09</p> <p>\u82e5\\(U\\)\u4e0e\\(W\\)\u5747\u4e3a\\(K^n\\)\u7684\u975e\u96f6\u5b50\u7a7a\u95f4\uff0c\u4e14\\(U \\subseteq W \\(\uff0c\u90a3\u4e48\\)U\\)\u7684\u57fa\\(\\mathbf{a_1,...,a_r}\\)\u53ef\u7531\\(W\\)\u7684\u57fa\\(\\mathbf{\\eta_1,...,\\eta_t}\\)\u7ebf\u6027\u8868\u51fa\uff0c\\(r \\leq t\\)\uff0c\u56e0\u6b64\\(dim \\ U \\leq dim \\ W\\)</p> <p>$Def.5  $ \u6570\u57df\\(K\\)\u4e0an\u5143\u9f50\u6b21\u7ebf\u6027\u65b9\u7a0b\u7ec4\\(AX=0\\)\u7684\u89e3\u7a7a\u95f4\\(W\\)\u7684\u7ef4\u6570\\(dim \\ W = n - rank(A)\\)</p> <p>\u8bc1\u660e\uff1b\u56e0\u4e3a\\(rank(A) = r\\)\uff0c\u5219\\(AX=0\\)\u53ef\u5199\u6210\u5982\u4e0b\uff1a</p> \\[ \\begin{cases} x_1= -b_{1,r+1} x_{r+1} - ... - b_{1,n} x_n \\\\ x_2= -b_{2,r+1} x_{r+1} - ... - b_{2,n} x_n \\\\ ... \\\\ x_r = -b_{r,r+1} x_{r+1} - ... - b_{r,n} x_n \\\\ x_{r+1} = 1 x_{r+1} + ... + 0 x_n \\\\ ... \\\\ x_n = 0 x_{r+1} + ... + 1 x_n\\\\ \\end{cases} \\] <p>\u53ef\u5f97\u7ebf\u6027\u65e0\u5173\u7684n-r\u4e2a\u89e3\\(\\mathbf{\\eta_1,...,\\eta_{n-r}}\\)\uff0c\u5373\uff1a</p> \\[ \\mathbf{\\eta_1} = \\begin{pmatrix} -b_{1,r+1} \\\\ ... \\\\ -b_{r,r+1} \\\\ 1 \\\\ 0 \\\\ ... \\\\ 0 \\end{pmatrix},...,\\mathbf{\\eta_{n-r}} = \\begin{pmatrix} -b_{1,r+1} \\\\ ... \\\\ -b_{r,r+1} \\\\ 0 \\\\ 0 \\\\ ... \\\\ 1 \\end{pmatrix} \\] <p>\u53d6\u5176\u4e2d\u4e00\u4e2a\u89e3\\(\\mathbf{\\eta} = (c_1,...,c_n)^T\\)\uff0c\u5219\uff1a</p> <p>$$ \\begin{aligned}</p> <p>\\mathbf{\\eta} =&amp; \\begin{pmatrix} c_1 \\ ... \\ c_r \\ c_{r+1} \\ ... \\ c_n \\end{pmatrix} =  \\begin{pmatrix} -b_{1,r+1} c_{r+1}&amp; -&amp; ...&amp; -&amp; b_{1,n} c_{n}&amp; \\ ... \\ -b_{r,r+1} c_{r+1}&amp; -&amp; ...&amp; -&amp; b_{r,n} c_{n}&amp; \\ 1 c_{r+1}&amp; +&amp; ...&amp; -&amp; 0 c_{n}&amp; \\ ... \\ 0 c_{r+1}&amp; +&amp; ...&amp; -&amp; 1 c_{n}&amp; \\ \\end{pmatrix} \\</p> <p>=&amp; c_{r+1} \\mathbf{\\eta}1 +...+ c \\end{aligned} $$} \\mathbf{\\eta}_{n-r</p> <p>\u663e\u7136\u6bcf\u4e00\u4e2a\u89e3\u90fd\u53ef\u7531\\(\\mathbf{\\eta_1,...,\\eta_{n-r}}\\)\u7ebf\u6027\u8868\u51fa(\u57fa\u7840\u89e3\u7cfb)\uff0c\u6240\u4ee5\\(\\mathbf{\\eta_1,...,\\eta_{n-r}}\\)\u662f\\(W\\)\u7684\u4e00\u4e2a\u57fa\uff0c\\(dim \\ W = n-rank(A)\\)</p> <p>$Exercise.  $ \u8bbe\\(A\\)\u662f\u6570\u57df\\(K\\)\u4e0a\\(s \\times n\\)\u77e9\u9635\uff0c\u8bc1\u660e\uff1a\u82e5\u5bf9\u4e8e\\(K^n\\)\u4e2d\u4efb\u610f\u4e00\u5217\u5411\u91cf\\(\\mathbf{\\eta}\\)\uff0c\u90fd\u6709\\(A \\mathbf{\\eta}=0\\)\uff0c\u5219\\(A = 0\\)\u3002</p> <p>\u5373\\(K^n\\)\u4e2d\u4efb\u610f\u4e00\u5217\u5411\u91cf\\(\\mathbf{\\eta}\\)\u90fd\u662f\\(AX=0\\)\u7684\u89e3\u3002\u89e3\u7a7a\u95f4\\(W=K^n\\)\uff0c\u53ef\u77e5\\(dim \\ W = n - rank(A) = n\\)\uff0c\u6240\u4ee5\\(rank(A) = 0\\)\uff0c\\(A=0\\)\u3002</p>"},{"location":"math/HigherAlgebra/chapter/1/#\u6b63\u4ea4\u77e9\u9635","title":"\u6b63\u4ea4\u77e9\u9635","text":"<p>\\(Def.1 \\ \\(\u5b9e\u6570\u57df\u4e0a\u7684n\u9636\u77e9\u9635\\)A\\)\u6ee1\u8db3\\(A^T A =I_n\\)\uff0c\u5219\\(A\\)\u4e3a\u6b63\u4ea4\u77e9\u9635\u3002\u82e5\\(A\\)\u53ef\u9006\uff0c\u5219\\(A^{-1} = A^T\uff0cAA^{-1}=I_n\\)\u3002</p> <ul> <li> <p>\u6b63\u4ea4\u77e9\u9635\\(A\\)\u7684\\(A^T\uff08A^{-1}\\)\u4e5f\u662f\u6b63\u4ea4\u77e9\u9635</p> </li> <li> <p>\u6b63\u4ea4\u77e9\u9635\u7684\u884c\u5217\u5f0f\\(|A| = \\pm 1\\)\uff0c\u56e0\u4e3a\\(det(A^T A) = det(I_n) = 1\\)\uff0c\u6240\u4ee5\\(det(A)^2 = 1\\)\uff0c\\(det(A) = \\pm 1\\)\u3002</p> </li> <li> <p>\u82e5\\(A\\)\u4e0e\\(B\\)\u4e3a\u6b63\u4ea4\u77e9\u9635\uff0c\u90a3\u4e48\\(AB\\)\u4e5f\u662f\u6b63\u4ea4\u77e9\u9635\uff0c\u56e0\u4e3a\\((AB)^T AB = A^T B^T AB = A^T I_n B = I_n\\)\u3002</p> </li> <li> <p>\u5bf9\u4e8e\u5b9e\u6570\u57df\u4e0a\u7684n\u9636\u77e9\u9635\\(A\\)\u7684\u884c\u5411\u91cf\u7ec4\\(\\mathbf{\\gamma_1,...,\\gamma_n}\\)\uff0c\u884c\u5217\u5411\u91cf\u7ec4\\(\\mathbf{a_1,...,a_n}\\)\u3002\u6ee1\u8db3\uff1a$$\\gamma_i \\gamma_j^T = \\delta^j_i, a_i a_j^T = \\delta^j_i,  1 \\leq i,j \\leq n $$</p> </li> </ul> <p>$Def. 2  $ \u5728\\(R^n\\)\u4e2d\u4efb\u53d6\\(\\mathbf{a},\\mathbf{b} \\in R^n\\)\uff0c\u89c4\u5b9a\uff1a\\((\\mathbf{a}, \\mathbf{b})=\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^T \\mathbf{b} = \\sum_{i=1}^n a_i b_i\\)\u3002\u5219\u79f0\\((\\mathbf{a}, \\mathbf{b})\\)\u4e3a\\(R^n\\)\u7684\u4e00\u4e2a\u6807\u51c6\u5185\u79ef\uff08\u5185\u79ef\uff09\u3002\u5728n\u7ef4\u5411\u91cf\u7a7a\u95f4\\(R^n\\)\u6709\u4e86\u6807\u51c6\u5185\u79ef\u540e\uff0c\u5219\\(R^n\\)\u6210\u4e3a\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u3002</p> <p>\u82e5\u975e\u96f6\u5411\u91cf\u7ec4\u4e2d\u7684\u5411\u91cf\u4e24\u4e24\u6b63\u4ea4\uff0c\u5219\u79f0\u4e3a\u6b63\u4ea4\u5411\u91cf\u7ec4\uff0c\u4e14\u8be5\u5411\u91cf\u7ec4\u4e00\u5b9a\u7ebf\u6027\u65e0\u5173\u3002\u82e5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\\(R^n\\)\u4e2d\uff0cn\u4e2a\u5411\u91cf\u7ec4\u6210\u7684\u6b63\u4ea4\u5411\u91cf\u7ec4\u4e00\u5b9a\u662f\\(R^n\\)\u7684\u57fa\uff08\u6b63\u4ea4\u57fa\uff09\u3002n\u4e2a\u5355\u4f4d\u5411\u91cf\u7ec4\u6210\u7684\u6b63\u4ea4\u5411\u91cf\u7ec4\u4e3a\\(R^n\\)\u7684\u6807\u51c6\u6b63\u4ea4\u57fa\u3002</p> <p>$Def. 3  $ \u65bd\u5bc6\u7279\u6b63\u4ea4\u5316\uff1a\u5047\u8bbe\\(\\mathbf{a_1,a_2,...,a_s}\\)\u4e3a\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\\(R^n\\)\u4e2d\u7684\u4e00\u4e2a\u7ebf\u6027\u65e0\u5173\u5411\u91cf\u7ec4\uff0c\u5219\uff1a</p> \\[ \\begin{cases} \\mathbf{\\beta_1} = \\mathbf{\\alpha_1} \\\\ \\mathbf{\\beta_2} = \\mathbf{\\alpha_2} - \\frac{(\\mathbf{\\alpha_2} ,\\mathbf{\\beta_1})}{(\\mathbf{\\beta_1},\\mathbf{\\beta_1})} \\mathbf{\\beta_1} \\\\ ...\\\\ \\mathbf{\\beta_s} = \\mathbf{\\alpha_s} - \\sum_{j=1}^{s-1} \\frac{(\\mathbf{\\alpha_s} ,\\mathbf{\\beta_j})}{(\\mathbf{\\beta_j},\\mathbf{\\beta_j})} \\mathbf{\\beta_j} \\\\ \\end{cases} \\] <p>\u901a\u8fc7\u65bd\u5bc6\u7279\u6b63\u4ea4\u5316\u65b9\u6cd5\uff0c\u5c06\u7ebf\u6027\u65e0\u5173\u5411\u91cf\u7ec4\\((\\mathbf{a_1,...,a_s})\\)\u6b63\u4ea4\u5316\uff0c\u5f97\u5230\u6b63\u4ea4\u5411\u91cf\u7ec4\\((\\mathbf{\\beta_1,...,\\beta_s})\\)\u3002</p> <p>\\(Exercise. \\ \\(\u8bbe\\)A\\)\u662f\u5b9e\u6570\u57df\u4e0a\u7684n\u9636\u53ef\u9006\u77e9\u9635\uff0c\u5219\\(A\\)\u53ef\u4ee5\u552f\u4e00\u5206\u89e3\u4e3a\u6b63\u4ea4\u77e9\u9635\\(T\\)\u4e0e\u4e3b\u5bf9\u89d2\u5143\u90fd\u4e3a\u6b63\u6570\u7684\u4e0a\u4e09\u89d2\u77e9\u9635\\(B\\)\u4e4b\u79ef:\\(A = TB\\)</p> <p>\u8bc1\u660e\uff1a\u8bbe\u53ef\u9006\u9635\\(A\\)\u7684\u884c\u5411\u91cf\u7ec4\u4e3a\\(\\mathbf{a_1,...,a_n}\\)\uff0c\u5219\\(\\mathbf{a_1,...,a_n}\\)\u7ebf\u6027\u65e0\u5173\uff0c\u4e14\\(\\mathbf{a_1,...,a_n}\\)\u662f\\(R^n\\)\u7684\u6807\u51c6\u6b63\u4ea4\u57fa\u3002\u8bbe\\(\\mathbf{a_1,...,a_n}\\)\u5bf9\u5e94\u7684\u6b63\u4ea4\u5411\u91cf\u7ec4\u4e3a\\(\\mathbf{\\beta_1,...,\\beta_n}\\)\uff0c\u6839\u636e\u65bd\u5bc6\u7279\u6b63\u4ea4\u5316\uff0c\u8bbe\\(b_{ji} = \\frac{(\\mathbf{a_j},\\mathbf{\\beta_i})}{(\\mathbf{\\beta_i},\\mathbf{\\beta_i})}, \\ i=2,3,...,n; \\ j=1,2,...,i-1\\)\uff0c\u5219\uff1a</p> \\[ \\begin{aligned} A &amp;= (\\mathbf{a_1,...,a_n}) = (\\mathbf{\\beta_1},\\mathbf{\\beta_2},...,\\mathbf{\\beta_n}) \\begin{pmatrix} 1 &amp; b_{12} &amp; b_{13} &amp; ... &amp; b_{1n} \\\\ 0 &amp; 1 &amp; b_{23} &amp; ... &amp; b_{2n} \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ... \\\\ 0 &amp; 0 &amp; 0 &amp; ... &amp; 1 \\end{pmatrix} \\\\ &amp;= (\\mathbf{\\eta_1},\\mathbf{\\eta_2},...,\\mathbf{\\eta_n})  \\begin{pmatrix} |\\beta_1| &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; |\\beta_2| &amp; ... &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp;  ... &amp; |\\beta_n| \\end{pmatrix} \\begin{pmatrix} 1 &amp; b_{12} &amp; b_{13} &amp; ... &amp; b_{1n} \\\\ 0 &amp; 1 &amp; b_{23} &amp; ... &amp; b_{2n} \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ... \\\\ 0 &amp; 0 &amp; 0 &amp; ... &amp; 1 \\end{pmatrix} \\\\ &amp;= (\\mathbf{\\eta_1},\\mathbf{\\eta_2},...,\\mathbf{\\eta_n})  \\begin{pmatrix} |\\beta_1| &amp; b_{12}|\\beta_1| &amp; b_{13}|\\beta_1| &amp; ... &amp; b_{1n}|\\beta_1| \\\\ 0 &amp; |\\beta_2| &amp; b_{23}|\\beta_2| &amp; ... &amp; b_{2n}|\\beta_2| \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ... \\\\ 0 &amp; 0 &amp; 0 &amp; ... &amp; |\\beta_n| \\end{pmatrix} \\\\ &amp;= TB \\\\ \\end{aligned} \\] <p>\u552f\u4e00\u6027\uff1a\\(TB = T_1 B_1 \\ \\rightarrow \\ T_1^{-1} T = B_1 B^{-1}\\)\u3002\u663e\u7136\\(T_1^{-1}T\\)\u5373\u4e3a\u6b63\u4ea4\u77e9\u9635\uff0c\u4e5f\u662f\u4e0a\u4e09\u89d2\u77e9\u9635\uff0c\u6613\u5f97\u5176\\(a^2_{kk}=1, \\ k=1,2,...,n\\)\uff0c\u5373\u4e3a\u5355\u4f4d\u77e9\u9635\uff0c\u6240\u4ee5\\(B_1 B^{-1} = I_n\\)\uff0c\\(B_1 = B\uff0cT_1 =T\\)\u3002</p> <p>\u4e0a\u8ff0\u5373\u4e3a\u65b9\u9635\u7684QR\u5206\u89e3\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u63a8\u5e7f\u5230\u975e\u65b9\u9635\uff0c\u8bbe\\(A\\)\u662f\u5b9e\u6570\u57df\u4e0a\u7684\\(m \\times n\\)\u77e9\u9635\uff08\\(m&gt;n\\)\uff09\uff0c\u5219\\(A\\)\u53ef\u4ee5\u552f\u4e00\u5206\u89e3\u4e3a\u5217\u5411\u91cf\u4e3a\u6b63\u4ea4\u5355\u4f4d\u5411\u91cf\u7ec4\u7684\\(Q_{m \\times n}\\)\u4e0e\u4e3b\u5bf9\u89d2\u5143\u5747\u4e3a\u6b63\u6570\u7684\\(n\\)\u9636\u4e0a\u4e09\u89d2\u77e9\u9635\\(R\\)\u4e4b\u79ef:\\(A = QR\\)\u3002</p> \\[ A = (\\mathbf{\\eta_1},\\mathbf{\\eta_2},...,\\mathbf{\\eta_n}) \\begin{pmatrix} |\\beta_1| &amp; b_{12}|\\beta_1| &amp; b_{13}|\\beta_1| &amp; ... &amp; b_{1n}|\\beta_1| \\\\ 0 &amp; |\\beta_2| &amp; b_{23}|\\beta_2| &amp; ... &amp; b_{2n}|\\beta_2| \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ... \\\\ 0 &amp; 0 &amp; 0 &amp; ... &amp; |\\beta_n| \\end{pmatrix} \\] <p>$Def. 4  $ \u5047\u8bbe\\(U\\)\u4e3a\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\\(R^n\\)\u7684\u4e00\u4e2a\u5b50\u7a7a\u95f4\uff0c\u82e5\u5411\u91cf\\(\\alpha\\)\u4e0e\\(U\\)\u4e2d\u6bcf\u4e00\u4e2a\u5411\u91cf\u6b63\u4ea4\uff0c\u5373\\(\\alpha \\perp U\\)\u3002\u4ee4\\(U^\\perp = \\{ \\mathbf{\\alpha} \\in R^n | \\mathbf{\\alpha} \\perp U \\}\\)\uff0c\u5219\\(U^\\perp\\)\u4e3a\\(R^n\\)\u7684\u4e00\u4e2a\u5b50\u7a7a\u95f4\uff0c\u79f0\u4e3a\\(U\\)\u7684\u6b63\u4ea4\u8865\u3002</p> <p>$Def. 5  $ \u8bbe\\(U\\)\u4e3a\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\\(R^n\\)\u7684\u4e00\u4e2a\u5b50\u7a7a\u95f4\uff0c\u4ee4</p> \\[ P_U : R^n \\rightarrow R^n, \\ \\mathbf{a} \\rightarrow \\mathbf{a_1} \\] <p>\u5176\u4e2d\\(\\mathbf{a_1} \\in U\\)\uff0c\u4e14\\(\\mathbf{a} - \\mathbf{a_1} \\in U^{\\perp}\\)\uff0c\u5219\u79f0\\(P_U\\)\u4e3a\\(U\\)\u7684\u6b63\u4ea4\u6295\u5f71\u3002\\(\\mathbf{a_1}\\)\u79f0\u4e3a\u5411\u91cf\\(\\mathbf{a}\\)\u5728\\(U\\)\u4e0a\u7684\u6b63\u4ea4\u6295\u5f71\u3002\u5bf9\u4e8e\\(\\mathbf{a} \\in R^n\\)\uff0c\u6709</p> \\[ |\\mathbf{a} - \\mathbf{a_1}| \\leq |\\mathbf{a} - \\mathbf{\\gamma}|, \\ \\forall \\mathbf{\\gamma} \\in U \\] <p>$Exercise.1  $ \u8bbe\\(A\\)\u662f\u5b9e\u6570\u57df\u4e0a\u7684\\(m \\times n\\)\u77e9\u9635\uff08\\(m&gt;n\\)\uff09\uff0c\\(\\beta \\in R^n\\)\u3002\u82e5\u6709\\(x_0 \\in R^n\\)\u4f7f\u5f97\\(|Ax_0 - \\beta|^2 \\leq |Ax - \\beta|, \\ \\forall x \\in R^n\\)\uff0c\u5219\\(x_0\\)\u4e3a\u7ebf\u6027\u65b9\u7a0b\\(Ax=\\beta\\)\u7684\u6700\u5c0f\u4e8c\u4e58\u89e3\uff0c\u8bc1\u660e\uff1a\\(x_0\\)\u662f\u7ebf\u6027\u65b9\u7a0b\\(Ax=\\beta\\)\u7684\u6700\u5c0f\u4e8c\u4e58\u89e3\u5f53\u4e14\u4ec5\u5f53\\(x_0\\)\u662f\\(A^T Ax =A^T \\beta\\)\u7684\u89e3</p> <p>\u8bbe\\(U=&lt;a_1,a_2,...,a_n&gt;\\)\u8868\u793a\\(A\\)\u7684\u5217\u7a7a\u95f4\uff0c\u5219</p> \\[ |Ax_0 - \\beta| \\leq |Ax - \\beta| \\rightarrow |Ax_0 - \\beta| \\leq |\\gamma - \\beta| , \\ \\forall \\gamma \\in U \\] <p>\u5373\\(Ax_0\\)\u4e3a\\(\\beta\\)\u5728\u5b50\u7a7a\u95f4\\(U\\)\u4e0a\u7684\u6b63\u4ea4\u6295\u5f71\uff0c\u6240\u4ee5\\(a_i^T (Ax_0 - \\beta) = 0, \\ i=1,2,...,n\\)\u3002\u6613\u5f97\\(A^T(Ax_0 - \\beta) = \\mathbf{0} \\ \\rightarrow \\ A^T A x_0 = A^T \\beta\\)\uff0c\u5373\\(x_0\\)\u662f\\(A^T Ax =A^T \\beta\\)\u7684\u89e3</p> <p>$Exercise.2  $ \u8bbe\\(A\\)\u662f\u5b9e\u6570\u57df\u4e0a\u7684\u5217\u6ee1\u79e9\\(m \\times n\\)\u77e9\u9635\uff08\\(m&gt;n\\)\uff09\uff0c\u5176\u5217\u7a7a\u95f4\u8bb0\u4f5c\\(U = &lt;a_1,a_2,...,a_n&gt; \\subset R^m\\)\u3002\u8bc1\u660e\\(P_A=A(A^TA)^{-1}A^T\\)\u4e3a\\(R^m\\)\u5728\\(U\\)\u4e0a\u7684\u6b63\u4ea4\u6295\u5f71</p> <p>\u4efb\u53d6\\(X \\in R^m\\)\uff0c\u7531\u4e8e\\(((A^TA)^{-1}A^T) X\\)\u4e3a\\(n \\times 1\\)\u77e9\u9635\uff0c\u53ef\u8bbe\u4e3a\\((c_1,c_2,...,c_n)^T\\)\uff0c\u5219:</p> \\[ P_A X = (\\mathbf{a_1,a_2,...,a_n}) \\begin{pmatrix} c_1\\\\ c_2 \\\\ ... \\\\c_n \\end{pmatrix} = c_1\\mathbf{a_1} + c_2\\mathbf{a_2} + ... + c_n\\mathbf{a_n} \\in U  \\] <p>\u518d\u8bc1\\(X - P_A X = (I_n - P_A)X \\in U^{\\perp} \\subseteq R^m\\)\uff0c\u5373</p> \\[ \\begin{pmatrix} \\mathbf{a_1}\\\\ \\mathbf{a_2} \\\\ ... \\\\ \\mathbf{a_n} \\end{pmatrix}(I_n - P_A)X = [A^T - A^TA(A^TA)^{-1}A^T]X = \\mathbf{0} \\] <p>\u56e0\u6b64\\((I_n - P_A)X \\in U^{\\perp}\\)\uff0c\u7efc\u4e0a\\(P_A=A(A^TA)^{-1}A^T\\)\u662f\\(R^m\\)\u5728\\(U\\)\u4e0a\u7684\u6b63\u4ea4\u6295\u5f71\u3002</p> <p>$Def. 5  $ \u82e5\u5b58\u5728\u4e00\u4e2a\u5bf9\u5e94\u6cd5\u5219\\(f\\)\uff0c\u4f7f\u5f97\u96c6\u5408\\(S\\)\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\\(a\\)\uff0c\u90fd\u6709\\(S'\\)\u4e2d\u552f\u4e00\u786e\u5b9a\u7684\u5143\u7d20\\(b\\)\u4e0e\u4e4b\u5bf9\u5e94\uff0c\u5219\u79f0\\(f\\)\u4e3a\\(S\\)\u5230\\(S'\\)\u7684\u6620\u5c04\uff0c\u8bb0\u4f5c\\(f: S \\rightarrow S'\\)\uff0c\\(a \\rightarrow b\\)\u3002\\(b\\)\u79f0\u4e3a\\(a\\)\u5728\\(f\\)\u4e0b\u7684\u8c61\uff0c\\(a\\)\u79f0\u4e3a\\(b\\)\u5728\\(f\\)\u4e0b\u7684\u4e00\u4e2a\u539f\u8c61\u3002</p> <p>\u6620\u5c04\\(f\\)\u53ef\u8bb0\u4f5c:</p> \\[ f(a) = b, \\ a \\in S \\] <p>\\(S\\)\u4e3a\u6620\u5c04\\(f\\)\u7684\u5b9a\u4e49\u57df\uff0c\\(S'\\)\u4e3a\u6620\u5c04\\(f\\)\u7684\u966a\u57df\uff0c\\(S\\)\u7684\u6240\u6709\u5143\u7d20\u5728\\(f\\)\u4e0b\u7684\u8c61\u7ec4\u6210\u7684\u96c6\u5408\u4e3a\\(f\\)\u7684\u503c\u57df\u6216\\(f\\)\u7684\u8c61\uff0c\u5373\\(f(S)\\)\u6216\\(Im f\\)\uff1a</p> \\[ f(S) = \\{ f(a) \\ | \\ a \\in S \\} = \\{ b \\in S' \\ | \\ \\exist a \\in S,f(a)=b \\} \\] <p>\u663e\u7136\\(f\\)\u7684\u503c\u57df\u662f\\(f\\)\u7684\u966a\u57df\u7684\u5b50\u96c6\u3002</p> <ul> <li> <p>\u82e5\\(f(S) = S'\\)\uff0c\u5219\\(f\\)\u4e3a\u6ee1\u5c04\uff0c\\(f\\)\u7684\u966a\u57df\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u81f3\u5c11\u6709\u4e00\u4e2a\u539f\u8c61</p> </li> <li> <p>\u82e5\u5bf9\u4e8e\\(a_1,a_2 \\in S\\)\uff0c\u6709\\(f(a_1)=f(a_2) \\ \\rightarrow \\ a_1=a_2\\)\uff0c\u5219\\(f\\)\u4e3a\u5355\u5c04\uff0c\\(f\\)\u7684\u5b9a\u4e49\u57df\u4e2d\u7684\u4e0d\u540c\u5143\u7d20\u7684\u8c61\u4e0d\u540c</p> </li> <li> <p>\u82e5\\(f\\)\u65e2\u662f\u5355\u5c04\u53c8\u662f\u6ee1\u5c04\uff0c\u5219\\(f\\)\u4e3a\u53cc\u5c04\uff0c\\(f\\)\u7684\\(S\\)\u4e0e\\(S'\\)\u4e00\u4e00\u5bf9\u5e94</p> </li> <li> <p>\u96c6\u5408\\(S\\)\u5230\u6570\u96c6\u7684\u4e00\u4e2a\u6620\u5c04\uff0c\u79f0\u4e3a\\(S\\)\u4e0a\u7684\u4e00\u4e2a\u51fd\u6570</p> </li> <li> <p>\u966a\u57df\\(S'\\)\u4e2d\u7684\u5143\u7d20\\(b\\)\u5728\u6620\u5c04\\(f\\)\u4e0b\u7684\u6240\u6709\u539f\u8c61\u7ec4\u6210\u7684\u96c6\u5408\u79f0\u4e3a\\(b\\)\u5728\\(f\\)\u4e0b\u7684\u539f\u8c61\u96c6\uff0c\u8bb0\u4f5c\\(f^{-1}(b)\\)</p> </li> </ul> <p>$Def. 6  $ \u6620\u5c04\\(g:S \\rightarrow S'\\)\u548c\\(f:S' \\rightarrow S''\\)\u5f97\u5230\\(S \\rightarrow S''\\)\u7684\u4e00\u4e2a\u6620\u5c04\\(fg\\)\uff0c\u79f0\u4e3a\\(f\\)\u4e0e\\(g\\)\u7684\u4e58\u79ef/\u5408\u6210\uff1a</p> \\[ (fg)(a) = f(g(a)), \\ \\forall a \\in S \\] <p>$Def. 7  $ \u8bbe\\(f:S \\rightarrow S'\\)\uff0c\u5b58\u5728\u4e00\u4e2a\\(g:S\u2019 \\rightarrow S\\)\uff0c\u4f7f\u5f97$$ fg = 1_{S} \\quad gf=1_{S'}\\(\\(\uff0c\u5219\u79f0\u6620\u5c04\\)f\\)\u662f\u53ef\u9006\u7684\uff0c\\(g\\)\u4e3a\\(f\\)\u7684\u4e00\u4e2a\u552f\u4e00\u9006\u6620\u5c04,\u53ef\u8bb0\u4f5c\\(f^{-1}\\)\u3002</p> <p>\u6620\u5c04\\(f:S \\rightarrow S'\\)\u53ef\u9006\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u4e3a\\(f\\)\u662f\u53cc\u5c04</p> <p>$Def. 8  $ \u6570\u57df\\(K\\)\u4e0a\u7684\u5411\u91cf\u7a7a\u95f4\\(K^n \\rightarrow K^s\\)\u7684\u4e00\u4e2a\u6620\u5c04\\(\\sigma\\)\u82e5\u4fdd\u6301\u52a0\u6cd5\u548c\u6570\u4e58\u8fd0\u7b97\uff0c\u5373\\(\\forall \\mathbf{a,b} \\in K^n, \\ k \\in K\\):</p> \\[ \\sigma(\\mathbf{a+b}) = \\sigma(\\mathbf{a}) + \\sigma(\\mathbf{b}), \\ \\sigma(k\\mathbf{a}) = k\\sigma(\\mathbf{a}) \\] <p>\u5219\\(\\sigma\\)\u4e3a\\(K^n\\)\u5230\\(K^s\\)\u7684\u4e00\u4e2a\u7ebf\u6027\u6620\u5c04\u3002\u663e\u7136\u77e9\u9635\\(A\\)\u662f\u4e00\u4e2a\u7ebf\u6027\u6620\u5c04\u3002</p> <p>\u5047\u8bbe\u6570\u57df\\(K\\)\u4e0an\u5143\u7ebf\u6027\u65b9\u7a0b\u7ec4\\(Ax=\\beta\\)\u6709\u89e3\uff0c\u5373\u5b58\u5728\\(\\gamma \\in K^n\\)\u4f7f\u5f97\\(A \\gamma = \\beta\\)\u3002\u6839\u636e\u7ebf\u6027\u6620\u5c04\u7684\u6982\u5ff5\uff0c\\(\\beta \\in Im \\ A\\)</p> <p>\u663e\u7136\\(\\beta \\in &lt;\\mathbf{a_1,a_2,...,a_n}&gt;\\)\uff0c\u56e0\u6b64\\(Im \\ A = &lt;\\mathbf{a_1,a_2,...,a_n}&gt;\\)\uff0c\u5373\u7ebf\u6027\u6620\u5c04\\(A\\)\u7684\u8c61\u7b49\u4e8e\\(A\\)\u7684\u5217\u7a7a\u95f4\uff0c\u56e0\u6b64\\(Im \\ A\\)\u662f\\(K^s\\)\u7684\u4e00\u4e2a\u5b50\u7a7a\u95f4\u3002</p> <p>$Def. 9  $ \u8bbe\\(sigma\\)\u662f\\(K^n \\rightarrow K^s\\)\u7684\u4e00\u4e2a\u6620\u5c04\uff0c\\(K^n\\)\u7684\u4e00\u4e2a\u5b50\u96c6\\(\\(\\{ \\alpha \\in K^n \\ | \\ \\sigma(\\alpha) = 0 \\}\\)\\)\u79f0\u4e3a\u6620\u5c04\\(\\sigma\\)\u7684\u6838\uff0c\u8bb0\u4f5c$ Ker  \\sigma$\u3002</p> <p>\u5982\u679c\\(\\sigma\\)\u662f\\(K^n \\rightarrow K^s\\)\u7684\u4e00\u4e2a\u7ebf\u6027\u6620\u5c04\uff0c\u5219\\(Ker \\ \\sigma\\)\u662f\\(K^n\\)\u7684\u4e00\u4e2a\u5b50\u7a7a\u95f4\u3002</p> <p>\u5bf9\u4e8e\u9f50\u6b21\u7ebf\u6027\u65b9\u7a0b\u7ec4\\(Ax=0\\)\uff0c\u7ebf\u6027\u6620\u5c04\\(A\\)\u7684\u6838\u5c31\u662f\u65b9\u7a0b\u7ec4\u7684\u89e3\u7a7a\u95f4\uff1a\\(Ker \\ A = W\\)</p> <p>\u7531\u4e8e\u4e0a\u8ff0\\(dim \\ W = n - rank(A) = n - dim&lt;a_1,a_2,...,a_n&gt;\\)\uff0c\u5373\u79e9-\u96f6\u5316\u5ea6\u5b9a\u7406\\(dim \\ Ker \\ A + dim \\ Im \\ A = dim \\ K^n\\)</p>"},{"location":"math/HigherAlgebra/chapter/1/#\u77e9\u9635\u7684\u76f8\u62b5\u4e0e\u76f8\u4f3c","title":"\u77e9\u9635\u7684\u76f8\u62b5\u4e0e\u76f8\u4f3c","text":"<p>$Def. 1  $ \u8bbe\u6570\u57df\\(K\\)\u4e0a\\(s \\times n\\)\u77e9\u9635\\(A\\)\u7684\u79e9\u4e3a\\(r&gt;0\\)\uff0c\u5219\\(A\\)\u76f8\u62b5\u4e8e\u77e9\u9635\\(\\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}\\)\uff0c\u79f0\u4e3a\\(A\\)\u7684\u76f8\u62b5\u6807\u51c6\u578b\u3002</p> <p>$Def. 2  $ \u6570\u57df\\(K\\)\u4e0a\u77e9\u9635\\(A\\)\u4e0e\\(B\\)\u76f8\u62b5\u5f53\u4e14\u4ec5\u5f53\u79e9\u76f8\u7b49\uff0c\\(rank(A) = rank(B)\\)</p> <p>\u56e0\u6b64\uff0c\u5b58\u5728\\(K\\)\u4e0a\u7684s\u9636\uff0cn\u9636\u53ef\u9006\u77e9\u9635\\(P,Q\\)\u4f7f\u5f97\uff1a</p> \\[ A = P \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} Q \\] <p>$Exercise. $ \u8bbe\\(A\\)\u4e3a\u6570\u57df\\(K\\)\u4e0a\u7684\\(s \\times n\\)\u77e9\u9635\uff0c\u8bc1\u660e:\\(rank(A)\\)\u5f53\u4e14\u4ec5\u5f53\u5b58\u5728\\(s \\times r\\)\u5217\u6ee1\u79e9\u77e9\u9635\\(B\\)\u4e0e\\(r \\times n\\)\u884c\u6ee1\u79e9\u77e9\u9635\\(C\\)\uff0c\u4f7f\u5f97\\(A=BC\\)</p> <p>\u5fc5\u8981\u6027\uff1a</p> \\[ \\begin{aligned} A =&amp; P \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} Q \\\\ =&amp; (P_{s \\times r}, \\ P_{s \\times (s-r)}) \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} \\begin{pmatrix} Q_{r \\times n} \\\\ Q_{(n-r) \\times n} \\end{pmatrix} \\\\ =&amp; (P_{s \\times r}, 0) \\begin{pmatrix} Q_{r \\times n} \\\\ Q_{(n-r) \\times n} \\end{pmatrix} = P_{s \\times r} Q_{r \\times n} = BC \\end{aligned} \\] <p>\u7531\u4e8e\\(P,Q\\)\u53ef\u9006\uff0c\u56e0\u6b64\\(rank(P_{s \\times r})=rank(Q_{r \\times n}) = r\\)\u3002</p> <p>\u5145\u5206\u6027\uff1a$ rank(B) + rank(C) - r  \\leq  rank(BC) \\leq rank(B) = r\\(\uff0c\u663e\u7136\\)rank(A) = rank(BC) =r$</p> <p>$Exercise. $ \u8bbe\\(B_1,B_2\\)\u662f\u6570\u57df\\(K\\)\u4e0a\u7684\\(s \\times r\\)\u5217\u6ee1\u79e9\u77e9\u9635\uff0c\u8bc1\u660e\uff1a\u5b58\u5728\\(K\\)\u4e0a\u7684s\u9636\u53ef\u9006\u77e9\u9635\\(P\\)\uff0c\u4f7f\u5f97\\(B_1 = PB_2\\)</p> <p>\u6613\u77e5:\u5b58\u5728s\u9636\u53ef\u9006\u77e9\u9635\u4f7f\u5f97\\(P_1 B_1 = \\begin{pmatrix} I_r \\\\ 0 \\end{pmatrix}\\)\uff0c\\(P_2 B_2 = \\begin{pmatrix} I_r \\\\ 0 \\end{pmatrix}\\)</p> <p>\u6240\u4ee5\\(P_1 B_1=P_2 B_2 \\ \\rightarrow \\ B_1= (P_1^{-1}P_2)B_2\\)\uff0c\u56e0\u6b64\\(P=P_1^{-1}P_2\\)\u4e3as\u9636\u53ef\u9006\u77e9\u9635\u3002</p> <p>\\(Exercise. \\(\u8bbe\\)K\\)\u4e0a\u7684\\(A_{s \\times n}\uff0cB_{n \\times m}\\)\uff0c\u8bc1\u660e\uff1a\\(ABX_{m \\times n} =A\\)\u6709\u89e31\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u662f\uff1a</p> \\[ rank(AB) = rank(A) \\] <p>\u56e0\u4e3a\u77e9\u9635\u65b9\u7a0b\\(ABX = A\\)\u6709\u89e3\uff0c\u5219\\(rank(AB) = rank(AB,A)\\)\uff0c\u8bf4\u660e$ rank(A) \\leq rank(AB) \\(\uff0c\u53c8\u56e0\u4e3a\\)rank(AB) \\leq rank(A)\\(\uff0c\u6240\u4ee5\\)rank(AB) = rank(A)$</p>"},{"location":"math/HigherAlgebra/chapter/1/#\u5e7f\u4e49\u9006\u77e9\u9635","title":"\u5e7f\u4e49\u9006\u77e9\u9635","text":"<p>\u5bf9\u4e8e\u7ebf\u6027\u65b9\u7a0b\u7ec4\\(Ax=\\beta\\)\uff0c\u82e5\\(A\\)\u4e0d\u53ef\u9006\u4f46\u6709\u89e3\uff0c\u9700\u8981\u5f15\u5165\u5e7f\u4e49\u9006\u77e9\u9635\\(A^{-}\\)\u6765\u7b80\u6d01\u8868\u793a\u89e3\u3002</p> <p>$Def. 1  $ \u8bbe\\(A\\)\u662f\u6570\u57df\\(K\\)\u4e0a\u7684\\(s \\times n\\)\u975e\u96f6\u77e9\u9635\uff0c\u5219</p> <p>$$</p> <p>AXA = A</p> <p>$$</p> <p>\u4e00\u5b9a\u6709\u89e3\u3002\u6bcf\u4e00\u4e2a\u89e3\u90fd\u79f0\u4e3a\\(A\\)\u7684\u4e00\u4e2a\u5e7f\u4e49\u9006\u77e9\u9635\\(A^{-}\\)\u3002</p> <p>\u56e0\u6b64\uff0c\\(Ax=\\beta\\)\u6709\u89e3\u65f6\uff0c\u5b83\u7684\u901a\u89e3\u4e3a\\(x = A^{-} \\beta\\)\u3002</p> <p>$Def. 2  $ \u6570\u57df\\(K\\)\u4e0an\u5143\u9f50\u6b21\u7ebf\u6027\u65b9\u7a0b\u7ec4\\(Ax=0\\)\u7684\u901a\u89e3\u4e3a\uff1a</p> \\[ x = (I_n - A^{-}A)Z \\] <p>\u5176\u4e2d\\(A^{-}\\)\u4e3a\\(A\\)\u4efb\u610f\u7ed9\u5b9a\u7684\u5e7f\u4e49\u9006\uff0c\\(Z\\)\u53d6\u904d\\(K^n\\)\u4e2d\u7684\u4efb\u610f\u5217\u5411\u91cf\u3002</p> <p>\u7ed3\u5408\u4e0a\u8ff0\u4e24\u4e2a\u5b9a\u4e49\uff0c\u53ef\u77e5\u5bf9\u4e8e\u6570\u57df\\(K\\)\u4e0a\u7684\u975e\u9f50\u6b21\u7ebf\u6027\u65b9\u7a0b\u7ec4\\(Ax=\\beta\\)\uff0c\u5b83\u7684\u901a\u89e3\u4e3a\uff1a</p> \\[ x = A^{-} \\beta + (I_n - A^{-}A)Z \\] <p>\u663e\u7136\u5e7f\u4e49\u9006\u5e76\u4e0d\u552f\u4e00</p> <p>$Def. 3  $ Penrose\u65b9\u7a0b\u7ec4\uff1a\u8bbe\\(A\\)\u662f\u590d\u6570\u57df\u4e0a\u7684\\(s \\times n\\)\u77e9\u9635\uff0c\u77e9\u9635\u65b9\u7a0b\u7ec4\uff1a</p> \\[ \\begin{cases} AXA = A \\\\ XAX = X \\\\ (AX)^* = AX \\\\ (XA)^* = XA \\end{cases} \\] <p>\u5f97\u5230\u7684\u89e3\u4e3a\\(A\\)\u7684\\(Moore-Penrose\\)\u5e7f\u4e49\u9006\uff0c\u8bb0\u4f5c\\(A^{+}\\)\u3002\u5176\u4e2d\\((AX)^*\\)\u8868\u793a\u628a\\(AX\\)\u7684\u6240\u6709\u5143\u7d20\u53d6\u5171\u8f6d\u590d\u6570\u518d\u8f6c\u7f6e\u5f97\u5230\u7684\u77e9\u9635\u3002</p> <p>$Def. 4  $ \u82e5\\(A\\)\u662f\u590d\u6570\u57df\u4e0a\u7684\\(s \\times n\\)\u975e\u96f6\u77e9\u9635\uff0c\u5219\\(A\\)\u7684Penrose\u65b9\u7a0b\u7ec4\u59cb\u7ec8\u6709\u552f\u4e00\u89e3\u3002\u8bbe\\(A=BC\\)\uff0c\u5176\u4e2d\\(B\\)\u662f\u5217\u6ee1\u79e9\u77e9\u9635\uff0c\\(C\\)\u662f\u884c\u6ee1\u79e9\u77e9\u9635\uff0c\u5219\u552f\u4e00\u89e3\u4e3a\uff1a</p> \\[ X = C^*(CC^*)^{-1} (B^*B)^{-1}B^* \\] <p>\u5373\u5bf9\u4e8e\u4efb\u610f\u7684\u590d\u77e9\u9635\\(A\\)\uff0c\\(A\\)\u7684\\(Moore-Penrose\\)\u5e7f\u4e49\u9006\\(A^+\\)\u5b58\u5728\u4e14\u552f\u4e00\u3002</p> <p>$Exercise. $ \u8bbe\\(B,C\\)\u5206\u522b\u4e3a\u590d\u6570\u57df\u4e0a\u7684\\(s \\times r,\\quad r \\times n\\)\u5217\u6ee1\u79e9\u3001\u884c\u6ee1\u79e9\u77e9\u9635\uff0c\u5219\uff1a</p> \\[ (BC)^+ = C^+ B^+ \\] <p>\u4e0a\u8ff0\u5df2\u77e5</p> \\[ A^+ = (BC)^+ = C^*(CC^*)^{-1} (B^*B)^{-1}B^* \\] <p>\u7531\u4e8e\\(B=BI_r\uff0cC=I_r C\\)\uff0c\u6240\u4ee5</p> \\[ \\begin{aligned} B^+ = I^*_r (I_r I_r^*)^{-1} (B^* B)^{-1} B^* = (B^* B)^{-1} B^* \\\\ C^+ = C^* (C C^*)^{-1}  \\\\ \\end{aligned} \\] <p>\u6240\u4ee5\\(C^+ B^+ = C^* (C C^*)^{-1} (B^* B)^{-1} B^* = (BC)^+\\)</p> <p>$Exercise. $ \u8bbe\\(A\\)\u5206\u522b\u4e3a\u590d\u6570\u57df\u4e0a\u7684\\(s \\times n\\)\u77e9\u9635\uff0c\u8bc1\u660e\uff1a\\(B\\)\u662f\\(A\\)\u7684\u4e00\u4e2a\u5e7f\u4e49\u9006\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u662f\uff1a</p> \\[ rank(A) + rank(I_n - BA) = n \\] <p>\u7531\u4e0a\u8ff0\u53ef\u77e5\uff0c\\(B\\)\u662f\\(AXA = A\\)\u7684\u89e3\uff0c\u5373\\(ABA = A \\ \\rightarrow \\ A(I_n - BA) = 0\\)\u3002\u56e0\u6b64\\(rank(A) + rank(I_n - BA) = n\\)\u3002</p> <p>$Exercise. $ \u8bbe\\(A\\)\u5206\u522b\u4e3a\u6570\u57df\u4e0a\u7684\\(s \\times n\\)\u975e\u96f6\u77e9\u9635\uff0c\u8bc1\u660e\uff1a$$rank(A^{-}A) = rank(A) $$</p> <p>\u5df2\u77e5\uff1a</p> \\[ A = P \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} Q, \\quad A^{-} = Q^{-1} \\begin{pmatrix} I_r &amp; B \\\\ C &amp; D \\end{pmatrix} P^{-1} \\] <p>\u5219\uff1a</p> \\[ A^{-}A = Q^{-1} \\begin{pmatrix} I_r &amp; B \\\\ C &amp; D \\end{pmatrix} P^{-1} P \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} Q = Q^{-1} \\begin{pmatrix} I_r &amp; 0 \\\\ C &amp; 0 \\end{pmatrix} Q \\] <p>\u6613\u5f97\\(rank(A^{-}A) = rank \\begin{pmatrix} I_r &amp; 0 \\\\ C &amp; 0 \\end{pmatrix} \\geq r \\(\uff0c\u53c8\u56e0\u4e3a\\)rank(A^{-}A) \\leq rank(A) =r\\)\uff0c\u6240\u4ee5\\(rank(A^{-}A) = rank(A) = r\\)</p> <p>$Exercise. $ \u8bbe\\(A,B,C\\)\u5206\u522b\u4e3a\u6570\u57df\u4e0a\u7684\\(s \\times n,l \\times m, s \\times m\\)\u975e\u96f6\u77e9\u9635\uff0c\u8bc1\u660e\uff1a\u5b58\u5728\u5e7f\u4e49\u9006\\(A^{-},B^{-}\\)\u4f7f\u5f97\uff1a</p> \\[ rank \\begin{pmatrix} A &amp; C \\\\ 0 &amp; B \\end{pmatrix} = rank(A) + rank(B) +rank[(I_s - AA^{-})C(I_m - B^{-}B)] \\] <p>\u5047\u8bbe\\(rank(A)=r,rank(B)=t\\)\uff0c\u5219\uff1a</p> \\[ A^{-} = Q^{-1}_1 \\begin{pmatrix} I_r &amp; G_1 \\\\ H_1 &amp; D_1 \\end{pmatrix} P_1^{-1}, \\quad B^{-} = Q^{-2}_2 \\begin{pmatrix} I_t &amp; G_2 \\\\ H_2 &amp; D_2 \\end{pmatrix} P_2^{-1} \\] <p>\u53d6\u5176\u4e2d\u7684\\(G_1=0,H_2=0\\):</p> \\[ AA^{-} = P_1 \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} P^{-1}_1, \\quad B^{-}B = Q^{-1}_2 \\begin{pmatrix} I_t &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} Q_2 \\] \\[ (I_s - AA^{-})C(I_m - B^{-}B) = P_1 \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; I_{s-r} \\end{pmatrix} P_1^{-1} C Q_2^{-1} \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; I_{m-t} \\end{pmatrix} Q_2 \\] <p>\u5047\u8bbe\\(P_1^{-1} C Q_2^{-1} =  \\begin{pmatrix} C_1 &amp; C_2 \\\\ C_3 &amp; C_4 \\end{pmatrix}\\)\uff0c\u5219:</p> \\[ (I_s - AA^{-})C(I_m - B^{-}B) = P_1 \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; C_4 \\end{pmatrix} Q_2 \\] <p>\u5373\uff1a\\(rank[(I_s - AA^{-})C(I_m - B^{-}B)] = rank(C_4)\\)</p> <p>$$ \\begin{aligned} &amp;\\begin{pmatrix} P_1^{-1} &amp; 0 \\ 0 &amp; P_2^{-1} \\end{pmatrix} \\begin{pmatrix} A &amp; C \\ 0 &amp; B \\end{pmatrix} \\begin{pmatrix} Q_1^{-1} &amp; 0 \\ 0 &amp; Q_2^{-1} \\end{pmatrix}= \\begin{pmatrix} P_1<sup>{-1}AQ_1</sup> &amp; P_1<sup>{-1}CQ_2</sup> \\ 0 &amp; P_2<sup>{-1}BQ_2</sup> \\} \\ \\end{pmatrix</p> <p>&amp;= \\begin{pmatrix}  I_r &amp; 0 &amp; C_1 &amp; C_2 \\ 0 &amp; 0 &amp; C_3 &amp; C_4 \\ 0 &amp; 0 &amp; I_t &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ \\end{pmatrix} </p> <p>= \\begin{pmatrix}  I_r &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; I_t &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; C_4 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ \\end{pmatrix} \\end{aligned} $$</p> <p>\u53ef\u77e5\\(rank \\begin{pmatrix} A &amp; C \\\\ 0 &amp; B \\end{pmatrix} = rank(A) + rank(B) +rank(C_4) = rank(A) + rank(B) +rank[(I_s - AA^{-})C(I_m - B^{-}B)]\\)</p>"},{"location":"math/HigherAlgebra/chapter/1/#\u77e9\u9635\u7684\u76f8\u4f3c","title":"\u77e9\u9635\u7684\u76f8\u4f3c","text":"<p>$Def. 1  $ \u8bbe\\(A\\)\u4e0e\\(B\\)\u90fd\u662f\u6570\u57df\u4e0a\u7684\\(n\\)\u9636\u77e9\u9635\uff0c\u82e5\u5b58\u5728\u6570\u57df\u4e0a\u7684\\(n\\)\u9636\u53ef\u9006\u77e9\u9635\\(P\\)\uff0c\u4f7f\u5f97\\(P^{-1}AP=B\\)\uff0c\u5219\u79f0\\(A\\)\u4e0e\\(B\\)\u76f8\u4f3c\uff0c\u8bb0\u4f5c\\(A \\sim B\\)\u3002</p> <ul> <li> <p>\u76f8\u4f3c\u77e9\u9635\u7684\u884c\u5217\u5f0f\u7684\u503c\u76f8\u7b49\uff1a\\(det(A) = det(B)\\)</p> </li> <li> <p>\u76f8\u4f3c\u77e9\u9635\u6216\u53ef\u9006\u6216\u4e0d\u53ef\u9006\uff0c\u82e5\u53ef\u9006\uff0c\u5219\u9006\u77e9\u9635\u4e5f\u76f8\u4f3c</p> </li> <li> <p>\u76f8\u4f3c\u77e9\u9635\u7684\u79e9\u76f8\u7b49\uff1a\\(rank(A) = rank(B)\\)</p> </li> <li> <p>\u76f8\u4f3c\u77e9\u9635\u7684\u8ff9\u76f8\u7b49\uff1a\\(tr(A) = tr(B)\\)</p> </li> <li> <p>\u8bbe\\(f(x)=a_0+a_1x+...+a_m x^m\\)\u662f\u6570\u57df\\(K\\)\u4e0a\u7684\u4e00\u5143\u591a\u9879\u5f0f\uff0c\u82e5\\(A \\sim B\\)\uff0c\u5219\\(f(A) \\sim f(B)\\)</p> </li> <li> <p>\u82e5\\(A_i \\sim B_i, \\ i=1,2\\)\uff0c\u5219\\(\\(\\begin{pmatrix} A_1 &amp; 0 \\\\ 0 &amp; A_2 \\end{pmatrix} \\sim \\begin{pmatrix} B_1 &amp; 0 \\\\ 0 &amp; B_2 \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u82e5\\(A\\)\u53ef\u9006\uff0c\u5219\\(AB \\sim BA\\)\u3002\u56e0\u4e3a\\(B = P \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} P^{-1} A^{-1} \\ \\rightarrow \\ BA = P \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} P^{-1} \\ \\rightarrow \\  P^{-1} (BA) P = \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} P^{-1} = (AB) P\\)</p> </li> </ul> <p>\u82e5n\u9636\u77e9\u9635\\(A\\)\u80fd\u591f\u76f8\u4f3c\u4e8e\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c\u5219\u79f0\u4e3a\\(A\\)\u53ef\u5bf9\u89d2\u5316\uff0c\\(P^{-1}AP=diag\\{ \\lambda_1, \\lambda_2, ...,\\lambda_n \\}\\)</p> <ul> <li>\u82e5\\(A\\)\u53ef\u5bf9\u89d2\u5316\uff0c\u5219\\(A \\sim A^T\\)</li> </ul> <p>$Def. 2  $ \u6570\u57df\\(K\\)\u4e0an\u9636\u77e9\u9635\\(A\\)\u53ef\u5bf9\u89d2\u5316\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\uff1a\\(K^n\\)\u4e2d\u6709n\u4e2a\u7ebf\u6027\u65e0\u5173\u7684\u5217\u5411\u91cf\\(\\mathbf{a_1,a_2,...,a_n}\\)\uff0c\u4ee5\u53ca\\(K\\)\u4e2d\u7684n\u4e2a\u6570\\(\\lambda_1, \\lambda_2, ..., \\lambda_n\\)\uff0c\u4f7f\u5f97$$A \\mathbf{a_i} = \\lambda_i \\mathbf{a_i}, \\quad i=1,2,...,n  $$</p> <p>\u4ee4\\(P = (\\mathbf{a_1,a_2,...,a_n})\\)\uff0c\u5219\\(\\(P^{-1}AP = diag\\{ \\lambda_1, \\lambda_2, ..., \\lambda_n \\}\\)\\)</p>"},{"location":"math/HigherAlgebra/chapter/1/#\u7279\u5f81\u503c","title":"\u7279\u5f81\u503c","text":"<p>$Def. 1  $ \u8bbe\\(A\\)\u662f\u6570\u57df\\(K\\)\u4e0a\u7684n\u9636\u77e9\u9635\uff0c\u82e5\\(K^n\\)\u4e2d\u6709\u975e\u96f6\u5217\u5411\u91cf\\(\\mathbf{a}\\)\u4f7f\u5f97\uff1a</p> \\[ A \\mathbf{a} = \\lambda_0 \\mathbf{a} \\] <p>\u5219\u79f0\\(\\lambda_0\\)\u662f\u77e9\u9635\\(A\\)\u7684\u4e00\u4e2a\u7279\u5f81\u503c\uff0c\\(\\mathbf{a}\\)\u662f\\(A\\)\u5bf9\u5e94\u4e8e\u7279\u5f81\u503c\\(\\lambda_0\\)\u7684\u7279\u5f81\u5411\u91cf\u3002</p> <ul> <li> <p>\\(\\lambda_0\\)\u662f\\(A\\)\u7684\u4e00\u4e2a\u7279\u5f81\u503c\u5f53\u4e14\u4ec5\u5f53\\(\\lambda_0\\)\u662f\u7279\u5f81\u591a\u9879\u5f0f\\(|\\lambda I - A|=0\\)\u5728\\(K\\)\u7684\u4e00\u4e2a\u6839</p> </li> <li> <p>\\(\\mathbf{a}\\)\u662f\\(A\\)\u7684\u5c5e\u4e8e\\(\\lambda_0\\)\u7684\u4e00\u4e2a\u7279\u5f81\u5411\u91cf\u5f53\u4e14\u4ec5\u5f53\\(\\mathbf{a}\\)\u662f\u9f50\u6b21\u7ebf\u6027\u65b9\u7a0b\u7ec4\\((\\lambda_0 I - A) \\mathbf{x} = \\mathbf{0}\\)\u7684\u4e00\u4e2a\u975e\u96f6\u89e3</p> </li> <li> <p>\u8bbe\\(A \\sim B\\)\uff0c\u5219\\(|\\lambda I -B|=|\\lambda I -A|\\)</p> </li> <li> <p>\u76f8\u4f3c\u77e9\u9635\u5177\u6709\u76f8\u540c\u7684\u7279\u5f81\u6570\uff08\u91cd\u6570\u4e5f\u76f8\u540c\uff09</p> </li> <li> <p>\u5bf9\u4e8e\u9f50\u6b21\u7ebf\u6027\u65b9\u7a0b\u7ec4\\((\\lambda_j I-A)x=0\\)\u7684\u4e00\u4e2a\u57fa\u7840\u89e3\u7cfb:\\(\\eta_1,\\eta_2,...,\\eta_t\\)\uff0c\u5219\\(A\\)\u5c5e\u4e8e\\(\\lambda_j\\)\u7684\u5168\u90e8\u7279\u5f81\u5411\u91cf\u7ec4\u6210\u7684\u96c6\u5408\u4e3a\uff1a</p> </li> </ul> \\[ \\{ k_1 \\eta_1 + k_2 \\eta_2+...+ k_t \\eta_t \\ | \\ k_1,k_2,...,k_t \\in K,\\text{\u4e14\u4e0d\u5168\u4e3a0} \\} \\] <ul> <li> <p>\\(\\lambda_1\\)\u4f5c\u4e3a\\(A\\)\u7684\u7279\u5f81\u591a\u9879\u5f0f\u7684\u6839\u7684\u91cd\u6570\u53eb\u505a\\(\\lambda_1\\)\u7684\u4ee3\u6570\u91cd\u6570\uff0c\u628a\\(A\\)\u7684\u5c5e\u4e8e\\(\\lambda_1\\)\u7279\u5f81\u5b50\u7a7a\u95f4\u7684\u7ef4\u6570\u53eb\u505a\\(\\lambda_1\\)\u7684\u51e0\u4f55\u91cd\u6570\uff08\u51e0\u4f55\u91cd\u6570\u4e0d\u8d85\u8fc7\u4ee3\u6570\u91cd\u6570\uff09</p> </li> <li> <p>\u8bbe\\(f(x)=a_0+a_1 x+...+a_m x^m\\)\u662f\u6570\u57df\\(K\\)\u4e0a\u7684\u4e00\u4e2a\u591a\u9879\u5f0f\u3002\u8bc1\u660e\uff1a\u82e5\\(\\lambda_0\\)\u662f\\(K\\)\u4e0an\u9636\u77e9\u9635\\(A\\)\u7684\u4e00\u4e2a\u7279\u5f81\u503c\uff0c\u4e14\\(\\mathbf{a}\\)\u662f\u5bf9\u5e94\u7684\u4e00\u4e2a\u7279\u5f81\u5411\u91cf\u3002\u90a3\u4e48\\(f(\\lambda_0)\\)\u662f\\(f(A)\\)\u7684\u4e00\u4e2a\u7279\u5f81\u503c\uff0c\u4e14\\(\\mathbf{a}\\)\u662f\\(f(A)\\)\u7684\u5c5e\u4e8e\\(f(\\lambda_0)\\)\u7684\u4e00\u4e2a\u7279\u5f81\u5411\u91cf\u3002</p> </li> </ul>"},{"location":"math/HigherAlgebra/chapter/1/#\u77e9\u9635\u53ef\u5bf9\u89d2\u5316\u7684\u6761\u4ef6","title":"\u77e9\u9635\u53ef\u5bf9\u89d2\u5316\u7684\u6761\u4ef6","text":"<p>$Def. 1  $ \u6570\u57df\\(K\\)\u4e0an\u9636\u77e9\u9635\\(A\\)\u53ef\u5bf9\u89d2\u5316\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u4e3a\uff1a\\(A\\)\u6709n\u4e2a\u7ebf\u6027\u65e0\u5173\u7684\u7279\u5f81\u5411\u91cf\\(\\mathbf{a_1,a_2,...,a_n}\\)\uff0c\u5373\uff1a</p> \\[ P=(\\mathbf{a_1,a_2,...,a_n}) \\] <p>\u5219\\(P^{-1} A P = diag \\{ \\lambda_1,\\lambda_2,...,\\lambda_n \\}\\)\uff0c\u5176\u4e2d\\(\\lambda_i\\)\u662f\\(\\mathbf{a_i}\\)\u6240\u5c5e\u7684\u7279\u5f81\u503c\u3002\u8fd9\u4e2a\u5bf9\u89d2\u77e9\u9635\\(diag \\{ \\lambda_1,\\lambda_2,...,\\lambda_n \\}\\)\u662f\\(A\\)\u7684\u76f8\u4f3c\u6807\u51c6\u5f62\u3002</p> <p>\u82e5\u5404\u7ec4\u5411\u91cf\u6570\u4e4b\u548c\\(r_1+r_2+...+r_m&lt;n\\)\uff0c\u5219\\(A\\)\u6ca1\u6709n\u4e2a\u7ebf\u6027\u65e0\u5173\u7684\u7279\u5f81\u5411\u91cf\uff0c\u56e0\u6b64\\(A\\)\u4e0d\u53ef\u5bf9\u89d2\u5316\u3002(\u4e0d\u540c\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf\u4e92\u76f8\u7ebf\u6027\u65e0\u5173)</p> <ul> <li>\u82e5\u6570\u57df\\(K\\)\u4e0a\u7684n\u9636\u77e9\u9635\\(A\\)\u53ef\u5bf9\u89d2\u5316\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\uff1a\\(A\\)\u7684\u7279\u5f81\u591a\u9879\u5f0f\u7684\u5168\u90e8\u590d\u6839\u5c5e\u4e8e\\(K\\)\uff0c\u4e14\u6bcf\u4e2a\u7279\u5f81\u503c\u7684\u51e0\u4f55\u91cd\u6570\u7b49\u4e8e\u4ee3\u6570\u91cd\u6570</li> </ul>"},{"location":"math/HigherAlgebra/chapter/1/#\u5b9e\u5bf9\u79f0\u77e9\u9635\u7684\u5bf9\u89d2\u5316","title":"\u5b9e\u5bf9\u79f0\u77e9\u9635\u7684\u5bf9\u89d2\u5316","text":"<p>\u5750\u6807\u53d8\u6362\u7684\u5b9e\u4f8b\uff1a\u5bf9\u4e8e\u4e8c\u6b21\u66f2\u7ebf\u5728\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4e0b\u7684\u65b9\u7a0b</p> \\[ x^2 + 4y^2 + z^2 - 4xy - 8xz - 4yz - 1 =0 \\] <p>\u901a\u8fc7\u76f4\u89d2\u5750\u6807\u53d8\u6362\uff0c\u6d88\u53bb\u65b9\u7a0b\u4e2d\u7684\u4ea4\u53c9\u9879\uff0c\u53ea\u7559\u4e0b\u5e73\u65b9\u9879(\\(\\mathbf{T}\\)\u662f\u6b63\u4ea4\u77e9\u9635\uff0c\\(T^{-1}=T^T\\))\uff1a</p> \\[ \\begin{pmatrix} x \\\\ y \\\\  z \\\\ \\end{pmatrix} = \\mathbf{T}  \\begin{pmatrix} x^* \\\\ y^* \\\\  z^* \\\\ \\end{pmatrix} \\] \\[ \\begin{aligned} x^2 + 4y^2 + z^2 - 4xy - 8xz - 4yz  =  \\\\ \\begin{pmatrix} x \\ \\ y \\ \\ z \\end{pmatrix} \\begin{pmatrix} 1 &amp; -2 &amp; -4 \\\\ -2 &amp; 4 &amp; -2 \\\\ -4 &amp; -2 &amp; 1 \\\\ \\end{pmatrix} \\begin{pmatrix} x \\\\  y \\\\  z \\\\ \\end{pmatrix} \\\\ \\end{aligned} \\] <p>\u4ee3\u5165\u540e\u5f97\u5230\uff1a</p> \\[ \\begin{pmatrix} x^* \\ \\ y^* \\ \\ z^* \\end{pmatrix} \\mathbf{T^T A T} \\begin{pmatrix} x^* \\\\  y^* \\\\  z^* \\\\ \\end{pmatrix} \\\\ \\] <p>\u663e\u7136\\(A\\)\u662f\u5b9e\u6570\u57df\u4e0a\u7684\u5bf9\u79f0\u77e9\u9635\uff08\\(A=\\bar{A}\\)\uff09\uff0c\u4e3a\u4e86\u4f7f\u5f97\u65b9\u7a0b\u53ea\u7559\u4e0b\u5e73\u65b9\u9879\uff0c\u9700\u8981\\(T^T A T\uff08T^{-1}AT\uff09\\)\u4e3a\u5bf9\u89d2\u77e9\u9635\uff0c\u5373\\(A\\)\u53ef\u5bf9\u89d2\u5316\u3002</p> <p>\u82e5n\u9636\u5b9e\u77e9\u9635\\(A,B\\)\u5b58\u5728\u4e00\u4e2an\u9636\u6b63\u4ea4\u77e9\u9635\\(T\\)\uff0c\u4f7f\u5f97\\(T^{-1}AT=B\\)\uff0c\u5219\\(A\\)\u6b63\u4ea4\u76f8\u4f3c\u4e8e\\(B\\)</p> <ul> <li> <p>\u5b9e\u5bf9\u79f0\u77e9\u9635\u7684\u7279\u5f81\u591a\u9879\u5f0f\u7684\u6bcf\u4e00\u4e2a\u590d\u6839\u90fd\u662f\u5b9e\u6570\uff0c\u4ece\u800c\u5b83\u4eec\u90fd\u662f\u7279\u5f81\u503c</p> </li> <li> <p>\u5b9e\u5bf9\u79f0\u77e9\u9635\\(A\\)\u7684\u5c5e\u4e8e\u4e0d\u540c\u7279\u5f81\u503c\u7684\u7279\u5f81\u5411\u91cf\u662f\u6b63\u4ea4\u7684\\((\\mathbf{a_1,a_2})=0\\)</p> </li> <li> <p>\u5b9e\u5bf9\u79f0\u77e9\u9635\u4e00\u5b9a\u6b63\u4ea4\u76f8\u4f3c\u4e8e\u5bf9\u89d2\u77e9\u9635\uff08\\(T^{-1}AT=diag\\{\\lambda_1,\\lambda_2,...,\\lambda_n \\}\\)\uff09</p> </li> <li> <p>n\u9636\u5b9e\u77e9\u9635\\(A\\)\u6b63\u4ea4\u76f8\u4f3c\u4e8e\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\\(D\\)\uff0c\u5219\\(A\\)\u4e00\u5b9a\u662f\u5bf9\u79f0\u77e9\u9635\uff0c\\(T^{-1}AT=D, \\ A^T = (TDT^{-1})^T = TDT^{-1} = A\\)</p> </li> </ul>"},{"location":"math/HigherAlgebra/chapter/1/#\u4e8c\u6b21\u578b","title":"\u4e8c\u6b21\u578b","text":"<p>$Def. 1  $ \u6570\u57df\\(K\\)\u4e0a\u7684\u4e00\u4e2an\u5143\u4e8c\u6b21\u578b\u662f\u7cfb\u6570\u5728\\(K\\)\u4e2d\u7684n\u4e2a\u53d8\u91cf\u7684\u4e8c\u6b21\u9f50\u6b21\u591a\u9879\u5f0f\uff1a</p> \\[ f(x_1,x_2,...,x_n) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{ij} x_i x_j \\] <p>\u5176\u4e2d\\(a_{ij}=a_{ji}, \\ 1 \\leq i,j \\leq n\\)</p> <p>\u6240\u6709\u7cfb\u6570\u53ef\u6392\u6210\u4e00\u4e2an\u9636\u77e9\u9635\\(A\\)\u3002\u79f0\u4e3a\u4e8c\u6b21\u578b\\(f(x_1,...,x_n)\\)\u7684\u77e9\u9635\uff0c\u663e\u7136\\(A\\)\u662f\u5bf9\u79f0\u4e14\u552f\u4e00\u7684\uff0c\u4e3b\u5bf9\u89d2\u5143\u662f\\(x_1^2,x^2_2,...,x^2_n\\)\u7684\u7cfb\u6570\uff0c\\((i,j)\\)\u5143\u662f\\(x_i x_j\\)\u7684\u7cfb\u6570\u7684\u4e00\u534a:</p> \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; ... &amp; a_{1n} \\\\ a_{12} &amp; a_{22} &amp; ... &amp; a_{2n} \\\\ ... &amp; ... &amp; ... &amp; ... \\\\ a_{n1} &amp; a_{n2} &amp; ... &amp; a_{nn}\\\\ \\end{pmatrix}, \\ X = \\begin{pmatrix} x_1 \\\\ x_2  \\\\ ...\\\\ x_n \\\\ \\end{pmatrix} \\] <p>\u56e0\u6b64\u4e8c\u6b21\u578b\\(f(x_1,...,x_n)\\)\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\\(\\(f(x_1,...,x_n) = X^T A X\\)\\)</p> <p>\u4ee4\\(Y=\\begin{pmatrix} y_1 \\\\ y_2  \\\\ ...\\\\ y_n \\\\ \\end{pmatrix}\\)\uff0c\u8bbe\\(C\\)\u662f\u6570\u57df\\(K\\)\u4e0a\u7684n\u9636\u53ef\u9006\u77e9\u9635\uff0c\u5219\u5b58\u5728:</p> \\[ X = CY \\] <p>\u79f0\u4e3a\u53d8\u91cf\\(x_1,x_2,...,x_n\\)\u5230\\(y_1,...,y_n\\)\u7684\u975e\u9000\u5316\u7ebf\u6027\u66ff\u6362</p> \\[ X^T AX = (CY)^T A (CY) = Y^T (C^T A C) Y \\] <p>\u8bb0\\(B=C^{T}AC\\)\uff0c\u5219\\(X^{T} A X = Y^{T} B Y\\)\uff0c\\(B\\)\u4e5f\u662f\u5bf9\u79f0\u77e9\u9635\u3002</p> <p>$Def .2  $ \u5bf9\u4e8e\u6570\u57df\\(K\\)\u4e0a\u7684\u4e24\u4e2an\u5143\u4e8c\u6b21\u578b\\(X^{T} A X\\)\u4e0e\\(Y^{T}BY\\)\uff0c\u82e5\u5b58\u5728\u4e00\u4e2a\u975e\u9000\u5316\u7ebf\u6027\u66ff\u6362\\(X=CY\\)\uff0c\u4f7f\u5f97\\(X^{T}AX\\)\u8f6c\u6362\u4e3a\\(Y^{T}BY\\)\uff0c\u90a3\u4e48\u4e24\u8005\u7b49\u4ef7\uff0c\u8bb0\u4f5c\\(\\(X^{T}AX \\simeq Y^{T}BY\\)\\)\u3002</p> <p>$Def.3  $ \u6570\u57df\\(K\\)\u4e0a\u4e24\u4e2an\u9636\u77e9\u9635\\(A,B\\)\uff0c\u82e5\u5b58\u5728\\(K\\)\u4e0a\u4e00\u4e2an\u9636\u53ef\u9006\u77e9\u9635\\(C\\)\uff0c\u4f7f\u5f97\uff1a</p> \\[ C^{T} A C = B \\] <p>\u5219\u79f0\\(A\\)\u4e0e\\(B\\)\u5408\u540c\uff0c\u8bb0\u4f5c\\(A \\simeq B\\)\u3002</p> <ul> <li> <p>\u5f53\u4e24\u4e2an\u5143\u4e8c\u6b21\u578b\\(X^{T}AX\uff0cY^{T}BY\\)\u7b49\u4ef7\u5f53\u4e14\u4ec5\u5f53n\u9636\u77e9\u9635\\(A\uff0cB\\)\u5408\u540c</p> </li> <li> <p>\u82e5\u4e8c\u6b21\u578b\\(X^{T}AX\\)\u7b49\u4ef7\u4e8e\u4e00\u4e2a\u53ea\u542b\u5e73\u65b9\u9879\u7684\u4e8c\u6b21\u578b\uff0c\u79f0\u4e3a\u6807\u51c6\u5f62</p> </li> <li> <p>\u82e5\u5bf9\u79f0\u77e9\u9635\\(A\\)\u5408\u540c\u4e8e\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c\u5219\u79f0\u4e3a\u5408\u540c\u6807\u51c6\u5f62</p> </li> </ul> <p>\u5bf9\u4e8en\u9636\u5b9e\u5bf9\u79f0\u77e9\u9635\\(A\\)\uff0c\u5b58\u5728\u4e00\u4e2an\u9636\u6b63\u4ea4\u77e9\u9635\\(T\\)\uff0c\u4f7f\u5f97\\(T^{-1}AT\\)\u662f\u5bf9\u89d2\u77e9\u9635\\(diag \\{ \\lambda_1,\\lambda_2,...,\\lambda_n \\}\\)\uff0c\u5176\u4e2d\\(\\lambda_1,\\lambda_2,...,\\lambda_n\\)\u662f\\(A\\)\u7684\u7279\u5f81\u503c\u3002\u56e0\u4e3a\\(T^{-1} = T^T\\)\uff0c\u56e0\u6b64\\(A\\)\u5408\u540c\u4e8e\\(diag \\{ \\lambda_1,\\lambda_2,...,\\lambda_n \\}\\)\u3002</p> <p>\u56e0\u6b64\uff0c\\(X=TY\\)\u79f0\u4e3a\u6b63\u4ea4\u66ff\u6362\u3002</p> <ul> <li> <p>\u6570\u57df\\(K\\)\u4e0a\u4efb\u4e00\u5bf9\u79f0\u77e9\u9635\u90fd\u5408\u540c\u4e8e\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635</p> </li> <li> <p>\u6570\u57df\\(K\\)\u4e0a\u4efb\u4e00n\u5143\u4e8c\u6b21\u578b\u90fd\u7b49\u4ef7\u4e8e\u4e00\u4e2a\u53ea\u542b\u5e73\u65b9\u9879\u7684\u4e8c\u6b21\u578b\u3002\uff08\u65b9\u6cd5\uff1a\u6210\u5bf9\u7684\u521d\u7b49\u884c\u3001\u5217\u53d8\u6362\u6cd5\uff09</p> </li> <li> <p>\u6570\u57df\\(K\\)\u4e0an\u5143\u4e8c\u6b21\u578b\\(X^{T}AX\\)\u7684\u4efb\u4e00\u6807\u51c6\u5f62\u4e2d\uff0c\u7cfb\u6570\u4e0d\u4e3a0\u7684\u5e73\u65b9\u9879\u4e2a\u6570\u7b49\u4e8e\u77e9\u9635\\(A\\)\u7684\u79e9\uff0c\\(rank(A)=r\\)</p> </li> </ul> <p>$Exercise. $ \u5c06\u4e8c\u6b21\u66f2\u9762\u65b9\u7a0b\u5316\u4f5c\u6807\u51c6\u65b9\u7a0b\uff1a</p> \\[ x^2 + 4y^2 + z^2 - 4xy - 8xz - 4yz + 2x + y + 2z - \\frac{25}{16} = 0 \\] <p>\u53d6\u4e8c\u6b21\u9879\u90e8\u5206\uff1a\\(f(x,y,z)=x^2 + 4y^2 + z^2 - 4xy - 8xz - 4yz\\)\uff0c\u4e8c\u6b21\u578b\u77e9\u9635\u4e3a\uff1a</p> \\[ \\begin{pmatrix} A = \\begin{pmatrix} 1 &amp; -2 &amp; -4 \\\\ -2 &amp; 4 &amp; -2 \\\\ -4 &amp; -2 &amp; 1 \\\\ \\end{pmatrix} \\\\ |\\lambda I - A| = (\\lambda +4 )(\\lambda - 5)^2 = 0 \\\\ \\end{pmatrix} \\] <p>\u901a\u8fc7\u7279\u5f81\u503c\u4e3a5\uff0c5\uff0c-4\uff0c\u6613\u5f97\u7279\u5f81\u5411\u91cf\\(a_1,a_2,a_3\\)\uff0c\u901a\u8fc7\u65bd\u5bc6\u7279\u6b63\u4ea4\u5316\u548c\u5355\u4f4d\u5316\u5f97\u5230\\(\\eta_1,\\eta_2,\\eta_3\\)\u3002\u56e0\u6b64\uff0c\u5f97\u5230\u6b63\u4ea4\u77e9\u9635\\(T\\):</p> \\[ \\begin{aligned} T =&amp; (\\eta_1,\\eta_2,\\eta_3) \\\\ T^{-1} A  T = T^{T}A &amp; T = diag\\{ 5,5,-4 \\} \\end{aligned} \\] <p>\u4f5c\u6b63\u4ea4\u66ff\u6362,\u4f7f\u5f97\u4e8c\u6b21\u578b\\(f(x,y,z)\\)\u5316\u4e3a\u6807\u51c6\u5f62\\(5{x'}^2 + 5{y'}^2-4{z'}^2\\)\uff1a</p> \\[ \\begin{pmatrix} x \\\\ y \\\\  z \\\\ \\end{pmatrix} = T \\begin{pmatrix} x' \\\\ y' \\\\  z' \\\\ \\end{pmatrix} \\] <p>\u5f97\u5230\u4e8c\u6b21\u66f2\u9762\u7684\u65b0\u65b9\u7a0b\uff1a</p> \\[ \\begin{aligned} 5{x'}^2 + 5{y'}^2 - 4{z'}^2 + 3z' - \\frac{25}{16} = 0 \\\\ 5{x'}^2 + 5{y'}^2 - 4{(z' - \\frac{3}{8})}^2 - 1 = 0 \\\\ \\end{aligned} \\] <p>\u4f5c\u79fb\u8f74\uff1a</p> \\[ \\begin{cases} x^* = x' \\\\ y^* = y' \\\\ z^* = z' - \\frac{3}{8} \\\\ \\end{cases} \\] <p>\u5219\u65b9\u7a0b\u8f6c\u53d8\u4e3a\uff1a\\(5 {x^*}^2 + 5 {y^*}^2 - 4{z^*}^2 = 1\\)</p> <p>\u56e0\u6b64\uff0c\u603b\u7684\u76f4\u89d2\u5750\u6807\u53d8\u6362\u516c\u5f0f\u4e3a\uff1a</p> \\[ \\begin{pmatrix} x \\\\ y \\\\  z \\\\ \\end{pmatrix} = T \\begin{pmatrix} x^* \\\\ y^* \\\\  z^* + \\frac{3}{8} \\\\ \\end{pmatrix}  = T \\begin{pmatrix} x^* \\\\ y^* \\\\  z^*  \\\\ \\end{pmatrix}  + T \\begin{pmatrix} 0 \\\\ 0 \\\\  \\frac{3}{8}  \\\\ \\end{pmatrix} = T \\begin{pmatrix} x^* \\\\ y^* \\\\  z^*  \\\\ \\end{pmatrix}  +  \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{8} \\\\  \\frac{1}{4} \\\\   \\end{pmatrix} \\] <p>$Exercise. $ \u8bbe\\(A\\)\u662f\u6570\u57df\\(K\\)\u4e0a\u7684n\u9636\u77e9\u9635\uff0c\u8bc1\u660e\uff1a\\(A\\)\u662f\u659c\u5bf9\u79f0\u77e9\u9635\u5f53\u4e14\u4ec5\u5f53\u5bf9\u4e8e\\(K^n\\)\u4e2d\u4efb\u4e00\u5217\u5411\u91cf\\(\\mathbf{a}\\)\uff0c\u6709\\(\\mathbf{a}^T A \\mathbf{a} = 0\\)</p> <ul> <li> <p>\u5fc5\u8981\u6027\uff1a\u56e0\u4e3a\\(A\\)\u662f\u659c\u5bf9\u79f0\u77e9\u9635\uff0c\u6240\u4ee5\\(A^T = -A\\)\uff0c\u56e0\u6b64\\((\\mathbf{a}^T A \\mathbf{a})^T = \\mathbf{a}^T A^T \\mathbf{a} = -\\mathbf{a}^T A \\mathbf{a} = 0\\)\uff0c\u56e0\u6b64\\(\\mathbf{a}^T A \\mathbf{a} = 0\\)</p> </li> <li> <p>\u5145\u5206\u6027\uff1a\u8bbe\\(A\\)\u7684\u5217\u5411\u91cf\\(\\mathbf{a_1,a_2,...,a_n}\\)\uff0c\u5df2\u77e5\\(\\epsilon_i^T A \\epsilon_i = \\epsilon_i^T \\mathbf{a_i} = a_{ii} = 0, \\quad i=1,2,...,n\\) \uff0c\\((\\epsilon_i + \\epsilon_j)^T A (\\epsilon_i + \\epsilon_j) = (\\epsilon_i + \\epsilon_j)^T (\\mathbf{a_i} + \\mathbf{a_j}) = a_{ii} + a_{ij} + a_{ji} + a_{jj} = a_{ij} + a_{ji}, \\quad i \\neq j\\)\uff0c\u56e0\u6b64\u662f\\(A\\)\u659c\u5bf9\u79f0\u77e9\u9635</p> </li> </ul> <p>\\(Exercise.\\)  \u8bben\u9636\u5b9e\u5bf9\u79f0\u77e9\u9635\\(A\\)\u7684\u5168\u90e8\u7279\u5f81\u503c\u6309\u5927\u5c0f\u987a\u5e8f\u6392\u6210\\(\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n\\)\u3002\u8bc1\u660e\uff1a\u5bf9\u4e8e\\(R^n\\)\u4e2d\u4efb\u4e00\u975e\u96f6\u5217\u5411\u91cf\\(\\mathbf{a}\\)\uff0c\u90fd\u6709\\(\\(\\lambda_n \\leq \\frac{\\mathbf{a}^T A \\mathbf{a}}{|\\mathbf{a}|^2} \\leq \\lambda_1\\)\\)</p> <p>\u56e0\u4e3a\\(A\\)\u4e3an\u9636\u5b9e\u5bf9\u79f0\u77e9\u9635\uff0c\u56e0\u6b64\u5b58\u5728\u6b63\u4ea4\u77e9\u9635\\(T\\)\uff0c\u4f7f\u5f97\\(T^{-1}AT = diag\\{ \\lambda_1,\\lambda_2,...,\\lambda_n \\}\\)\uff0c\u8bbe\\(\\mathbf{a}\\)\u4e3a\u4e00\u4e2a\\(R^n\\)\u4e2d\u7684\u975e\u96f6\u5217\u5411\u91cf\uff0c\\((T \\mathbf{a})^T = (b_1,b_2,...,b_n)\\)\uff0c\u56e0\u6b64\uff1a</p> \\[ \\begin{aligned} \\mathbf{a}^T A \\mathbf{a} =&amp; \\mathbf{a}^T T \\ diag\\{ \\lambda_1,\\lambda_2,...,\\lambda_n \\} \\ T \\mathbf{a} = (T \\mathbf{a})^T diag\\{ \\lambda_1,\\lambda_2,...,\\lambda_n \\} (T \\mathbf{a}) \\\\ =&amp; \\lambda_1 b^2_1 + \\lambda_2 b^2_2 + ... + \\lambda_n b^2_n \\\\ \\end{aligned} \\] <p>\u663e\u7136\uff0c\\(\\lambda_n |\\mathbf{a}|^2 \\leq \\mathbf{a}^T A \\mathbf{a} \\leq \\lambda_1 (b_1^2 + b_2^2 + ... + b_n^2) = \\lambda_1 |\\mathbf{a}|^2\\)</p> <p>\u56e0\u6b64\uff1a\\(\\(\\lambda_n \\leq \\frac{\\mathbf{a}^T A \\mathbf{a}}{|\\mathbf{a}|^2} \\leq \\lambda_1\\)\\)</p>"},{"location":"math/HigherAlgebra/chapter/1/#\u5b9e\u4e8c\u6b21\u578b\u7684\u89c4\u8303\u5f62","title":"\u5b9e\u4e8c\u6b21\u578b\u7684\u89c4\u8303\u5f62","text":"<p>\u5b9e\u4e8c\u6b21\u578b\\(X^T AX\\)\u7684\u89c4\u8303\u5f62\u662f\uff1a</p> \\[ z_1^2 + z_2^2 + ... + z_p^2 - z_{p+1}^2 - ... - z_r^2 \\] <p>\u89c4\u8303\u5f62\u53ea\u542b\u5e73\u65b9\u9879\uff0c\u4e14\u7cfb\u6570\u4e3a1\uff0c-1\uff0c0\uff0c\u7cfb\u6570\u4e3a1\u7684\u5e73\u65b9\u9879\u5199\u5728\u524d\u9762\uff0c\u5176\u4e2d\u5e73\u65b9\u9879\u7684\u4e2a\u6570\u7b49\u4e8e\u77e9\u9635\\(A\\)\u7684\u79e9\\(rank(A)\\)\uff0c</p> <p>\\(Def. 1 \\ \\(\u60ef\u6027\u5b9a\u7406\uff1an\u5143\u5b9e\u4e8c\u6b21\u578b\\)X^T AX\\)\u7684\u89c4\u8303\u5f62\u662f\u552f\u4e00\u7684\u3002</p> <ul> <li> <p>\u5176\u4e2d\uff0c\u7cfb\u6570\u4e3a+1\u7684\u5e73\u65b9\u9879\u4e2a\u6570\\(p\\)\u79f0\u4e3a\u4e8c\u6b21\u578b\u7684\u6b63\u60ef\u6027\u6307\u6570\uff0c\u7cfb\u6570\u4e3a-1\u7684\u5e73\u65b9\u9879\u4e2a\u6570\\(r-p\\)\u79f0\u4e3a\u4e8c\u6b21\u578b\u7684\u8d1f\u60ef\u6027\u6307\u6570\u3002\u6b63\u60ef\u6027\u6307\u6570\u51cf\u53bb\u8d1f\u60ef\u6027\u6307\u6570\u7684\u5dee\\(2p-r\\)\u79f0\u4e3a\\(X^TAX\\)\u7684\u7b26\u53f7\u5dee\u3002</p> </li> <li> <p>\u4e24\u4e2an\u5143\u5b9e\u4e8c\u6b21\u578b\u7b49\u4ef7$ \\leftrightarrow  \\(\u89c4\u8303\u5f62\u76f8\u540c\\) \\leftrightarrow  $\u79e9\u76f8\u7b49\uff0c\u4e14\u6b63\u60ef\u6027\u6307\u6570\u76f8\u7b49</p> </li> <li> <p>\u82e5\\(A\\)\u5408\u540c\u4e8e\u5bf9\u89d2\u77e9\u9635\\(diag\\{ 1,...,1,-1,...,-1,0,...,0 \\}\\)\uff081\u7684\u4e2a\u6570\u7b49\u4e8e\u6b63\u60ef\u6027\u6307\u6570\uff0c-1\u7684\u4e2a\u6570\u7b49\u4e8e\u8d1f\u60ef\u6027\u6307\u6570\uff09\uff0c\u5219\u5176\u4e3a\\(A\\)\u7684\u5408\u540c\u89c4\u8303\u5f62</p> </li> <li> <p>\u4e24\u4e2an\u9636\u5b9e\u5bf9\u79f0\u77e9\u9635\u5408\u540c$ \\leftrightarrow  $\u79e9\u76f8\u7b49\uff0c\u6b63\u60ef\u6027\u6307\u6570\u76f8\u7b49</p> </li> </ul>"},{"location":"math/HigherAlgebra/chapter/1/#\u590d\u4e8c\u6b21\u578b\u7684\u89c4\u8303\u5f62","title":"\u590d\u4e8c\u6b21\u578b\u7684\u89c4\u8303\u5f62","text":"<p>\u8bben\u5143\u590d\u4e8c\u6b21\u578b\\(X^T AX\\)\u901a\u8fc7\u4e00\u4e2a\u975e\u9000\u5316\u7ebf\u6027\u66ff\u6362\\(X=CY\\)\u8f6c\u6362\u6210\u6807\u51c6\u5f62\uff1a</p> \\[ d_1 y^2_1 + d_2 y^2_2 + ... + d_r y^2_r \\] <p>\\(r\\)\u5373\u4e3a\u4e8c\u6b21\u578b\u7684\u79e9\uff0c\\(d_i \\neq 0 , \\ i=1,2,...,r\\)\u3002\u5047\u8bbe\\(d_j=r_j (cos \\theta_j + i \\ sin\\theta_j), \\ 0 \\leq \\theta_j &lt; 2 \\pi\\)\uff0c\u6613\u5f97\uff1a</p> \\[ d_j = [\u00b1 \\sqrt{r_j} (cos \\frac{\\theta_j}{2} + i \\ \\frac{sin\\theta_j}{2})  ]^2 \\] <p>\u56e0\u6b64\u5c06\\(\\sqrt{r_j} (cos \\frac{\\theta_j}{2} + i \\ \\frac{sin\\theta_j}{2})\\)\u8bb0\u4f5c\\(\\sqrt{d_jd_j}\\)\uff0c\u5219\u518d\u4f5c\u975e\u9000\u5316\u7ebf\u6027\u66ff\u6362\u4e3a\uff1a</p> \\[ \\begin{cases} y_j = \\frac{1}{\\sqrt{d_j}} z_j, \\ j=1,2,...,r\\\\ y_l = z_l, \\ l=r+1,r+2,...,n \\end{cases} \\] <p>\u56e0\u6b64\\(X^T AX\\)\u7684\u6807\u51c6\u5f62\uff1a</p> \\[ z_1^2 + z_2^2 + ... + z_r^2 \\] <p>\u4fbf\u662f\u590d\u4e8c\u6b21\u578b\\(X^T AX\\)\u7684\u89c4\u8303\u578b\uff0c\u53ea\u542b\u5e73\u65b9\u9879\uff0c\u4e14\u7cfb\u6570\u4e3a1\u62160\u3002\u590d\u4e8c\u6b21\u578b\\(X^T AX\\)\u7684\u89c4\u8303\u578b\u5b8c\u5168\u7531\u79e9\u51b3\u5b9a\u3002</p> <ul> <li> <p>\u590d\u4e8c\u6b21\u578b\\(X^T AX\\)\u7684\u89c4\u8303\u5f62\u662f\u552f\u4e00\u7684</p> </li> <li> <p>\u4e24\u4e2an\u5143\u590d\u4e8c\u6b21\u578b\u7b49\u4ef7$ \\leftrightarrow  \\(\u89c4\u8303\u5f62\u76f8\u540c\\) \\leftrightarrow  $\u79e9\u76f8\u7b49</p> </li> <li> <p>\u4efb\u4e00n\u9636\u590d\u5bf9\u79f0\u77e9\u9635\\(A\\)\u5408\u540c\u4e8e\u5bf9\u89d2\u77e9\u9635\\(\\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0  \\end{pmatrix}\\)\uff0c\u5176\u4e2d\\(r=rank(A)\\)\u3002</p> </li> <li> <p>\u4e24\u4e2an\u9636\u590d\u5bf9\u79f0\u77e9\u9635\u5408\u540c$ \\leftrightarrow  $\u79e9\u76f8\u7b49</p> </li> </ul>"},{"location":"math/HigherAlgebra/chapter/1/#\u6b63\u5b9a\u4e8c\u6b21\u578b","title":"\u6b63\u5b9a\u4e8c\u6b21\u578b","text":"<p>\u82e5\u5bf9\u4e8e\\(R^n\\)\u4e2d\u4efb\u610f\u975e\u96f6\u5217\u5411\u91cf\\(\\mathbf{a}\\)\uff0c\u90fd\u7531\\(\\mathbf{a}^T A \\mathbf{a} &gt; 0\\)\uff0c\u5219\u79f0\\(X^T AX\\)\u4e3a\u6b63\u5b9a\u4e8c\u6b21\u578b\u3002</p> <ul> <li> <p>n\u5143\u5b9e\u4e8c\u6b21\u578b\\(X^T AX\\)\u6b63\u5b9a\uff0c\u5f53\u4e14\u4ec5\u5f53\u5176\u6b63\u60ef\u6027\u6307\u6570\u7b49\u4e8en\uff0c\u5176\u89c4\u8303\u5f62\u4e3a\uff1a$$y_1<sup>2+y_2</sup>2+...+y^2_n $$</p> </li> <li> <p>n\u5143\u5b9e\u4e8c\u6b21\u578b\\(X^T AX\\)\u662f\u6b63\u5b9a\u7684 $ \\leftrightarrow  $ \\(A\\)\u7684\u6b63\u60ef\u6027\u6307\u6570\u7b49\u4e8en $ \\leftrightarrow  $ \\(A\\)\u5408\u540c\u4e8e\u5bf9\u89d2\u77e9\u9635\\(I_n\\)\uff0c\u5373\\(A \\simeq I_n\\) $ \\leftrightarrow  $ \\(A\\)\u7684\u7279\u5f81\u503c\u5168\u4e3a\u6b63</p> </li> <li> <p>\u4e0e\u6b63\u5b9a\u77e9\u9635\u5408\u540c\u7684\u5b9e\u5bf9\u79f0\u77e9\u9635\u4e5f\u662f\u6b63\u5b9a\u77e9\u9635</p> </li> <li> <p>\u975e\u9000\u5316\u7ebf\u6027\u9000\u5316\u4e0d\u4f1a\u7ed9\u6539\u53d8\u5b9e\u4e8c\u6b21\u578b\u7684\u6b63\u5b9a\u6027</p> </li> <li> <p>\u6b63\u5b9a\u77e9\u9635\u7684\u884c\u5217\u5f0f\u5927\u4e8e0\uff0c\\(A=C^T I_n C\uff0c|A|=|C^T C| = |C|^2&gt;0\\)</p> </li> </ul> <p>$Def. 1  $ \u82e5\u5bf9\u4e8e\\(R^n\\)\u4e2d\u4efb\u4e00\u975e\u96f6\u5217\u5411\u91cf\\(\\mathbf{a}\\)\uff0c\u90fd\u6709\uff1a</p> \\[ \\mathbf{a}^T A \\mathbf{a} \\geq 0\uff0c\\quad (\\mathbf{a}^T A \\mathbf{a} &lt; 0, \\ \\mathbf{a}^T A \\mathbf{a} \\leq 0) \\] <p>\u5219\u8be5n\u5143\u5b9e\u4e8c\u6b21\u578b\\(X^T AX\\)\u79f0\u4e3a\u534a\u6b63\u5b9a\uff08\u8d1f\u5b9a\uff0c\u534a\u8d1f\u5b9a\uff09\u3002</p> <ul> <li>n\u5143\u5b9e\u4e8c\u6b21\u578b\\(X^T AX\\)\u662f\u534a\u6b63\u5b9a\\(\\ \\leftrightarrow \\$\u6b63\u60ef\u6027\u6307\u6570\u7b49\u4e8e\u79e9\\) \\leftrightarrow $\u89c4\u8303\u5f62\u4e3a\\(y_1^2+...+y_r^2\\) $ \\leftrightarrow $\u6807\u51c6\u5f62\u4e2dn\u4e2a\u7cfb\u6570\u5747\u975e\u8d1f</li> </ul> <p>$Def. 2  $ \u9ed1\u585e\uff08Hesse\uff09\u77e9\u9635:\u8bbe\u4e8c\u5143\u5b9e\u503c\u51fd\u6570\\(F(x,y)\\)\u6709\u4e00\u4e2a\u7a33\u5b9a\u70b9\\(\\mathbf{a}=(x_0,y_0)\\)\uff0c\u5373\u5728\u8be5\u70b9\u7684\u4e00\u9636\u504f\u5bfc\u6570\u5168\u4e3a0\uff0c\u8bbe\\(F(x,y)\\)\u5728\\((x_0,y_0)\\)\u7684\u4e00\u4e2a\u90bb\u57df\u91cc\u67093\u9636\u8fde\u7eed\u504f\u5bfc\u6570\uff0c\u4ee4\uff1a</p> \\[ \\mathbf{H} = \\begin{pmatrix} F''_{xx}(x_0,y_0) &amp; F''_{xy}(x_0,y_0) \\\\ F''_{xy}(x_0,y_0) &amp; F''_{yy}(x_0,y_0) \\end{pmatrix} \\] <p>\u79f0\\(\\mathbf{H}\\)\u4e3a\\(F(x,y)\\)\u5728\\((x_0,y_0)\\)\u5904\u7684\u9ed1\u585e\u77e9\u9635\u3002\u82e5\u9ed1\u585e\u77e9\u9635\\(\\mathbf{H}\\)\u6b63\u5b9a\uff0c\u5219\\(F(x,y)\\)\u5728\\((x_0,y_0)\\)\u5904\u5177\u6709\u6781\u5c0f\u503c\u3002\u82e5\u8d1f\u5b9a\uff0c\u5219\\(F(x,y)\\)\u5728\\((x_0,y_0)\\)\u5904\u5177\u6709\u6781\u5927\u503c\u3002</p> <p>\u5c06\\(F(x,y)\\)\u5728\\((x_0,y_0)\\)\u5904\u5c55\u5f00\u4e3aTalyor\u7ea7\u6570\uff0c\u5f97\uff1a</p> \\[ F(x_0+h, y_0+k) = F(x_0,y_0) + [h F'_x(x_0,y_0) + k F'_y(x_0,y_0)] + \\frac{1}{2} [h^2 F''_{xx}(x_0,y_0) + 2hk F''_{xy}(x_0,y_0) + k^2 F''_{yy}(x_0,y_0)] + R \\] <p>\u8bbe\\(a=F''_{xx}(x_0,y_0),b=F''_{xy}(x_0,y_0),c=F''_{yy}(x_0,y_0)\\)\uff0c\u5219\uff1a</p> \\[ f(h,k) = F(x_0+h, y_0+k)  - F(x_0,y_0)= ah^2 + 2bhk + ck^2 + R \\] <p>\u662f\\(h,k\\)\u7684\u5b9e\u4e8c\u6b21\u578b\uff0c\u5176\u77e9\u9635\u5c31\u662f\\(\\mathbf{H}\\)\u3002\u82e5\u6b63\u5b9a\uff0c\u5219\u5bf9\u4e8e\u8db3\u591f\u5c0f\u7684\\(|h|,|k| \\neq 0\\)\uff0c\u6709</p> \\[ F(x_0+h, y_0+k)  - F(x_0,y_0) &gt; 0 \\] <p>\u8868\u660e\\(F(x,y)\\)\u5728\\((x_0,y_0)\\)\u5904\u5177\u6709\u6781\u5c0f\u503c\u3002\u8d1f\u5b9a\u7684\u60c5\u51b5\u76f8\u53cd\uff0c\u8fbe\u5230\u6781\u5927\u503c\u3002</p> <p>\u63a8\u5e7f\u5230n\u5143\u5f62\u5f0f\uff0c\u8bbe\\(F(x_1,x_2,...,x_n)\\)\u5728\\(\\mathbf{a}=(a_1,a_2,...,a_n)\\)\u5904\u6709\u8fde\u7eed\u76843\u9636\u504f\u5bfc\u6570\uff0c\u5219\u9ed1\u585e\u77e9\u9635\\(\\mathbf{H}=(F''_{x_i x_j}(\\mathbf{a}))\\)\u3002</p>"},{"location":"math/HigherAlgebra/chapter/2/","title":"Higher Algebra: \u591a\u9879\u5f0f\u73af","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599:</p> <p></p>"},{"location":"math/HigherAlgebra/chapter/3/","title":"Higher Algebra: \u7ebf\u6027\u7a7a\u95f4","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599:</p> <p></p>"},{"location":"math/HigherAlgebra/chapter/3/#\u57fa\u4e0e\u7ef4\u6570","title":"\u57fa\u4e0e\u7ef4\u6570","text":""},{"location":"math/MathematicalAnalysis/","title":"\u6570\u5b66\u5206\u6790 Mathematical Analysis\uff1a\u4e2a\u4eba\u7b14\u8bb0","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599: Baby rudin\uff0c\u6885\u52a0\u5f3a\uff0c\u8c22\u60e0\u6c11 </p> <p>\u7ae0\u8282</p> <p> <ul> <li> <p> 1\uff1a </p> </li> <li> <p> 2\uff1a </p> </li> </ul> <p></p>"},{"location":"math/MathematicalMethodForPhysics/MMFP/","title":"\u6570\u5b66\u7269\u7406\u65b9\u6cd5\uff1aMathematical methods in Physics","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599:</p> <p>\u300a\u6570\u5b66\u7269\u7406\u65b9\u6cd5\uff08\u7b2c\u4e09\u7248\uff09\u300b\uff0c\u5434\u5d07\u8bd5 \u7f16</p> <p>\u300a\u6570\u5b66\u7269\u7406\u65b9\u6cd5\u300b\uff0c\u987e\u6a35 \u7f16 </p> <p>\u7ae0\u8282</p> <p> <ul> <li> <p> 1\uff1a </p> </li> <li> <p> 2\uff1a </p> </li> </ul> <p></p>"},{"location":"math/MathematicalMethodForPhysics/chapter/1/","title":"\u6570\u5b66\u7269\u7406\u65b9\u6cd5\uff1aMathematical methods in Physics","text":""},{"location":"math/MathematicalMethodForPhysics/chapter/1/#\u5fae\u5206\u7b97\u5b50nabla\u4e0e\u62c9\u666e\u62c9\u65af\u7b97\u5b50nabla2","title":"\u5fae\u5206\u7b97\u5b50\\(\\nabla\\)\u4e0e\u62c9\u666e\u62c9\u65af\u7b97\u5b50\\(\\nabla^2\\)","text":""},{"location":"math/MathematicalMethodForPhysics/chapter/1/#\u5085\u91cc\u53f6\u7ea7\u6570\u4ee5\u53ca\u53d8\u6362","title":"\u5085\u91cc\u53f6\u7ea7\u6570\u4ee5\u53ca\u53d8\u6362","text":""},{"location":"math/PRML/","title":"\ud83d\udd2dPRML:Pattern Recognition and Machine Learning","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p></p> <p></p> <p>Chapter</p> <p> <ul> <li> <p>Chapter 1 </p> </li> <li> <p>Chapter 2</p> </li> <li> <p>Chapter 3</p> </li> </ul> <p></p>"},{"location":"math/TensorCalculus/","title":"\u5f20\u91cf\u5206\u6790\uff1aTensor Analysis","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599:</p> <p></p> <p>\u7ae0\u8282</p> <p> <ul> <li> <p> 1\uff1a </p> </li> <li> <p> 2\uff1a </p> </li> </ul> <p></p>"},{"location":"math/TensorCalculus/begin/","title":"\u5f20\u91cf\u5206\u6790\uff1aTensor Analysis","text":""},{"location":"math/TensorCalculus/begin/#\u77e2\u91cf\u4e0e\u5f20\u91cf","title":"\u77e2\u91cf\u4e0e\u5f20\u91cf","text":""},{"location":"math/TensorCalculus/begin/#\u57fa\u7840\u8fd0\u7b97","title":"\u57fa\u7840\u8fd0\u7b97","text":"<p>\u7ef4\u6570\uff08dimension\uff09\uff1a\u4e00\u4e2a\u77e2\u91cf\u7a7a\u95f4\u6240\u5305\u542b\u7684\u6700\u5927\u7ebf\u6027\u65e0\u5173\u77e2\u91cf\u7684\u6570\u76ee\u3002\u5982\uff0c\u4e09\u7ef4\u7a7a\u95f4\u6700\u591a\u67093\u4e2a\u7ebf\u6027\u65e0\u5173\u7684\u77e2\u91cf</p> <p>\u53c9\u79ef\uff08cross product\uff09\uff1a\u4e24\u4e2a\u77e2\u91cf\\(u\\)\u548c\\(v\\)\u7684\u53c9\u79ef\\(u\u00d7v\\)\u662f\u4e00\u4e2a\u77e2\u91cf\\(w\\)\uff0c\u5176\u65b9\u5411\u5782\u76f4\u4e8e\\(u\\)\u548c\\(v\\)\u6240\u51b3\u5b9a\u7684\u5e73\u9762\u3002\u5176\u65b9\u5411\u7531\u53f3\u624b\u5b9a\u5219\u786e\u5b9a\u3002</p> \\[ w = u\u00d7v = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\ u_x &amp; u_y &amp; u_z \\\\ v_x &amp; v_y &amp; v_z \\\\ \\end{vmatrix}  \\] <p>\u53c9\u79ef\u7684\u6a21\u4e3a\\(|u\u00d7v|=|u||v| \\sin{(u,v)}\\)</p> <p>\u6df7\u5408\u79ef\uff1a\\([u \\ v \\ w] = (u \\times v) \\cdot w = u \\cdot (v \\times w) = \\begin{vmatrix} u_x &amp; u_y &amp; u_z \\\\ v_x &amp; v_y &amp; v_z \\\\ w_x &amp; w_y &amp; w_z \\\\ \\end{vmatrix} = \\begin{vmatrix} u_x &amp; v_x &amp; w_x \\\\ u_y &amp; v_y &amp; w_y \\\\ u_z &amp; v_z &amp; w_z \\\\ \\end{vmatrix} \\(\uff0c\u5176\u7269\u7406\u610f\u4e49\u4e3a\u4ee5\\)u\\)\uff0c\\(v\\)\uff0c\\(w\\)\u4e3a\u68f1\u7684\u5e73\u884c\u516d\u9762\u4f53\u7684\u4f53\u79ef\u3002\uff08\u5f53\\(u,v,w\\)\u6784\u6210\u53f3\u624b\u7cfb\u65f6\uff0c\u516d\u9762\u4f53\u4f53\u79ef\u4e3a\u6b63\uff0c\u5373\u6df7\u5408\u79ef\\([u \\ v \\ w]\\)\u4e3a\u6b63\uff09</p> <p>\u6df7\u5408\u79ef\u7684\u4e58\u6cd5\uff1a$[u  v  w] [u'  v'  w'] = \\begin{vmatrix} u \\cdot u' &amp; u \\cdot v' &amp; u \\cdot w' \\ v \\cdot u' &amp; v \\cdot v' &amp; v \\cdot w' \\ w \\cdot u' &amp; w \\cdot v' &amp; w \\cdot w' \\ \\end{vmatrix} $</p>"},{"location":"math/TensorCalculus/begin/#\u5e73\u9762\u659c\u89d2\u76f4\u7ebf\u5750\u6807\u7cfb","title":"\u5e73\u9762\u659c\u89d2\u76f4\u7ebf\u5750\u6807\u7cfb","text":"<p>\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u662f\u6807\u51c6\u7684\u6b63\u4ea4\u76f4\u7ebf\u5750\u6807\u7cfb\uff0c\u800c\u5e73\u9762\u659c\u89d2\u76f4\u7ebf\u5750\u6807\u7cfb\u7684\u5750\u6807\u8f74\u4e0d\u4e92\u76f8\u5782\u76f4\uff0c\u5b58\u5728\u5939\u89d2\\(\\phi(\\phi &lt; \\pi)\\)\u3002</p> <p>\u5982\u4e0b\u56fe\uff0c\u5e73\u9762\u5185\u7684\u76f4\u7ebf\u5750\u6807\u7cfb\\(x^1,x^2\\)\uff0c\u5176\u5750\u6807\u7ebf\u4e92\u4e0d\u6b63\u4ea4\u3002\u9009\u53d6\u6cbf\u5750\u6807\u7ebf\\(x^1,x^2\\)\u7684\u53c2\u8003\u77e2\u91cf\\(\\mathbf{g}_{1},\\mathbf{g}_{2}\\)</p> <p></p> <p>\u5e73\u9762\u4e2d\u7684\u4efb\u610f\u77e2\u91cf\\(\\mathbf{P}\\)\u53ef\u4ee5\u5206\u89e3\u4e3a\uff1a\\(\\mathbf{P} = P^1 \\mathbf{g}_1+ P^2 \\mathbf{g}_2 = \\sum^2_{\\alpha=1} P^\\alpha \\mathbf{g}_\\alpha\\)</p> <p>\u54d1\u6307\u6807:\u91c7\u7528\u7231\u56e0\u65af\u5766\u6c42\u548c\u7ea6\u5b9a\uff0c\u6211\u4eec\u53ef\u4ee5\u7701\u7565\u8868\u8fbe\u5f0f\u7684\u6c42\u548c\u7b26\u53f7\\(\\sum\\)\u3002\u5728\u540c\u4e00\u9879\u4e2d\uff0c\u4ee5\u4e00\u4e2a\u4e0a\u6307\u6807\u4e0e\u4e00\u4e2a\u4e0b\u6307\u6807\u6210\u5bf9\u51fa\u73b0\uff0c\u8868\u793a\u904d\u5386\u5176\u53d6\u503c\u8303\u56f4\u6c42\u548c\u3002\u5982\\(\\sum^2_{\\alpha=1} P^\\alpha \\mathbf{g}_\\alpha = P^\\alpha \\mathbf{g}_\\alpha = P^\\beta \\mathbf{g}_\\beta\\)</p> <p>\u518d\u5f15\u5165\u4e00\u5bf9\u4e8e\\(\\mathbf{g}_\\alpha(\\alpha=1,2)\\)\u5bf9\u5076\u7684\u53c2\u8003\u77e2\u91cf\\(\\mathbf{g}^\\alpha(\\alpha=1,2)\\)\uff0c\u5176\u4e2d\\(\\mathbf{g}^1\\)\uff0c\\(\\mathbf{g}^2\\)\u5206\u522b\u4e0e\\(\\mathbf{g}_2\\)\uff0c\\(\\mathbf{g}_1\\)\u76f8\u4e92\u6b63\u4ea4\uff0c\u5982\u4e0a\u56fe\u3002</p> \\[ |\\mathbf{g}^1| = \\frac{1}{|\\mathbf{g_1}| \\sin{(\\phi)}}\uff0c|\\mathbf{g}^2| = \\frac{1}{|\\mathbf{g_2}| \\sin{(\\phi)}} \\] <p>\u5176\u4e2d\uff0c\u6211\u4eec\u79f0\\(\\mathbf{g}_\\alpha(\\alpha=1,2)\\)\u4e3a\u534f\u53d8\u57fa\u77e2\u91cf\uff0c\\(\\mathbf{g}^\\beta(\\beta=1,2)\\)\u4e3a\u9006\u53d8\u57fa\u77e2\u91cf\u3002\u4e24\u8005\u7684\u5bf9\u5076\u6761\u4ef6:</p> \\[ \\mathbf{g}_\\alpha \\cdot \\mathbf{g}^\\beta = \\delta_\\alpha^\\beta(\\alpha,\\beta=1,2) =  \\begin{cases} 1, \\quad \\alpha = \\beta \\\\ 0, \\quad \\alpha \\neq \\beta \\end{cases} \\] <p>\\(\\delta_\\alpha^\\beta\\)\u4e3a\\(Kronecker \\ \\delta\\)(\u514b\u7f57\u5185\u514bDelta\u51fd\u6570)</p> <p>\u77e2\u91cf\u5bf9\u534f\u53d8\u57fa\u77e2\u91cf\u7684\u5206\u91cf\u6210\u4e3a\u77e2\u91cf\\(\\mathbf{P}\\)\u7684\u9006\u53d8\u5206\u91cf\\(P^\\alpha = \\mathbf{P} \\cdot \\mathbf{g}^\\alpha\uff08\\alpha=1,2\uff09\\)\u3002\u540c\u7406\uff0c\u77e2\u91cf\u5bf9\u9006\u53d8\u57fa\u77e2\u91cf\u7684\u5206\u91cf\u6210\u4e3a\u77e2\u91cf\\(\\mathbf{P}\\)\u7684\u534f\u53d8\u5206\u91cf\\(P_\\alpha = \\mathbf{P} \\cdot \\mathbf{g}_\\alpha\uff08\\alpha=1,2\uff09\\)</p>"},{"location":"math/TensorCalculus/begin/#\u4e09\u7ef4\u7a7a\u95f4\u7684\u659c\u89d2\u76f4\u7ebf\u5750\u6807\u7cfb","title":"\u4e09\u7ef4\u7a7a\u95f4\u7684\u659c\u89d2\u76f4\u7ebf\u5750\u6807\u7cfb","text":"<p>\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u659c\u89d2\u76f4\u7ebf\u5750\u6807\u7cfb\\(x^1,x^2,x^3\\)\uff0c\u5176\u5750\u6807\u7ebf\u4e92\u4e0d\u6b63\u4ea4\u3002\u5176\u7a7a\u95f4\u4e2d\u7684\u4efb\u610f\u4e00\u70b9\u53ef\u7528\u4ee5\u539f\u70b9\u4e3a\u8d77\u70b9\u7684\u77e2\u5f84\\(\\mathbf{r} = x^i \\mathbf{g}_i\uff08i=1,2,3\uff09\\)\u8868\u793a\u3002</p> <p>\u663e\u7136\uff0c\u4e09\u7ef4\u7a7a\u95f4\u7684\u534f\u53d8\u57fa\u77e2\u91cf\\(\\mathbf{g}_i = \\frac{\\partial \\mathbf{r}}{\\partial x^i}\\)\uff0c\u79f0\u4e3a\u81ea\u7136\u57fa\u77e2\u91cf\uff08\u5927\u5c0f\u4e3a\u6cbf\u7740\u5750\u6807\u7ebf\u7684\u5355\u4f4d\u957f\u5ea6\uff09\u3002</p> <p>\u5f53\\(\\mathbf{g}_1, \\mathbf{g}_2, \\mathbf{g}_3\\)\u6784\u6210\u53f3\u624b\u7cfb\u65f6\uff0c\u6df7\u5408\u79ef\u4e3a\u6b63\uff1a</p> \\[ [\\mathbf{g}_1 \\ \\mathbf{g}_2 \\ \\mathbf{g}_3] = \\delta_{ij} = \\sqrt{g} \\] <p>\u6839\u636e\u5bf9\u5076\u6761\u4ef6\uff1a$\\mathbf{g}^j \\cdot \\mathbf{g}i = \\delta^j_i (i,j=1,2,3) = G 1  0  0 \\ 0  1  0 \\ 0  0  1 \\ \\end{bmatrix} $},G_{3 \\times 3} = \\begin{bmatrix</p> <p></p> <p>\u6839\u636e\u51e0\u4f55\u5173\u7cfb\uff0c\u9006\u53d8\u57fa\u77e2\u91cf\\(\\mathbf{g}^i\\)\u4e0e\u5bf9\u5e94\u7684\u534f\u53d8\u57fa\u77e2\u91cf\\(\\mathbf{g}_i\\)\u7684\u5939\u89d2\u4e3a\\(\\phi\\)\uff0c\u5219\uff1a</p> \\[ |\\mathbf{g}^i| = \\frac{1}{|\\mathbf{g}_i| cos(\\phi)}  \\] <p>\u5bf9\u9006\u53d8\u57fa\u77e2\u91cf\u8fdb\u884c\u5206\u89e3\uff1a\\(\\mathbf{g}^i = \\sum_{j=1}^3 g^{ij} \\mathbf{g}_j  = g^{ij} \\mathbf{g}_j \\ (i = 1,2,3)\\)</p> <p>\u5bf9\u534f\u53d8\u57fa\u77e2\u91cf\u8fdb\u884c\u5206\u89e3\uff1a\\(\\mathbf{g}_i = g_{ij} \\mathbf{g}^j \\ (i = 1,2,3)\\)</p> <p>\u663e\u7136\uff0c\\(\\mathbf{g}_i \\cdot \\mathbf{g}_j = g_{ij} = g_{ji}\uff0c\\ \\mathbf{g}^i \\cdot \\mathbf{g}^j = g^{ij} = g^{ji}\\)</p> <p>\u7531\u7cfb\u6570\u6784\u6210\u7684\\(3 \\times 3\\)\u7684\u5bf9\u79f0\u77e9\u9635\\([g_{ij}]\uff0c[g^{ij}]\\)\u4e3a\u4e92\u9006\u77e9\u9635\uff1a</p> <p>$$ [g_{ij}]= \\begin{bmatrix} g_{11} &amp; g_{12} &amp; g_{13} \\ g_{21} &amp; g_{22} &amp; g_{23} \\ g_{31} &amp; g_{32} &amp; g_{33} \\ \\end{bmatrix} = \\begin{bmatrix} \\mathbf{g}_1 \\cdot \\mathbf{g}_1 &amp; \\mathbf{g}_1 \\cdot \\mathbf{g}_2 &amp; \\mathbf{g}_1 \\cdot \\mathbf{g}_3 \\ \\mathbf{g}_2 \\cdot \\mathbf{g}_1 &amp; \\mathbf{g}_2 \\cdot \\mathbf{g}_2 &amp; \\mathbf{g}_2 \\cdot \\mathbf{g}_3 \\ \\mathbf{g}_3 \\cdot \\mathbf{g}_1 &amp; \\mathbf{g}_3 \\cdot \\mathbf{g}_2 &amp; \\mathbf{g}_3 \\cdot \\mathbf{g}_3 \\ \\end{bmatrix}</p> <p>$$</p> <p>$$</p> <p>[g^{ij}] = \\begin{bmatrix} g^{11} &amp; g^{12} &amp; g^{13} \\ g^{21} &amp; g^{22} &amp; g^{23} \\ g^{31} &amp; g^{32} &amp; g^{33} \\ \\end{bmatrix} = \\begin{bmatrix} \\mathbf{g}^1 \\cdot \\mathbf{g}^1 &amp; \\mathbf{g}^1 \\cdot \\mathbf{g}^2 &amp; \\mathbf{g}^1 \\cdot \\mathbf{g}^3 \\ \\mathbf{g}^2 \\cdot \\mathbf{g}^1 &amp; \\mathbf{g}^2 \\cdot \\mathbf{g}^2 &amp; \\mathbf{g}^2 \\cdot \\mathbf{g}^3 \\ \\mathbf{g}^3 \\cdot \\mathbf{g}^1 &amp; \\mathbf{g}^3 \\cdot \\mathbf{g}^2 &amp; \\mathbf{g}^3 \\cdot \\mathbf{g}^3 \\ \\end{bmatrix} $$</p> \\[ \\begin{aligned} [g_{ij}][g^{ij}] =&amp; [g_{ik}g^{kj} ] \\ (i,j=1,2,3) \\\\ g_{ik}g^{kj} =&amp; g_{ik} \\mathbf{g}^k \\cdot \\mathbf{g}^j = \\mathbf{g}_i \\cdot \\mathbf{g}^j = \\delta_i^j \\\\ \\end{aligned} \\] <p>\u56e0\u6b64\\([g^{ij}] = [g_{ij}]^{-1}\\)</p> <p>\u79f0\\([g_{ij}]\\)\u4e3a\u5ea6\u91cf\u5f20\u91cf\u7684\u534f\u53d8\u5206\u91cf\uff0c\\([g^{ij}]\\)\u4e3a\u5ea6\u91cf\u5f20\u91cf\u7684\u9006\u53d8\u5206\u91cf\u3002</p> <p>\u663e\u7136\uff0c\\([g_{ij}] =  \\begin{bmatrix} \\mathbf{g}_1 \\\\ \\mathbf{g}_2 \\\\  \\mathbf{g}_3 \\\\ \\end{bmatrix}  \\cdot \\begin{bmatrix} \\mathbf{g}_1 \\ \\mathbf{g}_2 \\ \\mathbf{g}_3 \\end{bmatrix}\\)\uff0c\u5219\\(det([g_{ij}]) = [\\mathbf{g}_1 \\ \\mathbf{g}_2 \\ \\mathbf{g}_3]^2 = g\\)</p> <p>\\([\\mathbf{g}^1 \\ \\mathbf{g}^2 \\ \\mathbf{g}^3] = [\\mathbf{g}_1 \\ \\mathbf{g}_2 \\ \\mathbf{g}_3]^{-1} = \\frac{1}{\\sqrt{g}}\\)\uff0c\u56e0\u6b64\u82e5\u534f\u53d8\u91cf\u57fa\u77e2\u91cf\\(\\mathbf{g}_1,\\mathbf{g}_2,\\mathbf{g}_3\\)\u6784\u6210\u53f3\u624b\u7cfb\uff0c\\(g &gt; 0\\)\uff0c\u5219\u9006\u53d8\u57fa\u77e2\u91cf\\(\\mathbf{g}^1,\\mathbf{g}^2,\\mathbf{g}^3\\)\u4e5f\u6784\u6210\u53f3\u624b\u7cfb\u3002</p> <p>\u6307\u6807\u5347\u964d\u5173\u7cfb\uff1a\u5df2\u77e5\\(\\mathbf{P} = P^i \\mathbf{g}_i = P_j \\mathbf{g}^j \\(\uff0c\\)\\mathbf{P}\\)\u4e3a\u7a7a\u95f4\u4efb\u4e00\u77e2\u91cf\uff0c\u5219\u6709\uff1a</p> \\[ \\begin{aligned} P^i = \\mathbf{P} \\cdot \\mathbf{g}^i = P_k \\mathbf{g}^k \\cdot \\mathbf{g}^i = P_k g^{ki} \\\\ P_j = \\mathbf{P} \\cdot \\mathbf{g}_j = P^k \\mathbf{g}_k \\cdot \\mathbf{g}_j = P^k g_{kj} \\\\ \\end{aligned} \\] <p>\u5229\u7528\u534f\u53d8\u4e0e\u9006\u53d8\u5206\u91cf\uff0c\u53ef\u4ee5\u5c06\u5185\u79ef\u8fd0\u7b97\u5199\u6210\uff1a\\(\\mathbf{X} \\cdot \\mathbf{Y} = (x^i \\mathbf{g}_i) \\cdot (y_j \\mathbf{g}^j) = x^i y_i = x_i y^i = x^i y^i g_{ij} = g^{ij} x_i y_i\\)</p>"},{"location":"math/TensorCalculus/begin/#\u66f2\u7ebf\u5750\u6807\u7cfb","title":"\u66f2\u7ebf\u5750\u6807\u7cfb","text":"<p>\u8bbe\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u4efb\u610f\u70b9P\u7684\u4f4d\u7f6e\u7528\u56fa\u5b9a\u70b9O\u5230\u8be5\u70b9\u7684\u77e2\u5f84\\(\\mathbf{r}\\)\u8868\u793a\uff1a</p> \\[ \\mathbf{r} = \\mathbf{r} (x^1,x^2,x^3) \\] <p>\u501f\u52a9\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\\(x,y,z\\)\u4ee5\u53ca\u6807\u51c6\u6b63\u4ea4\u57fa\\(\\mathbf{i},\\mathbf{j},\\mathbf{k}\\)\u6765\u8868\u793a\uff1a</p> \\[ \\mathbf{r} = x(x^1,x^2,x^3) \\ \\mathbf{i} + y(x^1,x^2,x^3) \\ \\mathbf{j} + z(x^1,x^2,x^3) \\ \\mathbf{k}  \\] <ul> <li>\u8981\u6c42\uff1a\u66f2\u7ebf\u5750\u6807\\(x^i\\)\u4e0e\u7a7a\u95f4\u70b9\u4e00\u4e00\u5bf9\u5e94\uff0c\u5373\u51fd\u6570$ x(x<sup>1,x</sup>2,x<sup>3),y(x</sup>1,x<sup>2,x</sup>3),z(x<sup>1,x</sup>2,x<sup>3)\\(\u5728\\)x</sup>i$\u7684\u5b9a\u4e49\u57df\u5185\u5355\u503c\u3001\u8fde\u7eed\u5149\u6ed1\u4e14\u53ef\u9006:</li> </ul> \\[ det([\\frac{\\partial x^{k'}}{\\partial x^i}]) \\neq 0 \\quad (x^{k'} = x,y,z) \\] \\[ det([\\frac{\\partial x^i}{\\partial x^{k'}}]) \\neq 0  \\] <p>\u5373\u8be5\u66f2\u7ebf\u5750\u6807\u7cfb\u7684Jacobian\u884c\u5217\u5f0f\u4e0d\u4e3a\u96f6\u3002</p> <p>\u5047\u8bbe\u5750\u6807\u5b58\u5728\u5fae\u5c0f\u7684\u589e\u91cf\\(d \\mathbf{r} = \\frac{\\partial \\mathbf{r}}{\\partial x^i} dx^i = \\mathbf{g_i} dx_i\\)\uff0c\u5176\u4e2d\\(\\mathbf{g_i}= \\frac{\\partial \\mathbf{r}}{\\partial x^i} \\ (i=1,2,3)\\)\u4e3a\u534f\u53d8\u57fa\u77e2\u91cf\u6216\u81ea\u7136\u5c40\u90e8\u57fa\u77e2\u91cf\uff0c</p> <p>\\(\\mathbf{g_i} \\ (i=1,2,3)\\)\u6cbf\u7740P\u70b9\u76843\u6839\u5750\u6807\u7ebf\u7684\u5207\u7ebf\u5e76\u6307\u5411\\(x^i\\)\u589e\u52a0\u7684\u65b9\u5411\uff0c\u5f62\u6210\u4e86\u4e00\u7ec43\u4e2a\u975e\u5171\u9762\u7684\u6d3b\u52a8\u6807\u67b6\uff0c\u79f0\u4e3a\u7a7a\u95f4\u67d0\u70b9\u5173\u4e8e\u66f2\u7ebf\u7cfb\\((x^1,x^2,x^3)\\)\u7684\u5207\u6807\u67b6\u3002\u663e\u7136\uff0c\u57fa\u77e2\u91cf\\(\\mathbf{g_i}\\)\u4e0d\u662f\u5e38\u77e2\u91cf\uff0c\u968f\u7740\u7a7a\u95f4\u70b9\u4f4d\u7f6e\u7684\u53d8\u5316\u800c\u53d8\u5316\uff0c\u56e0\u6b64\u662f\u4e00\u79cd\u5c40\u90e8\u57fa\u77e2\u91cf\u3002</p> \\[ \\mathbf{g_i} = \\frac{\\partial x}{\\partial x^i} \\mathbf{i}+\\frac{\\partial y}{\\partial x^i} \\mathbf{j}+\\frac{\\partial z}{\\partial x^i} \\mathbf{k} \\ (i=1,2,3) \\] <p>\u6839\u636e\u5bf9\u5076\u6761\u4ef6\uff0c\u5f15\u5165\u4e00\u7ec4\u4e0e\\(\\mathbf{g_i}\\)\u5bf9\u5e94\u7684\u9006\u53d8\u57fa\u77e2\u91cf\\(\\mathbf{g^j} \\ (j=1,2,3)\\)\uff0c\u540c\u6837\u662f\u7a7a\u95f4\u6bcf\u4e00\u70b9\u7684\u5c40\u90e8\u6807\u67b6\u3002\u663e\u7136\uff0c\u9006\u53d8\u57fa\u77e2\u91cf\u662f\u5750\u6807\u9762\u7684\u5728\u67d0\u70b9\u7684\u68af\u5ea6\uff1a</p> \\[ \\mathbf{g^j} = \\nabla x^j = grad \\ x^j = \\frac{\\partial x^j}{\\partial x} \\mathbf{i}+\\frac{\\partial y^j}{\\partial y} \\mathbf{j}+\\frac{\\partial z^j}{\\partial z} \\mathbf{k} \\ (j=1,2,3) \\] <p>\u56e0\u6b64\u7ebf\u5143\\(d \\mathbf{r}\\)\u7684\u5206\u89e3\uff1a</p> \\[ d \\mathbf{r} = \\mathbf{g_i} dx^i = \\mathbf{g^j} dx_j \\] <p>\u6307\u6807\u5347\u964d\u5173\u7cfb\uff1a\\(dx_i = g_{ij} dx^j\\)\u3002</p>"},{"location":"math/TensorCalculus/begin/#\u6b63\u4ea4\u66f2\u7ebf\u5750\u6807\u7cfb","title":"\u6b63\u4ea4\u66f2\u7ebf\u5750\u6807\u7cfb","text":"<p>\u82e5\u66f2\u7ebf\u5750\u6807\u7cfb\\((x^1,x^2,x^3)\\)\u7684\u5750\u6807\u7ebf\u5904\u5904\u6b63\u4ea4\uff0c\u5219\u4e3a\u6b63\u4ea4\u66f2\u7ebf\u5750\u6807\u7cfb</p> <p>\u6b63\u4ea4\u7cfb\u6ee1\u8db3\uff1a\\(g_{ij}=g^{ij}=0\uff0ci \\neq j\uff0ci,j=1,2,3\\)\uff0c\u663e\u7136\u5ea6\u91cf\u5f20\u91cf\u77e9\u9635\\([g_{ij}],[g^{ij}]\\)\u4e3a\u5bf9\u89d2\u77e9\u9635\uff0c\u4e14\\(\\mathbf{g_i}\\)\u4e0e\\(\\mathbf{g^i}\\)\u5171\u7ebf\u3002</p> <p>\u7ebf\u5143\\(d \\mathbf{r}\\)\u7684\u957f\u5ea6\u5e73\u65b9\\(ds^2 = d \\mathbf{r} \\cdot d \\mathbf{r} = g_{11} (dx^1)^2 + g_{22} (dx^2)^2 + g_{33} (dx^3)^2 \\(\u3002\u5b9a\u4e49Lam\u00e9\u5e38\u6570\\)A_i (i=1,2,3) = \\sqrt{g_{ii}}\\)\u3002</p> <p>\u56e0\u6b64\\(ds^2  = (A_1 dx^1)^2 + (A_2 dx^2)^2 + (A_3 dx^3)^2\\)</p> <p>\u4e3a\u4e86\u6c42\u89e3\u6b63\u4ea4\u7cfb\u4e2d\u7684\u5ea6\u91cf\u5f20\u91cf\uff0c\u5148\u7531\u51e0\u4f55\u65b9\u6cd5\u5f97\u51fa\u77e2\u5f84\u7684\u5fae\u5206\u4e0e\u5750\u6807\u7684\u5fae\u5206\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u786e\u5b9a\\(A_i\\)\uff0c\u518d\u56e0\u6b64\u5f97\u5230\u5ea6\u91cf\u5f20\u91cf\u7684\u534f\u53d8\u5206\u91cf\\(g_{ij}\\)\uff0c\u8fdb\u800c\u4e5f\u5f97\u5230\u9006\u53d8\u5206\u91cf\\(g^{ij}\\)\u3002</p>"},{"location":"math/TensorCalculus/begin/#\u5750\u6807\u8f6c\u6362","title":"\u5750\u6807\u8f6c\u6362","text":"<p>\u5047\u8bbe\u6709\u4e00\u7ec4\u8001\u5750\u6807\u7cfb\\(x^i\\)\u548c\u65b0\u5750\u6807\u7cfb\\(x^{j'}\\)\uff0c\u4e4b\u95f4\u5b58\u5728\\(x^i(x^{j'})\\)\u6216\\(x^{j'}(x^i)\\)\u7684\u51fd\u6570\u5173\u7cfb\uff08\u6ee1\u8db3Jacobian\u884c\u5217\u5f0f\u4e0d\u4e3a\u96f6\uff09\u3002</p> <p>\u5c06\u65b0\u5750\u6807\u7684\u57fa\u77e2\u91cf\u5bf9\u8001\u5750\u6807\u7684\u57fa\u77e2\u91cf\u5206\u89e3\uff1a</p> \\[ \\mathbf{g}_{i'} = \\beta^j_{i'} \\mathbf{g}_j , \\quad \\mathbf{g}^{i'} = \\beta_j^{i'} \\mathbf{g}^j \\ (i'=1,2,3) \\] <p>\u5176\u4e2d\uff0c\\(\\beta^j_{i'}, \\ \\beta_j^{i'}\\)\u5206\u522b\u4e3a\u534f\u53d8\u8f6c\u6362\u7cfb\u6570\u548c\u9006\u53d8\u8f6c\u6362\u7cfb\u6570\uff0c\u5f62\u6210\\(3 \\times 3\\)\u77e9\u9635\u3002\u663e\u7136\uff0c\\([\\beta^j_{i'}]\\)\u4e0e\\([\\beta_j^{i'}]\\)\u4e92\u9006\u3002</p> \\[ \\delta^{j'}_{i'} = \\mathbf{g}_{i'} \\cdot \\mathbf{g}^{j'} = \\beta^k_{i'} \\mathbf{g}_{k}  \\cdot \\beta^{j'}_{l} \\mathbf{g}^{l} = \\beta^k_{i'} \\beta^{j'}_{l} \\delta^l_k = \\beta^k_{i'} \\beta^{j'}_{k} \\quad (i',j'=1,2,3) \\] <p>\u56e0\u6b64\u8001\u5750\u6807\u7684\u57fa\u77e2\u91cf\u5bf9\u65b0\u5750\u6807\u7684\u57fa\u77e2\u91cf\u5206\u89e3\uff1a</p> \\[ \\mathbf{g}_{j} = \\beta_j^{i'} \\mathbf{g}_{i'} , \\quad \\mathbf{g}^{j} = \\beta^j_{i'} \\mathbf{g}^{i'} \\ (j=1,2,3) \\] <p>\u65b0\u8001\u5750\u6807\u7684\u534f\u53d8\u57fa\u4e0e\u9006\u53d8\u57fa\u77e2\u91cf\u4e4b\u95f4\u7684\u8f6c\u6362\u53ea\u9700\u89819\u4e2a\u72ec\u7acb\u7684\u7cfb\u6570</p> <p>\u4e0a\u8ff0\u63d0\u5230\\(\\mathbf{g}_{i'} = \\frac{\\partial \\mathbf{r}}{\\partial x^{i'}} = \\frac{\\partial \\mathbf{r}}{\\partial x^{j}} \\frac{\\partial \\mathbf{x^{j}}}{\\partial x^{i'}} = \\frac{\\partial \\mathbf{x^{j}}}{\\partial x^{i'}} \\mathbf{g}_{j}\\) \\quad (i'=1,2,3)\u3002\u56e0\u6b64\u534f\u53d8\u8f6c\u6362\u7cfb\u6570:</p> \\[ \\beta^j_{i'} = \\frac{\\partial x^{j}}{\\partial x^{i'}} \\quad (i',j=1,2,3) \\] <p>\u9006\u53d8\u8f6c\u6362\u7cfb\u6570\uff1a</p> \\[ \\beta_j^{i'} = \\frac{\\partial x^{i'}}{\\partial x^{j}} \\quad (i',j=1,2,3) \\] <p>\u56e0\u6b64,\\(\\ \\mathbf{g}^{i'} = \\frac{\\partial \\mathbf{x^{i'}}}{\\partial x^{j}} \\mathbf{g}^{j} \\quad (i'=1,2,3)\\)</p> <p>\u5bf9\u7a7a\u95f4\u4e2d\u7684\u4efb\u610f\u77e2\u91cf\\(\\mathbf{v}\\)\uff0c\u5176\u5728\u65b0\u8001\u5750\u6807\u7cfb\u7684\u5206\u89e3\uff1a</p> \\[ \\begin{aligned} \\mathbf{v} &amp;= v_j \\mathbf{g}^j = v_{i'} \\mathbf{g}^{i'} \\\\ &amp;= v^j \\mathbf{g}_j = v^{i'} \\mathbf{g}_{i'} \\\\ \\end{aligned} \\] <p>\u7531\u4e0a\u5f0f\u663e\u7136\u53ef\u5f97\uff0c\\(v_{i'} = v_j \\beta^j_{i'} \\quad v^{i'} = v^j \\beta_j^{i'}\\)</p> <p>\u5ea6\u91cf\u5f20\u91cf\u5206\u91cf\u5728\u65b0\u8001\u5750\u6807\u7cfb\u4e2d\u7684\u8f6c\u6362\uff1a</p> \\[ \\begin{aligned} g_{i'j'} =&amp; \\mathbf{g}_{i'} \\cdot \\mathbf{g}_{j'} = \\beta^k_{i'} \\mathbf{g}_{k} \\cdot \\beta^l_{j'} \\mathbf{g}_{l} = \\beta^k_{i'} \\beta^l_{j'} g_{kl} \\quad (i',j'=1,2,3) \\\\ g_{ij} =&amp; \\beta^{k'}_{i} \\beta^{l'}_{j} g_{kl} \\quad (i,j=1,2,3) \\\\ g^{ij} =&amp; \\beta_{k'}^{i} \\beta_{l'}^{j} g^{k'l'} \\quad (i,j=1,2,3) \\\\ g^{i'j'} =&amp; \\beta_{k}^{i'} \\beta_{l}^{j'} g^{kl} \\quad (i',j'=1,2,3) \\\\ \\end{aligned} \\] <p>\u5047\u8bbe\u4efb\u610f\u66f2\u7ebf\u5750\u6807\u7cfb\\((x^1,x^2,x^3)\\)\u4e0e\u7b1b\u5361\u5c14\u6807\u51c6\u6b63\u4ea4\u5750\u6807\u7cfb\\((x^{1'}=x,y^{1'}=y,z^{1'}=z)\\)\u5b58\u5728\u8f6c\u6362\u5173\u7cfb\\(x^i(x,y,z)\\)\uff0c\u5750\u6807\u8f6c\u6362\u7cfb\u6570\\(\\beta^{k'}_i=\\partial x^{k'} / \\partial x^i\\)\uff0c\u5219\uff1a</p> \\[ \\begin{aligned} g_{11} =&amp; \\beta^{k'}_1 \\beta^{l'}_1 g_{k'l'} = \\beta^{1'}_1 \\beta^{1'}_1 g_{1'1'} + \\beta^{2'}_1 \\beta^{2'}_1 g_{2'2'} + \\beta^{3'}_1 \\beta^{3'}_1 g_{3'3'} = (\\frac{\\partial x}{\\partial x^{1}})^2 + (\\frac{\\partial y}{\\partial x^{1}})^2 + (\\frac{\\partial z}{\\partial x^{1}})^2 \\\\ g_{12} =&amp; g_{21} = \\beta^{k'}_1 \\beta^{l'}_2 g_{k'l'} =\\beta^{1'}_1 \\beta^{1'}_2 g_{1'1'} + \\beta^{2'}_1 \\beta^{2'}_2 g_{2'2'} + \\beta^{32'}_1 \\beta^{3'}_2 g_{3'3'} = (\\frac{\\partial x}{\\partial x^{1}})( \\frac{\\partial x}{\\partial x^{2}}) + (\\frac{\\partial y}{\\partial x^{1}}) (\\frac{\\partial y}{\\partial x^{2}}) + (\\frac{\\partial z}{\\partial x^{1}}) (\\frac{\\partial z}{\\partial x^{2}}) \\\\ ..... \\end{aligned} \\]"},{"location":"math/TensorCalculus/begin/#\u5e76\u77e2","title":"\u5e76\u77e2","text":"<p>\\(\\mathbf{a} \\otimes \\mathbf{b}\\)\u6216\\(\\mathbf{a}\\mathbf{b}\\)\u79f0\u4e3a\u5e76\u77e2\uff0c\u4e5f\u6210\u4e3a\u5f20\u91cf\u79ef</p> <p>\u8bbe\u77e2\u91cf\\(\\mathbf{a}=(a_1,a_2,a_3)^T, \\mathbf{b}=(b_1,b_2,b_3)^T\\)\u6784\u6210\\(3\\times 1\\)\u7684\u77e9\u9635\uff0c\u5219\u5176\u5e76\u77e2</p> \\[ \\mathbf{a} \\otimes \\mathbf{b}=  \\begin{bmatrix} a_1b_1 &amp; a_1b_2 &amp; a_1b_3 \\\\ a_2b_1 &amp; a_2b_2 &amp; a_2b_3 \\\\ a_3b_1 &amp; a_3b_2 &amp; a_3b_3 \\\\ \\end{bmatrix} =  \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\end{bmatrix}  \\begin{bmatrix} b_1 &amp; b_2 &amp; b_3 \\\\ \\end{bmatrix}  \\] <p>\u663e\u7136\uff0c\\(\\mathbf{a} \\otimes \\mathbf{b} \\neq \\mathbf{b} \\otimes \\mathbf{a}\\)\uff0c\u5e76\u77e2\u4e2d\u7684\u77e2\u91cf\u987a\u5e8f\u4e0d\u80fd\u968f\u610f\u8c03\u6362\u3002</p> \\[ (\\mathbf{a} \\otimes \\mathbf{b})^T = \\mathbf{b} \\otimes \\mathbf{a} \\] <p>\u5e76\u77e2\u7684\u70b9\u79ef\uff1a\u6307\u7684\u662f\u5c06\u76f8\u90bb\u7684\u4e24\u4e2a\u77e2\u91cf\u8fdb\u884c\u7f29\u5e76</p> \\[ \\begin{aligned} \\mathbf{u} \\cdot (\\mathbf{a} \\mathbf{b}) = (\\mathbf{u} \\cdot \\mathbf{a}) \\mathbf{b} \\\\ (\\mathbf{a} \\mathbf{b}) \\cdot \\mathbf{u} = (\\mathbf{b} \\cdot \\mathbf{u})\\mathbf{a}  \\\\ (\\mathbf{a} \\mathbf{b}) \\cdot (\\mathbf{c} \\mathbf{d} ) = (\\mathbf{b} \\cdot \\mathbf{c} ) \\mathbf{a} \\mathbf{d} \\\\  \\end{aligned} \\] <p>\u5e76\u77e2\u7684\u7f29\u5e76\uff1a\u82e5\u53d6\u67d0\u4e24\u4e2a\u77e2\u91cf\u8fdb\u884c\u70b9\u79ef\uff0c\u79f0\u4e3a\u7f29\u5e76\u3002\uff08\u4e00\u6b21\u7f29\u5e76\u4f7f\u5f97\u5e76\u77e2\u7684\u9636\u6570\u964d\u4f4e\u4e24\u9636\uff09</p> <p>\u5982\u8bbe\u56db\u9636\u5e76\u77e2\\(\\mathbf{abcd}\\),\u8fdb\u884c\u7f29\u5e76,\u964d\u4e3a\u4e8c\u9636\u5e76\u77e2\uff1a</p> \\[ \\mathbf{a} \\cdot \\mathbf{bcd} = (\\mathbf{a} \\cdot \\mathbf{b}) \\mathbf{cd}  \\] \\[ \\mathbf{abc} \\cdot \\mathbf{d}= (\\mathbf{c} \\cdot \\mathbf{d}) \\mathbf{ab}  \\] <p>\u53cc\u70b9\u79ef\u6307\u7684\u662f\u4e24\u4e2a\u5f20\u91cf\u5143\u7d20\u4e4b\u95f4\u7684\u4e58\u79ef\uff0c\u5bf9\u4e8e\u76f8\u540c\u5f62\u72b6\u7684\u77e9\u9635\\(A,B\\)\uff0c\u53cc\u70b9\u79ef\u8bb0\u4e3a\\(A:B = \\sum_{i.j} A_{ij}B_{ij}\\)</p> <p>\u4e24\u4e2a\u5e76\u77e2\u7684\u53cc\u70b9\u79ef\uff1a\u6307\u7684\u662f\u5c06\u5b83\u4eec\u6700\u90bb\u8fd1\u76844\u4e2a\u77e2\u91cf\u4e24\u4e24\u7f29\u5e76</p> <ul> <li>\u5e76\u8054\u5f0f\uff1a\\(\\mathbf{a} \\mathbf{b} : \\mathbf{c} \\mathbf{d} = (\\mathbf{a} \\cdot \\mathbf{c})( \\mathbf{b} \\cdot \\mathbf{d})\\)</li> <li>\u4e32\u8054\u5f0f\uff1a\\(\\mathbf{a} \\mathbf{b} \\cdot \\cdot \\ \\mathbf{c} \\mathbf{d} = (\\mathbf{b} \\cdot \\mathbf{c})( \\mathbf{a} \\cdot \\mathbf{d})\\)</li> </ul> <p>\u5bf9\u4e00\u4e2a\u4e8c\u9636\u5e76\u77e2\\(T=AB\\)\uff0c\u5f15\u5165\u4e24\u7ec4\u7ebf\u6027\u65e0\u5173\u7684\u57fa\u77e2\u91cf\\(\\mathbf{(a_1,a_2,a_3), (b_1,b_2,b_3)}\\)\uff0c\u5219:</p> \\[ \\mathbf{T} = (A^i \\mathbf{a}_i)(B^j \\mathbf{b}_j) = A^i B^j \\mathbf{a}_i \\mathbf{b}_j = T^{ij} \\mathbf{a}_i \\mathbf{b}_j \\] <p>\u6613\u5f97\\(T^{ij} = A^i B^j \\quad (i,j=1,2,3)\\)\u51719\u4e2a\u5206\u91cf\uff0c\u5e76\u4e14\\(\\mathbf{a}_i \\mathbf{b}_j\\)\u662f9\u4e2a\u7ebf\u6027\u65e0\u5173\u7684\u4e8c\u9636\u5e76\u57fa\u3002</p> <p>\u56e0\u6b64\uff0c\u5bf9\u4e8e\u4e09\u9636\u5e76\u77e2\\(\\mathbf{T = ABC} = T^{ijk} \\mathbf{a}_i \\mathbf{b}_j \\mathbf{c}_k\\)\uff0c\u5b58\u5728\\(3^3=27\\)\u4e2a\u5e76\u57fa\u548c\u5bf9\u5e94\u7684\u5206\u91cf\u3002</p>"},{"location":"math/TensorCalculus/begin/#\u5f20\u91cf\u7684\u6982\u5ff5","title":"\u5f20\u91cf\u7684\u6982\u5ff5","text":"<p>\u5f20\u91cf</p>"},{"location":"math/TensorNetwork/","title":"\u5f20\u91cf\u7f51\u7edc: Tensor Network","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599:</p> <p></p>"},{"location":"math/Time-serise/time-serise-chapter1/","title":"Time Serise learning","text":"<p>Override\ud83c\udf87</p> <p> <p>\u8bfe\u672c\uff1a\u300a\u65f6\u95f4\u5e8f\u5217\u4e0e\u673a\u5668\u5b66\u4e60\u300b\uff08\u5f20\u620e\u3001\u7f57\u9f50\uff09\u3001\u300a\u5e94\u7528\u65f6\u95f4\u5e8f\u5217\u300b\uff08\u4f55\u4e66\u5143\uff09</p> <p></p> <p>\u65f6\u95f4\u5e8f\u5217</p> <p> <p>\u6309\u7167\u65f6\u95f4\u6b21\u5e8f\u6392\u5217\u7684\u968f\u673a\u53d8\u91cf\u5e8f\u5217\u79f0\u4e3a\u65f6\u95f4\u5e8f\u5217\uff1a</p> \\[     X_1, X_2,...... \\] <p>\u82e5\\(x_1,x_2,...x_N\\)\u8868\u793a\u968f\u673a\u53d8\u91cf\u7684\u89c2\u6d4b\u503c\uff0c\u5219\u79f0\u4e4b\u4e3a\u65f6\u95f4\u5e8f\u5217(1, 1)\u7684N\u4e2a\u89c2\u6d4b\u6837\u672c\u3002</p> <p>\u82e5\u5b58\u5728\u4e24\u4e2a\u65f6\u95f4\u5e8f\u5217\\({X_t}\\)\u4e0e\\({Y_t}\\)\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u5173\u7cfb\uff0c\u5219\u53ef\u8868\u793a\u4e3a\uff1a</p> \\[     \\xi_t = (X_t,Y_t)^T, t=1,2,... \\] <p>\u8fd9\u4e2a\u5411\u91cf\u503c\u7684\u65f6\u95f4\u5e8f\u5217\u79f0\u4e3a\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217 </p>"},{"location":"math/Time-serise/time-serise-chapter1/#\u65f6\u95f4\u5e8f\u5217\u7684\u5206\u89e3","title":"\u65f6\u95f4\u5e8f\u5217\u7684\u5206\u89e3","text":"<p>\u5927\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\u7684\u89c2\u6d4b\u6837\u672c\u4f53\u73b0\u51fa:</p> <ul> <li> <p>\u8d8b\u52bf\u6027\\({T_t}\\)</p> </li> <li> <p>\u5b63\u8282\u6027\\({S_t}\\),\u8981\u6c42\\(\\sum_{j=1}^s S(t+j)=0, t=1,2,...\\)</p> </li> <li> <p>\u968f\u673a\u6027\\({R_t}\\)\uff0c\u8981\u6c42\\(E(R_t)=0, t=1,2,...\\)</p> </li> </ul> <p>\u56e0\u6b64\uff0c\u65f6\u95f4\u5e8f\u5217\u7684\u5206\u89e3\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> \\[     X_t = T_t + S_t + R_t, t=1,2,... \\] <p>\u5c06\u65f6\u95f4\u5e8f\u5217\u7684\u8d8b\u52bf\u9879\u3001\u5b63\u8282\u9879\u548c\u968f\u673a\u9879\u5206\u89e3\u51fa\u6765\u7684\u5de5\u4f5c\u79f0\u4e4b\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u89e3</p>"},{"location":"python/Python-is-important/","title":"\ud83d\udd17Python\u6821\u5185\u8bfe\u7a0b\u7b14\u8bb0","text":"<p>\u60f3\u8bf4\u7684\u8bdd</p> <p>\u4eba\u751f\u82e6\u77ed,\u6211\u5b66Pyhton.</p> <p> <p>\u6211\u7684\u57f9\u517b\u65b9\u6848\u4e2dPython\u7a0b\u5e8f\u8bbe\u8ba1\u662f\u5fc5\u4fee\u7684\u4e00\u95e8\u8bfe,\u672c\u9875\u7ed3\u5408\u8bfe\u5185\u8981\u6c42\u548cCS50p\u505a\u4e00\u4e2a\u6bd4\u8f83\u5b8c\u5584\u7684\u4e2a\u4eba\u7b14\u8bb0,\u7528\u4e8e\u5e94\u4ed8\u8bfe\u7a0b\u8003\u8bd5</p> <p>\u5b98\u65b9\u6587\u6863:https://docs.python.org/3/library/functions.html </p>"},{"location":"python/Python-is-important/#\u4e00\u4e9b\u96f6\u788e\u77e5\u8bc6\u70b9","title":"\ud83c\udfa8\u4e00\u4e9b\u96f6\u788e\u77e5\u8bc6\u70b9","text":""},{"location":"python/Python-is-important/#\u8bed\u53e5\u683c\u5f0f","title":"\u8bed\u53e5\u683c\u5f0f","text":"<pre><code>a=1;b=2;c=3         # \u7528\u5206\u53f7\u5728\u4e00\u884c\u4e2d\u8f93\u5165\u591a\u4e2a\u8bed\u53e5\n\nsum = a + b \\       # \u4f7f\u7528'\\'\u6765\u7eed\u884c\n    + c\n</code></pre>"},{"location":"python/Python-is-important/#\u79d1\u5b66\u8ba1\u6570\u6cd5e","title":"\u79d1\u5b66\u8ba1\u6570\u6cd5e","text":"<p>\u5bf9\u4e8e\u5f88\u5927\u7684\u6d6e\u70b9\u6570,\u53ef\u7528e\u4ee3\u66ff10,\u5176\u4e2de\u540e\u9762\u7684\u6570\u5b57\u5fc5\u987b\u4e3a\u6574\u6570 <pre><code>&gt;&gt;&gt;1.32e-1\n0.132\n&gt;&gt;&gt;1.32e0\n1.32\n</code></pre></p> <p>\u4e0d\u662f\u6240\u6709\u5b9e\u6570\u5728\u8ba1\u7b97\u673a\u4e2d\u90fd\u53ef\u4ee5\u7cbe\u786e\u8868\u8fbe,\u6240\u4ee5\u4f1a\u5b58\u5728\u8bef\u5dee</p>"},{"location":"python/Python-is-important/#\u590d\u6570-real--imag--j","title":"\u590d\u6570 real + imag * j","text":"<pre><code>&gt;&gt;&gt;type(3+4J)         # j/J\u865a\u90e8\u5b57\u6bcd\u5927\u5c0f\u5199\u5747\u53ef\ncomplex               # \u590d\u6570\u7684\u6570\u636e\u7c7b\u578b\n&gt;&gt;&gt;a = complex(3,4)\n&gt;&gt;&gt;a.real, a.imag     # \u590d\u6570\u7684\u5b9e\u90e8\u3001\u865a\u90e8\u4e3a\u6d6e\u70b9\u6570(float)\n(3.0,4.0)\n&gt;&gt;&gt;a.conjugate()\n3-4J\n</code></pre>"},{"location":"python/Python-is-important/#\u8fdb\u5236\u8f6c\u6362","title":"\u8fdb\u5236\u8f6c\u6362","text":"<ul> <li>0b,0B \u4e8c\u8fdb\u5236</li> <li>0x,0X \u5341\u516d\u8fdb\u5236</li> <li>0o,0O \u516b\u8fdb\u5236</li> </ul> <pre><code>&gt;&gt;&gt;list(bin(3),oct(10),hex(15))       # \u53ea\u80fd\u8f6c\u6362\u6574\u6570!\n('0b11', '0o12', '0xf')\n&gt;&gt;&gt;type(bin(3))      # \u8f93\u51fa\u7684\u662f\u5b57\u7b26\u4e32\nstr\n</code></pre> <p>int()\u8f6c\u6362\u8fdb\u5236(\u8f93\u51fa\u5341\u8fdb\u5236)</p> <pre><code>&gt;&gt;&gt;int('101',2)\n5\n&gt;&gt;&gt;int('0x1d',16)\n29\n&gt;&gt;&gt;int('0b101',2)\n5\n</code></pre>"},{"location":"python/Python-is-important/#evalstr\u51fd\u6570","title":"eval('str')\u51fd\u6570","text":"<p>\u5c06\u5b57\u7b26\u4e32\u5f53\u6210\u6709\u6548\u7684\u8868\u8fbe\u5f0f\u6765\u6c42\u503c\uff0c\u5e76\u8fd4\u56de\u8ba1\u7b97\u7ed3\u679c</p> <p>eval()\u51fd\u6570\u4f1a\u628a\u8f93\u5165\u7684\u5b57\u7b26\u4e32\u7684\u5f15\u53f7\u53bb\u6389\uff0c\u628a\u4e2d\u95f4\u7684\u5185\u5bb9\u5f53\u6210Python\u7684\u4ee3\u7801\uff0c\u5e76\u6267\u884c\u8fd9\u6bb5\u4ee3\u7801\u8fd4\u56de\u6267\u884c\u7ed3\u679c <pre><code>&gt;&gt;&gt;eval(\"254+1\")\n255\n&gt;&gt;eval(\"'+' * 5\")\n+++++\n&gt;&gt;&gt;eval('[1,2,3,4,5]')\n[1, 2, 3, 4, 5]\n</code></pre></p>"},{"location":"python/Python-is-important/#\u7a7a\u503cnone","title":"\u7a7a\u503cNone","text":"<p>\u4e0d\u540c\u4e8e0\u3001False\u6216\u7a7a\u5b57\u7b26\u4e32\uff0cNone\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u6570\u636e\u7c7b\u578b\uff08NoneType\uff09\u3002\u53ea\u6709None\u53ef\u4ee5\u7b49\u4e8eNone\u3002 <pre><code>&gt;&gt;&gt;type(None)\nNoneType\n&gt;&gt;&gt;None == 0 or [None] == [] or '' == 'None' # None\u8868\u793a\u7a7a\u503c\u4f46\u4e0d\u7b49\u4e8e\u7a7a\nFalse\n&gt;&gt;&gt;1 &gt; None        # None \u4e0d\u80fd\u4f5c\u5927\u5c0f\u6bd4\u8f83\nTypeError: '&gt;' not supported between instances of 'int' and 'NoneType'\n</code></pre></p>"},{"location":"python/Python-is-important/#\u5b57\u7b26\u4e32\u65e0\u6cd5\u4fee\u6539","title":"\u5b57\u7b26\u4e32\u65e0\u6cd5\u4fee\u6539","text":"<pre><code>&gt;&gt;&gt;s = 'abc'\n&gt;&gt;&gt;s[0] = 'k'\nTypeError: 'str' object does not support item assignment\n&gt;&gt;&gt;s[4] = 'u'            #\u6dfb\u52a0\u4e5f\u4e0d\u884c\nTypeError: 'str' object does not support item assignment\n</code></pre>"},{"location":"python/Python-is-important/#ascii\u7801-unicode\u7f16\u7801-utf-8\u7f16\u7801","title":"\ud83d\udce0Ascii\u7801-Unicode\u7f16\u7801-UTF-8\u7f16\u7801","text":"<p>pta\u548c\u8bfe\u672c\u4e0a\u6709\u76f8\u5173\u9898\u76ee,\u56e0\u6b64\u505a\u7b80\u5355\u7684\u6982\u5ff5\u533a\u5206</p> <ul> <li> <p>\"Ascii\u7801\":\u7528\u4e8c\u8fdb\u5236\u7f16\u7801\u6765\u8868\u793a\u975e\u6570\u503c\u7684\u6587\u5b57\u4e0e\u7b26\u53f7(\u6570\u5b57\u5316),\u5982\u4eca\u666e\u904d\u91c7\u7528\u7684\u5c31\u662fASCII(\u7f8e\u56fd\u4fe1\u606f\u4ea4\u6362\u6807\u51c6\u4ee3\u7801),\u67097\u4f4d\u4e8c\u8fdb\u5236\u4e0e\u516b\u4f4d\u4e8c\u8fdb\u5236\u4e24\u79cd\u7248\u672c(\u56fd\u9645\u901a\u75287\u4f4d,\u6709\\(2^7 = 128\\)\u4e2a\u5143\u7d20),\u4e0e Unicode \u7f16\u7801\u548c UTF-8 \u7f16\u7801\u517c\u5bb9</p> <p>\u5b57\u7b26 A:65,a:97,Z:90,z:122</p> </li> <li> <p>\"Unicode\u7f16\u7801\":\u5305\u542b\u4e86\u4e16\u754c\u4e0a\u51e0\u4e4e\u6240\u6709\u7684\u5b57\u7b26\uff0c\u5305\u62ec\u5404\u79cd\u8bed\u8a00\u7684\u6587\u5b57\u3001\u7b26\u53f7\u3001\u8868\u60c5\u7b26\u53f7\u7b49\u3002\u5b83\u662f\u76ee\u524d\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u5b57\u7b26\u7f16\u7801\u65b9\u6848\uff0c\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u4e4b\u95f4\u7684\u4ea4\u6d41\u9700\u6c42\u3002Python3\u4e2d\u7684\u5b57\u7b26\u4e32\u662fUnicode\u5b57\u7b26\u4e32\u800c\u4e0d\u662f\u5b57\u8282\u6570\u7ec4.</p> </li> <li> <p>\"UTF-8\":\u662f Unicode \u7f16\u7801\u7684\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0f,\u4f7f\u75281\u52304\u4e2a\u5b57\u8282\u6765\u8868\u793a\u4e0d\u540c\u7684\u5b57\u7b26,\u7279\u70b9\u662f\u5bf9\u4e0d\u540c\u8303\u56f4\u7684\u5b57\u7b26\u4f7f\u7528\u4e0d\u540c\u957f\u5ea6\u7684\u7f16\u7801(\u4e0d\u7b49\u957f).\u5bf9\u4e8e0x00-0x7f\u4e4b\u95f4\u7684\u5b57\u7b26,UTF8\u4e0eascii\u7f16\u7801\u5b8c\u5168\u4e00\u81f4.</p> </li> </ul>"},{"location":"python/Python-is-important/#ord\u4e0echr","title":"ord()\u4e0echr()","text":"<p>\u7528\u4e8e\u5c06\u5b57\u7b26\u8f6c\u6362\u4e3a\u6574\u6570(ord())\uff0c\u4ee5\u53ca\u5c06\u6574\u6570\u8f6c\u6362\u4e3a\u5b57\u7b26(chr()) <pre><code># \u5c06\u5b57\u7b26\u8f6c\u6362\u4e3aUnicode\u4ee3\u7801\u70b9\u503c\n&gt;&gt;&gt;ord('a')\n97\n# \u5c06\u6574\u6570\u8f6c\u6362\u4e3a\u5b57\u7b26\n&gt;&gt;&gt;chr(65)\nA\n&gt;&gt;&gt;ord('ab')\nTypeError: ord() expected a character, but string of length 2 found\n&gt;&gt;&gt;ord('a'and'b')\n98\n&gt;&gt;&gt;ord('\u6211')    # \u8f93\u51fa\u5bf9\u5e94\u7684Unicode\u7801\n25105\n&gt;&gt;&gt;chr(20014)\n'\u4e30'\n</code></pre></p>"},{"location":"python/Python-is-important/#int--float","title":"int--float","text":"<pre><code>&gt;&gt;&gt;32.2//6-24//6\n1.0                 # 32.2\u4e3afloat \u8f93\u51fa\u6d6e\u70b9\u578b1.0\n</code></pre>"},{"location":"python/Python-is-important/#printobjects-sep--endn-filenone-flushfalse","title":"\u270dprint(*objects,\u00a0sep='\u00a0',\u00a0end='\\n',\u00a0file=None,\u00a0flush=False):","text":"<p>print\u51fd\u6570\u81ea\u5e26sep='\u00a0',\u8f93\u51fa\u591a\u4e2a\u53d8\u91cf\u65f6\u6709\u7a7a\u683c\u9694\u5f00</p>"},{"location":"python/Python-is-important/#\u5b57\u7b26\u4e32str","title":"\ud83d\udcd2\u5b57\u7b26\u4e32str","text":"<p>Python3d\u7684\u5b57\u7b26\u4e32\u7f3a\u7701\u7f16\u7801\u4e3aUnicode\u7f16\u7801</p> <p>\u7279\u6027</p> <ul> <li>\u53ef\u62fc\u63a5:     <pre><code>&gt;&gt;&gt;\"abc\" + '123'\n'abc123'\n&gt;&gt;&gt;\"abc\" + 123\nTypeError: can only concatenate str (not \"int\") to str\n</code></pre></li> <li>\u53ef\u590d\u5236:     <pre><code>&gt;&gt;&gt;'abc' * 5\n'abcabcabcabcabc'\n&gt;&gt;&gt;abc' * 0\n''     # \u7a7a\u5b57\u7b26\n</code></pre></li> </ul>"},{"location":"python/Python-is-important/#\u5b57\u7b26\u4e32\u8f6c\u6362\u65b9\u6cd5","title":"\u5b57\u7b26\u4e32\u8f6c\u6362\u65b9\u6cd5","text":"<p> <p>\u65b9\u6cd5\u5c06\u5b57\u7b26\u4e32\u8f6c\u6362\u8f93\u51fa,\u4f46\u6ca1\u6709\u6539\u53d8\u539f\u53d8\u91cf</p> \u65b9\u6cd5 \u529f\u80fd x.capitalize() #\u8fd4\u56de\u9996\u5b57\u6bcd\u5927\u5199\u5176\u4ed6\u5b57\u6bcd\u90fd\u5c0f\u5199\u7684\u5b57\u7b26\u4e32 x.casefold() #\u8fd4\u56de\u6240\u6709\u5b57\u6bcd\u90fd\u5c0f\u5199\u7684\u5b57\u7b26\u4e32 x.title() #\u8fd4\u56de\u6bcf\u4e2a\u5355\u8bcd\u9996\u5b57\u6bcd\u5927\u5199\u5176\u4ed6\u5b57\u6bcd\u5c0f\u5199\u7684\u5b57\u7b26\u4e32 x.upper() / x.lower() #\u5c06\u6240\u6709\u5b57\u6bcd\u90fd\u8f6c\u6362\u6210\u5927/\u5c0f\u5199 x.swapcase() #\u53cd\u8f6c\u5b57\u7b26\u4e32\u5927\u5c0f\u5199 x.strip()/.rstrip()/.lstrip() #\u53ea\u53bb\u9664\u5b57\u7b26\u4e32\u4e24\u7aef/\u53f3\u8fb9/\u5de6\u8fb9\u7684\u7a7a\u683c x.center(width, [fillchar]) \u5b57\u7b26\u4e32\u5c45\u4e2d\u5bf9\u9f50\u7684\u65b9\u6cd5,\u5728\u5b57\u7b26\u4e32\u4e24\u4fa7\u586b\u5145\u6307\u5b9a\u7684\u5b57\u7b26 <pre><code>&gt;&gt;&gt;name = 'zju hello'\n&gt;&gt;&gt;name.capitalize()          # \u8f6c\u6362\u4e3a\u9996\u5b57\u6bcd\u5927\u5199\u7684\u5b57\u7b26\u4e32\n'Zju hello'             \n&gt;&gt;&gt;name.title()               # \u6bcf\u4e2a\u5355\u8bcd\u9996\u5b57\u6bcd\u5927\u5199,\u4ec5\u8bc6\u522b\u7a7a\u683c\u9694\u5f00\u7684\u5355\u8bcd\n'Zju Hello'\n&gt;&gt;&gt;name.upper()\n'ZJU HELLO'\n&gt;&gt;&gt;block = '  twist zz '\n&gt;&gt;&gt;block.strip()\n'twist zz'                     # \u4ec5\u53bb\u9664\u5b57\u7b26\u4e32\u4e24\u7aef\u7a7a\u683c\n&gt;&gt;&gt;sentence = \"Python is awesome\"\n&gt;&gt;&gt;sentence.center(24, '*')\n'***Python is awesome****'\n</code></pre>"},{"location":"python/Python-is-important/#\u9690\u5f0f\u5b57\u7b26\u4e32\u8fde\u63a5","title":"\u9690\u5f0f\u5b57\u7b26\u4e32\u8fde\u63a5","text":"<p>\u5982\u679c\u4e24\u4e2a\u5b57\u7b26\u4e32\u76f4\u63a5\u76f8\u90bb\uff0c\u5b83\u4eec\u4f1a\u81ea\u52a8\u8fde\u63a5\u5728\u4e00\u8d77  <pre><code>&gt;&gt;&gt;'aa' 'b'\n'aab'\n</code></pre></p>"},{"location":"python/Python-is-important/#\u539f\u59cb\u5b57\u7b26\u4e32","title":"\u539f\u59cb\u5b57\u7b26\u4e32","text":"<p>\u4f7f\u7528r\u5b57\u7b26\u8f6c\u6362\u4e3a\u539f\u59cb\u5b57\u7b26\u4e32,\u5b57\u7b26\u4e32\u4e2d\u7684'\\n'\u7b49\u8f6c\u4e49\u5b57\u7b26\u5c06\u88ab\u8bc6\u522b\u4e3a\u666e\u901a\u5b57\u7b26 <pre><code>sentence_1 = '''hello      \nworld'''                         #\u7528''' '''\u6765\u8868\u793a\u8de8\u591a\u884c\u7684\u957f\u5b57\u7b26\u4e32\nsentence_2 = 'hello\\nworld'\nsentence_3 = r'hello\\nworld'     # \u8f6c\u4e3a\u539f\u59cb\u5b57\u7b26\u4e32\n&gt;&gt;&gt;sentence_1 == sentence_2\nTrue\n&gt;&gt;&gt;sentence_2 == sentence_3\nFalse\n</code></pre></p>"},{"location":"python/Python-is-important/#\u62c6\u5206\u66ff\u6362\u67e5\u627e","title":"\u62c6\u5206,\u66ff\u6362,\u67e5\u627e","text":"<pre><code>S = 'sha bby'\n&gt;&gt;&gt;S.split(' ')        # \u5c06\u5b57\u7b26\u4e32\u62c6\u5206\u4f4dlist\n['sha', 'bby']\n&gt;&gt;&gt;S.replace(' bby','rk')      # \u66ff\u6362\u6307\u5b9a\u5b57\u7b26\u4e32\n'shark'\n&gt;&gt;&gt;(S*4).replace(' bby','rk')   # \u4f1a\u65e0\u5dee\u522b\u66ff\u6362\u6240\u6709\u6307\u5b9a\u5b57\u7b26\u4e32\n'sharksharksharkshark'\n&gt;&gt;&gt;(S*4).count('bby')           # \u7edf\u8ba1\u67d0\u6307\u5b9a\u5b57\u7b26\u4e32\u51fa\u73b0\u6b21\u6570\n4\n&gt;&gt;&gt;(S*4).find('bby')            # \u67e5\u627e \u67d0\u6307\u5b9a\u5b57\u7b26\u4e32\u7b2c\u4e00\u6b21\u51fa\u73b0\u7684\u5730\u65b9\n4\n&gt;&gt;&gt;(S*4).find('bby',5,15)       # \u6307\u5b9a\u8303\u56f4\n11\n&gt;&gt;&gt;(S*4).find('bby',1,2)        # \u65e0\u6cd5\u627e\u5230\u5219\u8f93\u51fa-1\n-1\n</code></pre>"},{"location":"python/Python-is-important/#\u5b57\u7b26\u4e32\u4e0d\u53ef\u4fee\u6539","title":"\u5b57\u7b26\u4e32\u4e0d\u53ef\u4fee\u6539","text":"<pre><code>&gt;&gt;&gt;s = 'hello'\n&gt;&gt;&gt;s[0] = 'f'\n# \u5bf9\u5b57\u7b26\u4e32\u5143\u7d20\u7684\u8d4b\u503c\u65f6\u975e\u6cd5\u7684\n</code></pre>"},{"location":"python/Python-is-important/#\u6d6e\u70b9\u6570\u76f8\u5173","title":"\ud83c\udf26\u6d6e\u70b9\u6570\u76f8\u5173","text":"<pre><code>&gt;&gt;&gt;divmod(9.0,2.5)      # \u6c42(\u5546,\u4f59\u6570)\n(3.0, 1.5)              # \u8303\u56f4tuple\u7c7b\u578b\n&gt;&gt;&gt;round(4.55,1)              # \u56db\u820d\u4e94\u5165 round( number[,\u00a0ndigits=None] ) ndigits\u63a7\u5236\u4fdd\u7559\u5230\u5c0f\u6570\u70b9\u540e\u51e0\u4f4d\n4 # \u800c\u4e0d\u662f3.6, Python 3.x\u91c7\u7528\u201cround half to even\u201d,\u5c06.5\u7684\u503c\u820d\u5165\u5230\u6700\u8fd1\u7684\u5076\u6570\u7ed3\u679c\uff0c\u800c\u4e0d\u662f\u5411\u4e0a\u820d\u5165  \n&gt;&gt;&gt;abs(-44)\n44\n</code></pre>"},{"location":"python/Python-is-important/#\u5e03\u5c14\u503c\u76f8\u5173bool","title":"\u274c\u5e03\u5c14\u503c\u76f8\u5173bool","text":""},{"location":"python/Python-is-important/#\u5b57\u7b26\u4e32\u4e0e\u6570\u5b57\u4e0d\u80fd\u6bd4\u8f83\u5927\u5c0f","title":"\u5b57\u7b26\u4e32\u4e0e\u6570\u5b57\u4e0d\u80fd\u6bd4\u8f83\u5927\u5c0f","text":"<pre><code>&gt;&gt;&gt;\"hello\" &gt; 3 \nTypeError:unorderable types:str()&gt;int()\n</code></pre>"},{"location":"python/Python-is-important/#\u77ed\u8def\u8fd0\u7b97\u7b26andor","title":"\u77ed\u8def\u8fd0\u7b97\u7b26'and'\u3001'or'","text":"<p>'and'\u3001'or'\u4f1a\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u88ab\u8ba1\u7b97\u7684\u8868\u8fbe\u5f0f\u7684\u503c:</p> <ul> <li> <p>'and'\u8fd0\u7b97\u7b26\uff1a   \u5982\u679c\u6240\u6709\u8868\u8fbe\u5f0f\u90fd\u4e3a\u771f\uff08True\uff09\uff0c\u5219\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u8868\u8fbe\u5f0f\u7684\u503c\uff1b\u5982\u679c\u6709\u4efb\u4f55\u4e00\u4e2a\u8868\u8fbe\u5f0f\u4e3a\u5047\uff08False\uff09\uff0c\u5219\u8fd4\u56de\u7b2c\u4e00\u4e2a\u4e3a\u5047\u7684\u8868\u8fbe\u5f0f\u7684\u503c\u3002#\u5f53\\(A_1\\) and \\(B_2\\)\u65f6,\u82e5\\(A_1\\)\u4e3a\u771f,\u5219\u8fd4\u56de\\(B_2\\),\u5426\u5219\u8fd4\u56de\\(A_1\\).</p> <pre><code>    # and \u8fd0\u7b97\u7b26\u793a\u4f8b\n    a = 1\n    b = 5\n    print(a and b)  # \u8f93\u51fa5\n    print(a and False)  # \u8f93\u51faFalse\n    print(a and None)  # \u8f93\u51faNone\n    print(False and a)  # \u8f93\u51faFalse\n    print(None and a)  # \u8f93\u51faNone\n    print(True and 'a' and 2 and 5&gt;2)  # \u8f93\u51faTrue\n</code></pre> </li> <li> <p>'or'\u8fd0\u7b97\u7b26\uff1a   \u5982\u679c\u6709\u4efb\u4f55\u4e00\u4e2a\u8868\u8fbe\u5f0f\u4e3a\u771f\uff08True\uff09\uff0c\u5219\u8fd4\u56de\u7b2c\u4e00\u4e2a\u4e3a\u771f\u7684\u8868\u8fbe\u5f0f\u7684\u503c\uff1b\u5982\u679c\u6240\u6709\u8868\u8fbe\u5f0f\u90fd\u4e3a\u5047\uff08False\uff09\uff0c\u5219\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u8868\u8fbe\u5f0f\u7684\u503c\u3002#\u5f53\\(A_1\\) and \\(B_2\\)\u65f6,\u82e5\\(A_1\\)\u4e3a\u771f,\u5219\u8fd4\u56de\\(A_1\\),\u5426\u5219\u8fd4\u56de\\(B_2\\).     <pre><code># or \u8fd0\u7b97\u7b26\u793a\u4f8b\nprint(2 or False)  # \u8f93\u51fa2\nprint(0 or False)  # \u8f93\u51faFalse\nprint(0 or False or {} or 4)  # \u8f93\u51fa4\n</code></pre></p> </li> </ul> <p>\u6240\u6709\u975e\u96f6\u503c\u5728\u53c2\u4e0e\u903b\u8f91\u8fd0\u7b97\u65f6\u4f5c\u4e3aTrue\u7684\u903b\u8f91\u91cf,0\u4f5c\u4e3aFalse</p>"},{"location":"python/Python-is-important/#\u5e03\u5c14\u503c\u53ef\u4f5c\u6570\u5b57\u8fd0\u7b97true\u4e3a1false\u4e3a0","title":"\u5e03\u5c14\u503c\u53ef\u4f5c\u6570\u5b57\u8fd0\u7b97,True\u4e3a1,false\u4e3a0","text":"<pre><code>&gt;&gt;&gt;True &gt; 0.5\nTrue\n&gt;&gt;&gt;True &gt; False\nTrue\n&gt;&gt;&gt; True &gt; None\nTypeError: '&gt;' not supported between instances of 'bool' and 'NoneType'\n</code></pre>"},{"location":"python/Python-is-important/#\u5217\u8868list","title":"\ud83d\udcd1\u5217\u8868list","text":""},{"location":"python/Python-is-important/#\u7a7a\u5217\u8868","title":"\u7a7a\u5217\u8868[]","text":"<pre><code>&gt;&gt;&gt;type([])\nlist\n&gt;&gt;&gt;[] + 'a'        # \u7a7a\u5217\u8868\u4e0d\u80fd\u76f4\u63a5\u52a0\u5b57\u7b26\u4e32\nTypeError: can only concatenate list (not \"str\") to list\n&gt;&gt;&gt;[] + ['a']\n['a']\n&gt;&gt;&gt;['a'] * 2\n['a', 'a']\n&gt;&gt;&gt;[] == 0 or [] == None or [] == {} \nFalse\n&gt;&gt;&gt;[] == list()\nTrue\n&gt;&gt;&gt;bool([])   # [] and 1&gt;2\nFalse\n</code></pre>"},{"location":"python/Python-is-important/#\u5217\u8868\u63a8\u5bfc\u5f0f","title":"\u5217\u8868\u63a8\u5bfc\u5f0f","text":"<p>\u7528\u4e8e\u521b\u5efa\u5217\u8868\u7684\u7d27\u51d1\u8bed\u6cd5,\u6839\u636e\u73b0\u6709\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\uff08\u5982\u5217\u8868\u3001\u5143\u7ec4\u3001\u96c6\u5408\u7b49\uff09\u5feb\u901f\u6784\u5efa\u65b0\u7684\u5217\u8868 <pre><code>old_list = [1, 2, 3, 4, 5]\nnew_list = [x * 2 for x in old_list if x % 2 == 0]\n&gt;&gt;&gt;new_list \n[4, 8]\n</code></pre></p>"},{"location":"python/Python-is-important/#\u540c\u65f6\u5bf9\u4e24\u4e2a\u6216\u591a\u4e2a\u53ef\u8fed\u4ee3\u5bf9\u8c61\u7684\u5217\u8868\u63a8\u5bfc\u5f0f","title":"\u540c\u65f6\u5bf9\u4e24\u4e2a\u6216\u591a\u4e2a\u53ef\u8fed\u4ee3\u5bf9\u8c61\u7684\u5217\u8868\u63a8\u5bfc\u5f0f","text":"<p><pre><code>list1 = [1, 2, 3]\nlist2 = ['a', 'b', 'c']\ncombined_list = [(x, y) for x in list1 for y in list2 if x % 2 == 0 and y != 'a']\n&gt;&gt;&gt;combined_list\n[(2, 'b'), (2, 'c')]\n</code></pre> Tips:\u4f46\u5047\u5982\u4e24\u4e2a\u53ef\u8fed\u4ee3\u5bf9\u8c61\u7684\u957f\u5ea6\u4e0d\u540c,\u5219\u4f1a\u5bf9\u4f46\u7684\u5bf9\u8c61\u91cd\u590d\u8f93\u51fa <pre><code>list1 = [1, 2, 3]\nlist2 = ['a', 'b']\ncombined_list = [(x, y) for x in list1 for y in list2]\n&gt;&gt;&gt;combined_list\n[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b'), (3, 'a'), (3, 'b')]\n</code></pre></p>"},{"location":"python/Python-is-important/#\u540c\u4e00\u5e8f\u5217\u591a\u79cd\u5f62\u5f0f\u7ec4\u6210","title":"\u540c\u4e00\u5e8f\u5217\u591a\u79cd\u5f62\u5f0f\u7ec4\u6210","text":"<pre><code>&gt;&gt;&gt;[1/i if i % 2 == 2 else -1/i for i in range(1,6)]\n[-1.0, -0.5, -0.3333333333333333, -0.25, -0.2]\n&gt;&gt;&gt;[1/i if i % 2 == 0 else -1/i for i in range(1,10) if i % 2 ==1]\n[-1.0, -0.3333333333333333, -0.2, -0.14285714285714285, -0.1111111111111111]\n</code></pre>"},{"location":"python/Python-is-important/#\u5207\u7247\u64cd\u4f5c","title":"\u5207\u7247\u64cd\u4f5c","text":"0 1 2 3 4 a b c d e -5 -4 -3 -2 -1 <pre><code>lis = [i for i in range(1,6)]\n&gt;&gt;&gt;lis[::-1]                         # \u5c06\u5143\u7d20\u5012\u5e8f \n[5, 4, 3, 2, 1]\n&gt;&gt;&gt;lis[1:3:-1]                       # \u6b65\u957f\u4e3a\u8d1f\u6570,\u4f46\u7d22\u5f15\u6b63\u5411,\u5219\u8fd4\u56de\u7a7a\u5217\u8868[]\n[]\n</code></pre>"},{"location":"python/Python-is-important/#ba-\u4e0eba-\u7684\u533a\u5206","title":"b=a \u4e0eb=a[:] \u7684\u533a\u5206","text":"<ul> <li> <p>b = a \u662f\u5c06b\u6307\u5411a\u5217\u8868\u7684\u5f15\u7528,\u56e0\u6b64a,b\u4f1a\u540c\u6b65\u6539\u53d8\u5143\u7d20 <pre><code>a = list(range(1,5))\nb = a \nb += [99]\n&gt;&gt;&gt;a\n[1, 2, 3, 4, 99]\n</code></pre></p> </li> <li> <p>b = a[:]\u521b\u5efa\u4e86\u4e00\u4e2aa\u7684\u526f\u672c\u5e76\u5c06\u5176\u8d4b\u503c\u7ed9b,a\u4e0eb\u4e0d\u4f1a\u4e92\u76f8\u5f71\u54cd</p> </li> <li> <p>b = a[1]\u662f\u5bf9a\u5217\u8868\u4e2d\u7d22\u5f15\u4e3a1\u7684\u5143\u7d20\u7684\u5f15\u7528,\u7136\u800c\u4fee\u6539\u4e86b\u7684\u503c,a\u8868\u4e2d\u7684\u5143\u7d20\u5e76\u4e0d\u4f1a\u53d7\u5230\u5f71\u54cd</p> </li> </ul>"},{"location":"python/Python-is-important/#del-\u5220\u9664\u5143\u7d20","title":"del \u5220\u9664\u5143\u7d20","text":"<pre><code>&gt;&gt;&gt;name = ['Alice', 'Kim', 'Karl', 'John']\n&gt;&gt;&gt;del name[2]\n&gt;&gt;&gt;name\n['Alice','Kim','John']\n</code></pre>"},{"location":"python/Python-is-important/#\u5217\u8868\u5e38\u7528\u65b9\u6cd5","title":"\u5217\u8868\u5e38\u7528\u65b9\u6cd5","text":"\u65b9\u6cd5 \u529f\u80fd str.append(x) \u5728\u672b\u5c3e\u8ffd\u52a0x str.clear \u6e05\u9664\u6240\u6709\u5143\u7d20 str.count(x) \u7edf\u8ba1\u5217\u8868\u4e2dx\u51fa\u73b0\u7684\u6b21\u6570 str.copy() \u521b\u5efa\u5217\u8868\u5907\u4efd str.remove(x) \u5220\u9664\u503c\u4e3ax\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20 str.sort() \u5217\u8868\u5143\u7d20\u6392\u5217 str.reverse() \u5012\u5e8f\u5217\u8868 str.insert(index,x) \u5728index\u5904\u63d2\u5165x str.index(value[,start[,stop]]) \u5728\u6307\u5b9a\u8303\u56f4\u5185\u5bfb\u627evalue\u7b2c\u4e00\u6b21\u51fa\u73b0\u7684\u4e0b\u6807 str.extend() \u5c06\u5217\u8868x\u6269\u5145\u5230str\u4e2d(\u7c7b\u4f3c.append(),\u80fd\u591f\u4e00\u6b21\u6dfb\u52a0\u591a\u4e2a\u5143\u7d20) str.sort(reverse=False) \u5217\u8868\u5143\u7d20\u6392\u5e8f,\u9ed8\u8ba4\u5347\u5e8f(False) <pre><code>&gt;&gt;&gt;a = [2,3,5,7,11,13]\n&gt;&gt;&gt;a.remove(4)               # \u65e0\u6cd5\u6e05\u9664\u5217\u8868\u4e2d\u6ca1\u6709\u7684\u5143\u7d20\nValueError: list.remove(x): x not in list\n\n&gt;&gt;&gt;a.reverse()               # \u5c06\u5217\u8868\u53cd\u8f6c,\u4f1a\u76f4\u63a5\u6539\u53d8a\u5217\u8868\u7684\u503c\n&gt;&gt;&gt;a\n[13, 11, 7, 5, 3, 2]\n\n&gt;&gt;&gt;a.index(6)                # \u65e0\u6cd5\u627e\u51fa\u5217\u8868\u4e2d\u6ca1\u6709\u7684\u5143\u7d20\nValueError: 6 is not in list\n\n&gt;&gt;&gt;a = [1,2,3,5,4,8,0]\n&gt;&gt;&gt;type(a.sort(reverse = True)),type(a.remove(4)),type(a.clear())\n(NoneType, NoneType, NoneType)\n\n#a.sort()\u65b9\u6cd5\u4f1a\u76f4\u63a5\u4fee\u6539\u5217\u8868a\uff0c\u5e76\u8fd4\u56deNone\u3002\u5bf9\u4e8e\u4f1a\u6539\u53d8\u6570\u636e\u7684\u51fd\u6570\u548c\u65b9\u6cd5\uff08\u5982list.sort()\uff09\uff0c\u901a\u5e38\u4f1a\u8fd4\u56deNone\u7c7b\u578b\uff0c\u4ee5\u63d0\u793a\u5b83\u4eec\u662f\u5728\u539f\u5730\u4fee\u6539\u6570\u636e\n</code></pre>"},{"location":"python/Python-is-important/#\u805a\u5408join\u51fd\u6570","title":"\u805a\u5408.join()\u51fd\u6570","text":"<p>.join()\u51fd\u6570\u53ef\u5c06\u4e00\u4e2a\u5217\u8868\u7684\u5b57\u7b26\u4e32\u7c7b\u578b\u5143\u7d20\u7ec4\u5408\u4e3a\u5355\u72ec\u4e00\u4e2a\u5b57\u7b26\u4e32&lt;\uff0fB&gt;: <p><code>\u683c\u5f0f:&lt;\u5206\u9694\u7b26&gt;.join(x)</code></p> <pre><code>&gt;&gt;&gt;a = ['Hello','world','!!']\n&gt;&gt;&gt;':'.join(a)\n'Hello:world:!!'\n&gt;&gt;&gt;b = ['Hello',123]\n&gt;&gt;&gt;' '.join(b)                     # \u82e5\u5217\u8868\u4e2d\u5b58\u5728\u975estr\u5143\u7d20,\u5219\u4f1a\u62a5\u9519\nTypeError: sequence item 1: expected str instance, int found\n</code></pre>"},{"location":"python/Python-is-important/#for\u5faa\u73af\u8bed\u53e5-important","title":"\ud83d\udd87for\u5faa\u73af\u8bed\u53e5 (important)","text":"<p>for\u5faa\u73af\u8bed\u53e5\u7528\u4e8e\u8fed\u4ee3\u5e8f\u5217\uff08\u53ef\u4ee5\u662f\u5217\u8868\u3001\u5143\u7ec4\u3001\u5b57\u5178\u3001\u96c6\u5408\u6216\u5b57\u7b26\u4e32\uff09\u4e2d\u7684\u6bcf\u4e2a\u9879\u76ee</p> <ul> <li> <p>\u904d\u5386\u5b57\u7b26\u4e32: \u5373\u4f7f\u5b57\u7b26\u4e32\u662f\u4e0d\u53ef\u53d8\u5bf9\u8c61\uff0c\u5b83\u4eec\u4e5f\u662f\u53ef\u8fed\u4ee3\u5bf9\u8c61\uff0c\u53ef\u4ee5\u4f7f\u7528for\u5faa\u73af\u904d\u5386\u5b57\u7b26\u4e32\u4e2d\u7684\u6bcf\u4e2a\u5b57\u7b26\u3002 <pre><code>word = \"banana\"\nfor x in word:\n    print(x)\n</code></pre></p> </li> <li> <p>range(start=0, stop, step=1)\u51fd\u6570: \u53ef\u7528\u4e8e\u6307\u5b9a\u5faa\u73af\u7684\u6b21\u6570,\u6839\u636estep\u9012\u589e/\u9012\u51cf\u5230\u6307\u5b9a\u6570\u5b57stop\u7ed3\u675f(\u4f46\u662f\u4e0d\u5305\u62ecstop) <pre><code>&gt;&gt;&gt;type(range(1,5))\n&lt;class 'range'&gt;          # \u4f5c\u4e3a\u4e00\u79cd\u7279\u6b8a\u7684range\u5bf9\u8c61,\u4ee3\u8868\u7279\u5b9a\u533a\u95f4\u7684\u5e8f\u5217\n&gt;&gt;&gt;list(range(1,5))\n[1, 2, 3, 4]             # \u4e0d\u5305\u62ec5\n</code></pre></p> </li> <li> <p>\u5728for\u5faa\u73af\u4e2d\u4f7f\u7528else: \u5728for\u5faa\u73af\u7ed3\u675f\u540e\u6267\u884celse\u4e2d\u7684\u4ee3\u7801\u5757\u3002\u4f46\u5982\u679c\u5faa\u73af\u88abbreak\u8bed\u53e5\u4e2d\u6b62\uff0c\u5219\u4e0d\u4f1a\u6267\u884celse\u5757\u3002 <pre><code>for x in range(6):\n    sum += x\nelse:\n    print(\"Finally finished!\")\n</code></pre></p> </li> <li> <p>break\u8bed\u53e5/continue\u8bed\u53e5: break\u8bed\u53e5\u53ef\u7528\u4e8e\u63d0\u524d\u9000\u51fa\u5faa\u73af(\u4e0eif\u8bed\u53e5\u914d\u5408) continue\u8bed\u53e5\u7528\u4e8e\u8df3\u8fc7\u5f53\u524d\u8fed\u4ee3\u5e76\u7ee7\u7eed\u4e0b\u4e00\u6b21\u8fed\u4ee3(\u8df3\u8fc7\u7279\u5b9a\u503c) <pre><code>fruits = [\"apple\", \"banana\", \"cherry\"]\nfor x in fruits:\n    print(x)\n    if x == \"banana\":\n        break\n\nfor x in fruits:\n    if x == \"banana\":\n        continue\n    print(x)\n</code></pre></p> </li> <li>\u904d\u5386\u5217\u8868list()</li> </ul>"},{"location":"python/Python-is-important/#f-string-important","title":"\u2728F-string (important)","text":"<p>F-string\u662f\u4e00\u79cd\u683c\u5f0f\u5316\u5b57\u7b26\u4e32\u5e38\u91cf\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u5728\u5b57\u7b26\u4e32\u4e2d\u63d2\u5165\u53d8\u91cf\u6216\u8868\u8fbe\u5f0f\uff0c\u5e76\u4ee5\u6307\u5b9a\u7684\u683c\u5f0f\u8f93\u51fa\u3002</p> <p>\u5728\u683c\u5f0f\u5316\u7684\u5b57\u7b26\u4e32\u4e2d\uff0c\u4f7f\u7528\u5927\u62ec\u53f7 {} \u6765\u8868\u793a\u9700\u8981\u88ab\u66ff\u6362\u7684\u5b57\u6bb5\uff0c\u800c\u4e0d\u5728\u5927\u62ec\u53f7 {} \u5185\u7684\u5185\u5bb9\u5c06\u4f1a\u88ab\u539f\u6837\u8f93\u51fa\u3002\u5982\u679c\u60f3\u8981\u8f93\u51fa\u5927\u62ec\u53f7 {} \u5b57\u7b26\u672c\u8eab\uff0c\u53ef\u4ee5\u4f7f\u7528\u53cc\u5c42\u5927\u62ec\u53f7 {{}} \u5c06\u5176\u5305\u88f9\u8d77\u6765\u3002\u901a\u8fc7\u5728\u5927\u62ec\u53f7 {} \u4e2d\u76f4\u63a5\u586b\u5165\u8981\u66ff\u6362\u7684\u5185\u5bb9\uff0c\u6211\u4eec\u53ef\u4ee5\u8f7b\u677e\u5730\u751f\u6210\u683c\u5f0f\u5316\u540e\u7684\u5b57\u7b26\u4e32\u3002</p>"},{"location":"python/Python-is-important/#\u5e38\u7528\u683c\u5f0ffds","title":"\u5e38\u7528\u683c\u5f0f(f,d,s)","text":"<p><pre><code>float_0 = 2305.665\nint_0 = 230\nword = 'machinewjq'\n\n&gt;&gt;&gt;f'Results of the {float_0:.2f}'        # :.?f \u4fdd\u7559n\u4f4d\u5c0f\u6570(Banker's rounding)\n&gt;&gt;&gt;'Results of the 2305.66'\n&gt;&gt;&gt;f'Results of the {int_0:.2f}'\n&gt;&gt;&gt;'Results of the 230.00'                # int\u8f6cfloat\u8f93\u51fa\n\n&gt;&gt;&gt;f'Results of the {float_0:10d}'            # \u6d6e\u70b9\u6570\u65e0\u6cd5\u4f7f\u7528'\u6700\u5c0f\u5b57\u7b26\u5bbd\u5ea6\u8bbe\u7f6ed'\n&gt;&gt;&gt;ValueError: Unknown format code 'd' for object of type 'float'\n&gt;&gt;&gt;f'Results of the {int_0:10d}'\n&gt;&gt;&gt;'Results of the       2305'\n&gt;&gt;&gt;f'Results of the {int_0:3d}'               # \u5f53\u8bbe\u7f6e\u5bbd\u5ea6\u5c0f\u4e8e\u539f\u5b57\u7b26\u4e32,\u5219\u4e0d\u4f1a\u53d8\u52a8\n&gt;&gt;&gt;'Results of the       2305'\n</code></pre> \u53ef\u77e5f\u4e0ed\u4e92\u65a5,\u4e0d\u80fd\u4e00\u8d77\u4f7f\u7528(f\u7528\u4e8efloat,d\u7528\u4e8eint,\u5b57\u7b26\u4e32\u4f7f\u7528s) <pre><code>&gt;&gt;&gt;f'this is {int_0:#&gt;10}'\n'this is #######230'          # \u53f3\u5bf9\u9f50,#\u586b\u5145\n&gt;&gt;&gt;f'this is {int_0:-^10}'\n'this is ---230----'          # \u5c45\u4e2d,-\u586b\u5145\n\ntable = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 7678}\nfor name, phone in table.items():\n    print(f'{name:10} ==&gt; {phone:10d}')\n#Sjoerd     ==&gt;       4127\n#Jack       ==&gt;       4098\n#Dcab       ==&gt;       7678\n</code></pre></p> <p>\u4e00\u79cd\u7279\u6b8a\u4f7f\u7528</p> <p>\u5bf9\u4e8e\u5b57\u7b26\u4e32str <pre><code>name = \"Alice\"\nformatted_name = f\"{name:0&gt;8s}\"     #\u5b57\u7b26\u4e32\u4f7f\u7528s\n&gt;&gt;&gt;formatted_name\n'000Alice'\n&gt;&gt;&gt;'{:&gt;08s}'.format(bin(31)[2:])\n'00011111'\n&gt;&gt;&gt;'{:08s}'.format(bin(31)[2:])\n'11111000'                      # \u5b57\u7b26\u4e32s\u9ed8\u8ba4\u5de6\u5bf9\u9f50\n&gt;&gt;&gt;'{:8s}'.format(bin(31)[2:])\n'11111   '\n&gt;&gt;&gt;'{:8d}'.format(123)\n'     123'                     # \u6574\u6570d\u9ed8\u8ba4\u53f3\u5bf9\u9f50 \n&gt;&gt;&gt;'{:08d}'.format(123)\n'00000123'\n</code></pre> \u5176\u4e2d\u7684{:0ns}\u3001{:0nd}\u662f\u76f8\u8f83\u4e8e{:ns}\u3001{:nd}\u7684\u4e00\u79cd\u7279\u6b8a\u7528\u6cd5,\u8868\u793a\u75280\u586b\u5145,\u800c\u9ed8\u8ba4\u4e3a\u7528\u7a7a\u683c\u586b\u5145.</p> <p></p>"},{"location":"python/Python-is-important/#\u5206\u9694\u7b26\u5c0f\u6570\u70b9\u79d1\u5b66\u8ba1\u6570\u6cd5","title":"\u5206\u9694\u7b26/\u5c0f\u6570\u70b9/\u79d1\u5b66\u8ba1\u6570\u6cd5","text":"<pre><code>this_num = 1234.5678\n&gt;&gt;&gt;f'this number is {this_num:,}'    #\u5343\u4f4d\u5206\u9694\u7b26\n'this number is 1,234.5678'\n\n&gt;&gt;&gt;f'this number is {this_num:.2e}'   # \u79d1\u5b66\u8ba1\u6570\u6cd5+\u4fdd\u75592\u4f4d\u5c0f\u6570\n'this number is 1.23e+03'\n</code></pre>"},{"location":"python/Python-is-important/#ifelse\u6761\u4ef6f-string","title":"if/else\u6761\u4ef6f-string","text":"<pre><code>score = 65\n&gt;&gt;&gt;f'student {'pass' if score &gt; 60 else 'fail'}'\n'student pass'\n</code></pre>"},{"location":"python/Python-is-important/#tuple-\u5143\u7ec4-important","title":"\ud83d\udcadtuple \u5143\u7ec4 (important!)","text":"<p>tuple\u662f\u4e0d\u53ef\u4fee\u6539\u7684\u4efb\u4f55\u7c7b\u522b\u7684\u6570\u636e\u5e8f\u5217</p>"},{"location":"python/Python-is-important/#\u4ece\u51fd\u6570\u4e2d\u8fd4\u56de\u591a\u4e2a\u503c\u65f6\u901a\u5e38\u4f1a\u4f7f\u7528tuple","title":"\u4ece\u51fd\u6570\u4e2d\u8fd4\u56de\u591a\u4e2a\u503c\u65f6\uff0c\u901a\u5e38\u4f1a\u4f7f\u7528tuple","text":"<pre><code>def xxx():\n    name = \"John\"\n    age = 22\n    return name, age    # \u5b9e\u9645\u4e0a,\u4f7f\u7528\u9017\u53f7\u5206\u9694\u5373\u53ef\u521b\u5efa\u5143\u7ec4\n\nresult = xxx()\nprint(result)         # ('John', 30)\n</code></pre>"},{"location":"python/Python-is-important/#\u66f4\u65b0\u5143\u7ec4","title":"\u66f4\u65b0\u5143\u7ec4","text":"<p>\u867d\u7136\u5143\u7ec4\u662f\u4e0d\u53ef\u66f4\u6539\u7684\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u5c06\u5143\u7ec4\u8f6c\u6362\u4e3a\u5217\u8868\u3001\u4fee\u6539\u5217\u8868\uff0c\u7136\u540e\u518d\u8f6c\u6362\u56de\u5143\u7ec4\u6765\u95f4\u63a5\u66f4\u65b0\u5143\u7ec4</p> <pre><code>my_tuple = (1, 2, 3)\n# \u5c06\u5143\u7ec4\u8f6c\u6362\u4e3a\u5217\u8868\uff0c\u66f4\u65b0\u5217\u8868\uff0c\u7136\u540e\u518d\u8f6c\u6362\u56de\u5143\u7ec4\nmy_list = list(my_tuple)\nmy_list[1] = 4\nupdated_tuple = tuple(my_list)\nprint(updated_tuple)  # \u8f93\u51fa: (1, 4, 3)\n</code></pre>"},{"location":"python/Python-is-important/#\u5143\u7ec4\u5e38\u7528\u51fd\u6570\u65b9\u6cd5","title":"\u5143\u7ec4\u5e38\u7528\u51fd\u6570/\u65b9\u6cd5","text":"<p>sorted()\u4f1a\u628atuple\u7b49\u7c7b\u578b\u8f6c\u6362\u4e3alist, \u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u65b0\u7684\u5df2\u6392\u5e8f\u7684\u5217\u8868\uff0c\u4e0d\u4f1a\u6539\u53d8\u539f\u59cb\u7684\u5143\u7ec4</p> <pre><code>my_tuple = (3, 1, 4, 2)\n# \u5bf9\u5143\u7ec4\u4e2d\u7684\u5143\u7d20\u8fdb\u884c\u6392\u5e8f\nsorted_tuple = sorted(my_tuple) \nprint(sorted_tuple)  \n# \u8f93\u51fa: [1, 2, 3, 4]\n\nmy_tuple_A = (1, 4, 2, 'a')              # \u5f53\u5143\u7ec4\u4e2dstr\u4e0eint/float\u6df7\u5408\u65f6,\u4e0d\u80fd\u4f7f\u7528sorted()\u6392\u5e8f\nsorted_tuple_A = sorted(my_tuple_A)\n# TypeError: '&lt;' not supported between instances of 'str' and 'int'\n\nmy_tuple_B = ('b', 'a', 'z')              # \u5f53\u5143\u7ec4\u4e2dstr\u4e0eint/float\u6df7\u5408\u65f6,\u4e0d\u80fd\u4f7f\u7528sorted()\u6392\u5e8f\nsorted_tuple_B = sorted(my_tuple_B)\n# \u8f93\u51fa: ['A', 'a', 'b', 'z']\n</code></pre> <p>\u56e0\u4e3atuple\u4e0d\u53ef\u4fee\u6539\u5143\u7d20\u7684\u503c,.append(), .remove()\u7b49\u65b9\u6cd5\u5747\u4e0d\u9002\u7528,\u67e5\u627e\u7c7b\u65b9\u6cd5\u5219\u53ef\u4ee5\u4f7f\u7528:</p> \u65b9\u6cd5 \u529f\u80fd tuple.count(x) \u8ba1\u7b97x\u5143\u7d20\u51fa\u73b0\u7684\u6b21\u6570 tuple.index(x) \u8ba1\u7b97x\u5143\u7d20\u7b2c\u4e00\u6b21\u51fa\u73b0\u7684\u4e0b\u6807"},{"location":"python/Python-is-important/#\u96c6\u5408set","title":"\ud83c\udf65\u96c6\u5408set()","text":"<p>\u96c6\u5408\u662f\u4e00\u79cd\u65e0\u5e8f\u7684\u4e0d\u91cd\u590d\u5143\u7d20\u5e8f\u5217,,\u53ef\u4ee5\u6dfb\u52a0/\u5220\u9664\u5143\u7d20,\u7c7b\u4f3c\u6570\u5b66\u4e2d\u7684\u96c6\u5408\u6982\u5ff5</p> <p>\u5e38\u7528\u65b9\u6cd5:.add(), .remove(), sum(), max()/min()</p> \u65b9\u6cd5 \u529f\u80fd set_1.issuperset(set_2) / set_1 &gt;= set_2 \u5224\u65ads2\u662fs1\u7684\u8d85\u96c6 set_1.issubset(set_2) / set_1 &lt;= set_2 \u5224\u65ads1\u662fs2\u7684\u5b50\u96c6 set_1.union(set_3) \u6c42\u5e76\u96c6 set_1.intersection(set_2) \u6c42\u4ea4\u96c6 set_1.defference(set_2) \u6c42\u5dee\u96c6 <pre><code>set_1 = {2,3,5,7,11}\nset_2 = {2,3,4,5,6,7,8,9,10,11}\nset_3 = {'a','b','c'}\n\n&gt;&gt;&gt;set_2.issuperset(set_1), set_1.issubset(set_2)\n(True, True)\n&gt;&gt;&gt;union(set_1,set_3)\n{11, 2, 3, 5, 7, 'a', 'b', 'c'}\n&gt;&gt;&gt;set_1.intersection(set_2)\n{2, 3, 5, 7, 11}\n&gt;&gt;&gt;set_2.difference(set_1)\n{4, 6, 8, 9, 10}\n&gt;&gt;&gt;set_1.difference(set_2)       # \u82e5set1\u4e3aset2\u5b50\u96c6\nset()\n</code></pre>"},{"location":"python/Python-is-important/#\u5b57\u5178dict","title":"\ud83c\udf8d\u5b57\u5178dict()","text":""},{"location":"python/Python-is-important/#get\u65b9\u6cd5\u67e5\u627e\u5b57\u5178","title":".get()\u65b9\u6cd5\u67e5\u627e\u5b57\u5178","text":"<pre><code>dictionary.get(keyname, value=None)\n\n- 'keyname'\uff1a\u5fc5\u9700\uff0c\u8981\u8fd4\u56de\u5176\u503c\u7684\u952e\u540d\u3002\n\n- 'value'\uff1a\u53ef\u9009\uff0c\u5982\u679c\u6307\u5b9a\u7684\u952e\u4e0d\u5b58\u5728\uff0c\u5219\u8fd4\u56de\u8be5\u503c\u4f5c\u4e3a\u9ed8\u8ba4\u503c\u3002\u5982\u679c\u4e0d\u63d0\u4f9b\u8be5\u53c2\u6570\uff0c\u5219\u9ed8\u8ba4\u503c\u4e3a None\u3002\n</code></pre> <pre><code># \u521b\u5efa\u4e00\u4e2a\u5b57\u5178\nmy_dict = {'name': 'Alice', 'age': 25, 'city': 'New York'}\n\n# \u4f7f\u7528.get()\u65b9\u6cd5\u5b89\u5168\u5730\u68c0\u7d22\u503c\nprint(my_dict.get('name', 'Unknown'))  # \u8f93\u51fa\uff1a'Alice'\nprint(my_dict.get('gender', 'Unknown'))  # \u8f93\u51fa\uff1a'Unknown'\n</code></pre> <p>Tips: {}\u521b\u5efa\u7684\u662f\u4e00\u4e2a\u7a7a\u5b57\u5178\u800c\u4e0d\u662f\u7a7a\u96c6\u5408</p>"},{"location":"python/Python-is-important/#dict\u521b\u5efa\u5b57\u5178","title":"dict()\u521b\u5efa\u5b57\u5178","text":"<pre><code>&gt;&gt;&gt;url = dict(baidu = 'www.xxx.com', sina = 'hahaha')         # \u6b63\u786e\n&gt;&gt;&gt;url\n{'baidu': 'www.xxx.com', 'sina': 'hahaha'}\n\n&gt;&gt;&gt;url_2 = dict('baidu' = 'www.xxx.com', 'sina' = 'hahaha')   # \u9519\u8bef\nSyntaxError: expression cannot contain assignment, perhaps you meant \"==\"?\n\n&gt;&gt;&gt;url_3 = {'baidu' : 'www.xxx.com', 'sina' : 'hahaha'}       # \u6b63\u786e\n\n&gt;&gt;&gt;url_4 = {key: value for key, value in [('baidu', 'www.xxx.com'), ('sina', 'hahaha')]#\u5fc5\u987b\u4e3a\u5217\u8868}                              # \u63a8\u5bfc\u5f0f\u65b9\u6cd5\n</code></pre>"},{"location":"python/Python-is-important/#\u68c0\u6d4b\u952e","title":"\u68c0\u6d4b\u952e","text":"<pre><code>in\u4e0enot in\u8fd0\u7b97\u7b26\u53ef\u4ee5\u68c0\u6d4b\u5b57\u5178\u4e2d\u662f\u5426\u5b58\u5728\u67d0\u4e2a\u952e,\u8fd4\u56deTrue/False\n</code></pre> <pre><code>&gt;&gt;&gt;url = dict(baidu = 'www.xxx.com', sina = 'hahaha') \n&gt;&gt;&gt;'baidu' in url\n True\n&gt;&gt;&gt;'www.xxx.com' in url\nFalse\n</code></pre>"},{"location":"python/Python-is-important/#for-in\u5faa\u73af\u904d\u5386\u5b57\u5178\u952e","title":"for-in\u5faa\u73af\u904d\u5386\u5b57\u5178(\u952e)","text":"<pre><code>name = {'mood':'happy','health':'good'}\nfor i in name:\n    print(i+':'+name[i])           # \u6309\u7167\u952e\u904d\u5386,\u800c\u4e0d\u662f\u952e\u503c\n\n#mood:happy\n#health:good\n</code></pre>"},{"location":"python/Python-is-important/#\u5b57\u5178\u89e3\u5305unpack","title":"\u5b57\u5178\u89e3\u5305unpack","text":"<p>\u89e3\u5305\u64cd\u4f5c\u5c06\u5b57\u5178\u4e2d\u7684\u952e\u503c\u5bf9\u62c6\u5206\u5e76\u4f20\u9012\u7ed9\u51fd\u6570\u6216\u53d8\u91cf,\u4f7f\u7528**\u64cd\u4f5c\u7b26\u6765\u5b9e\u73b0</p> <pre><code>def print_info(name, age):\n    print(f\"Name: {name}, Age: {age}\")\n\nperson = {'name': 'Alice', 'age': 30}\n\nprint_info(**person)\n\n#Name: Alice\n#Age: 30\n</code></pre> <p>**\u89e3\u5305\u53ef\u4ee5\u7528\u6765\u66f4\u65b0\u5b57\u5178 \u5f53dict1\u4e0edict2\u6709\u76f8\u540c\u7684\u952e\u65f6,\u7531\u540e\u4e00\u4e2a\u4ee3\u66ff\u524d\u4e00\u4e2a:</p> <pre><code>dict1 = {'a': 1, 'b': 2}\ndict2 = {'b': 3, 'c': 4}\nmerged_dict = {**dict1, **dict2}\nprint(merged_dict)  # \u8f93\u51fa: {'a': 1, 'b': 3, 'c': 4}\n</code></pre>"},{"location":"python/Python-is-important/#\u89e3\u5305\u5230\u5217\u8868\u6216\u5143\u7ec4","title":"\u89e3\u5305\u5230\u5217\u8868\u6216\u5143\u7ec4","text":"<pre><code>my_dict = {'a': 1, 'b': 2, 'c': 3}\nkeys_list = [*my_dict]  # \u89e3\u5305\u952e\u5230\u5217\u8868\nvalues_tuple = (*my_dict.values(),)  # \u89e3\u5305\u503c\u5230\u5143\u7ec4,\u6ce8\u610f\u7ed3\u5c3e\u9700\u8981\u9017\u53f7','\n\nprint(keys_list)  \n# \u8f93\u51fa: ['a', 'b', 'c']\nprint(values_tuple)  \n# \u8f93\u51fa: (1, 2, 3)\n</code></pre>"},{"location":"python/Python-is-important/#\u5b57\u5178\u65b9\u6cd5\u51fd\u6570-important-","title":"\u5b57\u5178\u65b9\u6cd5/\u51fd\u6570 (important !)","text":"\u65b9\u6cd5/\u51fd\u6570 \u529f\u80fd dict.keys() dict.values() dict.get(key,value) dict1.update(dict2)"},{"location":"python/Python-is-important/#del\u64cd\u4f5c","title":"\u274cdel\u64cd\u4f5c","text":"<p>del\u8bed\u53e5\u662f\u7528\u4e8e\u5220\u9664\u5bf9\u8c61\u7684\u5173\u952e\u5b57\u3002\u53ef\u4ee5\u7528\u4e8e\u5220\u9664\u53d8\u91cf\u3001\u5217\u8868\u3001\u5b57\u5178\u7b49\u4e0d\u540c\u7c7b\u578b\u7684\u5bf9\u8c61\u3002</p>"},{"location":"python/cs224n-notebook/","title":"\ud83d\udee3[Deep Learning]Stanford CS224n:Natural Language Processing","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224n/index.html</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  https://www.bilibili.com/video/BV1jt421L7ui/(B\u7ad9\u53cc\u8bed\u7cbe\u7ffb) https://www.showmeai.tech/tutorials/36?articleId=231(\u8bfe\u4ef6\u7ffb\u8bd1+\u77e5\u8bc6\u68b3\u7406)</p> <p></p> <p></p> <p>Chapter</p> <p> <ul> <li>Chapter 1\uff1aImage Classification with Linear Classifiers </li> </ul> <p></p>"},{"location":"python/cs224w/","title":"\ud83d\udd2dStanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\u56fe\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u6570\u636e\u7ed3\u6784\uff0c\u53ef\u4ee5\u7528\u4e8e\u5efa\u6a21\u8bb8\u591a\u771f\u5b9e\u4e16\u754c\u7684\u573a\u666f\uff0c\u56fe\u80fd\u591f\u5bf9\u6837\u672c\u4e4b\u95f4\u7684\u5173\u7cfb\u4fe1\u606f\u8fdb\u884c\u5efa\u6a21\u3002\u4f46\u662f\u771f\u5b9e\u56fe\u7684\u6570\u636e\u91cf\u5e9e\u5927\uff0c\u52a8\u8f84\u4e0a\u4ebf\u8282\u70b9\u3001\u800c\u4e14\u5185\u90e8\u62d3\u6251\u7ed3\u6784\u590d\u6742\uff0c\u5f88\u96be\u5c06\u4f20\u7edf\u7684\u56fe\u5206\u6790\u65b9\u6cd5\u5982\u6700\u77ed\u8def\u5f84\u3001DFS\u3001BFS\u3001PageRank \u7b49\u7b97\u6cd5\u5e94\u7528\u5230\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u3002\u56e0\u6b64\u6709\u7814\u7a76\u8005\u63d0\u51fa\u5c06\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u56fe\u6570\u636e\u7ed3\u5408\u8d77\u6765\uff0c\u5373\u56fe\u673a\u5668\u5b66\u4e60\uff0c\u8fd9\u9010\u6e10\u6210\u4e3a\u8fd1\u5e74\u6765\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u80a1\u70ed\u6f6e\uff0c\u7279\u522b\u662f\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u3002</p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p></p> <p></p> <p>Chapter</p> <p> <ul> <li> <p>Chapter 1\uff1aNode Embeddings </p> </li> <li> <p>Chapter 2\uff1aGraph Neural Networks</p> </li> <li> <p>Chapter 3\uff1aA Single Layer of a GNN</p> </li> <li> <p>Chapter 4\uff1aPrediction with GNN</p> </li> <li> <p>Chapter 5\uff1aGraph Isomorphism Network(GIN)</p> </li> <li> <p>Chapter 6\uff1aHeterogeneous Graphs</p> </li> <li> <p>Chapter 7\uff1aHeterogeneous Graphs Transformer</p> </li> <li> <p>Chapter 8\uff1aKnowledge Graphs</p> </li> <li> <p>Chapter 9\uff1aReasoning over KGs</p> </li> <li> <p>Chapter 10\uff1a</p> </li> <li> <p>Chapter 11\uff1aGeometric Deep Learning</p> </li> </ul> <p></p>"},{"location":"python/cs231n/","title":"\ud83d\udd2dStanford CS231n:Deep Learning for Computer Vision","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttps://cs231n.stanford.edu/</p> <p>2024\u7248PPT: https://cs231n.stanford.edu/slides/2024/</p> <p></p> <p>Chapter</p> <p> <ul> <li> <p>Chapter 1\uff1aImage Classification with Linear Classifiers </p> </li> <li> <p>Chapter 2\uff1aRegularization and Optimization</p> </li> <li> <p>Chapter 3\uff1aNeural Networks and Backpropagation</p> </li> <li> <p>Chapter 4\uff1aImageClassification with CNNs</p> </li> <li> <p>Chapter 5\uff1aCNN Architectures</p> </li> <li> <p>Chapter 6: recurrent nerual network</p> </li> <li> <p>Chapter 7: Attention and Transformers</p> </li> <li> <p>Chapter 8:  Object Detection and Image Segmentation</p> </li> </ul> <p></p>"},{"location":"python/d2l/","title":"\ud83d\udd2dDive into Deeplearing","text":"<p>\u60f3\u8bf4\u7684\u8bdd</p> <p> &lt;\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60&gt;\u662f\u5165\u95e8\u6df1\u5ea6\u5b66\u4e60\u975e\u5e38\u597d\u7684\u6559\u6750,\u7528\u4e8e\u57fa\u7840\u77e5\u8bc6\u7684\u5b66\u4e60 <p>\u5b98\u65b9\u6587\u6863:https://zh.d2l.ai/</p> <p>PyTorch\u7248\u89c6\u9891\u6559\u7a0b\uff1ahttps://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497</p> <p></p> <p>Chapter</p> <p> <p>\u4e0d\u5b8c\u5168\u6309\u7167\u5b98\u65b9\u6587\u6863\u7684\u7ae0\u8282\uff0c\u5168\u6309\u6211\u81ea\u5df1\u7684\u5b66\u4e60\u8282\u594f\u6765\u5206\u8282\u5b66\u4e60</p> <ul> <li>Chapter 0\uff1aPyTorch\u53cd\u5411\u4f20\u64ad</li> <li>Chapter 1\uff1alinear regression and multilayer perceptron</li> <li>Chapter 2\uff1aconvolutional neural network</li> <li>Chapter 3\uff1arecurrent neural network</li> </ul> <p></p>"},{"location":"python/Pytorch/","title":"\u90a3\u4e9b\u4f60\u5e94\u8be5\u4f1a\u7684Pytorch\u64cd\u4f5c","text":"<p>Part.1</p> <p> <ul> <li> <p> 1\uff1a\u81ea\u5b9a\u4e49\u6c42\u5bfc/\u53cd\u5411\u4f20\u64ad </p> </li> <li> <p> 2\uff1aDropout\u4e0eDropConnect </p> </li> <li> <p> 3\uff1a </p> </li> <li> <p> 4\uff1a </p> </li> <li> <p> 5\uff1a </p> </li> <li> <p> 6\uff1a </p> </li> <li> <p> 7\uff1a </p> </li> <li> <p> 8\uff1a </p> </li> <li> <p> 9\uff1a</p> </li> <li> <p> 10\uff1a </p> </li> <li> <p> 11\uff1a </p> </li> <li> <p> 12\uff1a </p> </li> <li> <p> 13\uff1a </p> </li> <li> <p> 14\uff1a </p> </li> <li> <p> 15\uff1a </p> </li> <li> <p> 16\uff1a </p> </li> <li> <p> 17\uff1a </p> </li> </ul>"},{"location":"python/Pytorch/code/1/","title":"\u90a3\u4e9b\u4f60\u5e94\u8be5\u4f1a\u7684Pytorch\u64cd\u4f5c","text":""},{"location":"python/Pytorch/code/1/#chapter-1\u81ea\u5b9a\u4e49\u6c42\u5bfc\u53cd\u5411\u4f20\u64ad\u65b9\u5f0f","title":"\u3010Chapter 1\u3011\uff1a\u81ea\u5b9a\u4e49\u6c42\u5bfc/\u53cd\u5411\u4f20\u64ad\u65b9\u5f0f","text":"<pre><code>import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass Exmaple(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x): #\n    ''' \u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570\u7684\u8ba1\u7b97\u8fc7\u7a0b '''\n        ctx.save_for_backward(x)\n        # \u5c06\u8f93\u5165\u4fdd\u5b58\u8d77\u6765\uff0c\u5728backward\u65f6\u4f7f\u7528\n        output = x.clamp(min=0)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n    ''' \n    \u6839\u636e\u94fe\u5f0f\u6c42\u5bfc\uff0cdloss / dx = (dloss / doutput) * (doutput / dx)\n\n    \u5176\u4e2d\u7684dloss / doutput\u5c31\u662f\u8f93\u5165\u7684\u53c2\u6570grad_output\n    '''\n        x, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input_ &lt; 0] = 0               \n        return grad_input\n</code></pre> <p>\u4e0b\u9762\u5c55\u793a<code>EfficientNet</code>\u4e2d\u81ea\u5b9a\u4e49\u7684<code>Swish</code>\u6fc0\u6d3b\u51fd\u6570</p> \\[     Swish(x) = x \\cdot sigmoid(x)     \\] <pre><code># An ordinary implementation of Swish function\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\n# A memory-efficient implementation of Swish function\nclass SwishImplementation(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_tensors[0]\n        sigmoid_i = torch.sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\nclass MemoryEfficientSwish(nn.Module):\n    def forward(self, x):\n        return SwishImplementation.apply(x)\n</code></pre> <ul> <li> <p><code>ctx</code>\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5bf9\u8c61\uff0c\u53ef\u4ee5\u5229\u7528<code>save_for_backward</code>\u6765\u4fdd\u5b58<code>tensors</code>\uff0c\u5728<code>backward</code>\u9636\u6bb5\u53ef\u4ee5\u8fdb\u884c\u83b7\u53d6</p> </li> <li> <p><code>ctx.needs_input_grad</code>\u4f5c\u4e3a\u4e00\u4e2a<code>bool</code>\u4e5f\u53ef\u4ee5\u7528\u6765\u63a7\u5236\u6bcf\u4e00\u4e2a<code>input</code>\u662f\u5426\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\uff1a<code>ctx.needs_input_grad[0] = False</code>\uff0c\u8868\u793a<code>forward</code>\u91cc\u7684\u7b2c\u4e00\u4e2a<code>input</code>\u4e0d\u9700\u8981\u68af\u5ea6\uff0c\u82e5\u6b64\u65f6\u6211\u4eec<code>return</code>\u8fd9\u4e2a\u4f4d\u7f6e\u7684\u68af\u5ea6\u503c\u7684\u8bdd\uff0c\u4e3a<code>None</code>\u5373\u53ef</p> </li> </ul> <p>\u4f8b1\uff1a\u7f16\u5199\\(y = e^x\\)\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff1a</p> <pre><code>class Expfunc(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        output = torch.exp(x)\n        ctx.save_for_backward(x)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, =ctx.saved_tensors\n        return grad_output * torch.exp(x)\n</code></pre> <p>\u4f8b2\uff1a\u7f16\u5199\\(y = xW^T + b\\)\u7684\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b</p> <pre><code>class LinearFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, w, bias=None):\n        ctx.save_for_backward(x, w, bias)\n        if bias != None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, w, bias = ctx.saved_tensors\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output @ weight   # \u590d\u5408\u51fd\u6570\u6c42\u5bfc\uff0c\u94fe\u5f0f\u6cd5\u5219\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t() @ x   # \u590d\u5408\u51fd\u6570\u6c42\u5bfc\uff0c\u94fe\u5f0f\u6cd5\u5219\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0).squeeze(0)\n\n        return grad_input, grad_weight, grad_bias\n</code></pre>"},{"location":"python/Pytorch/code/2/","title":"\u90a3\u4e9b\u4f60\u5e94\u8be5\u4f1a\u7684Pytorch\u64cd\u4f5c","text":""},{"location":"python/Pytorch/code/2/#chapter-2dropout\u4e0edropconnect","title":"\u3010Chapter 2\u3011\uff1aDropout\u4e0eDropConnect","text":""},{"location":"python/cs224w-notebook/chapter1/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p>Optional Readings\uff1a</p> <p>DeepWalk: Online Learning of Social Representations</p> <p>node2vec: Scalable Feature Learning for Networks</p> <p>Network Embedding as Matrix Factorization</p> <p></p> <p><code>NetworkX</code>\u63d0\u4f9b\u4e86\u591a\u4e2a\u7c7b\u6765\u5b58\u50a8\u4e0d\u540c\u7c7b\u578b\u7684\u56fe\uff0c\u5982\u6709\u5411\u56fe\u548c\u65e0\u5411\u56fe\u3002\u5b83\u8fd8\u63d0\u4f9b\u4e86\u7528\u4e8e\u521b\u5efa\u591a\u91cd\u56fe\uff08\u6709\u5411\u56fe\u548c\u65e0\u5411\u56fe\uff09\u7684\u7c7b\u3002</p> <pre><code>import networkx as nx\n\n# Create an undirected graph G\nG = nx.Graph()\nprint(G.is_directed())  # False\n\n# Create a directed graph H\nH = nx.DiGraph()\nprint(H.is_directed())  # True\n\n# Add graph level attribute\nG.graph[\"Name\"] = \"Bar\"\nprint(G.graph)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter1/#node","title":"Node","text":"<pre><code># Add one node with node level attributes\nG.add_node(0, feature=5, label=1)\n\n# Get attributes of the node 0\nnode_0_attr = G.nodes[0] # {'feature': 5, 'label': 1}\n\nG.nodes(data=True) # {0: {'feature': 5, 'label': 1}} \n\nG.add_nodes_from([\n  (1, {\"feature\": 1, \"label\": 1}),\n  (2, {\"feature\": 2, \"label\": 2})\n]) # add more nodes through list\n\n# Get number of nodes\nnum_nodes = G.number_of_nodes()\n\n# Loop through all nodes \nfor node in G.nodes(data=True):\n    print(node)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter1/#edge","title":"Edge","text":"<pre><code># Add one edge with edge weight\nG.add_edge(0, 1, weight=0.5)\n\nG.add_edges_from([\n  (1, 2, {\"weight\": 0.3}),\n  (2, 0, {\"weight\": 0.1})\n])\n\nfor edge in G.edges():\n  print(edge)\n\nnum_edges = G.number_of_edges()\n\nnx.draw(G, with_labels=True)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter1/#nodesrelation","title":"nodes'relation","text":"<pre><code>G.degree[1] # node's degree\nG.degree() # [(0, 2), (1, 2), (2, 2)]\n\nfor neighbor in G.neighbors(1):\n    print(neighbor) \n# 0 2\n</code></pre> <p>PyTorch Geometric (PyG)\u662f\u5173\u4e8ePytorch\u7684\u56fe\u6df1\u5ea6\u5b66\u4e60\u62d3\u5c55\u5e93\u3002</p> <pre><code>import torch\nfrom torch_geometric.datasets import KarateClub \n# \u7a7a\u624b\u9053\u4ff1\u4e50\u90e8\u7684 34 \u540d\u6210\u5458\u7684\u793e\u4ea4\u7f51\u7edc\n\ndataset = KarateClub()\nprint(len(dataset), dataset.num_features, dataset.num_classes) # 1 34 4\n# \u6570\u636e\u96c6\u5305\u542b\u7684\u56fe\u6570\uff0c\u8282\u70b9\u8868\u793a\u5411\u91cf\u7684\u7ef4\u5ea6\uff0c\u8282\u70b9\u7c7b\u522b\u6570\n\ngraph = dataset[0]\nprint(graph)\n# Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34]) \n# train_mask \u9644\u52a0\u5c5e\u6027\uff0c\u63cf\u8ff0\u4e86\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u54ea\u4e9b\u8282\u70b9\u7684\u793e\u533a\u5206\u914d\nprint(graph.num_nodes, graph.num_edges) # 34 156\n\ngraph.has_isolated_nodes() # False \ngraph.has_self_loops() # False\ngraph.is_undirected() # True \n</code></pre>"},{"location":"python/cs224w-notebook/chapter1/#visualize-the-graph","title":"Visualize the graph","text":"<pre><code>import matplotlib.pyplot as plt\nfrom torch_geometric.utils import to_networkx\n\n# Visualization function for NX graph or PyTorch tensor\ndef visualize(h, color, epoch=None, loss=None, accuracy=None):\n    plt.figure(figsize=(7,7))\n    plt.xticks([])\n    plt.yticks([])\n\n    if torch.is_tensor(h):\n        h = h.detach().cpu().numpy()\n        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n        if epoch is not None and loss is not None and accuracy['train'] is not None and accuracy['val'] is not None:\n            plt.xlabel((f'Epoch: {epoch}, Loss: {loss.item():.4f} \\n'\n                       f'Training Accuracy: {accuracy[\"train\"]*100:.2f}% \\n'\n                       f' Validation Accuracy: {accuracy[\"val\"]*100:.2f}%'),\n                       fontsize=16)\n    else:\n        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=False,\n                         node_color=color, cmap=\"Set2\")\n    plt.show()\n\nG = to_networkx(data, to_undirected=True)\nvisualize(G, color=data.y)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter1/#node-embeddings","title":"Node Embeddings","text":"<p>\u5728\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u539f\u59cb\u6570\u636e\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b<code>feature engineering</code>\uff08\u6bd4\u5982\u63d0\u53d6\u7279\u5f81\u7b49\uff09\uff0c\u4f46\u662f\u73b0\u5728\u6211\u4eec\u4f7f\u7528\u8868\u793a\u5b66\u4e60<code>representation learning</code>\u7684\u65b9\u5f0f\u6765\u81ea\u52a8\u5b66\u4e60\u5230\u6570\u636e\u7684\u7279\u5f81\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u3002</p> <p>\u56fe\u7684\u8868\u793a\u5b66\u4e60\uff1aMap nodes into an embedding space, similarity of embeddings between nodes indicates their similarity in the network.For exmaple : Both nodes are close to each other(connected by an edge).</p> <p></p> <p></p> <p>we assume an graph \\(G\\): \\(V\\) is the vertex set and \\(A\\) is the adjacency matrix (assume binary).</p> <p>the adjacency matrix(\u90bb\u63a5\u77e9\u9635): \u8868\u793a\u9876\u70b9\u4e4b\u95f4\u76f8\u90bb\u5173\u7cfb\u7684\u77e9\u9635\uff0c\u82e5\u4e24\u4e2a\u9876\u70b9\u76f8\u90bb\uff0c\u5219\u5bf9\u5e94\u4f4d\u7f6e\u7684\u503c\u4e3a1\uff0c\u5426\u5219\u4e3a0\u3002</p> <p></p> <p>Our goal is to encode nodes making the similarity in the embedding space reflect the similarity in the graph.</p> <p></p> <p>So the definition of similarity function is the key(a  measure of similarity in the original network).</p> \\[ similarity(u,v)  \\approx  \\mathbf{z}_v^T \\mathbf{z}_u \\] <p>Encoder: maps each node to a low-dimensional vector \\(\\mathbf{z}_v\\).we assume \\(Z \\in \\mathbb{R}^{d \\times |V|}\\) as the matrix of embeddings and \\(v \\in \\mathbb{I}^{|V|}\\) as indicator vector.</p> \\[ ENC(v) = \\mathbf{z}_v \\stackrel{simplest}{=} \\mathbf{Z} \\cdot v \\] <p></p>"},{"location":"python/cs224w-notebook/chapter1/#random-walks","title":"Random Walks","text":"<p>\\(P(v|\\mathbf{z}_u)\\)\u662f\u4ece\\(u\\)\u5f00\u59cb\u968f\u673a\u6e38\u8d70\u80fd\u5230\\(v\\)\u7684\u6982\u7387\uff0c\u8861\u91cf\\(u\\)\u548c\\(v\\)\u7684\u76f8\u4f3c\u5ea6\uff0c\u7528\u8282\u70b9embedding\u5411\u91cf\u76f8\u4f3c\u6027\u7b97\u6982\u7387\u3002 </p> <p></p> \\[ \\mathbf{z}^T_u \\mathbf{z}_v = \\text{the probability that u and v co-occur on a random walk over the graph} \\] <p></p> <p>Why random walks?</p> <pre><code>- Expressivity: Flexible stochastic definition of node similarity that incorporates both local and higher-order neighborhood information\n\n- Idea: if random walk starting from node \ud835\udc96 visits \ud835\udc97 with high probability, \ud835\udc96 and \ud835\udc97 are similar (high-order multi-hop information)\n\n- Efficiency: Do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks\n</code></pre> <p>The definition of nearby nodes and our goal to learn a mapping:</p> <ul> <li> <p>\\(N_R(u)\\): neighbourhood of \\(u\\) which can be obtained by random walk</p> </li> <li> <p>\\(f:u \u2192 \\mathbb{R}^d:f(u)=\\mathbf{z}_u\\) </p> </li> <li> <p>Log-likelihood objective:</p> <p>$\\mathop{\\arg\\max}\\limits_{z} \\mathop{\\sum}\\limits_{u \\in V}  \\log P(N_R(u) | \\mathbf{z}_u) $</p> <p>Equivalently,</p> <p>\\(\\mathop{\\arg\\min}\\limits_{z} \u2112 = \\mathop{\\sum}\\limits_{u \\in V} \\mathop{\\sum}\\limits_{v \\in N_R(u)} - \\log P(v | \\mathbf{z}_u)\\)</p> <p>\u4f7f\u76f8\u90bb\u7684nodes\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u6700\u5927\u5316</p> </li> </ul> <p></p> <ul> <li> <p>Parameterize \\(P(v|\\mathbf{z}_u)\\) as a softmax function:</p> \\[ P(v|\\mathbf{z}_u) = \\frac{\\exp(\\mathbf{z}_u^T \\mathbf{z}_v)}{\\sum_{n \\in V} \\exp(\\mathbf{z}_{u}^T \\mathbf{z}_n)} \\] </li> </ul> <p>so,the log-likelihood objective can transfer to:</p> \\[ \u2112 = \\mathop{\\sum}\\limits_{u \\in V} \\mathop{\\sum}\\limits_{v \\in N_R(u)} - \\log \\frac{\\exp(\\mathbf{z}_u^T \\mathbf{z}_v)}{\\sum_{n \\in V} \\exp(\\mathbf{z}_{u}^T \\mathbf{z}_n)} \\] <p></p> <p>Obviously \\(O(|V|^2)\\) complexity, doing this naively is too expensive.</p> <p></p> <p></p> <p>\u5c42\u6b21Softmax\uff08Hierarchical Softmax\uff09\u4f18\u5316\u7b97\u6cd5\uff0c\u907f\u514d\u8ba1\u7b97\u6240\u6709\u8bcd\u7684softmax</p> <p>\u4e0a\u8ff0\u7684\u968f\u673a\u6e38\u8d70\u7b56\u7565\u662f\u5b8c\u5168\u968f\u673a\u7684\uff0c\u56fa\u5b9a\u957f\u5ea6\u7684\u6e38\u8d70\uff0c\u662f\u5426\u9700\u8981\u6539\u8fdb\uff1f</p>"},{"location":"python/cs224w-notebook/chapter1/#deepwalkrandwalk--skip-gram","title":"DeepWalk\uff1aRandWalk + Skip-Gram","text":"<p>Code\uff1ahttps://github.com/phanein/deepwalk</p> <p>DeepWalk\u5c06\u56fe\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff08Word2Vec\uff09\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u968f\u673a\u6e38\u8d70\u5c06\u56fe\u7ed3\u6784\u8f6c\u5316\u4e3a\u8282\u70b9\u5e8f\u5217\uff0c\u7136\u540e\u4f7f\u7528Skip-Gram\u6a21\u578b\u8bad\u7ec3\u8bcd\u5d4c\u5165\uff0c\u7528\u4e8e\u5b66\u4e60\u7f51\u7edc\u4e2d\u9876\u70b9\u7684\u6f5c\u5728\u8868\u793a\u3002</p> <p></p> <p>\u2460\u4ece\u7f51\u7edc\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u5f00\u59cb\u5206\u522b\u8fdb\u884cRandomWalk\u91c7\u6837\uff0c\u5f97\u5230\u5c40\u90e8\u76f8\u5173\u8054\u7684\u8bad\u7ec3\u6570\u636e\uff1b</p> <p>\u2461\u5bf9\u91c7\u6837\u6570\u636e\u8fdb\u884cSkipGram\u8bad\u7ec3\uff0c\u5c06\u79bb\u6563\u7684\u7f51\u7edc\u8282\u70b9\u8868\u793a\u6210\u5411\u91cf\u5316\uff0c\u6700\u5927\u5316\u8282\u70b9\u5171\u73b0\uff0c\u4f7f\u7528Hierarchical Softmax\u6765\u505a\u8d85\u5927\u89c4\u6a21\u5206\u7c7b\u7684\u5206\u7c7b\u5668</p>"},{"location":"python/cs224w-notebook/chapter1/#node2vecbiased-walks","title":"Node2Vec\uff1aBiased Walks","text":"<p>Code\uff1ahttps://github.com/aditya-grover/node2vec</p> <p>Node2Vec\u548c\u968f\u673a\u6e38\u8d70\u7684\u533a\u522b\u662f\u5982\u4f55\u5b9a\u4e49\u76f8\u90bb\u8282\u70b9\u96c6\u2014\u2014\u4ee5\u53ca\u5982\u4f55\u5b9a\u4e49\u968f\u673a\u6e38\u8d70\u7684\u7b56\u7565(\u504f\u968f\u673a\u6e38\u8d70)</p> <p></p> <ul> <li> <p>BFS\uff1a\u8282\u70b9\u529f\u80fd\u89d2\u8272structural equivalence</p> </li> <li> <p>DFS\uff1a\u540c\u8d28\u793e\u7fa4homophily</p> </li> </ul> <p>Node2Vec gives two parameters to control the random walk:</p> <ul> <li> <p>Return parameter \\(p\\): probability of returning to the previous node</p> </li> <li> <p>In-out parameter \\(q\\): the \u201cratio\u201d of BFS vs. DFS</p> </li> </ul> <p>\u5f15\u5165\u8fd9\u4e24\u4e2a\u8d85\u53c2\u6570\\(p\uff0cq\\)\uff0c\u6765\u63a7\u5236\u968f\u673a\u6e38\u8d70\u7684\u7b56\u7565\u3002\u5047\u8bbe\u5f53\u524d\u968f\u673a\u6e38\u8d70\u7ecf\u8fc7\u8fb9\\((t,v)\\)\u5230\u8fbe\u8282\u70b9\\(v\\)\u3002\u5219\u8f6c\u79fb\u7b56\u7565\u9075\u5faa\u4ee5\u4e0b\u516c\u5f0f\uff1a\\(\\pi_{vx}=\\alpha_{pq}(t,x) \\cdot w_{vx}\\)\uff0c\u8f6c\u79fb\u7b56\u7565\u4e3a\\(\\alpha_{pq}(t,x)\\)\uff0c\\(w_vx\\)\u662f\u8282\u70b9\\(v\\)\u4e0e\\(x\\)\u4e4b\u95f4\u7684\u8fb9\u6743\u3002\\(d_{tx}\\)\u4e3a\u8282\u70b9\\(t\\)\u548c\\(x\\)\u4e4b\u95f4\u7684\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\uff1a</p> \\[ \\alpha_{pq}(t,x) =  \\begin{cases} \\frac{1}{p}, &amp; if \\ \\  d_{tx}=0 .\\\\ 1, &amp; if \\ \\ d_{tx}=1  .\\\\ \\frac{1}{q}, &amp; if \\ \\ d_{tx}=2 .\\\\ \\end{cases} \\] <p> </p> <p>Core idea: Embedding nodes so that distances in embedding space reflect node similarities in the original network.</p> <p>Alias\u91c7\u6837</p> <p>Node2vecWalk\u4e2d\u4e0d\u518d\u662f\u968f\u673a\u62bd\u53d6\u90bb\u63a5\u70b9\uff0c\u800c\u662f\u6309\u6982\u7387\u62bd\u53d6\u3002Alias\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u4e00\u4e2a\u975e\u5747\u5300\u5206\u5e03\u8f6c\u5316\u4e3a\u591a\u4e2a\u5747\u5300\u5206\u5e03\u7684\u7ec4\u5408\uff0c\u80fd\u591f\u52a0\u5feb\u91c7\u6837\u901f\u5ea6\uff0c\u521d\u59cb\u5316\u540e\u7684\u91c7\u6837\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a\\(O(1)\\)\uff0c\u9700\u8981\u5b58\u50a8<code>accepet</code>\u4e0e<code>alias</code>\u4e24\u4e2a\u6570\u7ec4\uff0c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3a\\(O(2N)\\)\u3002</p> <p>\u7ed9\u5b9a\u5982\u4e0b\u79bb\u6563\u6982\u7387\u5206\u5e03\uff0c\u6709\\(N\\)\u4e2a\u53ef\u80fd\u53d1\u751f\u7684\u4e8b\u4ef6\u3002\u6bcf\u5217\u77e9\u5f62\u9762\u79ef\u8868\u793a\u8be5\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\uff0c\u67f1\u72b6\u56fe\u4e2d\u6240\u6709\u77e9\u5f62\u7684\u9762\u79ef\u4e4b\u548c\u4e3a1\u3002</p> <p></p> <p>\u518d\u6839\u636e\u8fd9\u4e2a\u77e9\u5f62\uff0c\u8f6c\u6362\u6210\u76f8\u5e94\u7684<code>Accept</code>\u8868\u548c<code>Alias</code>\u8868\u3002</p> <p></p> <p>\u5c06\u6bcf\u4e2a\u4e8b\u4ef6\u7684\u53d1\u751f\u7684\u6982\u7387\u4e58\u4ee5\\(N\\)\uff0c\u6b64\u65f6\u4f1a\u6709\u90e8\u5206\u77e9\u5f62\u7684\u9762\u79ef\u5927\u4e8e1\uff0c\u90e8\u5206\u77e9\u5f62\u7684\u9762\u79ef\u5c0f\u4e8e1\u3002\u5207\u5272\u9762\u79ef\u5927\u4e8e1\u7684\u77e9\u5f62\uff0c\u586b\u8865\u5230\u9762\u79ef\u5c0f\u4e8e1\u7684\u77e9\u5f62\u4e0a\uff0c\u5e76\u4e14\u6bcf\u4e00\u5217\u81f3\u591a\u7531\u4e24\u4e2a\u4e8b\u4ef6\u7684\u77e9\u5f62\u6784\u6210\uff0c\u6700\u7ec8\u7ec4\u6210\u4e00\u4e2a\u9762\u79ef\u4e3a\\(1 \\times N\\)\u7684\u77e9\u5f62\u3002</p> <p></p> <p>\u9996\u5148\u4ece\\(1\\)~\\(N\\)\u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570i\uff0c\u51b3\u5b9a\u4ece\\(1 \\times N\\)\u77e9\u5f62\u4e2d\u9009\u62e9\u7b2c\u51e0\u5217\uff0c\u518d\u751f\u6210\u4e00\u4e2a\u5747\u5300\u968f\u673a\u6570\\(u \\in (0,1)\\)\uff0c\u82e5\u82e5<code>u &lt; Accept[i]</code>\uff0c\u5219\u91c7\u6837<code>i</code>\u5bf9\u5e94\u7684\u4e8b\u4ef6\uff0c\u5426\u5219\u91c7\u6837<code>Alias[i]</code>\u3002</p> <p>\u56e0\u4e3a\u8be5\u91c7\u6837\u8fc7\u7a0b\u4e0d\u9700\u8981\u6839\u636e\u968f\u673a\u6982\u7387\u5728\u533a\u5206\u5ea6\u4e3a\\(N\\)\u7684\u7ebf\u6bb5\u4e2d\u5bfb\u627e\uff0c\u53ea\u9700\u89812\u90091\uff0c\u6240\u4ee5\u590d\u6742\u5ea6\u964d\u4f4e\u81f3\\(O(1)\\)\uff0c\u8fd9\u4e5f\u662f\u5176\u4f18\u4e8e\u4f20\u7edf\u91c7\u6837\u7684\u539f\u56e0\u3002</p> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter1/#matrix-factorization","title":"Matrix Factorization","text":"<p>\u5728Deepwalk\u548cNode2Vec\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u968f\u673a\u6e38\u8d70\u5f97\u5230\u8282\u70b9\u5e8f\u5217\uff0c\u4f7f\u5f97\u8282\u70b9\u76f8\u4f3c\u5ea6\uff08node similarity\uff09\u7684\u5b9a\u4e49\u66f4\u52a0\u590d\u6742</p>"},{"location":"python/cs224w-notebook/chapter1/#limitations","title":"Limitations","text":"<ul> <li> <p>\u65e0\u6cd5\u7acb\u523b\u6cdb\u5316\u5230\u65b0\u52a0\u5165\u7684\u8282\u70b9\uff0c\u65e0\u6cd5\u5904\u7406\u52a8\u6001\u7f51\u7edc\uff08Cannot obtain embeddings for nodes not in the training set. Cannot apply to new graphs, evolving graphs\uff09</p> </li> <li> <p>Cannot capture structural similarity</p> </li> </ul> <p></p> <ul> <li>\u4ec5\u4ec5\u4f7f\u7528\u4e86\u8282\u70b9\u4e4b\u95f4\u7684\u8fde\u63a5\u4fe1\u606f(Cannot utilize node, edge and graph features)</li> </ul>"},{"location":"python/cs224w-notebook/chapter1/#embedding-entire-graphs","title":"Embedding Entire Graphs","text":"<p>The Goal: Embed a subgraph(\u5b50\u56fe) \\(G\\) into a low-dimensional space \\(\\mathbb{R}^d\\)</p> <ul> <li>Approach 1: \u76f4\u63a5\u5bf9\u6240\u6709\u8282\u70b9\u952e\u5165\u6c42\u548c/\u5e73\u5747</li> </ul> \\[ z_G = \\sum_{v \\in G} z_v \\] <ul> <li>Approach 2: \u5f15\u5165\u4e00\u4e2a\u865a\u62df\u8282\u70b9\uff08virtual node\uff09\uff0c\u6c42\u51fa\u865a\u62df\u8282\u70b9\u7684\u5d4c\u5165\u6765\u4ee3\u66ff\u5b50\u56fe\u7684\u5d4c\u5165</li> </ul> <p></p> <ul> <li>Approach 3: Anonymous Walks(\u533f\u540d\u968f\u673a\u6e38\u8d70)</li> </ul> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter1/#implement-a-node-embedding-model","title":"Implement a node embedding model","text":"<p>we usually use <code>nn.Embedding</code> to create an embedding layer.</p> <pre><code>emb_sample = nn.Embedding(num_embedding=4, embedding_dim=8)\n\nemb_sample.weight.data.shape\n# torch.Size([4, 8])\n\nemb_sample.weight \n# Parameter containing:\n#tensor([[-0.6076,  0.6043, -0.9907, -0.2478, -1.1062, -0.8926, -1.8408, -1.9413],\n#        [-0.7810, -0.4061,  0.5273, -0.5027, -1.2964, -0.8965,  0.2274,  0.5739],\n#        [-0.9227, -0.9500,  0.4401,  0.0330, -0.8612, -0.7178, -1.0709, -0.8083],\n#        [-1.4989, -0.2842, -0.3344, -0.5047, -0.4363, -1.3749, -0.6169, -2.5654]],\n#       requires_grad=True)\n</code></pre> <pre><code>def create_node_emb(num_node=34, embedding_dim=8):\n    emb = nn.Embedding(num_embeddings=num_node, embedding_dim=embedding_dim)\n    emb.weight.data = torch.rand(num_node, embedding_dim)\n    # emb.weight.data.normal_()\n    return emb\n\nemb = create_node_emb()\n</code></pre>"},{"location":"python/cs224w-notebook/chapter1/#train-the-embedding","title":"Train the embedding","text":"<pre><code>import torch.nn as nn\nfrom torch.optim import SGD\nfrom torch.nn.functional import sigmoid\n\ndef accuracy(pred, label):\n    return sum(torch.round(pred) == label) / len(pred)\n\ndef train(emb, loss_fn, train_label, train_edge):\n    epochs = 500\n    learning_rate = 0.1\n\n    optimizer = SGD(emb.parameters(), lr=learning_rate, momentum=0.9)\n\n    for i in range(epochs):\n        optimizer.zero_grad()\n        pred = sigmoid(torch.sum(emb(train_edge)[0].mul(emb(train_edge)[1]),1))\n        loss = loss_fn(pred, train_label)        \n        loss.backward()  # Derive gradients.\n        optimizer.step()  # Update parameters based on gradients.                \n        print(\"Epoch {} Loss: {}, Accuracy: {}\".format(i,loss,accuracy(pred, train_label)))\n    return emb    \n</code></pre>"},{"location":"python/cs224w-notebook/chapter10/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p>"},{"location":"python/cs224w-notebook/chapter10/#subgraph","title":"Subgraph","text":"<p>Def 1.:Node-induced subgraph</p> <p>Given a graph \\(G=(V,E)\\), a subgraph \\(G'=(V',E')\\) is a node induced subgraph iff \\(V'\\subseteq V\\) and \\(E=\\{(u,v) \\in E|u,v \\in V'\\}\\).</p> <p>induced subgraph(\u8bf1\u5bfc\u5b50\u56fe)\uff1a\u8be5\u56fe\u9876\u70b9\u7684\u5b50\u96c6\u548c\u8be5\u5b50\u96c6\u7684\u6240\u6709\u8fb9\u7684\u96c6\u5408\uff08determined by nodes\uff09</p> <p>Def 2.:Edge-induced subgraph</p> <p>\\(G'=(V',E')\\) is an edge induced subgraph iff \\(E'\\subseteq E\\) and \\(V'=\\{ v \\in V | (v,u) \\in E' for some u \\}\\)</p> <p>non-induced subgraph(\u975e\u8bf1\u5bfc\u5b50\u56fe)\uff1a\u8be5\u56fe\u8fb9\u7684\u5b50\u96c6\u548c\u8be5\u5b50\u96c6\u7684\u5bf9\u5e94\u9876\u70b9\u7684\u96c6\u5408\uff08determined by edges\uff09</p>"},{"location":"python/cs224w-notebook/chapter10/#graph-isomorphism-\u56fe\u540c\u6784","title":"Graph Isomorphism (\u56fe\u540c\u6784)","text":"<p>\u5373\\(G_1\\)\u4e2d\u7684\u8282\u70b9\u80fd\u4e00\u4e00\u6620\u5c04\u5230\\(G_2\\)\u4e2d\u7684\u8282\u70b9\uff0c\u4f7f\u8282\u70b9\u4e4b\u95f4\u5bf9\u5e94\u7684\u8fb9\u5173\u7cfb\u4e5f\u80fd\u540c\u65f6\u6620\u5c04\u5230\u53e6\u4e00\u4e2a\u56fe\u6240\u5bf9\u5e94\u7684\u8282\u70b9\u4e4b\u95f4</p> <p></p> <p>\u56e0\u4e3a\u8282\u70b9\u6ca1\u6709\u56fa\u5b9a\u987a\u5e8f\uff0c\u6240\u4ee5\u6211\u4eec\u4e0d\u77e5\u9053\u8282\u70b9\u4e4b\u95f4\u662f\u600e\u4e48\u6620\u5c04\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u904d\u5386\u6240\u6709\u53ef\u80fd\u3002\u68c0\u9a8c\u56fe\u662f\u5426\u540c\u6784\u7684\u95ee\u9898\u662f\u5426NP-hard\u672a\u77e5\uff0c\u4f46\u81f3\u4eca\u6ca1\u6709\u63d0\u51fapolynomial algorithm\u3002</p>"},{"location":"python/cs224w-notebook/chapter10/#network-motifs-\u7f51\u7edc\u6a21\u4f53","title":"Network motifs (\u7f51\u7edc\u6a21\u4f53)","text":"<p>Network motifs: \u201crecurring, significant patterns of interconnections\u201d </p> <p></p> <p></p> <ul> <li>Graph-level Subgraph Frequency Definition</li> </ul> <p></p> <ul> <li>Node-level Subgraph Frequency Definition</li> </ul> <p></p> <p>\u8fd9\u79cd\u5b9a\u4e49\u5bf9\u5f02\u5e38\u503c\u6bd4\u8f83\u9c81\u68d2.\u5982\u5728\u56fe\u4f8b\u4e2d\uff0cStar subgraph\u4ee5\u4e2d\u5fc3\u8282\u70b9\u4e3aanchor\uff0c\u5176\u5728\\(G_T\\)\u4e2d\u7684frequency\u5c31\u662f1\uff1b\u82e5\u4ee5\u5176\u5916\u56f4\u8282\u70b9\u4f5c\u4e3aanchor\uff0c\u5219\u5176frequency\u5c31\u662f100.</p> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter10/#define-random-graphs","title":"Define Random Graphs","text":"<ul> <li>Erd\u0151s\u2013R\u00e9nyi(ER) random graphs:</li> </ul> <p>\\(G_{n,p}\\):undirected graph on \\(n\\) nodes where each \\((u,v)\\) appears i.i.d.(\u72ec\u7acb\u540c\u5206\u5e03) with probability \\(p\\).</p> <p></p> <p>Goal:Generate a random graph with a given degree sequence \\(k_1,k_2,...,k_N\\).(Configuration model)</p> <p></p> <p></p> <p>Z-score captrues statistical significance of motif \\(i\\):</p> \\[ Z_i = \\frac{N_i^{real} - \\bar N_i^{rand}}{std(N_i^{rand})} \\] <ul> <li> <p>\\(N_i^{real}\\) is motif \\(i\\) in graph \\(G\\).</p> </li> <li> <p>\\(\\bar N_i^{rand}\\) is the average number of motif \\(i\\) in random graphs with the same degree sequence as \\(G\\).</p> </li> </ul> <p>Network significance profile(\\(SP\\)):</p> \\[ SP_i = \\frac{Z_i}{\\sqrt{\\sum_j Z_j^2}} \\] <p>\\(SP\\) is a vector of normalized Z-scores,the dim depends on number of motifs considered.</p> <p></p>"},{"location":"python/cs224w-notebook/chapter11/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p>"},{"location":"python/cs224w-notebook/chapter11/#geometric-graphs","title":"Geometric Graphs","text":"<p>A geometric graph \\(G=(A,S,X)\\) is a graph where each node is embeddedd in \\(d\\)-dimensional Euclidean space:</p> <p></p> <ul> <li> <p>\\(A\\): an \\(n \\times n\\) adjacency matrix</p> </li> <li> <p>\\(S \\in \\mathbb{R}^{n \\times f}\\): scalar features</p> </li> <li> <p>\\(X \\in \\mathbb{R}^{n \\times d}\\): tensor features(e.g.,coordinates)</p> </li> </ul> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter11/#geometric-gnns","title":"Geometric GNNs","text":"<ul> <li> <p>Invariant GNNs for learning invariant scalar features</p> </li> <li> <p>Equivariant GNNs for learning equivariant tensor features</p> </li> </ul>"},{"location":"python/cs224w-notebook/chapter11/#invariant-gnns-schnet","title":"Invariant GNNs: SchNet","text":""},{"location":"python/cs224w-notebook/chapter2/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p>"},{"location":"python/cs224w-notebook/chapter2/#the-limitation-of-node-embedding","title":"The limitation of node embedding","text":"<ul> <li> <p>\\(O(|V|d)\\) parameters are needed\uff1aNo sharing of parameters between nodes\uff0cevery node has its own unique embedding</p> </li> <li> <p>Have no ability to generate embeddings for nodes that are not in the training set</p> </li> <li> <p>Do not incorporate structural node features (e.g. node type, node degree)</p> </li> </ul>"},{"location":"python/cs224w-notebook/chapter2/#permutation-invariance\u7f6e\u6362\u4e0d\u53d8\u6027","title":"Permutation Invariance(\u7f6e\u6362\u4e0d\u53d8\u6027)","text":"<p>For order plan 1 and order plan 2, graph and node representation should be the same, but the node embeddings are different.</p> <p>Consider we learn a function \\(f:\\mathbb{R}^{|V| \\times m}\\times \\mathbb{R}^{|V| \\times |V|}\\) to map the graph \\(G=(A,X)\\) to a vector \\(\\mathbb{R}^d\\), then the function \\(f\\) should be permutation invariant: \\(f(A,X) = f(A',X')=f(PAP^T,PX)\\) for any permutation \\(P\\).</p> <p>Permutation \ud835\udc43: a shuffle of the node order.Example:\\((A,B,C)-&gt;(B,C,A)\\).</p> <p>for different order of nodes, the adjacency matrix \\(A\\) is different, but the output of \\(f\\) should be the same!.</p> <p></p>"},{"location":"python/cs224w-notebook/chapter2/#permutation-equivariant\u7f6e\u6362\u7b49\u53d8\u6027","title":"Permutation Equivariant(\u7f6e\u6362\u7b49\u53d8\u6027)","text":"<p>Consider we learn a function \\(f:\\mathbb{R}^{|V| \\times m}\\times \\mathbb{R}^{|V| \\times |V|}\\) to map the graph \\(G=(A,X)\\) to a vector \\(\\mathbb{R}^{|V| \\times d}\\).then the function \\(f\\) should be permutation equivariant: \\(Pf(A,X) =f(PAP^T,PX)\\) for any permutation \\(P\\).</p> <p> </p> <p>Idea: Node\u2019s neighborhood defines a computation graph</p> <p> </p> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter2/#graph-neural-networks","title":"Graph Neural Networks","text":"\\[ \\begin{aligned} h_v^{(0)} =&amp; x_v \\\\ h_v^{(k+1)} =&amp; \\sigma(W_k \\sum_{u\\in N(v)} \\frac{h_u^{(k)}}{|N(v)|} + B_k h_v^{(k)}), \u2200k \\in \\{ 0,...,k-1 \\}\\\\ z_v =&amp; h_v^{(K)}(\\text{Final node embedding})\\\\ \\end{aligned} \\] <p>\u8bbe\\(H^{(k)}=[h_1^{(k)},...,h_{|V|}^{(k)}]^T\\)\uff0c\u5219\\(\\sum_{u \\in N_v} h_u^{(k)}=A_{v,:}H^{(k)}\\)</p> <p>A \u4e3a\u4e00\u4e2a\u7a00\u758f\u7684\u5355\u4f4d\u77e9\u9635\uff0cExample:\\(\\begin{bmatrix} 1 &amp; 0 &amp; ... &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; ... &amp; 0 &amp; 1 &amp; 0 \\\\ ...  \\\\ 1 &amp; 0 &amp; ... &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix}\\)</p> <p>\u8bbe\u5bf9\u89d2\u77e9\u9635\uff08diagonal matrix\uff09\\(D\\),\u5373\\(D_{v,v}=Deg(v)=|N(v)|\\),\u5219\\(D_{v,v}^{-1}=1/|N(v)|\\).</p> <p>Therefore,\\(\\sum_{u \\in N(v)} \\frac{h_u^{(k-1)}}{|N(v)|} \\rightarrow H^{(k+1)} = D^{-1}AH^{(k)}\\)</p> <p>so\uff0c$H^{(k+1)} = \\sigma (D^{-1} A H^{(k)} W_k^T + H^{(k)} B_k^T) $</p> <p></p>"},{"location":"python/cs224w-notebook/chapter2/#graph-unsupervised-training","title":"Graph unsupervised training","text":""},{"location":"python/cs224w-notebook/chapter2/#graph-supervised-training","title":"Graph supervised training","text":"<pre><code>import torch\nfrom torch.nn import Linear\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(1234)\n        self.conv1 = GCNConv(dataset.num_features, 4)\n        self.conv2 = GCNConv(4, 4)\n        self.conv3 = GCNConv(4, 2)\n        self.classifier = Linear(2, dataset.num_classes)\n\n    def forward(self, x, edge_index):\n        h = self.conv1(x, edge_index)\n        h = h.tanh()\n        h = self.conv2(h, edge_index)\n        h = h.tanh()\n        h = self.conv3(h, edge_index)\n        h = h.tanh()  # Final GNN embedding space.\n\n        # Apply a final (linear) classifier.\n        out = self.classifier(h)\n\n        return out, h\n\nmodel = GCN()\n\n_, h = model(data.x, data.edge_index)\nprint(f'Embedding shape: {list(h.shape)}')\n\nvisualize(h, color=data.y)\n</code></pre> <p>\u6839\u636e\u4e0a\u8ff0\uff0cGNN\u7684\u76ee\u6807\u662f\u83b7\u53d6\u4e00\u4e2a\u8f93\u5165\u56fe\\(G=(\\mathbb{V,E})\\)\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8282\u70b9\\(v_i \\in \\mathbb{V}\\)\u90fd\u6709\u4e00\u4e2a\u8f93\u5165\u7279\u5f81\u5411\u91cf\\(X_i\\)\uff0c\u4ee5\u6b64\u5b66\u4e60\u4e00\u4e2a\u51fd\u6570$f_G : \\mathbb{V} \\times \\mathbb{R}^{d_1} \\to \\mathbb{R}^{d_2} $\uff0c\u8be5\u51fd\u6570\u63a5\u6536\u4e00\u4e2a\u8282\u70b9\u53ca\u5176\u7279\u5f81\u5411\u91cf\u4ee5\u53ca\u56fe\u7ed3\u6784\uff0c\u5e76\u8f93\u51fa\u4e00\u4e2a\u5d4c\u5165\uff0c\u5373\u4e00\u4e2a\u4ee5\u5bf9\u6211\u4eec\u7684\u4e0b\u6e38\u4efb\u52a1\u6709\u7528\u7684\u65b9\u5f0f\u8868\u793a\u8be5\u8282\u70b9\u7684\u5411\u91cf\u3002</p> <pre><code>criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n\ndef train(data):\n    optimizer.zero_grad()  # Clear gradients.\n    out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n    loss.backward()  # Derive gradients.\n    optimizer.step()  # Update parameters based on gradients.\n\n    accuracy = {}\n    # Calculate training accuracy on our four examples\n    predicted_classes = torch.argmax(out[data.train_mask], axis=1) # [-0.6, 0.2, 0.7, 0.1] -&gt; 2\n    target_classes = data.y[data.train_mask]\n    accuracy['train'] = torch.mean(\n        torch.where(predicted_classes == target_classes, 1, 0).float())\n\n    # Calculate validation accuracy on the whole graph\n    predicted_classes = torch.argmax(out, axis=1)\n    target_classes = data.y\n    accuracy['val'] = torch.mean(\n        torch.where(predicted_classes == target_classes, 1, 0).float())\n\n    return loss, h, accuracy\n\nfor epoch in range(1,501):\n    loss, h, accuracy = train(data)\n    # Visualize the node embeddings every 10 epochs\n    if epoch % 10 == 0:\n        visualize(h, color=data.y, epoch=epoch, loss=loss, accuracy=accuracy)\n</code></pre> <p></p>"},{"location":"python/cs224w-notebook/chapter2/#comparison-with-other-methods","title":"comparison with other methods","text":"<p>\u4ee5 <code>karate club</code>\u4e3a\u4f8b\uff1a</p> <pre><code>G = nx.karate_club_graph()\n</code></pre>"},{"location":"python/cs224w-notebook/chapter2/#q1average-degree-if-a-graph","title":"Q1:Average Degree if a graph","text":"<pre><code>def average_degree(num_edges, num_nodes):\n    average_degree = 0\n    average_degree = round(2* num_edges / num_nodes)\n    return average_degree\n\nnum_edges = G.number_of_edges()\nnum_nodes = G.number_of_nodes()\navg_degree = average_degree(num_edges, num_nodes)\nprint(f'Average Degree:{avg_degree}')\n</code></pre>"},{"location":"python/cs224w-notebook/chapter2/#q2the-average-clustering-coefficient-of-the-graph","title":"Q2:the average clustering coefficient of the graph","text":"<p>\u5e73\u5747\u805a\u7c7b\u7cfb\u6570(average clustering coefficient):\u63cf\u8ff0\u4e00\u4e2a\u56fe\u4e2d\u7684\u9876\u70b9\u4e4b\u95f4\u96c6\u6210\u56e2(clique)\u7684\u7a0b\u5ea6\u7cfb\u6570\u3002\u5373\u4e00\u4e2a\u70b9\u7684\u90bb\u63a5\u70b9\u4e4b\u95f4\u76f8\u4e92\u8fde\u63a5\u7684\u7a0b\u5ea6\u3002\u82e5\u4e00\u4e2a\u8282\u70b9\\(i\\)\u7684\u5ea6\u4e3a\\(k_i\\)\uff0c\\(e_i\\)\u4e3a\u8be5\u8282\u70b9\u4e0e\u90bb\u5c45\u4e4b\u95f4\u5b58\u5728\u7684\u8fb9\u6570\uff0c\u5219\\(\\(C_i=\\frac{2e_i}{k_i(k_i-1)}\\)\\)</p> <pre><code>def average_clustering_coefficient(G):\n    avg_clustering_coef = 0\n    avg_clustering_coef = round(nx.average_clustering(G), 2)\n    return avg_clustering_coef\n\navg_cluster_coef = average_clustering_coefficient(G)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter2/#q3pagerank","title":"Q3:PageRank","text":"<ul> <li>\u7ecf\u5178\u56fe\u7b97\u6cd5\uff1aPageRank</li> </ul> <p>PageRank measures importance of nodes in a graph based on its link structure.</p> <p>Core idea:</p> <ul> <li> <p>The more pages link to this page, the more important it is;</p> </li> <li> <p>A link from an important page is worth more.</p> </li> </ul> <p>If a page \\(i\\) with importance \\(r_i\\) has \\(d_i\\) out-links, then each link gets \\(\\frac{r_i}{d_i}\\) votes.Thus, the importance of a page \\(j\\), represented as \\(r_j\\) is the sum of the votes on its in links.</p> \\[ r_j = \\sum_{i \\rightarrow j} \\frac{r_i}{d_i} \\] <p>, where \\(d_i\\) is the out degree of node \\(i\\).</p> <p>The PageRank algorithm (used by Google) outputs a probability distribution which represent the likelihood of a random surfer clicking on links will arrive at any particular page.</p> <p>At each time step, the random surfer has two options:</p> <ul> <li> <p>with prob. \\(\\beta\\), follow a link at random</p> </li> <li> <p>with prob. \\(1- \\beta\\), jump to a random page</p> </li> </ul> \\[ r_j = \\beta \\sum_{i \\rightarrow j} \\frac{r_i}{d_i} + (1-\\beta) \\frac{1}{N} \\] <p>What is the PageRank value for node 0 after one PageRank iteration?</p> <pre><code>def one_iter_pagerank(G, beta, r0, node_id):\n    r1 = (1 - beta) / G.number_of_nodes()\n    for neighbor in G.neighbors(node_id):\n        r1 += beta * r0 / G.degree(neighbor)\n    # PageRank_list = nx.pagerank(G, alpha=beta)\n    return round(r1, 2)\n\nbeta = 0.8\nr0 = 1 / G.number_of_nodes()\nnode = 0\nr1 = one_iter_pagerank(G, beta, r0, node)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter2/#q4the-raw-closeness-centrality","title":"Q4:the (raw) closeness centrality","text":"<p>\u90bb\u8fd1\u4e2d\u5fc3\u5ea6(closeness centrality)\u8861\u91cf\u7f51\u7edc\u4e2d\u8282\u70b9\u5230\u5176\u4ed6\u8282\u70b9\u7684\u5e73\u5747\u8ddd\u79bb,\u8ddd\u79bb\u8d8a\u77ed\u8868\u793a\u8282\u70b9\u8d8a\u63a5\u8fd1\u7f51\u7edc\u4e2d\u7684\u5176\u4ed6\u8282\u70b9,\u5176Closeness Centrality\u503c\u8d8a\u9ad8\u3002\\(\\(c(v) = \\frac{1}{\\sum_{u \\neq v}\\text{shortest path length between } u \\text{ and } v}\\)\\)</p> <pre><code>def closeness_centrality(G, node=5):\n    clossness = nx.clossness_centrality(G, node) / (len(nx.node_connected_component(G, node)) - 1)\n    clossness = round(clossness, 2)\n    return clossness\n\nnode = 5\ncloseness = closeness_centrality(G,  node=node)\n</code></pre> <p><code>nx.clossness_centrality</code>\u8f93\u51fa\u7684\u662f\\(\\frac{n-1}{\\sum_{u \\neq v} d(u,v)}\\)\uff0c\u5176\u4e2d\\(d(u,v)\\)\u8868\u793a\u8282\u70b9\\(u\\)\u548c\\(v\\)\u4e4b\u95f4\u7684\u6700\u77ed\u8def\u5f84\u957f\u5ea6\u3002</p>"},{"location":"python/cs224w-notebook/chapter2/#q5get-the-edge_list-transform-it-into-torchlongtensor","title":"Q5:get the edge_list ,transform it into torch.LongTensor","text":"<pre><code>def graph_to_edge_list(G):\n    return [edge for edge in G.edges()]\ndef edge_list_to_tensor(edge_list):\n    return torch.LongTensor(edge_list).t()\n\npos_edge_list = graph_to_edge_list(G)\npos_edge_index = edge_list_to_tensor(pos_edge_list)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter2/#q6negative-sampling","title":"Q6:negative sampling","text":"<p>\"Negative\" edges \u6307\u7684\u662f\u56fe\u4e2d\u4e0d\u5b58\u5728\u7684\u8fb9\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u8d1f\u6837\u672c</p> <pre><code>import random\n\ndef sample_negative_edges(G, num_neg_samples):\n    neg_edge_list = [random.sample(list(enumerate(nx.non_edges(G))), num_neg_samples)[i][1] for i in range(num_neg_samples)]\n    return neg_edge_list\n\ndef can_be_negative(G, edge):\n    return not G.has_edge(*edge)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter3/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p>"},{"location":"python/cs224w-notebook/chapter3/#a-gnn-layer","title":"A GNN Layer","text":""},{"location":"python/cs224w-notebook/chapter3/#message-computation","title":"Message Computation","text":"<ul> <li>Message function: \\(m_u^{(l)} = MSG^{(l)} (h_u^{(l-1)})\\), For example: A Linear layer $m_u^{(l)} = W^{(l)} h^{(l-1)} $</li> </ul> <p>Intuition: Each node will create a message, which will be sent to other nodes later</p> <p></p>"},{"location":"python/cs224w-notebook/chapter3/#aggregation","title":"Aggregation","text":"<ul> <li>Aggregation function: \\(h_v^{(l)} = AGG^{(l)} (m_{v1}^{(l)}, m_{v2}^{(l)}, \\ldots, m_{vk}^{(l)})\\), For example: Summing up all messages \\(h_v^{(l)} = \\sum_{u \\in N(v)} m_u^{(l)}\\)</li> </ul> <p>Intuition: Node \ud835\udc63 will aggregate the messages from its neighbors </p> <p>In order to consider the node \\(v\\) itself, we can include \\(h_v^{(l-1)}\\) when computing \\(h_v^{(l)}\\).For example: </p> \\[ h_v^{(l)} = Concat(AGG({m_u^{(l)}}, u \\in N(v)), m_v^{(l)}) \\] <p></p>"},{"location":"python/cs224w-notebook/chapter3/#graph-convolutional-network-gcn","title":"Graph Convolutional Network (GCN)","text":"<p>\u8bba\u6587\u6807\u9898: Semi-Supervised Classification with Graph Convolutional Networks</p> <p>\u5047\u8bbe\u4e00\u4e2a\u4e0b\u56fe\u8fd9\u6837\u7684\u56fe\u7ed3\u6784\uff0c\u5b58\u5728ABCDE\u4e94\u4e2a\u8282\u70b9\uff0c\u8282\u70b9\u4e4b\u95f4\u5b58\u5728\u8fde\u63a5</p> <p></p> <p>\u56e0\u6b64\uff0c\u53ef\u4ee5\u8fd9\u6837\u805a\u5408\u4fe1\u606f\uff1a</p> <p></p> <p>\u4f46\u663e\u7136\u8fd9\u6837\u7684\u805a\u5408\u65b9\u5f0f\u8fc7\u4e8e\u7b80\u5355\uff0c\u6ca1\u6709\u8003\u8651\u5230\u8282\u70b9\u81ea\u8eab\u4fe1\u606f\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u6539\u8fdb\u90bb\u63a5\u77e9\u9635\uff1a</p> \\[ \\widetilde{A} = A + I \\] <p>\u5c06\u8282\u70b9\u81ea\u8eab\u4e0e\u5176\u8fd1\u90bb\u7c97\u66b4\u7684\u52a0\u548c\u7684\u805a\u5408\u65b9\u6cd5\u663e\u7136\u662f\u6709\u95ee\u9898\u7684\uff0c\u76f8\u5f53\u4e8e\u6211\u4eec\u53d8\u76f8\u7684\u6539\u53d8\u4e86\u7279\u5f81\u7684\u91cf\u7ea7\uff0c\u968f\u7740\u8fed\u4ee3\u7684\u589e\u52a0\uff0c\u7279\u5f81\u91cf\u7ea7\u4f1a\u53d8\u5f97\u8d8a\u6765\u8d8a\u5927\u3002\u56e0\u6b64\u6211\u4eec\u5f15\u5165\\(D=Deg(N(v))= \\sum_j A_{ij}\\)\u7684\u5ea6\u77e9\u9635\uff08\u4e0e\u8be5\u8282\u70b9\u76f8\u90bb\u8282\u70b9\u7684\u6570\u636e\uff09,\u5e76\u4e14\u8003\u8651\u81ea\u8eab\u4fe1\u606f\uff0c\u5728\\(D\\)\u4e0a\u52a0\u5165\u5355\u4f4d\u77e9\u9635\\(I\\)\uff08\u81ea\u73af\uff09\uff1a\\(\\widetilde{D}=D+I=\\sum_j \\widetilde{A}_{ij}\\)</p> <p>\u6700\u7ec8\uff0c\u6211\u4eec\u5f97\u5230\uff1a\\(\\widetilde{D}^{-1} \\widetilde{A} X\\)</p> <p>\\(\\widetilde{D}^{-1}\\)\u4f5c\u4e3a\u4e00\u4e2a\u521d\u7b49\u77e9\u9635\uff0c\u5de6\u4e58\\(\\widetilde{D}^{-1}\\)\u76f8\u5f53\u4e8e\u884c\u53d8\u6362\uff0c\u5373\\(\\widetilde{A} X\\) \u6bcf\u4e00\u884c\u9664\u4ee5\u5ea6.</p> <ul> <li>Renormalization trick: \\(\\widetilde{D}^{-\\frac{1}{2}} \\widetilde{A} \\widetilde{D}^{-\\frac{1}{2}} X\\)</li> </ul> <p>\\(\\widetilde{A}\\)\u5de6\u53f3\u90fd\u4e58\u4ee5\u4e00\u4e2a\u521d\u7b49\u77e9\u9635\\(\\widetilde{D}^{-\\frac{1}{2}}\\)\uff0c\\(\\widetilde{A}_{ij}\\)\u8868\u793a\\(\\widetilde{A}\\)\u4e2d\u7b2c\\(i\\)\u884c\u7b2c\\(j\\)\u5217\u7684\u5143\u7d20\uff0c\u5373\u4e3a\\(i\\)\u8282\u70b9\u4e0e\\(j\\)\u8282\u70b9\u7684\u5173\u7cfb\u3002\u5de6\u4fa7\u7684\\(\\widetilde{D}^{-\\frac{1}{2}}\\)\u5f88\u597d\u7406\u89e3\uff0c\u5373\u662f\u5bf9\u91cf\u5ea6\u7684\u5f52\u4e00\u5316\uff0c\u800c\u53f3\u4fa7\u7684\\(\\widetilde{D}^{-\\frac{1}{2}}\\)\u5219\u662f\u5bf9\u8282\u70b9\u95f4\u5173\u7cfb\u7684\u5f52\u4e00\u5316\uff0c\u5373\u662f\u5c06\u8282\u70b9\u7684\u90bb\u63a5\u8282\u70b9\u5bf9\u5176\u7684\u201c\u8d21\u732e\u201d\u8fdb\u884c\u6807\u51c6\u5316\u3002</p> <p>\u6bd4\u5982\u8bf4App\u4e0a\u7684\u7528\u6237\u4e4b\u95f4\u7684\u5173\u6ce8\u5173\u7cfb\u662f\u5929\u7136\u7684\u540c\u6784\u56fe\uff0c\u5047\u8bbe\u6211\u4eec\u8981\u505a\u8282\u70b9\u5206\u7c7b\u5224\u5b9a\u67d0\u4e2a\u7528\u6237A\u662f\u4e0d\u662f\u7b97\u6cd5\u5de5\u7a0b\u5e08\uff0c\u5e76\u4e14\u5047\u8bbeA\u7528\u6237\u4ec5\u4ec5\u548c\u53e6\u4e00\u4e2a\u7b97\u6cd5\u5de5\u7a0b\u5e08B\u4ee5\u53ca10\u4e2a\u730e\u5934\u6709\u5173\u6ce8\u7684\u5173\u7cfb\u3002\u76f4\u89c2\u4e0a\uff0c\u730e\u5934\u5bf9\u7528\u6237A\u7684\u8282\u70b9\u7c7b\u522b\u7684\u5224\u5b9a\u7684\u8d21\u732e\u5e94\u8be5\u662f\u5f88\u5c0f\u7684\uff0c\u56e0\u4e3a\u730e\u5934\u5f80\u5f80\u4f1a\u548c\u7b97\u6cd5\uff0c\u5f00\u53d1\uff0c\u6d4b\u8bd5\uff0c\u4ea7\u54c1\u7b49\u4e0d\u540c\u7c7b\u578b\u7684\u7528\u6237\u6709\u5173\u8054\u5173\u7cfb\uff0c\u4ed6\u4eec\u5bf9\u4e8e\u7528\u6237A\u7684\u201c\u5fe0\u8bda\u5173\u8054\u5ea6\u201d\u662f\u5f88\u4f4e\u7684\uff0c\u800c\u5bf9\u4e8e\u7b97\u6cd5\u5de5\u7a0b\u5e08B\u800c\u8a00\uff0c\u5047\u8bbe\u4ed6\u4ec5\u4ec5\u548cA\u6709\u5173\u8054\uff0c\u90a3\u4e48\u660e\u663eB\u5bf9A\u7684\u201c\u5fe0\u8bda\u5173\u8054\u5ea6\u201d\u662f\u5f88\u9ad8\u7684\uff0cB\u7684Node Features\u4ee5\u53caB\u548cA\u7684\u5173\u8054\u5173\u7cfb\u5728\u5bf9A\u8fdb\u884c\u8282\u70b9\u5206\u7c7b\u7684\u65f6\u5019\u5e94\u8be5\u662f\u66f4\u91cd\u8981\u7684\uff01</p> <ul> <li>GCN\u7b97\u6cd5\u6d41\u7a0b</li> </ul> <p>\u56fe\u7f51\u7edc\u7684\u8ba1\u7b97\u5c31\u662f\u4e0d\u65ad\u8003\u8651\u90bb\u5c45\u53ca\u81ea\u8eab\u4fe1\u606f\u7684\u4e00\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u6bcf\u8fdb\u884c\u4e00\u6b21\u8fed\u4ee3\u5c31\u662f\u4e00\u6b21\u7279\u5f81\u91cd\u7ec4\uff0c\u4e0b\u4e00\u5c42\u7684\u7279\u5f81\u4e3a\u4e0a\u4e00\u5c42\u7279\u5f81\u7684\u56fe\u5377\u79ef\uff1a</p> \\[ X^{k+1} = \\widetilde{D}^{-\\frac{1}{2}} \\widetilde{A} \\widetilde{D}^{-\\frac{1}{2}} X^k \\] <p>\u4ee4renormalization\u4e4b\u540e\u7684\u77e9\u9635\u4e3a\\(\\hat{A}\\):</p> \\[ X^{k+1} = \\hat{A} X^k \\] <p>\u4e00\u4e2a\u4e24\u5c42\u7684GCN\u7f51\u7edc\u4e3a(\\(W\\)\u4e3a\u6743\u91cd\u77e9\u9635)\uff1a</p> \\[ Z = \\hat{A} \\sigma (\\hat{A} X^k W^{(0)}) W^{(1)} \\] utils <pre><code>def encode_onehot(labels): #\u72ec\u70ed\u7f16\u7801\n    classes = set(labels)\n    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)} #\u5bf9\u89d2\u5355\u4f4d\u77e9\u9635\n    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n    return labels_onehot\n\n\ndef load_data(path=\"../data/cora/\", dataset=\"cora\"):\n    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n    print('Loading {} dataset...'.format(dataset))\n\n    idx_features_labels = np.genfromtxt(f\"{path}{dataset}.content\", dtype=np.dtype(str))\n    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) #\u7a00\u758f\u77e9\u9635\n    labels = encode_onehot(idx_features_labels[:, -1])\n\n    # build graph\n    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n    idx_map = {j: i for i, j in enumerate(idx)}\n    edges_unordered = np.genfromtxt(f\"{path}{dataset}.cites\", dtype=np.int32)\n    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n                     dtype=np.int32).reshape(edges_unordered.shape)\n    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n    # sp.coo_matrix((data,(row,col)), shape=, dtype=)\n    # build symmetric adjacency matrix\n    adj = adj + adj.T.multiply(adj.T &gt; adj) - adj.multiply(adj.T &gt; adj)\n    # \u5bf9\u79f0\u5316\u90bb\u63a5\u77e9\u9635(\u65e0\u5411\u56fe)\n    features = normalize(features)\n    adj = normalize(adj + sp.eye(adj.shape[0]))\n\n    idx_train = range(140)\n    idx_val = range(200, 500)\n    idx_test = range(500, 1500)\n\n    features = torch.FloatTensor(np.array(features.todense()))\n    labels = torch.LongTensor(np.where(labels)[1])\n    adj = sparse_mx_to_torch_sparse_tensor(adj)\n\n    idx_train = torch.LongTensor(idx_train)\n    idx_val = torch.LongTensor(idx_val)\n    idx_test = torch.LongTensor(idx_test)\n\n    return adj, features, labels, idx_train, idx_val, idx_test\n\n\ndef normalize(mx):\n    \"\"\"Row-normalize sparse matrix\"\"\"\n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1/2).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv) #\u5bf9\u89d2\u77e9\u9635\n    mx = r_mat_inv.dot(mx.dot(r_mat_inv))\n    return mx\n\ndef accuracy(output, labels):\n    preds = output.max(1)[1].type_as(labels)\n    correct = preds.eq(labels).double()\n    correct = correct.sum()\n    return correct / len(labels)\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    # COO\u683c\u5f0f(.tocoo())\u7528\u4e09\u4e2a\u6570\u7ec4\u6765\u8868\u793a\u77e9\u9635\uff1a\n    # \u884c\u7d22\u5f15\u6570\u7ec4.row\u3001\u5217\u7d22\u5f15\u6570\u7ec4.col\u3001\u6570\u636e\u6570\u7ec4.data\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n</code></pre> Model <pre><code>class GraphConvolution(Module):\n    \"\"\"\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self): # \u521d\u59cb\u5316\u6743\u91cd\u548c\u504f\u7f6e\u53c2\u6570\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj):\n        support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)  #torch.spmm\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + str(self.in_features) + ' -&gt; ' \\\n               + str(self.out_features) + ')'\n\nclass GCN(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(GCN, self).__init__()\n\n        self.gc1 = GraphConvolution(nfeat, nhid)\n        self.gc2 = GraphConvolution(nhid, nclass)\n        self.dropout = dropout\n\n    def forward(self, x, adj):\n        x = F.relu(self.gc1(x, adj))\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.gc2(x, adj)\n        return F.log_softmax(x, dim=1)\n</code></pre> Train <pre><code>from __future__ import division\nfrom __future__ import print_function\nimport time\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom pygcn.utils import load_data, accuracy\nfrom pygcn.models import GCN\n\n# Training settings\nparser = argparse.ArgumentParser()\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='Disables CUDA training.')\nparser.add_argument('--fastmode', action='store_true', default=False,\n                    help='Validate during training pass.')\nparser.add_argument('--seed', type=int, default=42, help='Random seed.')\nparser.add_argument('--epochs', type=int, default=200,\n                    help='Number of epochs to train.')\nparser.add_argument('--lr', type=float, default=0.01,\n                    help='Initial learning rate.')\nparser.add_argument('--weight_decay', type=float, default=5e-4,\n                    help='Weight decay (L2 loss on parameters).')\nparser.add_argument('--hidden', type=int, default=16,\n                    help='Number of hidden units.')\nparser.add_argument('--dropout', type=float, default=0.5,\n                    help='Dropout rate (1 - keep probability).')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n# Load data\nadj, features, labels, idx_train, idx_val, idx_test = load_data()\n\n# Model and optimizer\nmodel = GCN(nfeat=features.shape[1],\n            nhid=args.hidden,\n            nclass=labels.max().item() + 1,\n            dropout=args.dropout)\noptimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n\nif args.cuda:\n    model.cuda()\n    features = features.cuda()\n    adj = adj.cuda()\n    labels = labels.cuda()\n    idx_train = idx_train.cuda()\n    idx_val = idx_val.cuda()\n    idx_test = idx_test.cuda()\n\n\ndef train(epoch):\n    t = time.time()\n    model.train()\n    optimizer.zero_grad()\n    output = model(features, adj)\n    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n    acc_train = accuracy(output[idx_train], labels[idx_train])\n    loss_train.backward()\n    optimizer.step()\n\n    if not args.fastmode:\n        # Evaluate validation set performance separately,\n        # deactivates dropout during validation run.\n        model.eval()\n        output = model(features, adj)\n\n    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n    acc_val = accuracy(output[idx_val], labels[idx_val])\n    print('Epoch: {:04d}'.format(epoch+1),\n          'loss_train: {:.4f}'.format(loss_train.item()),\n          'acc_train: {:.4f}'.format(acc_train.item()),\n          'loss_val: {:.4f}'.format(loss_val.item()),\n          'acc_val: {:.4f}'.format(acc_val.item()),\n          'time: {:.4f}s'.format(time.time() - t))\n\n\ndef test():\n    model.eval()\n    output = model(features, adj)\n    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n    acc_test = accuracy(output[idx_test], labels[idx_test])\n    print(\"Test set results:\",\n          \"loss= {:.4f}\".format(loss_test.item()),\n          \"accuracy= {:.4f}\".format(acc_test.item()))\n\n\n# Train model\nt_total = time.time()\nfor epoch in range(args.epochs):\n    train(epoch)\nprint(\"Optimization Finished!\")\nprint(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n\n# Testing\ntest()\n</code></pre>"},{"location":"python/cs224w-notebook/chapter3/#graphsagegraph-sample-and-aggregate","title":"GraphSAGE\uff08Graph Sample and Aggregate\uff09","text":"<p>\u8bba\u6587\u6807\u9898: Inductive Representation Learning on Large Graphs</p> <p><code>GCN</code>\u672c\u8eab\u6709\u4e00\u4e2a\u5c40\u9650\uff0c\u5373\u6ca1\u6cd5\u5feb\u901f\u8868\u793a\u65b0\u8282\u70b9\u3002<code>GCN</code>\u9700\u8981\u628a\u6240\u6709\u8282\u70b9\u90fd\u53c2\u4e0e\u8bad\u7ec3\uff08\u6574\u4e2a\u56fe\u90fd\u4e22\u8fdb\u53bb\u8bad\u7ec3\uff09\u624d\u80fd\u5f97\u5230<code>node embedding</code>\uff0c\u5982\u679c\u65b0<code>node</code>\u6765\u4e86\uff0c\u6ca1\u6cd5\u5f97\u5230\u65b0<code>node</code>\u7684<code>embedding</code>\u3002\u6240\u4ee5\u8bf4\uff0c<code>GCN</code>\u662f<code>transductive</code>\u7684\u3002\uff08<code>Transductive</code>\u4efb\u52a1\u662f\u6307\uff1a\u8bad\u7ec3\u9636\u6bb5\u4e0e\u6d4b\u8bd5\u9636\u6bb5\u90fd\u57fa\u4e8e\u540c\u6837\u7684\u56fe\u7ed3\u6784\uff09</p> <p>\u800c<code>GraphSAGE</code>\u662f<code>inductive</code>\u7684\u3002<code>inductive</code>\u4efb\u52a1\u662f\u6307\uff1a\u8bad\u7ec3\u9636\u6bb5\u4e0e\u6d4b\u8bd5\u9636\u6bb5\u9700\u8981\u5904\u7406\u7684graph\u4e0d\u540c\u3002\u901a\u5e38\u662f\u8bad\u7ec3\u9636\u6bb5\u53ea\u662f\u5728\u5b50\u56fe\uff08<code>subgraph</code>\uff09\u4e0a\u8fdb\u884c\uff0c\u6d4b\u8bd5\u9636\u6bb5\u9700\u8981\u5904\u7406\u672a\u77e5\u7684\u9876\u70b9\u3002</p> <p>\u8981\u60f3\u5f97\u5230\u65b0\u8282\u70b9\u7684\u8868\u793a\uff0c\u9700\u8981\u8ba9\u65b0\u7684<code>node</code>\u6216\u8005<code>subgraph</code>\u53bb\u548c\u5df2\u7ecf\u4f18\u5316\u597d\u7684<code>node embedding</code>\u53bb\u201c\u5bf9\u9f50\u201d\u3002\u7136\u800c\u6bcf\u4e2a\u8282\u70b9\u7684\u8868\u793a\u90fd\u662f\u53d7\u5230\u5176\u4ed6\u8282\u70b9\u7684\u5f71\u54cd\uff08\u7275\u4e00\u53d1\u800c\u52a8\u5168\u8eab\uff09\uff0c\u56e0\u6b64\u6dfb\u52a0\u4e00\u4e2a\u8282\u70b9\uff0c\u610f\u5473\u7740\u8bb8\u8bb8\u591a\u591a\u4e0e\u4e4b\u76f8\u5173\u7684\u8282\u70b9\u7684\u8868\u793a\u90fd\u5e94\u8be5\u8c03\u6574\u3002</p> <p><code>GraphSAGE</code>\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5f52\u7eb3\u5f0f\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u70b9\u4fe1\u606f\u7279\u5f81\u4e3a\u672a\u51fa\u73b0\u8fc7\u7684\uff08unseen\uff09\u7ed3\u70b9\u751f\u6210\u7ed3\u70b9\u5411\u91cf\uff0c\u8fd9\u4e00\u65b9\u6cd5\u4e3a\u540e\u6765\u7684 <code>PinSage</code>\uff08<code>GCN</code> \u5728\u5546\u4e1a\u63a8\u8350\u7cfb\u7edf\u9996\u6b21\u6210\u529f\u5e94\u7528\uff09\u63d0\u4f9b\u4e86\u57fa\u7840\u3002</p> <p>GraphSAGE\u7684\u6838\u5fc3\u601d\u60f3\u5728\u4e8e\u5148\u4f7f\u7528\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u91c7\u6837\u56fa\u5b9a\u6570\u91cf\u7684\u90bb\u5c45\u7ed3\u70b9\uff1b\u7136\u540e\u8fdb\u884c\u805a\u5408</p> <p></p> <ul> <li> <p>\u5728\u56fe\u4e2d\u968f\u673a\u91c7\u6837\u82e5\u5e72\u7ed3\u70b9\uff0c\u7ed3\u70b9\u6570\u4e3a\u8d85\u53c2\u6570<code>batch_size</code>\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u88ab\u91c7\u6837\u7684\u7ed3\u70b9\u53c8\u968f\u673a\u9009\u53d6\u5b83\u4eec\u56fa\u5b9a\u4e2a\u6570\u7684\u90bb\u5c45\u8282\u70b9\uff0c\u8fd9\u4e2a\u53ef\u4ee5\u662f\u4e00\u9636\u90bb\u5c45\u4e5f\u53ef\u4ee5\u662f\u4e8c\u9636\u90bb\u5c45\uff0c\u6700\u540e\u6784\u6210\u8fdb\u884c\u5377\u79ef\u64cd\u4f5c\u7684\u56fe\u3002</p> </li> <li> <p>\u5c06\u90bb\u5c45\u8282\u70b9\u7684\u4fe1\u606f\u901a\u8fc7<code>aggregate</code>\u51fd\u6570\u805a\u5408\u8d77\u6765\u66f4\u65b0\u521a\u521a\u91c7\u6837\u7684\u7ed3\u70b9\u3002</p> </li> <li> <p>\u8ba1\u7b97\u91c7\u6837\u7ed3\u70b9\u5904\u7684\u635f\u5931\uff0c\u5982\u679c\u662f\u65e0\u76d1\u7763\u4efb\u52a1\uff0c\u5219\u76ee\u6807\u8bbe\u4e3a\u56fe\u4e0a\u90bb\u5c45\u7ed3\u70b9\u7684\u7f16\u7801\u76f8\u4f3c\uff08\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u6211\u4eec\u5e0c\u671b\u90bb\u5c45\u8282\u70b9\u7684\u7f16\u7801\u76f8\u4f3c\uff0c\u90a3\u4e48\u6211\u4eec\u5c31\u53ef\u4ee5\u5c06\u805a\u5408\u540e\u7684\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5f97\u5b83\u4eec\u5728\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u8ddd\u79bb\u66f4\u52a0\u63a5\u8fd1\uff0c\u4ece\u800c\u589e\u5f3a\u8282\u70b9\u8868\u793a\u7684\u76f8\u4f3c\u6027\uff09\uff1b\u5982\u679c\u662f\u6709\u76d1\u7763\u4efb\u52a1\uff0c\u5219\u6839\u636e\u6709\u76d1\u7763\u7ed3\u70b9\u7684\u6807\u7b7e\u548c\u6700\u540e\u7684\u503c\u8ba1\u7b97loss\u6765\u66f4\u65b0\u53c2\u6570\u3002</p> </li> </ul> <p>\u90bb\u5c45\u8282\u70b9\u7684\u9009\u53d6</p> <p>\u672c\u6587\u4e2d\u91c7\u7528\u7684\u662f\u5bf9\u90bb\u5c45\u8282\u70b9\u7684\u5747\u5300\u91c7\u6837\uff08\u56fa\u5b9a\u5927\u5c0f\uff09\uff0c\u6bcf\u4e00\u8df3\u62bd\u6837\u7684\u90bb\u5c45\u6570\u91cf\u4e0d\u591a\u4e8e\\(S_K\\)\u4e2a\uff0c\u5982\u679c\u4e0d\u662f\u56fa\u5b9a\u91c7\u6837\u7684\u8bdd\uff0c\u5355\u4e2a\u6279\u6b21\u7684\u5185\u5b58\u548c\u9884\u671f\u8fd0\u884c\u65f6\u95f4\u90fd\u662f\u4e0d\u53ef\u9884\u6d4b\u7684\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u662f\\(O(|V|)\\)\uff0c\u5f53\u56fe\u89c4\u6a21\u5f88\u5927\u65f6\u5f88\u5bb9\u6613\u8bad\u7ec3\u4e0d\u52a8\u6a21\u578b\u3002</p> <p></p> <p>\u968f\u7740\u5c42\u6570K\u7684\u589e\u52a0\uff0c\u53ef\u4ee5\u805a\u5408\u8d8a\u6765\u8d8a\u8fdc\u8ddd\u79bb\u7684\u4fe1\u606f\u3002\u8fd9\u662f\u56e0\u4e3a\uff0c\u867d\u7136\u6bcf\u6b21\u9009\u62e9\u90bb\u5c45\u7684\u65f6\u5019\u5c31\u662f\u4ece\u5468\u56f4\u7684\u4e00\u9636\u90bb\u5c45\u4e2d\u5747\u5300\u5730\u91c7\u6837\u56fa\u5b9a\u4e2a\u6570\u4e2a\u90bb\u5c45\uff0c\u4f46\u662f\u7531\u4e8e\u8282\u70b9\u7684\u90bb\u5c45\u4e5f\u805a\u5408\u4e86\u5176\u90bb\u5c45\u7684\u4fe1\u606f\uff0c\u8fd9\u6837\uff0c\u5728\u4e0b\u4e00\u6b21\u805a\u5408\u65f6\uff0c\u8be5\u8282\u70b9\u5c31\u4f1a\u63a5\u6536\u5230\u5176\u90bb\u5c45\u7684\u90bb\u5c45\u7684\u4fe1\u606f\uff0c\u4e5f\u5c31\u662f\u805a\u5408\u5230\u4e86\u4e8c\u9636\u90bb\u5c45\u7684\u4fe1\u606f\u4e86\u3002</p> <ul> <li>Mean Aggregate</li> </ul> \\[ h_v^k \\leftarrow  \\sigma(W \\cdot mean(\\{h_v^{k-1}\\} \\cup \\{h_u^{k-1},\\forall u \\in N(v)\\}) ) \\] <p>\u5f53\u524d\u8282\u70b9\\(v\\)\u672c\u8eab\u548c\u5b83\u6240\u6709\u7684\u90bb\u5c45\u5728\\(k-1\\)\u5c42\u7684<code>embedding</code>\u7684<code>mean</code>\uff0c\u7136\u540e\u7ecf\u8fc7<code>MLP+sigmoid</code>\u3002\uff08\u8fd9\u4e2a\u805a\u5408\u51fd\u6570\u6bd4\u8f83\u7c97\u7cd9\uff09</p> <ul> <li>LSTM Aggregate</li> </ul> <p>\u4f7f\u7528<code>LSTM</code>\u5bf9\u90bb\u5c45\u7ed3\u70b9\u4fe1\u606f\u8fdb\u884c\u805a\u5408\uff0c\u62e5\u6709\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u5730\u662f\uff0c\u56e0\u4e3a LSTM \u7684\u5e8f\u5217\u6027\uff0c\u8fd9\u4e2a\u805a\u5408\u51fd\u6570\u4e0d\u5177\u5907\u5bf9\u79f0\u6027\u3002\u6587\u7ae0\u4e2d\u4f7f\u7528\u5bf9\u90bb\u5c45\u7ed3\u70b9\u968f\u673a\u6392\u5217\u7684\u65b9\u6cd5\u6765\u5c06\u5176\u5e94\u7528\u4e8e\u65e0\u5e8f\u96c6\u5408\u3002 </p> <ul> <li>pooling Aggregate</li> </ul> \\[ Aggregate^{pool}_k = max({\\sigma(W_{pool} h^k_{u_i} + b), \\cup \\{h_u^{k-1},\\forall u \\in N(v)}) \\] <p>\u628a\u8282\u70b9\\(v\\)\u7684\u6240\u6709\u90bb\u5c45\u8282\u70b9\u90fd\u72ec\u7acb\u5730\u8f93\u5165\u4e00\u4e2a<code>MLP+sigmoid</code>\u5f97\u5230\u4e00\u4e2a\u5411\u91cf\uff0c\u6700\u540e\u628a\u6240\u6709\u90bb\u5c45\u7684\u5411\u91cf\u505a\u4e00\u4e2a<code>element-wise</code>(\u9010\u5143\u7d20)\u7684<code>max-pooling</code>\u3002(\u5b9e\u9a8c\u53d1\u73b0<code>max pooling</code>\u548c<code>mean pooling</code>\u6548\u679c\u4e0a\u57fa\u672c\u4e00\u81f4)</p> <p></p> <p></p> <p>\u5bf9\u4e8e\u90bb\u5c45\u8282\u70b9\u7684\u91c7\u6837\uff0c\u6ee1\u8db3\\(K=2,S_1 \\cdot S_2 &lt;= 500\\)\u8868\u73b0\u6bd4\u8f83\u597d</p> <p>\u5bf9\u4e8e\u805a\u5408\u51fd\u6570\u7684\u6bd4\u8f83\u4e0a\uff0c<code>LSTM aggregator</code> \u548c <code>Pooling aggregator</code> \u8868\u73b0\u6700\u597d\uff0c\u4f46\u662f\u524d\u8005\u6bd4\u540e\u8005\u6162\u5927\u7ea6\u4e24\u500d\u3002</p> <ul> <li>GraphSAGE\u7684\u65e0\u76d1\u7763\u5b66\u4e60</li> </ul> <p>\u5bf9\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\u5e94\u8be5\u8ba9\u4e34\u8fd1\u7684\u8282\u70b9\u7684\u62e5\u6709\u76f8\u4f3c\u7684\u8868\u793a\uff0c\u53cd\u4e4b\u5e94\u8be5\u8868\u793a\u5927\u4e0d\u76f8\u540c</p> \\[ J_G(z_u)=-log(\\sigma(z_u^T z_v)) -Q\\cdot \\mathbb{E}_{v_n \u223c P_n(v)} log(\\sigma(z_u^T z_{v_n})) \\] <p>\\(P_n\\):\u8d1f\u91c7\u6837\u5206\u5e03\uff0c\\(Q\\):\u8d1f\u91c7\u6837\u6837\u672c\u7684\u6570\u91cf</p> <p>\u5bf9\u4e8e\u6709\u76d1\u7763\u5b66\u4e60\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528cross-entropy loss\u7b49\u5e38\u89c4\u635f\u5931\u51fd\u6570\uff0c\u4e0a\u9762\u7684\u8fd9\u4e2aloss\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u8f85\u52a9loss\u3002</p> model <pre><code>class Classification(nn.Module):\n\n    def __init__(self, emb_size, num_classes):\n        super(Classification, self).__init__()\n\n        #self.weight = nn.Parameter(torch.FloatTensor(emb_size, num_classes))\n        self.layer = nn.Sequential(\n                                nn.Linear(emb_size, num_classes)      \n                                #nn.ReLU()\n                            )\n        self.init_params() #\u521d\u59cb\u5316\n\n    def init_params(self):\n        for param in self.parameters():\n            if len(param.size()) == 2:\n                nn.init.xavier_uniform_(param)\n\n    def forward(self, embeds):\n        logists = torch.log_softmax(self.layer(embeds), dim=1)\n        return logists\n\n# class Classification(nn.Module):\n\n#   def __init__(self, emb_size, num_classes):\n#       super(Classification, self).__init__()\n\n#       self.weight = nn.Parameter(torch.FloatTensor(emb_size, num_classes))\n#       self.init_params()\n\n#   def init_params(self):\n#       for param in self.parameters():\n#           nn.init.xavier_uniform_(param)\n\n#   def forward(self, embeds):\n#       logists = torch.log_softmax(torch.mm(embeds,self.weight), 1)\n#       return logists\n\nclass UnsupervisedLoss(object):\n    \"\"\"docstring for UnsupervisedLoss\"\"\"\n    def __init__(self, adj_lists, train_nodes, device):\n        super(UnsupervisedLoss, self).__init__()\n        '''\n        Args:\n            Q: \u8c03\u6574\u8d1f\u6837\u672c\u5f97\u5206\u7684\u7f29\u653e\n            N_WALKS: \u968f\u673a\u6e38\u8d70\u7684\u6b21\u6570\n            WALK_LEN: \u968f\u673a\u6e38\u8d70\u7684\u957f\u5ea6\n            N_WALK_LEN: \u8ba1\u7b97\u8d1f\u6837\u672c\u65f6\uff0c\u5141\u8bb8\u6269\u5c55\u7684\u6b65\u6570\uff0c\u7528\u4e8e\u5bfb\u627e\u8fdc\u79bb\u5f53\u524d\u8282\u70b9\u7684\u8d1f\u6837\u672c\n            MARGIN: \u8fb9\u754c\u53c2\u6570\n            adj_lists: \u8bbf\u95ee\u8282\u70b9\u7684\u90bb\u63a5\u5173\u7cfb\n            train_nodes: \u8bad\u7ec3\u8282\u70b9\n        '''\n\n        self.Q = 10\n        self.N_WALKS = 6\n        self.WALK_LEN = 1\n        self.N_WALK_LEN = 5\n        self.MARGIN = 3\n        self.adj_lists = adj_lists\n        self.train_nodes = train_nodes\n        self.device = device\n\n        self.target_nodes = None\n        self.positive_pairs = []\n        self.negtive_pairs = []\n        self.node_positive_pairs = {}\n        self.node_negtive_pairs = {}\n        self.unique_nodes_batch = []\n\n    def get_loss_sage(self, embeddings, nodes): # \u57fa\u4e8eSAGE\u6a21\u578b\u8ba1\u7b97\u635f\u5931\n        assert len(embeddings) == len(self.unique_nodes_batch)\n        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n\n        nodes_score = []\n        assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n        for node in self.node_positive_pairs:\n            pps = self.node_positive_pairs[node]\n            nps = self.node_negtive_pairs[node]\n            if len(pps) == 0 or len(nps) == 0:\n                continue\n\n            # Q * Exception(negative score)\n            indexs = [list(x) for x in zip(*nps)]\n            node_indexs = [node2index[x] for x in indexs[0]]\n            neighb_indexs = [node2index[x] for x in indexs[1]]\n            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n            neg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)), 0)\n            #print(neg_score)\n\n            # multiple positive score\n            indexs = [list(x) for x in zip(*pps)]\n            node_indexs = [node2index[x] for x in indexs[0]]\n            neighb_indexs = [node2index[x] for x in indexs[1]]\n            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n            pos_score = torch.log(torch.sigmoid(pos_score))\n            #print(pos_score)\n\n            nodes_score.append(torch.mean(- pos_score - neg_score).view(1,-1))\n\n        loss = torch.mean(torch.cat(nodes_score, 0))\n\n        return loss\n\n    def get_loss_margin(self, embeddings, nodes): # Margin loss\n        '''\n        embeddings: \u8282\u70b9\u5d4c\u5165\u5411\u91cf\n        nodes: \u8282\u70b9\u5217\u8868\n        self.unique_nodes_batch: \u5305\u542b\u6240\u6709\u552f\u4e00\u8282\u70b9\u7684\u5217\u8868\uff0c\u7528\u4e8e\u7d22\u5f15\u8282\u70b9\n        self.node_positive_pairs / self.node_negtive_pairs: \u8282\u70b9\u7684\u6b63\u8d1f\u6837\u672c\u5bf9\n        '''\n        assert len(embeddings) == len(self.unique_nodes_batch)\n\n        # \u786e\u4fdd nodes \u4e2d\u7684\u8282\u70b9\u987a\u5e8f\u4e0e unique_nodes_batch \u4e00\u81f4\n        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))] \n\n        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n\n        nodes_score = []\n\n        # \u786e\u4fdd\u6bcf\u4e2a\u8282\u70b9\u6709\u5bf9\u5e94\u7684\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\n        assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n\n        for node in self.node_positive_pairs:\n            pps = self.node_positive_pairs[node]\n            nps = self.node_negtive_pairs[node]\n            if len(pps) == 0 or len(nps) == 0:\n                continue\n            indexs = [list(x) for x in zip(*pps)]\n            node_indexs = [node2index[x] for x in indexs[0]]\n            neighb_indexs = [node2index[x] for x in indexs[1]]\n\n            #  \u8ba1\u7b97\u6b63\u6837\u672c\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\n            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n            # \u9009\u62e9\u6700\u5c0f\u7684\u5bf9\u6570\u6982\u7387\u503c\u5417, \u63d0\u9ad8\u6a21\u578b\u5bf9\u56f0\u96be\u6837\u672c\u7684\u654f\u611f\u5ea6\n            pos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)\n\n            indexs = [list(x) for x in zip(*nps)]\n            node_indexs = [node2index[x] for x in indexs[0]]\n            neighb_indexs = [node2index[x] for x in indexs[1]]\n            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n            neg_score, _ = torch.max(torch.log(torch.sigmoid(neg_score)), 0)\n\n            # self.MARGIN \u7c7b\u4f3cSVM\u4e2d\u7684\u8f6f\u95f4\u9694\n            nodes_score.append(torch.max(torch.tensor(0.0).to(self.device), neg_score - pos_score + self.MARGIN).view(1,-1))\n            # nodes_score.append((-pos_score - neg_score).view(1,-1))\n\n        loss = torch.mean(torch.cat(nodes_score, 0),0)\n\n        # loss = -torch.log(torch.sigmoid(pos_score))-4*torch.log(torch.sigmoid(-neg_score))\n        return loss\n\n    def extend_nodes(self, nodes, num_neg=6): \n        ''' \n        \u4e3a\u6bcf\u4e2a\u76ee\u6807\u8282\u70b9\u751f\u6210\u6b63\u8d1f\u6837\u672c\u8282\u70b9\u5bf9\n        '''\n        self.positive_pairs = []\n        self.node_positive_pairs = {}\n        self.negtive_pairs = []\n        self.node_negtive_pairs = {}\n\n        self.target_nodes = nodes\n        self.get_positive_nodes(nodes)\n        # print(self.positive_pairs)\n        self.get_negtive_nodes(nodes, num_neg)\n        # print(self.negtive_pairs)\n\n        self.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x]) | set([i for x in self.negtive_pairs for i in x]))\n        assert set(self.target_nodes) &lt; set(self.unique_nodes_batch)\n        return self.unique_nodes_batch\n\n    def get_positive_nodes(self, nodes):\n        return self._run_random_walks(nodes)\n\n    def get_negtive_nodes(self, nodes, num_neg):\n        for node in nodes:\n            neighbors = set([node]) \n            frontier = set([node])\n\n            # \u90bb\u5c45\u968f\u673a\u6e38\u8d70\n            for i in range(self.N_WALK_LEN):\n                current = set()\n                for outer in frontier:\n                    current |= self.adj_lists[int(outer)]\n                frontier = current - neighbors\n                neighbors |= current\n\n            far_nodes = set(self.train_nodes) - neighbors\n            neg_samples = random.sample(far_nodes, num_neg) if num_neg &lt; len(far_nodes) else far_nodes\n            self.negtive_pairs.extend([(node, neg_node) for neg_node in neg_samples])\n            self.node_negtive_pairs[node] = [(node, neg_node) for neg_node in neg_samples]\n        return self.negtive_pairs\n\n    def _run_random_walks(self, nodes):\n        for node in nodes:\n            if len(self.adj_lists[int(node)]) == 0:\n                continue\n            cur_pairs = []\n            for i in range(self.N_WALKS):\n                curr_node = node\n                for j in range(self.WALK_LEN):\n                    neighs = self.adj_lists[int(curr_node)]\n                    next_node = random.choice(list(neighs))\n                    # self co-occurrences are useless\n                    if next_node != node and next_node in self.train_nodes:\n                        self.positive_pairs.append((node,next_node))\n                        cur_pairs.append((node,next_node))\n                    curr_node = next_node\n\n            self.node_positive_pairs[node] = cur_pairs\n        return self.positive_pairs\n\nclass SageLayer(nn.Module):\n    \"\"\"\n    Encodes a node's using 'convolutional' GraphSage approach\n    \"\"\"\n    def __init__(self, input_size, out_size, gcn=False): \n        super(SageLayer, self).__init__()\n\n        self.input_size = input_size\n        self.out_size = out_size\n        self.gcn = gcn\n        self.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gcn else 2 * self.input_size))\n        self.init_params()\n\n    def init_params(self):\n        for param in self.parameters():\n            nn.init.xavier_uniform_(param)\n\n    def forward(self, self_feats, aggregate_feats, neighs=None):\n        \"\"\"\n        Generates embeddings for a batch of nodes.\n\n        nodes    -- list of nodes\n        \"\"\"\n        if not self.gcn:\n            combined = torch.cat([self_feats, aggregate_feats], dim=1)\n        else:\n            combined = aggregate_feats\n        combined = F.relu(self.weight.mm(combined.t())).t()\n        return combined\n\nclass GraphSage(nn.Module):\n    \"\"\"docstring for GraphSage\"\"\"\n    def __init__(self, num_layers, input_size, out_size, raw_features, adj_lists, device, gcn=False, agg_func='MEAN'):\n        super(GraphSage, self).__init__()\n\n        self.input_size = input_size\n        self.out_size = out_size\n        self.num_layers = num_layers\n        self.gcn = gcn\n        self.device = device\n        self.agg_func = agg_func\n        self.raw_features = raw_features\n        self.adj_lists = adj_lists\n\n        for index in range(1, num_layers+1):\n            layer_size = out_size if index != 1 else input_size\n            setattr(self, 'sage_layer'+str(index), SageLayer(layer_size, out_size, gcn=self.gcn))\n\n    def forward(self, nodes_batch):\n        \"\"\"\n        Generates embeddings for a batch of nodes.\n        nodes_batch -- batch of nodes to learn the embeddings\n        \"\"\"\n        lower_layer_nodes = list(nodes_batch)\n        nodes_batch_layers = [(lower_layer_nodes,)]\n        # self.dc.logger.info('get_unique_neighs.')\n        for i in range(self.num_layers):\n            lower_samp_neighs, lower_layer_nodes_dict, lower_layer_nodes= self._get_unique_neighs_list(lower_layer_nodes)\n            nodes_batch_layers.insert(0, (lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict))\n\n        assert len(nodes_batch_layers) == self.num_layers + 1\n\n        pre_hidden_embs = self.raw_features\n        for index in range(1, self.num_layers+1):\n            nb = nodes_batch_layers[index][0]\n            pre_neighs = nodes_batch_layers[index-1]\n            # self.dc.logger.info('aggregate_feats.')\n            aggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs) # \n            sage_layer = getattr(self, 'sage_layer'+str(index))\n            if index &gt; 1:\n                nb = self._nodes_map(nb, pre_hidden_embs, pre_neighs)\n            # self.dc.logger.info('sage_layer.')\n            cur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb],\n                                        aggregate_feats=aggregate_feats)\n            pre_hidden_embs = cur_hidden_embs\n\n        return pre_hidden_embs\n\n    def _nodes_map(self, nodes, hidden_embs, neighs):\n        layer_nodes, samp_neighs, layer_nodes_dict = neighs\n        assert len(samp_neighs) == len(nodes)\n        index = [layer_nodes_dict[x] for x in nodes]\n        return index\n\n    def _get_unique_neighs_list(self, nodes, num_sample=10) -&gt; list(set), dict, list:\n        '''\n        \u8fdb\u884c\u90bb\u5c45\u91c7\u6837\uff0c\u8fd4\u56de\uff1a\n            samp_neighs: \u6bcf\u4e2a\u8282\u70b9\u7684\u90bb\u5c45\u96c6\u5408\u5217\u8868\uff08\u5305\u542b\u81ea\u8eab\uff09\n            unique_nodes: \u88ab\u9009\u4e2d\u7684\u90bb\u5c45\u8282\u70b9\uff08\u5b57\u5178\uff09\n            _unique_nodes_list: \u88ab\u9009\u4e2d\u7684\u90bb\u5c45\u8282\u70b9\uff08\u5217\u8868\uff09\n        '''\n        _set = set\n        to_neighs = [self.adj_lists[int(node)] for node in nodes]\n        if not num_sample is None:\n            _sample = random.sample\n            samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) &gt;= num_sample else to_neigh for to_neigh in to_neighs] #\u968f\u673a\u91c7\u6837\n        else:\n            samp_neighs = to_neighs\n        samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)] # \u5c06\u6bcf\u4e2a\u8282\u70b9\u81ea\u8eab\u4e5f\u52a0\u5165\u5176\u90bb\u5c45\u96c6\u5408\n        _unique_nodes_list = list(set.union(*samp_neighs)) # \u5e76\u96c6\uff0c\u5f97\u5230\u6240\u6709\u90bb\u5c45\u8282\u70b9\n        i = list(range(len(_unique_nodes_list)))\n        unique_nodes = dict(list(zip(_unique_nodes_list, i)))\n        return samp_neighs, unique_nodes, _unique_nodes_list # lower_samp_neighs, lower_layer_nodes_dict, lower_layer_nodes\n\n    def aggregate(self, nodes, pre_hidden_embs, pre_neighs, num_sample=10):\n        # \u805a\u5408\u51fd\u6570\n        '''\n        pre_hidden_embs: \u6240\u6709\u8282\u70b9\u7684\u9884\u8bad\u7ec3\u5d4c\u5165\u77e9\u9635\n        unique_nodes\uff1a\u6240\u6709\u552f\u4e00\u90bb\u5c45\u8282\u70b9\u7684 ID \u5230\u5176\u5728 unique_nodes_list \u4e2d\u7d22\u5f15\u7684\u6620\u5c04\uff08\u5b57\u5178\uff09 idx -&gt; index\n        unique_nodes_list\uff1a\u6240\u6709\u552f\u4e00\u90bb\u5c45\u8282\u70b9\u7684 ID \u5217\u8868\n        samp_neighs\uff1a\u6bcf\u4e2a\u8282\u70b9\u5bf9\u5e94\u7684\u91c7\u6837\u90bb\u5c45\u8282\u70b9 ID \u5217\u8868\n        '''\n        unique_nodes_list, samp_neighs, unique_nodes = pre_neighs\n\n        assert len(nodes) == len(samp_neighs)\n        indicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))]\n        assert (False not in indicator)\n\n        if not self.gcn:\n            samp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]\n        # self.dc.logger.info('2')\n        if len(pre_hidden_embs) == len(unique_nodes):\n            embed_matrix = pre_hidden_embs\n        else:\n            embed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)] #\u53d6\u9700\u8981\u7684\u8282\u70b9\u5d4c\u5165\u77e9\u9635\n        # self.dc.logger.info('3')\n        mask = torch.zeros(len(samp_neighs), len(unique_nodes))\n        # \u63a9\u7801\u6307\u793a\u54ea\u4e9b\u90bb\u5c45\u8282\u70b9\u53c2\u4e0e\u805a\u5408\n        # mask \u77e9\u9635\u7684\u6bcf\u4e00\u884c\u5bf9\u5e94\u4e00\u4e2a\u5f85\u5904\u7406\u8282\u70b9\uff0c\u6bcf\u4e00\u5217\u5bf9\u5e94\u4e00\u4e2a\u552f\u4e00\u7684\u90bb\u5c45\u8282\u70b9\u3002\n        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh] # \u6240\u6709\u9700\u8981\u53c2\u4e0e\u805a\u5408\u7684\u90bb\u5c45\u8282\u70b9\u5728 unique_nodes\u4e2d\u7684\u7d22\u5f15\n        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))] # \u6240\u6709\u9700\u8981\u8fdb\u884c\u90bb\u5c45\u8282\u70b9\u7279\u5f81\u805a\u5408\u7684\u8282\u70b9\u5728 samp_neighs \u4e2d\u7684\u7d22\u5f15\n        '''\n        row_indices -&gt; [[0,0,0,...],[1,1,1,...],[2,2,2,...],...]    \n        '''\n        mask[row_indices, column_indices] = 1\n        # self.dc.logger.info('4')\n        # Example:\u8282\u70b9i\u7684\u90bb\u5c45\u8282\u70b9\u5217\u8868\u4e2d\u5305\u542b\u8282\u70b9n\uff0c\u90a3\u4e48 mask[i, unique_nodes[n]]\u88ab\u8bbe\u7f6e\u4e3a 1\uff0c\u8868\u793a\u8be5\u90bb\u5c45\u8282\u70b9\u53c2\u4e0e\u8282\u70b9i\u7684\u7279\u5f81\u805a\u5408\u3002\n\n        if self.agg_func == 'MEAN':\n            num_neigh = mask.sum(1, keepdim=True)\n            mask = mask.div(num_neigh).to(embed_matrix.device)\n            aggregate_feats = mask.mm(embed_matrix)\n\n        elif self.agg_func == 'MAX':\n            # print(mask)\n            indexs = [x.nonzero() for x in mask==1]\n            # nonzero() \u65b9\u6cd5\u4f1a\u8fd4\u56de\u6570\u7ec4\u4e2d\u975e\u96f6\u5143\u7d20\u7684\u7d22\u5f15\uff0c\u4ee5\u5143\u7ec4\u7684\u5f62\u5f0f\u8fd4\u56de\n            aggregate_feats = []\n            # self.dc.logger.info('5')\n            for feat in [embed_matrix[x.squeeze()] for x in indexs]:\n                if len(feat.size()) == 1:\n                    aggregate_feats.append(feat.view(1, -1))\n                else:\n                    aggregate_feats.append(torch.max(feat,0)[0].view(1, -1))\n            aggregate_feats = torch.cat(aggregate_feats, 0)\n        # self.dc.logger.info('6')\n        return aggregate_feats\n</code></pre>"},{"location":"python/cs224w-notebook/chapter3/#graph-attention-networks","title":"Graph Attention Networks","text":"<p>\u8bba\u6587\u6807\u9898: Graph Attention Networks</p> <p></p> <ul> <li>Attention coefficients \\(e_{vu}\\) across pairs of nodes \\(u\\) and \\(v\\) based on their features \\(h_u\\) and \\(h_v\\).</li> </ul> \\[ e_{vu} = a(W^{l}h_u{l-1}, W^{l}h_v{l-1}) \\] <p>\\(e_{vu}\\) indicates the importance of node \\(v\\) to node \\(u\\).</p> <ul> <li>Normalize the attention coefficients to obtain the attention coefficients \\(\\alpha_{vu}\\).</li> </ul> \\[ a_{vu} = \\frac{exp(e_{vu})}{\\sum_{k \\in N(v)} exp(e_{vk})} \\] <ul> <li>Weighted sum based on the final attention weights \\(\\alpha_{vu}\\).</li> </ul> \\[ h_v^l = \\sigma( \\sum_{u \\in N(v)} \\alpha_{vu} W^l h_u^{l-1}) \\] <p> </p> <p>\u672c\u6587\u63d0\u51fa\u7684Graph Attention Layers\u7684\u8f93\u5165\u4e3a\u4e00\u7ec4\u8282\u70b9\u7684\u7279\u5f81\uff1a\\(\\mathbf{h} = \\{ h_1,h_2,...,h_N \\},h_i \\in \\mathbb{R}^F\\)\uff0c\\(N\\)\u4e3a\u8282\u70b9\u4e2a\u6570\uff0c\\(F\\)\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u7279\u5f81\u6570\u3002\u8be5\u5c42\u8f93\u51fa\u4e3a\u4e00\u7ec4\u65b0\u7684\u8282\u70b9\u7279\u5f81$\\mathbf{h}' = { h_1',h_2',...,h_N' },h_i' \\in \\mathbb{R}^{F'} $</p> <p>\u5c06\u8f93\u5165\u7279\u5f81\u8f6c\u6362\u4e3a\u9ad8\u5c42\u7279\u5f81\uff0c\u6211\u4eec\u63d0\u4f9b\u4e00\u4e2a\u5171\u4eab\u53c2\u6570\u7684\u7ebf\u6027\u53d8\u6362\\(\\mathbf{W}\\):</p> \\[ e_{ij} = a(\\mathbf{W} h_i, \\mathbf{W} h_j) \\] <p>\\(e_{ij}\\)\u662f\u4ece\u4e00\u4e2a\u8282\u70b9\u5230\u53e6\u4e00\u4e2a\u8282\u70b9\u7684\u6ce8\u610f\u529b\u5206\u6570\uff08importance\uff09\uff0c\\(a\\)\u662f\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\\(a \\in \\mathbf{R}^{2F'}\\)</p> \\[ e_{ij} = LeakyReLU(\\mathbf{a}^T [\\mathbf{W} h_i || \\mathbf{W} h_j]) \\] <p>\u56e0\u6b64:</p> \\[ \\alpha_{ij} = \\frac{exp (LeakyReLU(\\mathbf{a}^T [\\mathbf{W} h_i || \\mathbf{W} h_j]))}{\\sum_{k \\in \\mathcal{N}(i)} exp(LeakyReLU(\\mathbf{a}^T [\\mathbf{W} h_i || \\mathbf{W} h_k]))} \\] <p>\u5f97\u5230\u5404\u4e2a\u90bb\u8282\u70b9\u7684\\(\\alpha_{ij}\\)\u540e\uff0c\u6211\u4eec\u5bf9\u5176\u8fdb\u884c\u52a0\u6743\u6c42\u548c:</p> \\[ h_i' = \\sigma(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{W} h_j) \\] <ul> <li>Multi-head Graph Attention</li> </ul> <p>\u4e3a\u4e86\u7a33\u5b9a <code>attention</code> \u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u6211\u4eec\u53d1\u73b0\u5c06\u6211\u4eec\u7684\u673a\u5236\u62d3\u5c55\u5230 <code>multi-head attention</code> \u662f\u6709\u597d\u5904\u7684:</p> \\[ h_i' = \\mathop{\\Big|\\Big|}\\limits_{k=1}^K  \\sigma(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^k \\mathbf{W}^k h_j) \\] <p>\u5018\u82e5\u6211\u4eec\u5728\u6700\u540e\u8f93\u51fa\u5c42\u6267\u884c\u8be5 <code>multi-head attention</code>\u5219<code>concat</code>\u64cd\u4f5c\u4e0d\u662f\u5fc5\u987b\u7684\uff0c\u53ef\u4ee5\u4f7f\u7528<code>Average</code>\u4ee3\u66ff\uff0c\u63a8\u8fdf\u6267\u884c\u6700\u7ec8\u975e\u7ebf\u6027:</p> \\[ h_i' =  \\sigma( \\frac{1}{K} \\sum_{k=1}^{K}  \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^k \\mathbf{W}^k h_j) \\] layers <pre><code>class GraphAttentionLayer(nn.Module):\n    \"\"\"\n    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n    \"\"\"\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(GraphAttentionLayer, self).__init__()\n        self.dropout = dropout\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n        self.concat = concat\n\n        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\n    def forward(self, h, adj):\n        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), W.shape: (in_features, out_features) --&gt; Wh.shape: (N, out_features)\n        e = self._prepare_attentional_mechanism_input(Wh) # e.shape (N, N)\n\n        zero_vec = -9e15*torch.ones_like(e)\n        attention = torch.where(adj &gt; 0, e, zero_vec) # \u9010\u5143\u7d20\u64cd\u4f5c e if adj &gt; 0 else -9e15\n        attention = F.softmax(attention, dim=1) # atten.shape (N, N)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n        h_prime = torch.matmul(attention, Wh) # atten.shape (N, N), Wh.shape: (N, out_features) -&gt; h_prime.shape: (N, out_features)\n\n        if self.concat:\n            return F.elu(h_prime)\n        else:\n            return h_prime\n\n    def _prepare_attentional_mechanism_input(self, Wh):\n        # Wh.shape (N, out_feature)\n        # self.a.shape (2 * out_feature, 1)\n        # Wh1&amp;2.shape (N, 1)\n        # e.shape (N, N)\n        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n        # broadcast add\n        '''\n        a = torch.tensor([[1],[2], [3], [4], [5]])\n        b = torch.tensor([[1], [2], [3], [4], [5]])\n        a+b.T\n        &gt;&gt;&gt; tensor([[ 2,  3,  4,  5,  6],\n                    [ 3,  4,  5,  6,  7],\n                    [ 4,  5,  6,  7,  8],\n                    [ 5,  6,  7,  8,  9],\n                    [ 6,  7,  8,  9, 10]])\n        '''\n        e = Wh1 + Wh2.T\n        return self.leakyrelu(e)\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -&gt; ' + str(self.out_features) + ')'\n\nclass SpecialSpmmFunction(torch.autograd.Function): \n    # \u5728\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u4e2d\u53ea\u5bf9\u7a00\u758f\u533a\u57df\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\n    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n    @staticmethod\n    def forward(ctx, indices, values, shape, b):\n        assert indices.requires_grad == False\n        a = torch.sparse_coo_tensor(indices, values, shape)  #\u7a00\u758f\u5f20\u91cf\n        ctx.save_for_backward(a, b)\n        ctx.N = shape[0] # \u5c06\u7a00\u758f\u77e9\u9635\u7684\u7b2c\u4e00\u7ef4\u5927\u5c0f\u4fdd\u5b58\u5230 ctx.N \u4e2d\uff0c\u7528\u4e8e\u540e\u7eed\u8ba1\u7b97\n        return torch.matmul(a, b)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # grad_output\u8f93\u51fa\u5f20\u91cf\u7684\u68af\u5ea6\n        a, b = ctx.saved_tensors\n        grad_values = grad_b = None # \u5206\u522b\u8868\u793avalues\uff08\u7a00\u758f\u77e9\u9635\u4e2d\u975e\u96f6\u5143\u7d20\uff09\u7684\u68af\u5ea6\u548cb\u7684\u68af\u5ea6\u3002\n        if ctx.needs_input_grad[1]: # \u68c0\u67e5\u662f\u5426\u9700\u8981\u8ba1\u7b97values\u7684\u68af\u5ea6\n            grad_a_dense = grad_output.matmul(b.t()) # \u8ba1\u7b97\u7a00\u758f\u77e9\u9635a\u7684\u68af\u5ea6, a_dense\u8868\u793aa\u7684\u5bc6\u96c6\u5f62\u5f0f\u3002\n            # \u6839\u636e\u94fe\u5f0f\u6cd5\u5219\uff0ca\u7684\u68af\u5ea6\u7b49\u4e8e\u8f93\u51fa\u68af\u5ea6 grad_output \u4e58\u4ee5 output \u5bf9 a \u7684\u504f\u5bfc\u6570\n            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :] \n            # \u8f6c\u6362\u4e3a\u7ebf\u6027\u7d22\u5f15\n            grad_values = grad_a_dense.view(-1)[edge_idx]\n        if ctx.needs_input_grad[3]: # \u68c0\u67e5\u662f\u5426\u9700\u8981\u8ba1\u7b97b\u7684\u68af\u5ea6\u3002\n            grad_b = a.t().matmul(grad_output)\n        return None, grad_values, None, grad_b\n\n\nclass SpecialSpmm(nn.Module):\n    def forward(self, indices, values, shape, b):\n        return SpecialSpmmFunction.apply(indices, values, shape, b)\n\n\nclass SpGraphAttentionLayer(nn.Module):\n    \"\"\"\n    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n    \"\"\"\n\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(SpGraphAttentionLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n        self.concat = concat\n\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_normal_(self.W.data, gain=1.414)\n\n        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n        nn.init.xavier_normal_(self.a.data, gain=1.414)\n\n        self.dropout = nn.Dropout(dropout)\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n        self.special_spmm = SpecialSpmm()\n\n    def forward(self, input, adj):\n        dv = 'cuda' if input.is_cuda else 'cpu'\n\n        N = input.size()[0]\n        edge = adj.nonzero().t() \n        # [\u6240\u6709\u8fb9\u7684\u8d77\u70b9\u8282\u70b9\u7d22\u5f15,\n        #   \u6240\u6709\u8fb9\u7684\u7ec8\u70b9\u8282\u70b9\u7d22\u5f15]\n\n        h = torch.mm(input, self.W)\n        # h: N x out\n        assert not torch.isnan(h).any()\n\n        # Self-attention on the nodes - Shared attention mechanism\n        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t() \n        # edge: 2*D x E \n        # h[edge[0, :], :] \u53d6\u51fa\u6240\u6709\u8d77\u70b9\u8282\u70b9\u7684\u7279\u5f81\uff0ch[edge[1, :], :] \u53d6\u51fa\u6240\u6709\u7ec8\u70b9\u8282\u70b9\u7684\u7279\u5f81\u3002\n\n        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n        assert not torch.isnan(edge_e).any()\n        # edge_e: E\n\n        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv)) \n        # \u6bcf\u4e2a\u8282\u70b9\u7684\u6ce8\u610f\u529b\u6743\u91cd\u4e4b\u548c e_rowsum\n        # e_rowsum: N x 1\n\n        edge_e = self.dropout(edge_e)\n        # edge_e: E\n\n        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h) \n        # \u52a0\u6743\u90bb\u5c45\u7279\u5f81\u4e4b\u548c h_prime\n        assert not torch.isnan(h_prime).any()\n        # h_prime: N x out\n\n        h_prime = h_prime.div(e_rowsum)\n        # h_prime: N x out\n        assert not torch.isnan(h_prime).any()\n\n        if self.concat:\n            # if this layer is not last layer,\n            return F.elu(h_prime)\n        else:\n            # if this layer is last layer,\n            return h_prime\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -&gt; ' + str(self.out_features) + ')'\n</code></pre> model <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom layers import GraphAttentionLayer, SpGraphAttentionLayer\n\n\nclass GAT(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        \"\"\"Dense version of GAT.\"\"\"\n        super(GAT, self).__init__()\n        self.dropout = dropout\n\n        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n        for i, attention in enumerate(self.attentions):\n            self.add_module('attention_{}'.format(i), attention)\n            # .add_module()\u5c06\u6bcf\u4e2a\u56fe\u6ce8\u610f\u529b\u5c42\u6dfb\u52a0\u5230\u6a21\u5757\u4e2d\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u56fe\u6ce8\u610f\u529b\u5c42\u6dfb\u52a0\u4e00\u4e2a\u552f\u4e00\u7684\u540d\u79f0\u3002\n\n        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n        # \u5c06nheads\u4e2a\u56fe\u6ce8\u610f\u529b\u5c42\u7684\u8f93\u51fa\u7279\u5f81\u62fc\u63a5\u8d77\u6765\uff0c\u5e76\u6620\u5c04\u5230 nclass \u7ef4\u7684\u8f93\u51fa\u7279\u5f81\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = F.elu(self.out_att(x, adj))\n        return F.log_softmax(x, dim=1)\n\n\nclass SpGAT(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        \"\"\"Sparse version of GAT.\"\"\"\n        super(SpGAT, self).__init__()\n        self.dropout = dropout\n\n        self.attentions = [SpGraphAttentionLayer(nfeat, \n                                                 nhid, \n                                                 dropout=dropout, \n                                                 alpha=alpha, \n                                                 concat=True) for _ in range(nheads)]\n        for i, attention in enumerate(self.attentions):\n            self.add_module('attention_{}'.format(i), attention)\n\n        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n                                             nclass, \n                                             dropout=dropout, \n                                             alpha=alpha, \n                                             concat=False)\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = F.elu(self.out_att(x, adj))\n        return F.log_softmax(x, dim=1)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter3/#the-over-smoothing-problem","title":"The over-smoothing problem","text":"<p>When we stack many <code>GNNLayers</code> together, the output of the network will suffer from <code>the over-smoothing problem</code>: all the node embeddings converge to the same value</p> <p>But we want to see the diffierences between different nodes.</p> <p>Why does this problem happen: Receptive field of a GNN</p> <ul> <li>Receptive field of a GNN: the set of nodes that determine the embedding of a node of interest.</li> </ul> <p></p> <p>In a 3-layer GNN, the receptive field overlap for two nodes(\u611f\u53d7\u91ce\u91cd\u53e0\u8fc7\u5927). The shared neighbors quickly grows when the number of <code>GNNlayers &gt;= 3</code>.So we should be cautious when adding GNNLayers.</p> <p>If two nodes have highly-overlapped receptive fields, then their embeddings are highly similar</p> <ul> <li>Goal:Making a shallow GNN more expressive.</li> </ul> <p></p> <p></p> <p></p> <p> </p> <p></p>"},{"location":"python/cs224w-notebook/chapter3/#manipulate-graphs","title":"Manipulate Graphs","text":""},{"location":"python/cs224w-notebook/chapter3/#feature-augmentation-on-graphs","title":"Feature Augmentation on Graphs","text":""},{"location":"python/cs224w-notebook/chapter3/#add-virtual-nodesedges-augment-sparse-graphs","title":"Add Virtual Nodes/Edges: Augment sparse graphs","text":"<ul> <li>Add virtual edges</li> </ul> <ul> <li>Add virtual nodes</li> </ul>"},{"location":"python/cs224w-notebook/chapter3/#node-neighborhood-sampling","title":"Node Neighborhood Sampling","text":""},{"location":"python/cs224w-notebook/chapter4/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p>"},{"location":"python/cs224w-notebook/chapter4/#prediction-with-gnn","title":"Prediction with GNN","text":"<ul> <li>Node-level prediction</li> </ul> <p>After GNN computation, we can get node embeddings:\\(\\{ \\mathbf{h}_v^{l} \\in \\mathbb{R}^d, \\forall v \\in G \\}\\)</p> \\[ \\hat{y}^v = Head_{node} (\\mathbf{h}_v^{l}) = \\mathbf{W}^{H} \\mathbf{h}_v^{l} \\] <p>\u5176\u4e2d\\(\\hat{y}^v \\in \\mathbb{R}^k\\)\u6765\u8868\u793a\u8981\u5206\u7c7b\u7684k\u7c7b</p> <ul> <li>Edge-level prediction</li> </ul> \\[ \\hat{y}^{uv} = Head_{edge} (\\mathbf{h}_v^{l}, \\mathbf{h}_v^{l}) \\] <p>(1)Concatenation + Linear</p> \\[ \\hat{y}^{uv} = Linear(Concat(\\mathbf{h}_v^{l}, \\mathbf{h}_u^{l})) \\] <p>(2) Dot product</p> \\[ \\hat{y}^{uv} = (\\mathbf{h}_u^{l})^T \\mathbf{h}_v^{l} \\] <p>This approach only applies to 1-way prediction (e.g., link prediction: predict the existence of an edge)</p> <p></p> <ul> <li>Graph-level prediction</li> </ul> \\[ \\hat{y}^{G} = Head_{graph} ( \\{\\mathbf{h}_v^{l}, \\forall v \\in G \\}) \\] <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter4/#hierarchical-pooling-\u5206\u5c42\u6c60\u5316\u65b9\u6cd5","title":"Hierarchical Pooling: \u5206\u5c42\u6c60\u5316\u65b9\u6cd5","text":"<p>\u8bba\u6587\u5730\u5740\uff1aHierarchical Graph Representation Learning with Differentiable Pooling</p> <p>Code: Diffpool</p> <p></p> <p></p> <p>\u5bf9\u4e8e\u5e38\u89c1\u7684\u56fe\u5206\u7c7b\u4efb\u52a1\uff0c\u6807\u51c6\u7684\u65b9\u6cd5\u662f\u4e3a\u56fe\u4e2d\u7684\u6240\u6709\u8282\u70b9\u751f\u6210<code>embedding</code>\uff0c\u7136\u540e\u5bf9\u6240\u6709\u8282\u70b9\u7684<code>embedding</code>\u8fdb\u884c\u5168\u5c40<code>pooling</code>\u3002\u4f46\u8fd9\u79cd\u5168\u5c40pooling\u65b9\u6cd5\u5ffd\u7565\u4e86\u56fe\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5c42\u6b21\u7ed3\u6784\u3002\u800c<code>DiffPool</code>\u662f\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u56fe\u6c60\u5316\u6a21\u5757\uff0c\u53ef\u4ee5\u5c42\u6b21\u5316\u548c\u7aef\u5230\u7aef\u65b9\u5f0f\u9002\u5e94\u5404\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002</p> <p>\u5b9a\u4e49\u56fe\\(\\mathbf{G}:(A, F)\\)\uff0c\u5176\u4e2d\\(A \\in \\{ 0, 1 \\}^{n \\times n}\\)\u662f\u4e00\u4e2a\u90bb\u63a5\u77e9\u9635\uff0c\\(F \\in \\mathbb{R}^{n \\times d}\\)\u662f\u8282\u70b9\u7684\u7279\u5f81\u77e9\u9635\u3002\u5047\u8bbe\u4e00\u4e2a\u6709\u6807\u7b7e\u7684\u56fe\u96c6\u4e3a\\(\\mathcal{D}=\\left\\{\\left(G_{1}, y_{1}\\right),\\left(G_{2}, y_{2}\\right), \\ldots\\right\\}\\)\u3002</p> <p>\u56fe\u5206\u7c7b\u4efb\u52a1\u7684\u76ee\u6807\u662f\u5b66\u4e60\u4e00\u4e2a\u6620\u5c04\u51fd\u6570\uff1a\\(f : \\mathcal{G} \\rightarrow \\mathcal{Y}\\)\u3002</p> \\[ H^{(k)} = M(A,H^{(k-1);\\theta^{(k)}}) \\] <p>\u5176\u4e2d\\(H^{k} \\in \\mathbb{R}^{n \\times d}\\)\u662f\u7b2ck\u5c42\u8282\u70b9\u8868\u793a\uff0c\\(M\\)\u662f\u6d88\u606f\u4f20\u9012\u51fd\u6570\uff0c\u4f9d\u8d56\u4e8e\u90bb\u63a5\u77e9\u9635\u548c\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\\(\\theta^{(k)}\\)\u3002\u5982<code>GCN</code>\u5b9e\u73b0\u7684\u4e3a\uff1a</p> \\[ H^{(k)} = \\operatorname{ReLU}\\left(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(k-1)} W^{(k-1)}\\right) \\] <p>\u5176\u4e2d\\(\\tilde{A} = A+I, \\tilde{D}=\\sum_j \\tilde{A}_{ij}\\)\u3002\u672c\u6587\u4e2d\u7684<code>DiffPool</code>\u53ef\u4ee5\u4f7f\u7528\u4efb\u610f\u4e00\u4e2a\u6d88\u606f\u4f20\u9012\u51fd\u6570\u6a21\u5757\\(M\\)\uff0c\u53ef\u5199\u6210\\(Z=GNN(A, X)\\)\u3002\\(Z=H^{(k)} \\in \\mathbb{R}^{n \\times d}\\)\uff0c\u5176\u4e2d\\(K\\)\u7684\u8303\u56f4\u901a\u5e38\u4e3a2~6\u3002</p> <p>\u672c\u6587\u7684\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u901a\u7528\u7684\u3001\u7aef\u5230\u7aef\u7684\u3001\u53ef\u5fae\u7684\u80fd\u591fstack\u591a\u4e2aGNN\u7684\u5c42\u6b21\u5316\u6a21\u578b</p> <p>\u6211\u4eec\u5bfb\u6c42\u4e00\u79cd\u7b56\u7565\u6765\u5728\\(n\\)\u4e2a\u8282\u70b9\u7684\u539f\u56fe\u57fa\u7840\u4e0a\u751f\u6210\\(m&lt;n\\)\u4e2a\u8282\u70b9\u7684\u65b0\u7c97\u5316\u56fe\uff0c\u5e76\u4e14\u5177\u6709\u5199\u7684\u90bb\u63a5\u77e9\u9635\\(A'\\)\u4ee5\u53ca\u8282\u70b9<code>embedding</code> \\(Z' \\in \\mathbb{R}^{m \\times d}\\)\uff0c\u518d\u8f93\u5165\u5230\u53e6\u4e00\u4e2a<code>GNNlayer</code>\u4e2d\uff0c\u6574\u4e2a\u8fc7\u7a0b\u53ef\u4ee5\u91cd\u590d\\(L\\)\u6b21\uff0c\u751f\u6210\u4e00\u4e2a\u5177\u6709\\(L\\)\u4e2a<code>GNNlayer</code>\u7684\u6a21\u578b\u3002</p> \\[ \\begin{aligned} X^{(l+1)} &amp;= {S^{(l)}}^T Z^{(l)} \\in \\mathbb{R}^{n_{l+1} \\times d} \\\\ A^{(l+1)} &amp;= {S^{(l)}}^T A^{(l)} S^{(l)} \\in \\mathbb{R}^{n_{l+1} \\times n_{l+1}}\\\\ \\end{aligned} \\] <p>\u5176\u4e2d\\(S^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l+1}}\\)\u5b9a\u4e49\u4e3a\u7b2c\\(l\\)\u5c42\u5b66\u5230\u7684<code>cluster assignment matrix</code>\uff0c\\(n_l\\)\u8868\u793a\u5728\u7b2c\\(l\\)\u5c42\u7684\u8282\u70b9\u6570\uff0c\\(n_{l+1}\\)\u8868\u793a\u5728\u7b2c\\(l+1\\)\u5c42\u7684\u8282\u70b9\u6570\uff08\\(n_l &gt; n_{l+1}\\)\uff09</p> \\[ \\begin{aligned} Z^{(l)} &amp;= GNN_{l,embed}(A^{(l)}, X^{(l)}) \\\\ S^{(l)} &amp;= \\operatorname{Softmax}\\left(GNN_{l,pool}(A^{(l)}, X^{(l)}) \\right) \\\\ \\end{aligned} \\] <p>\u5176\u4e2d\uff0c<code>softmax</code>\u662f\u5bf9\u6bcf\u4e00\u884c\u8fdb\u884c<code>softmax</code>\u3002\u5f97\u5230\\(l\\)\u5c42\u5404\u4e2a\u8282\u70b9\u5212\u5206\u5230\\(l+1\\)\u5c42\u5404\u4e2a<code>cluster</code>\u7684\u6982\u7387\u3002</p> <p>\u5728\u5012\u6570\u7b2c\u4e8c\u5c42\u7684\\(S\\)\u88ab\u6307\u5b9a\u4e3a\u4e00\u4e2a\u5168\u4e3a\\(1\\)\u7684\u5217\u5411\u91cf\uff0c\u8868\u793a\u6700\u540e\u4e00\u5c42\u53ea\u6709\u4e00\u4e2a\u7c7b\u522b\u3002\u6240\u6709\u7684\u8282\u70b9\u88ab\u5206\u914d\u5230\u8fd9\u4e2a\u7c7b\u522b\u4ea7\u751f\u7684embedding\u4ee3\u8868\u6574\u4e2a\u56fe\u3002</p> <p>\u5728\u5b9e\u8df5\u4e2d\uff0c\u4ec5\u4f7f\u7528\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u68af\u5ea6\u4fe1\u53f7\u6765\u8bad\u7ec3<code>pooling GNN</code>\u53ef\u80fd\u5f88\u56f0\u96be\u3002\u56e0\u6b64\u5f15\u5165\u4e24\u4e2a\u6b63\u5219\u5316\\(L_{LP}\\)\u548c\\(L_E\\):</p> <ul> <li><code>Auxiliary Link Prediction Objective</code>: \\(L_{LP}=|| A^{(l)} - S^{(l)} {S^{(l)}}^T ||_F (\uff0cwhere \\(|| \\cdot ||_F\\) denotes the Frobenius norm\u3002Frobenius norm \u8868\u793a\u77e9\u9635\u4e2d\u6bcf\u4e2a\u5143\u7d20\u5e73\u65b9\u548c\u518d\u5f00\u65b9\uff0c\u5373\\)L_{LP}\\)\u8868\u793a\\(A^{(l)}\\)\u4e0e\\(S^{(l)}{S^{(l)}}^T\\)\u4e4b\u95f4\u7684\u8ddd\u79bb\u8981\u5c3d\u53ef\u80fd\u63a5\u8fd1\u3002</li> </ul> <p>\\(S^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l+1}}\\)\u662f\u7b2c\\(l\\)\u5c42\u7684<code>assignment matrix</code>\uff0c\u8868\u793a\u5c06\u7b2cl\u5c42\u7684\\(n_l\\)\u4e2a\u8282\u70b9\u5206\u914d\u5230\u7b2c\\(l\\)\u5c42\u7684\\(n_{l+1}\\)\u4e2a\u8282\u70b9\u3002\\(S^{(l)}_{ik}\\)\u8868\u793a\u5c06\u7b2cl\u5c42\u7684\u7b2c\\(i\\)\u4e2a\u8282\u70b9\u5206\u914d\u5230\u7b2c\\(l+1\\)\u5c42\u7684\u7b2c\\(k\\)\u4e2acluster\u7684\u6982\u7387\u3002\\({(S^{(l)} {S^{(l)}}^T)}_{ij} = \\sum_k S^{(l)}_{ik} {S^{(l)}_{kj}}^T\\)\uff0c\u5373i\u8282\u70b9\u4e0ej\u8282\u70b9\u6620\u5c04\u5230\u4e0b\u4e00\u5c42\u540c\u4e00\u4e2acluster\u7684\u6982\u7387\u5bf9\u5e94\u76f8\u4e58\u518d\u76f8\u52a0\uff0c\u4e24\u4e2a\u8282\u70b9\u4e4b\u95f4\u6620\u5c04\u5230\u540c\u4e00\u4e2acluster\u7684\u6982\u7387\u8d8a\u5927\uff0c \\({(S^{(l)} {S^{(l)}}^T)}_{ij}\\)\u6570\u503c\u8d8a\u5927\u3002\u901a\u8fc7\u6700\u5c0f\u5316\\(L_{LP}\\)\uff0c\u53ef\u4ee5\u4f7f\u5f97\u8fde\u63a5\u5f3a\u5ea6\u8d8a\u5927\u7684\u4e24\u8282\u70b9\u66f4\u5bb9\u6613\u88ab\u6620\u5c04\u5230\u540c\u4e00\u4e2acluster\u4e0a\u3002</p> <ul> <li><code>Entropy Regularization</code>: \\(L_E = \\frac{1}{n} \\sum_{i=1}^n H(S_i)\\)</li> </ul> <p>\\(S_i\\)\u8868\u793a\u7b2ci\u4e2a\u8282\u70b9\u6620\u5c04\u5230\u4e0b\u4e00\u5c42\u6bcf\u4e2acluster\u7684\u6982\u7387\uff0c\u6240\u4ee5\u5e94\u8be5\u662f\u4e00\u4e2a\u63a5\u8fd1<code>one-hot</code>\u7684\u5411\u91cf\uff0c\u6700\u4f18\u60c5\u5f62\u662f\u7b2ci\u4e2a\u8282\u70b9\u53ea\u6620\u5c04\u5230\u4e0b\u4e00\u5c42\u7684\u4e00\u4e2acluster\u3002\\(H(S_i)\\)\u8868\u793a\u71b5\u51fd\u6570\uff0c\u71b5\u8868\u793a\u4f53\u7cfb\u5206\u5e03\u7684\u6df7\u4e71\u7a0b\u5ea6\uff0c\u901a\u8fc7\u51cf\u5c0f\u71b5\u7684\u65b9\u5f0f\u51cf\u5c11\u6620\u5c04\u5206\u5e03\u7684\u4e0d\u786e\u5b9a\u6027\u3002</p> \\[ \\text{Information Entropy:} \\ H(X) = - \\sum_{i=1}^n p(x_i) \\log{(p(x_i))} \\] <p>\u6700\u4f18\u60c5\u5f62\u662f\uff0c\u7b2ci\u4e2a\u8282\u70b9\u53ea\u6620\u5c04\u5230\u4e0b\u4e00\u5c42\u7684\u4e00\u4e2acluster\uff0c\u71b5\\(H(S_i)\\)\u4e3a0\u3002</p>"},{"location":"python/cs224w-notebook/chapter4/#dataset-splitfixed-splitrandom-split","title":"Dataset Split:Fixed split/Random split","text":""},{"location":"python/cs224w-notebook/chapter4/#seting-up-link-prediction-task","title":"Seting up Link Prediction Task","text":""},{"location":"python/cs224w-notebook/chapter5/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter5/#neighborhood-aggregation","title":"Neighborhood Aggregation","text":"<ul> <li>Observation: Neighbor aggregation can be abstracted as a function over a multi-set (a set with repeating elements).</li> </ul> <ul> <li>GCN</li> </ul> <ul> <li>GraphSAGE</li> </ul>"},{"location":"python/cs224w-notebook/chapter5/#designing-most-expressive-gnns","title":"Designing Most Expressive GNNs","text":""},{"location":"python/cs224w-notebook/chapter5/#graph-isomorphism-networkgin","title":"Graph Isomorphism Network(GIN)","text":"<p>GIN(\u56fe\u540c\u6784\u7f51\u7edc)'s neighbor aggregation function is injective, so GIN is the most expressive GNN</p> <p></p> <ul> <li>1-Weisfeiler-Lehman\uff08Color refinement algorithm\uff09\u7b97\u6cd5</li> </ul> <p></p> <p> </p> <p>GIN uses a NN to model the injective HASH function</p> \\[ \\begin{aligned} &amp;GINconv(c^{(k)}(v),\\{ c^{(k)}(u)_{u \\in N(v)} \\}) \\\\ =&amp; MLP_{\\phi} \\Big( (1+\\epsilon)MLP_f (c^{(k)}(v))+ \\sum_{u\\in N(v)} MLP_f (c^{(k)}(u)) \\Big)\\\\ \\end{aligned} \\] <p>where \\(\\epsilon\\) is a learnable parameter.</p> <p></p> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter5/#general-tips","title":"General tips","text":""},{"location":"python/cs224w-notebook/chapter5/#understand-gin","title":"Understand GIN","text":"<p>\u8bba\u6587\u5730\u5740\uff1aHow Powerful are Graph Neural Networks</p> <pre><code>class S2VGraph(object):\n    def __init__(self, g, label, node_tags=None, node_features=None):\n        '''\n            g: a networkx graph\n            label: an integer graph label\n            node_tags: a list of integer node tags\n            node_features: a torch float tensor, one-hot representation of the tag that is used as input to neural nets\n            edge_mat: a torch long tensor, contain edge list, will be used to create torch sparse tensor\n            neighbors: list of neighbors (without self-loop)\n        '''\n        self.label = label # \u56fe\u7684\u6807\u7b7e\n        self.g = g # networkx graph type\n        self.node_tags = node_tags # \u8282\u70b9\u6807\u7b7e\n        self.neighbors = []\n        self.node_features = 0 # \u8282\u70b9\u7279\u5f81\n        self.edge_mat = 0\n        self.max_neighbor = 0\n'''\nCOLLAB \u662f\u4e00\u4e2a\u79d1\u5b66\u5408\u4f5c\u6570\u636e\u96c6\u3002\u56fe\u5bf9\u5e94\u4e8e\u7814\u7a76\u4eba\u5458\u7684\u81ea\u6211\u7f51\u7edc\uff0c\u5373\u7814\u7a76\u4eba\u5458\u53ca\u5176\u5408\u4f5c\u8005\u662f\u8282\u70b9\uff0c\u8fb9\u8868\u793a\u4e24\u4e2a\u7814\u7a76\u4eba\u5458\u4e4b\u95f4\u7684\u5408\u4f5c\u3002\u7814\u7a76\u4eba\u5458\u7684\u81ea\u6211\u7f51\u7edc\u6709\u4e09\u4e2a\u53ef\u80fd\u7684\u6807\u7b7e\uff0c\u5373\u9ad8\u80fd\u7269\u200b\u200b\u7406\u3001\u51dd\u805a\u6001\u7269\u7406\u548c\u5929\u4f53\u7269\u7406\uff0c\u8fd9\u4e9b\u662f\u7814\u7a76\u4eba\u5458\u6240\u5c5e\u7684\u9886\u57df\u3002\u8be5\u6570\u636e\u96c6\u6709 5,000\u4e2a\u56fe\uff0c\u6bcf\u4e2a\u56fe\uff08graphs\uff09\u90fd\u6709\u6807\u7b7e 0\u30011 \u6216 2\u3002\n\nCOLLAB\u6570\u636e\u96c6\uff08.txt\uff09\u683c\u5f0f\uff1a\n5000 # \u56fe\u7684\u6570\u91cf\n45 0 # \u7b2c\u4e00\u4e2a\u56fe\u7684\u8282\u70b9\u6570\u548c\u6807\u7b7e\n0 44 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n0 44 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n0 44 0 1 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n.. ... ... \n\n\u7b2c\u4e00\u6b21\u63a5\u89e6\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u636e\u96c6\uff0c\u5fc5\u987b\u641e\u6e05\u695a\u6570\u636e\u96c6\u7684\u683c\u5f0f\uff0c\u624d\u80fd\u6b63\u786e\u5730\u8bfb\u53d6\u6570\u636e\u3002\n'''\n\n\ndef load_data(dataset, degree_as_tag):\n    '''\n        dataset: name of dataset\n        test_proportion: ratio of test train split\n        seed: random seed for random splitting of dataset\n    '''\n    print('loading data')\n    g_list = [] # \u5b58\u653e\u56fe\u5bf9\u8c61\n    label_dict = {} # \u5b58\u653e\u6807\u7b7e\u6620\u5c04\n    feat_dict = {} # \u5b58\u653e\u8282\u70b9\u7279\u5f81\u6620\u5c04\n\n    with open('dataset/%s/%s.txt' % (dataset, dataset), 'r') as f:\n        n_g = int(f.readline().strip())\n        for i in range(n_g):\n            row = f.readline().strip().split()\n            n, l = [int(w) for w in row] # number of nodes and label\n            if not l in label_dict:\n                mapped = len(label_dict)\n                label_dict[l] = mapped\n            g = nx.Graph() # NetworkX\n            node_tags = [] # \u8282\u70b9\u6807\u7b7e\u5217\u8868\n            node_features = [] # \u8282\u70b9\u7279\u5f81\u5217\u8868\n            n_edges = 0 # \u8fb9\u6570\n            for j in range(n):\n                g.add_node(j) # .add_node\u6dfb\u52a0\u8282\u70b9\n                row = f.readline().strip().split()\n                tmp = int(row[1]) + 2 # \u7528\u4e8e\u786e\u5b9a\u8282\u70b9\u5c5e\u6027\u7684\u6570\u91cf\n                if tmp == len(row):\n                    # no node attributes \u6ca1\u6709\u8282\u70b9\u5c5e\u6027\n                    row = [int(w) for w in row]\n                    attr = None\n                else:\n                    row, attr = [int(w) for w in row[:tmp]], np.array([float(w) for w in row[tmp:]])\n                if not row[0] in feat_dict:\n                    mapped = len(feat_dict)\n                    feat_dict[row[0]] = mapped\n                node_tags.append(feat_dict[row[0]]) # \u8282\u70b9\u6807\u7b7e\n\n                if tmp &gt; len(row): \n                    node_features.append(attr)\n\n                n_edges += row[1] # \u52a0\u8fb9\n                for k in range(2, len(row)):\n                    g.add_edge(j, row[k]) # .add_edge\u6dfb\u52a0\u8fb9\n            '''\n            g.add_node\n            g.add_edge\n            '''\n\n            if node_features != []:\n                node_features = np.stack(node_features)\n                node_feature_flag = True\n            else:\n                node_features = None\n                node_feature_flag = False\n\n            assert len(g) == n\n\n            g_list.append(S2VGraph(g, l, node_tags)) #(\u56fe\uff0c\u56fe\u6807\u7b7e\uff0c\u8282\u70b9\u6807\u7b7e)\n\n    #add labels and edge_mat       \n    for g in g_list:\n        g.neighbors = [[] for i in range(len(g.g))]\n        # \u4e3a\u6bcf\u4e2a\u56fe\u7684\u6bcf\u4e2a\u8282\u70b9\u521d\u59cb\u5316\u90bb\u5c45\u5217\u8868\n        for i, j in g.g.edges():\n            g.neighbors[i].append(j)\n            g.neighbors[j].append(i)\n        # \u4e3a\u6bcf\u4e2a\u8282\u70b9\u586b\u5145\u90bb\u5c45\u5217\u8868\uff08\u65e0\u5411\u56fe\uff09\n        degree_list = []\n        for i in range(len(g.g)):\n            g.neighbors[i] = g.neighbors[i]\n            degree_list.append(len(g.neighbors[i])) # \u6bcf\u4e2a\u8282\u70b9\u7684\u5ea6\u6570\n        g.max_neighbor = max(degree_list)\n\n        g.label = label_dict[g.label]\n\n        edges = [list(pair) for pair in g.g.edges()]\n        # \u8fb9\u7684\u8282\u70b9\u5bf9\n        edges.extend([[i, j] for j, i in edges])\n        # \u65e0\u5411\u56fe\n        deg_list = list(dict(g.g.degree(range(len(g.g)))).values())\n        # \u8ba1\u7b97\u6240\u6709\u8282\u70b9\u7684\u5ea6\u6570\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u5217\u8868\n        g.edge_mat = torch.LongTensor(edges).transpose(0,1)\n        # \u7528\u4e8e\u751f\u6210\u7a00\u758f\u77e9\u9635A\n\n    if degree_as_tag:\n        for g in g_list:\n            g.node_tags = list(dict(g.g.degree).values())\n\n    #Extracting unique tag labels   \n    tagset = set([])\n    for g in g_list:\n        tagset = tagset.union(set(g.node_tags))\n\n    tagset = list(tagset)\n    tag2index = {tagset[i]:i for i in range(len(tagset))}\n\n    for g in g_list:\n        g.node_features = torch.zeros(len(g.node_tags), len(tagset))\n        g.node_features[range(len(g.node_tags)), [tag2index[tag] for tag in g.node_tags]] = 1\n\n\n    print('# classes: %d' % len(label_dict))\n    print('# maximum node tag: %d' % len(tagset))\n\n    print(\"# data: %d\" % len(g_list))\n\n    return g_list, len(label_dict)\n\ndef separate_data(graph_list, seed, fold_idx):\n    assert 0 &lt;= fold_idx and fold_idx &lt; 10, \"fold_idx must be from 0 to 9.\"\n    skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = seed)\n\n    labels = [graph.label for graph in graph_list]\n    idx_list = []\n    for idx in skf.split(np.zeros(len(labels)), labels):\n        idx_list.append(idx)\n    train_idx, test_idx = idx_list[fold_idx]\n\n    train_graph_list = [graph_list[i] for i in train_idx]\n    test_graph_list = [graph_list[i] for i in test_idx]\n\n    return train_graph_list, test_graph_list\n\nclass MLP(nn.Module):\n    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n        '''\n            num_layers: number of layers in the neural networks (EXCLUDING the input layer). If num_layers=1, this reduces to linear model.\n            input_dim: dimensionality of input features\n            hidden_dim: dimensionality of hidden units at ALL layers\n            output_dim: number of classes for prediction\n            device: which device to use\n        '''\n\n        super(MLP, self).__init__()\n\n        self.linear_or_not = True #default is linear model\n        self.num_layers = num_layers\n\n        if num_layers &lt; 1:\n            raise ValueError(\"number of layers should be positive!\")\n        elif num_layers == 1:\n            #Linear model\n            self.linear = nn.Linear(input_dim, output_dim)\n        else:\n            #Multi-layer model\n            self.linear_or_not = False\n            self.linears = torch.nn.ModuleList()\n            self.batch_norms = torch.nn.ModuleList()\n\n            self.linears.append(nn.Linear(input_dim, hidden_dim))\n            for layer in range(num_layers - 2):\n                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n            self.linears.append(nn.Linear(hidden_dim, output_dim))\n\n            for layer in range(num_layers - 1):\n                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n\n    def forward(self, x):\n        if self.linear_or_not:\n            #If linear model\n            return self.linear(x)\n        else:\n            #If MLP\n            h = x\n            for layer in range(self.num_layers - 1):\n                h = F.relu(self.batch_norms[layer](self.linears[layer](h)))\n            return self.linears[self.num_layers - 1](h)\n\nclass GraphCNN(nn.Module):\n    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, output_dim, final_dropout, learn_eps, graph_pooling_type, neighbor_pooling_type, device):\n        '''\n            num_layers: number of layers in the neural networks (INCLUDING the input layer)\n            num_mlp_layers: number of layers in mlps (EXCLUDING the input layer)\n            input_dim: dimensionality of input features\n            hidden_dim: dimensionality of hidden units at ALL layers\n            output_dim: number of classes for prediction\n            final_dropout: dropout ratio on the final linear layer\n            learn_eps: If True, learn epsilon to distinguish center nodes from neighboring nodes. If False, aggregate neighbors and center nodes altogether. \n            neighbor_pooling_type: how to aggregate neighbors (mean, average, or max)\n            graph_pooling_type: how to aggregate entire nodes in a graph (mean, average)\n            device: which device to use\n        '''\n\n        super(GraphCNN, self).__init__()\n\n        self.final_dropout = final_dropout\n        self.device = device\n        self.num_layers = num_layers\n        self.graph_pooling_type = graph_pooling_type\n        self.neighbor_pooling_type = neighbor_pooling_type\n        self.learn_eps = learn_eps\n        self.eps = nn.Parameter(torch.zeros(self.num_layers-1))\n        # learn_eps \u662f\u5426\u6dfb\u52a0\u81ea\u73af\n\n        ###List of MLPs\n        self.mlps = torch.nn.ModuleList()\n\n        ###List of batchnorms applied to the output of MLP (input of the final prediction linear layer)\n        self.batch_norms = torch.nn.ModuleList()\n\n        for layer in range(self.num_layers-1):\n            if layer == 0:\n                self.mlps.append(MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim))\n            else:\n                self.mlps.append(MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim))\n\n            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n\n        #Linear function that maps the hidden representation at dofferemt layers into a prediction score\n        self.linears_prediction = torch.nn.ModuleList()\n        for layer in range(num_layers):\n            if layer == 0:\n                self.linears_prediction.append(nn.Linear(input_dim, output_dim))\n            else:\n                self.linears_prediction.append(nn.Linear(hidden_dim, output_dim))\n\n\n    def __preprocess_neighbors_maxpool(self, batch_graph): \n        #\u4e3a\u6700\u5927\u6c60\u5316\u521b\u5efa\u586b\u5145\u7684\u90bb\u5c45\u5217\u8868\n        ###create padded_neighbor_list in concatenated graph\n\n        #compute the maximum number of neighbors within the graphs in the current minibatch\n        max_deg = max([graph.max_neighbor for graph in batch_graph])\n\n        padded_neighbor_list = []\n        start_idx = [0]\n        '''\n        \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u8282\u70b9\u7d22\u5f15\u504f\u79fb\u91cf\uff1f\n\n        \u4e3a\u6bcf\u4e2a\u56fe\u7684\u90bb\u5c45\u8282\u70b9\u6dfb\u52a0\u504f\u79fb\u91cf\u662f\u4e3a\u4e86\u786e\u4fdd\u90bb\u5c45\u8282\u70b9\u5728\u6279\u5904\u7406\u4e2d\u7684\u5168\u5c40\u7d22\u5f15\u662f\u552f\u4e00\u7684\u548c\u6b63\u786e\u7684\n\n        \u5728\u5904\u7406\u591a\u4e2a\u56fe\u65f6\uff0c\u56fe\u7684\u8282\u70b9\u7d22\u5f15\u662f\u5c40\u90e8\u7684\u3002\u6bcf\u4e2a\u56fe\u7684\u8282\u70b9\u7d22\u5f15\u4ece0\u5f00\u59cb\uff0c\u800c\u5728\u6279\u5904\u7406\u7684\u4e0a\u4e0b\u6587\u4e2d\uff0c\u6240\u6709\u56fe\u7684\u8282\u70b9\u7d22\u5f15\u9700\u8981\u5408\u5e76\u6210\u4e00\u4e2a\u7edf\u4e00\u7684\u7d22\u5f15\u7a7a\u95f4\u3002\u901a\u8fc7\u52a0\u4e0a start_idx[i]\uff0c\u53ef\u4ee5\u5c06\u5f53\u524d\u56fe\u7684\u90bb\u5c45\u8282\u70b9\u7d22\u5f15\u8f6c\u6362\u4e3a\u5168\u5c40\u7d22\u5f15\uff0c\u907f\u514d\u7d22\u5f15\u51b2\u7a81\u3002\n        '''\n\n        for i, graph in enumerate(batch_graph):\n            start_idx.append(start_idx[i] + len(graph.g))\n            #\u66f4\u65b0 start_idx\u5217\u8868\uff0c\u5c06\u5f53\u524d\u56fe\u7684\u8282\u70b9\u6570\u6dfb\u52a0\u5230\u524d\u4e00\u4e2a\u56fe\u7684\u7ed3\u675f\u7d22\u5f15\uff0c\u4ee5\u4fbf\u4e3a\u4e0b\u4e00\u4e2a\u56fe\u7684\u8282\u70b9\u5206\u914d\u6b63\u786e\u7684\u7d22\u5f15\u3002\n            padded_neighbors = []\n            for j in range(len(graph.neighbors)):\n                #\u7b2ci\u4e2a\u56fe\u7b2cj\u4e2a\u8282\u70b9\u7684\u90bb\u5c45\u5217\u8868\n                #add off-set values to the neighbor indices\n                pad = [n + start_idx[i] for n in graph.neighbors[j]]\n                #padding, dummy data is assumed to be stored in -1\n                #\u4e3a\u6bcf\u4e2a\u90bb\u5c45\u8282\u70b9\u6dfb\u52a0\u504f\u79fb\u91cf\uff0c\u786e\u4fdd\u5b83\u4eec\u7684\u7d22\u5f15\u5728\u6574\u4e2a\u6279\u6b21\u4e2d\u7684\u6b63\u786e\u4f4d\u7f6e\u3002(\u5c06\u90bb\u5c45\u8282\u70b9\u7684\u7d22\u5f15\u8f6c\u6362\u4e3a\u5168\u5c40\u7d22\u5f15)\n\n                pad.extend([-1]*(max_deg - len(pad)))\n                #\u5c06\u90bb\u5c45\u5217\u8868\u586b\u5145\u5230 max_deg \u7684\u957f\u5ea6\uff0c\u586b\u5145\u90e8\u5206\u7528 -1 \u8868\u793a\uff0c\u8868\u793a\u65e0\u6548\u7684\u90bb\u5c45\n                #Add center nodes in the maxpooling if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n                if not self.learn_eps:\n                    pad.append(j + start_idx[i])\n\n                padded_neighbors.append(pad)\n            padded_neighbor_list.extend(padded_neighbors)\n        '''\n        output:padded_neighbor_list-&gt;[num_nodes, max_degree]\n        '''\n        return torch.LongTensor(padded_neighbor_list)\n\n\n    def __preprocess_neighbors_sumavepool(self, batch_graph):\n        ###create block diagonal sparse matrix\n        # \u6784\u5efa\u4e00\u4e2a\u7528\u4e8e\u6c42\u548c\u6216\u5e73\u5747\u6c60\u5316\u7684\u5757\u5bf9\u89d2\u7a00\u758f\u77e9\u9635\n        edge_mat_list = []\n        start_idx = [0]\n        for i, graph in enumerate(batch_graph):\n            start_idx.append(start_idx[i] + len(graph.g))\n            edge_mat_list.append(graph.edge_mat + start_idx[i])\n            # \u5c06\u5f53\u524d\u56fe\u7684\u8fb9\u77e9\u9635 graph.edge_mat \u7684\u7d22\u5f15\u52a0\u4e0a\u5f53\u524d\u56fe\u7684\u8d77\u59cb\u7d22\u5f15\uff0c\u4ee5\u4fbf\u5c06\u6240\u6709\u56fe\u5408\u5e76\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u7d22\u5f15\u7a7a\u95f4\u3002\n        Adj_block_idx = torch.cat(edge_mat_list, 1)\n        Adj_block_elem = torch.ones(Adj_block_idx.shape[1])\n\n        #Add self-loops in the adjacency matrix if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n\n        if not self.learn_eps:\n            num_node = start_idx[-1]\n            self_loop_edge = torch.LongTensor([range(num_node), range(num_node)])\n            elem = torch.ones(num_node)\n            Adj_block_idx = torch.cat([Adj_block_idx, self_loop_edge], 1)\n            Adj_block_elem = torch.cat([Adj_block_elem, elem], 0)\n\n        Adj_block = torch.sparse.FloatTensor(Adj_block_idx, Adj_block_elem, torch.Size([start_idx[-1],start_idx[-1]]))\n\n        return Adj_block.to(self.device)\n\n\n    def __preprocess_graphpool(self, batch_graph):\n        ###create sum or average pooling sparse matrix over entire nodes in each graph (num graphs x num nodes)\n\n        start_idx = [0]\n\n        #compute the padded neighbor list\n        for i, graph in enumerate(batch_graph):\n            start_idx.append(start_idx[i] + len(graph.g))\n\n        idx = []\n        elem = []\n        #idx\u7528\u4e8e\u5b58\u50a8\u7a00\u758f\u77e9\u9635\u4e2d\u975e\u96f6\u5143\u7d20\u7684\u7d22\u5f15\uff0celem\u7528\u4e8e\u5b58\u50a8\u5bf9\u5e94\u7684\u6743\u91cd\n        for i, graph in enumerate(batch_graph):\n            ###average pooling   \u6743\u91cd\u4e3a 1 / len(graph.g)\n            if self.graph_pooling_type == \"average\":\n                elem.extend([1./len(graph.g)]*len(graph.g))\n\n            else:\n            ###sum pooling   \u6743\u91cd\u4e3a 1 \n                elem.extend([1]*len(graph.g))\n\n            idx.extend([[i, j] for j in range(start_idx[i], start_idx[i+1], 1)])\n            #\u5c06\u6bcf\u4e2a\u8282\u70b9\u7684\u5168\u5c40\u7d22\u5f15\u6dfb\u52a0\u5230idx\u5217\u8868\u4e2d\uff0c\u683c\u5f0f\u4e3a: \n            # [[\u56fe\u7d22\u5f15, \u8282\u70b9\u7d22\u5f15], ...]\uff0c\u56fe\u4e0e\u5176\u7684\u8282\u70b9\u7d22\u5f15\u5bf9\u5e94\n        elem = torch.FloatTensor(elem)\n        idx = torch.LongTensor(idx).transpose(0,1)\n        graph_pool = torch.sparse.FloatTensor(idx, elem, torch.Size([len(batch_graph), start_idx[-1]]))\n        '''\n        output:graph_pool \u5173\u4e8e\u56fe\u4e0e\u8282\u70b9\u5173\u7cfb\u7684\u7a00\u758f\u77e9\u9635\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5143\u7d20\u8868\u793a\u4e00\u4e2a\u8282\u70b9\u5b58\u5728\u4e8e\u54ea\u4e2a\u56fe\n        '''\n        return graph_pool.to(self.device)\n\n    def maxpool(self, h, padded_neighbor_list):\n        ###Element-wise minimum will never affect max-pooling\n        #\u5bf9\u56fe\u4e2d\u7684\u8282\u70b9\u7279\u5f81\u8fdb\u884c\u6700\u5927\u6c60\u5316\n        '''\n        h: \u7279\u5f81\u77e9\u9635\n        padded_neighbor_list: \u90bb\u5c45\u5217\u8868\n        '''\n        dummy = torch.min(h, dim = 0)[0]\n        h_with_dummy = torch.cat([h, dummy.reshape((1, -1)).to(self.device)])\n        #\u5c06\u539f\u7279\u5f81\u77e9\u9635h\u548c\u865a\u62df\u8282\u70b9dummy\u7ed3\u5408\uff0c\u5f62\u6210\u65b0\u7684\u7279\u5f81\u77e9\u9635 h_with_dummy\n        pooled_rep = torch.max(h_with_dummy[padded_neighbor_list], dim = 1)[0] \n        # dummy\u5bf9\u5e94padded_neighbor_list\u4e2d\u7684\u5143\u7d20-1\n        '''\n        output: pooled_rep \u901a\u8fc7\u6700\u5927\u6c60\u5316\u4ece\u90bb\u8282\u70b9\u7279\u5f81\u4e2d\u63d0\u53d6\u4fe1\u606f\n        '''\n        return pooled_rep\n\n    def next_layer_eps(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n        ###pooling neighboring nodes and center nodes separately by epsilon reweighting. \n\n        if self.neighbor_pooling_type == \"max\":\n            ##If max pooling\n            pooled = self.maxpool(h, padded_neighbor_list)\n        else:\n            #If sum or average pooling\n            pooled = torch.spmm(Adj_block, h)\n            if self.neighbor_pooling_type == \"average\":\n                #If average pooling\n                degree = torch.spmm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))\n                pooled = pooled/degree\n\n        #Reweights the center node representation when aggregating it with its neighbors\n        pooled = pooled + (1 + self.eps[layer])*h\n        pooled_rep = self.mlps[layer](pooled)\n        h = self.batch_norms[layer](pooled_rep)\n\n        #non-linearity\n        h = F.relu(h)\n        return h\n\n    def next_layer(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n        ###pooling neighboring nodes and center nodes altogether  \n        # \u5c06\u90bb\u5c45\u8282\u70b9\u548c\u4e2d\u5fc3\u8282\u70b9\u7684\u8868\u793a\u4e00\u8d77\u5904\u7406    \n        if self.neighbor_pooling_type == \"max\":\n            ##If max pooling\n            pooled = self.maxpool(h, padded_neighbor_list)\n        else:\n            #If sum or average pooling\n            pooled = torch.spmm(Adj_block, h)\n            if self.neighbor_pooling_type == \"average\":\n                #If average pooling\n                degree = torch.spmm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))\n                pooled = pooled/degree\n\n        #representation of neighboring and center nodes \n        pooled_rep = self.mlps[layer](pooled)\n\n        h = self.batch_norms[layer](pooled_rep)\n\n        #non-linearity\n        h = F.relu(h)\n        return h\n\n    '''\n    - next_layer_eps \u5728\u805a\u5408\u65f6\u5173\u6ce8\u4e8e\u4e2d\u5fc3\u8282\u70b9\u7684\u91cd\u6743\u91cd\uff0c\u786e\u4fdd\u5728\u6c60\u5316\u65f6\u8003\u8651\u4e2d\u5fc3\u8282\u70b9\u7684\u7279\u5f81\u3002\n    - next_layer \u5219\u662f\u5c06\u90bb\u5c45\u548c\u4e2d\u5fc3\u8282\u70b9\u7684\u7279\u5f81\u76f4\u63a5\u5408\u5e76\u8fdb\u884c\u5904\u7406\u3002\n    '''\n\n    def forward(self, batch_graph):\n        X_concat = torch.cat([graph.node_features for graph in batch_graph], 0).to(self.device)\n        graph_pool = self.__preprocess_graphpool(batch_graph)\n\n        if self.neighbor_pooling_type == \"max\":\n            padded_neighbor_list = self.__preprocess_neighbors_maxpool(batch_graph)\n        else:\n            Adj_block = self.__preprocess_neighbors_sumavepool(batch_graph)\n\n        #list of hidden representation at each layer (including input)\n        hidden_rep = [X_concat]\n        h = X_concat\n\n        for layer in range(self.num_layers-1):\n            if self.neighbor_pooling_type == \"max\" and self.learn_eps:\n                h = self.next_layer_eps(h, layer, padded_neighbor_list = padded_neighbor_list)\n            elif not self.neighbor_pooling_type == \"max\" and self.learn_eps:\n                h = self.next_layer_eps(h, layer, Adj_block = Adj_block)\n            elif self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n                h = self.next_layer(h, layer, padded_neighbor_list = padded_neighbor_list)\n            elif not self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n                h = self.next_layer(h, layer, Adj_block = Adj_block)\n\n            hidden_rep.append(h)\n\n        score_over_layer = 0\n\n        #perform pooling over all nodes in each graph in every layer\n        for layer, h in enumerate(hidden_rep):\n            pooled_h = torch.spmm(graph_pool, h)\n            #graph_pool\u662f\u4e00\u4e2a\u7a00\u758f\u77e9\u9635\uff0c\u8868\u793a\u8282\u70b9\u5728\u56fe\u4e2d\u7684\u805a\u5408\u65b9\u5f0f\u3002\u901a\u8fc7\u77e9\u9635\u4e58\u6cd5torch.spmm\uff08\u7a00\u758f\u77e9\u9635\u4e0e\u7a20\u5bc6\u77e9\u9635\u7684\u4e58\u6cd5\uff09\uff0c\u5c06\u5f53\u524d\u5c42\u7684\u9690\u85cf\u8868\u793ah\u8fdb\u884c\u6c60\u5316\u3002\n            score_over_layer += F.dropout(self.linears_prediction[layer](pooled_h), self.final_dropout, training = self.training)\n\n        return score_over_layer\n</code></pre> <p>\u5bf9\u4e8e\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\uff0c\u8282\u70b9\u8868\u793a\\(h_v^{(k)}\\)\u4f5c\u4e3a\u9884\u6d4b\u7684\u8f93\u5165\uff1b\u5bf9\u4e8e\u56fe\u5206\u7c7b\u4efb\u52a1\uff0cREADOUT\u51fd\u6570\u805a\u5408\u4e86\u6700\u540e\u4e00\u4fa7\u8fed\u4ee3\u8f93\u51fa\u7684\u8282\u70b9\u8868\u793a\\(h_v^{(k)}\\),\u5e76\u751f\u6210\u56fe\u8868\u793a\\(h_G\\):</p> \\[ h_G = READOUT( \\{ h_v^{(k)} | v \\in G \\} ) \\] <p>READOUT\u51fd\u6570\u662f\u5177\u6709\u6392\u5217\u4e0d\u53d8\u6027\u7684\u51fd\u6570\uff0c\u5982sum,average, max-pooling...</p> <p>\u800c\u672c\u6587\u63d0\u51faREADOUT\u51fd\u6570\u4f7f\u7528<code>Concat+Sum</code>,\u5bf9\u6bcf\u6b21\u8fed\u4ee3\u6240\u5f97\u5230\u7684\u6240\u6709\u8282\u70b9\u7279\u5f81\u6c42\u548c\u4ee5\u5f97\u5230\u56fe\u7684\u7279\u5f81\uff0c\u6700\u540e\u62fc\u63a5\u8d77\u6765</p> \\[ h_G = Concat(Sum(\\{ h_v^{(k)} | v \\in G \\})| k = 0,1,...,K)  \\] <p>\u4e09\u79cd\u4e0d\u540c\u7684aggregate\u51fd\u6570\uff1a</p> <ul> <li> <p>sum\uff1a\u5b66\u4e60\u5168\u90e8\u7684\u6807\u7b7e\u4ee5\u53ca\u6570\u91cf\uff0c\u53ef\u4ee5\u5b66\u4e60\u7cbe\u786e\u7684\u7ed3\u6784\u4fe1\u606f\uff08\u4e0d\u4ec5\u4fdd\u5b58\u4e86\u5206\u5e03\u4fe1\u606f\uff0c\u8fd8\u4fdd\u5b58\u4e86\u7c7b\u522b\u4fe1\u606f\uff09</p> </li> <li> <p>mean\uff1a\u5b66\u4e60\u6807\u7b7e\u7684\u6bd4\u4f8b\uff08\u6bd4\u5982\u4e24\u4e2a\u56fe\u6807\u7b7e\u6bd4\u4f8b\u76f8\u540c\uff0c\u4f46\u662f\u8282\u70b9\u6709\u500d\u6570\u5173\u7cfb\uff09\uff0c\u504f\u5411\u5b66\u4e60\u5206\u5e03\u4fe1\u606f</p> </li> <li> <p>max\uff1a\u5b66\u4e60\u6700\u5927\u6807\u7b7e\uff0c\u5ffd\u7565\u591a\u6837\uff0c\u504f\u5411\u5b66\u4e60\u6709\u4ee3\u8868\u6027\u7684\u5143\u7d20\u4fe1\u606f</p> </li> </ul> <p></p> <p></p> <ul> <li>\u501f\u52a9<code>DGL</code>\u5e93\u7684\u7b80\u6d01\u5b9e\u73b0</li> </ul> <pre><code>import argparse\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom dgl.data import GINDataset\nfrom dgl.dataloading import GraphDataLoader\nfrom dgl.nn.pytorch.conv import GINConv\nfrom dgl.nn.pytorch.glob import SumPooling\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n\nclass MLP(nn.Module):\n    \"\"\"Construct two-layer MLP-type aggreator for GIN model\"\"\"\n\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.linears = nn.ModuleList()\n        # two-layer MLP\n        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))\n        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))\n        self.batch_norm = nn.BatchNorm1d((hidden_dim))\n\n    def forward(self, x):\n        h = x\n        h = F.relu(self.batch_norm(self.linears[0](h)))\n        return self.linears[1](h)\n\n\nclass GIN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.ginlayers = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        num_layers = 5\n        # five-layer GCN with two-layer MLP aggregator and sum-neighbor-pooling scheme\n        for layer in range(num_layers - 1):  # excluding the input layer\n            if layer == 0:\n                mlp = MLP(input_dim, hidden_dim, hidden_dim)\n            else:\n                mlp = MLP(hidden_dim, hidden_dim, hidden_dim)\n            self.ginlayers.append(\n                GINConv(mlp, learn_eps=False)\n            )  # set to True if learning epsilon\n            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n        # linear functions for graph sum poolings of output of each layer\n        self.linear_prediction = nn.ModuleList()\n        for layer in range(num_layers):\n            if layer == 0:\n                self.linear_prediction.append(nn.Linear(input_dim, output_dim))\n            else:\n                self.linear_prediction.append(nn.Linear(hidden_dim, output_dim))\n        self.drop = nn.Dropout(0.5)\n        self.pool = (\n            SumPooling()\n        )  # change to mean readout (AvgPooling) on social network datasets\n\n    def forward(self, g, h):\n        # list of hidden representation at each layer (including the input layer)\n        hidden_rep = [h]\n        for i, layer in enumerate(self.ginlayers):\n            h = layer(g, h)\n            h = self.batch_norms[i](h)\n            h = F.relu(h)\n            hidden_rep.append(h)\n        score_over_layer = 0\n        # perform graph sum pooling over all nodes in each layer\n        for i, h in enumerate(hidden_rep):\n            pooled_h = self.pool(g, h)\n            score_over_layer += self.drop(self.linear_prediction[i](pooled_h))\n        return score_over_layer\n\n\ndef split_fold10(labels, fold_idx=0):\n    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n    idx_list = []\n    for idx in skf.split(np.zeros(len(labels)), labels):\n        idx_list.append(idx)\n    train_idx, valid_idx = idx_list[fold_idx]\n    return train_idx, valid_idx\n\n\ndef evaluate(dataloader, device, model):\n    model.eval()\n    total = 0\n    total_correct = 0\n    for batched_graph, labels in dataloader:\n        batched_graph = batched_graph.to(device)\n        labels = labels.to(device)\n        feat = batched_graph.ndata.pop(\"attr\")\n        total += len(labels)\n        logits = model(batched_graph, feat)\n        _, predicted = torch.max(logits, 1)\n        total_correct += (predicted == labels).sum().item()\n    acc = 1.0 * total_correct / total\n    return acc\n\n\ndef train(train_loader, val_loader, device, model):\n    # loss function, optimizer and scheduler\n    loss_fcn = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n\n    # training loop\n    for epoch in range(350):\n        model.train()\n        total_loss = 0\n        for batch, (batched_graph, labels) in enumerate(train_loader):\n            batched_graph = batched_graph.to(device)\n            labels = labels.to(device)\n            feat = batched_graph.ndata.pop(\"attr\")\n            logits = model(batched_graph, feat)\n            loss = loss_fcn(logits, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        scheduler.step() #\u66f4\u65b0optimizer\u7684learning rate\n        train_acc = evaluate(train_loader, device, model)\n        valid_acc = evaluate(val_loader, device, model)\n        print(\n            \"Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(\n                epoch, total_loss / (batch + 1), train_acc, valid_acc\n            )\n        )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset\",\n        type=str,\n        default=\"MUTAG\",\n        choices=[\"MUTAG\", \"PTC\", \"NCI1\", \"PROTEINS\"],\n        help=\"name of dataset (default: MUTAG)\",\n    )\n    args = parser.parse_args()\n    print(f\"Training with DGL built-in GINConv module with a fixed epsilon = 0\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # load and split dataset\n    dataset = GINDataset(\n        args.dataset, self_loop=True, degree_as_nlabel=False\n    )  # add self_loop and disable one-hot encoding for input features\n    labels = [l for _, l in dataset]\n    train_idx, val_idx = split_fold10(labels)\n\n    # create dataloader\n    train_loader = GraphDataLoader(\n        dataset,\n        sampler=SubsetRandomSampler(train_idx),\n        batch_size=128,\n        pin_memory=torch.cuda.is_available(),\n    )\n    val_loader = GraphDataLoader(\n        dataset,\n        sampler=SubsetRandomSampler(val_idx),\n        batch_size=128,\n        pin_memory=torch.cuda.is_available(),\n    )\n\n    # create GIN model\n    in_size = dataset.dim_nfeats\n    out_size = dataset.gclasses\n    model = GIN(in_size, 16, out_size).to(device)\n\n    # model training/validating\n    print(\"Training...\")\n    train(train_loader, val_loader, device, model)\n</code></pre>"},{"location":"python/cs224w-notebook/chapter6/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p>"},{"location":"python/cs224w-notebook/chapter6/#limitations-of-graph-neural-networks","title":"Limitations of Graph Neural Networks","text":"<p>A perfect GNN should build an injective function between neighborhood structure (regardless of Shops) and node embeddings</p> <p></p> <p></p> <p></p> <p></p> <p>GNN\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u5927\u5c0f\u89c4\u5219\u56fe\u4e2d\u7684\u8282\u70b9</p> <p> </p> <p>\u5728\u94fe\u8def\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4e0d\u80fd\u533a\u5206\u90bb\u57df\u7ed3\u6784\u76f8\u540c\u4f46\u5230\u6e90\u8282\u70b9\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\u4e0d\u540c\u7684\u5019\u9009\u8282\u70b9</p> <p> </p> <p>\u5728\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5b83\u4eec\u4e0d\u80fd\u533a\u5206\u6b63\u5219\u56fe</p> <p>d-\u6b63\u5219\u56fe\u6307\u7684\u662f\uff0c\u5bf9\u4e8e\u4e00\u5f20\u65e0\u5411\u7684graph\uff0c\u6bcf\u4e2a\u9876\u70b9\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u90bb\u8282\u70b9\uff0c \u5373\u6bcf\u4e2a\u9876\u70b9\u5177\u6709\u76f8\u540c\u7684\u5ea6\u3002 \u5bf9\u4e8e\u4e00\u5f20\u6709\u5411\u7684\u6b63\u5219\u56fe\uff0c\u6bcf\u4e2a\u9876\u70b9\u7684\u51fa\u5ea6\u548c\u5165\u5ea6\u90fd\u76f8\u540c\u3002</p> <p></p> <p>\u4e0a\u4e00\u8282\u63d0\u5230\u7684GIN:</p> \\[ c^{(l+1)}_v = MLP \\Big( (1+\\epsilon)c^{(l)}_v + \\sum_{u \\in \\mathcal{N}(v)} c^{(l)}_u \\Big) \\] <p>\u5c06MLP\u7684\u7b2c\u4e00\u5c42\u5355\u72ec\u62c6\u89e3\uff0c\u516c\u5f0f\u53ef\u8868\u793a\u4e3a\uff1a</p> \\[ c^{(l+1)}_v = MLP_{-1} \\Big( \\sigma \\Big( (1+\\epsilon)c^{(l)}_v + \\sum_{u \\in \\mathcal{N}(v)} c^{(l)}_u \\Big) \\Big) \\] <p>\u6211\u4eec\u53ef\u4ee5\u4ee5\u77e9\u9635\u5f62\u5f0f\u7f16\u5199:</p> \\[ \\begin{aligned} C^{(l+1)} &amp;= MLP_{-1} \\Big( \\sigma \\Big( C^{(l)} W_0^{(l)} + A C^{(l)} W_1^{(l)}  \\Big) \\Big) \\\\ &amp;= MLP_{-1} \\Big( \\sigma \\Big( \\sum_{k=0}^1 A^k C^{(l)} W_k^{(l)} \\Big) \\Big) \\end{aligned} \\] <p>\u5176\u4e2d\\(C^{(l)} \\in \\mathbb{R}^{N \\times d}, \\ C^{(l)}[v,:] = c^{(l)}_v\\)\uff0c\\(A \\in \\{ 0,1 \\}^{N \\times N}\\)\u662f\u8be5\u56fe\u7684\u90bb\u63a5\u77e9\u9635\u3002</p> <p>\u6211\u4eec\u5c06\u90bb\u63a5\u77e9\u9635\u8fdb\u884c\u5947\u5f02\u503c\u5206\u89e3\uff1a</p> \\[ A = V \u039b V^T \\] <p>\u5176\u4e2d\\(V=[v_1,...,v_n]\\)\u4e3a\u7279\u5f81\u5411\u91cf\u7684\u6b63\u4ea4\u77e9\u9635\uff0c\\(\u039b\\)\u662f\u7279\u5f81\u503c\u5bf9\u89d2\u77e9\u9635\\(\\{ \\lambda_n \\}^N_{n=1}\\)\u3002</p> <ul> <li> <p>\u90bb\u63a5\u77e9\u9635\u7684\u7279\u5f81\u503c\u5206\u89e3\uff08\u8c31\u5206\u89e3\uff09\u662f\u56fe\u7684\u666e\u904d\u7279\u5f81\uff08universal characterization of the graph\uff09</p> </li> <li> <p>\u4e0d\u540c\u7684\u56fe\u5177\u6709\u4e0d\u540c\u7684\u8c31\u5206\u89e3\uff08Different graphs have different spectral decompositions\uff09</p> </li> <li> <p>\u56fe\u4e2d\u7684\u73af\u7684\u6570\u91cf\u53ef\u4ee5\u770b\u4f5c\u56fe\u7684\u90bb\u63a5\u77e9\u9635\u7684\u7279\u5f81\u503c\u4e0e\u7279\u5f81\u5411\u91cf\u7684\u51fd\u6570</p> </li> </ul> <p>\u8c31\u65b9\u6cd5\uff0c\u5f85\u8865\u5145</p>"},{"location":"python/cs224w-notebook/chapter6/#feature-augmentation-structurally-aware-gnns","title":"Feature Augmentation: Structurally-Aware GNNs","text":"<p>\u57fa\u4e8e\u6807\u6ce8\u63d0\u5347\u7684ID-GNN:Identity-aware Graph Neural Networks</p> <p>ID-GNN\u7684\u6df1\u5165\u4ee3\u7801\u89e3\u8bfb</p> <p></p> <p>In fact, certain structures are hard to learn by GNNs</p> <p></p> <p>\u7531\u524d\u51e0\u8282\u53ef\u77e5\uff0c\u6d88\u606f\u4f20\u9012GNNs\u7684\u8868\u8fbe\u80fd\u529b\u4e0a\u9650\u662f<code>1-Weisfeiler-Lehman\uff081-WL\uff09</code>(\u56fe\u540c\u6784\u68c0\u9a8c)\uff0c\u8fd9\u610f\u5473\u7740\u666e\u901aGNN\u4e0d\u80fd\u9884\u6d4b\u8282\u70b9\u805a\u7c7b\u7cfb\u6570\u548c\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\uff0c\u4e5f\u4e0d\u80fd\u533a\u5206\u4e0d\u540c\u7684\u6b63\u5219\u56fe\u3002</p> <p></p> <p>\u6b63\u5219\u56fe\u6307\u7684\u662f\u5404\u9876\u70b9\u7684\u5ea6\u5747\u76f8\u540c\u7684\u65e0\u5411\u7b80\u5355\u56fe\uff0c\u5bf9\u4e8e\u5e38\u89c4GNN\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u90bb\u57df\u7ed3\u6784\u7684\u8282\u70b9\u53ef\u80fd\u62e5\u6709\u76f8\u540c\u7684\u8ba1\u7b97\u56fe\uff0c\u4ece\u800c\u51fa\u73b0\u96be\u4ee5\u533a\u5206\u7684\u73b0\u8c61\u3002</p> <p>\u8bba\u6587\u4e2d\u63d0\u51fa<code>Identity-aware GraphNeural Networks\uff08ID-GNNs\uff09</code>\uff0c\u6bd41-WL\u5177\u6709\u66f4\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\u3002</p> <p>ID-GNN\u901a\u8fc7\u5728\u6d88\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u5f52\u7eb3\u5730\uff08inductively\uff09\u8003\u8651\u8282\u70b9\u7684\u8eab\u4efd\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7684GNN\u4f53\u7cfb\u7ed3\u6784\u3002\u4e3a\u4e86\u83b7\u5f97\u4e00\u4e2a\u7ed9\u5b9a\u7684\u8282\u70b9\u7684\u5d4c\u5165\uff0cID-GNN\u9996\u5148\u63d0\u53d6\u4ee5\u8be5\u8282\u70b9\u4e3a\u4e2d\u5fc3\u7684Ego-Networks\uff08\u5373\u81ea\u6211\u7f51\u7edc\uff09\uff0c\u7136\u540e\u8fdb\u884c\u591a\u8f6e\u7684\u5f02\u6784\u6d88\u606f\u4f20\u9012\uff0c\u5728\u81ea\u6211\u7f51\u7edc\u7684\u4e2d\u5fc3\u8282\u70b9\u4e0a\u5e94\u7528\u4e0e\u5176\u4ed6\u8282\u70b9\u4e0d\u540c\u7684\u53c2\u6570\uff08\u5373\u81ea\u6211\u7f51\u7edc\u7684\u4e2d\u5fc3\u8282\u70b9\u4e0e\u5176\u4ed6\u8282\u70b9\u7684\u805a\u5408\u51fd\u6570\u4e0d\u4e3a\u540c\u4e00\u4e2a\uff09\u3002 \u540c\u65f6\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u5feb\u7684ID-GNN-fast\uff0c\u5b83\u5c06\u8282\u70b9\u8eab\u4efd\u4fe1\u606f\u4f5c\u4e3a\u589e\u5f3a\u7684\u8282\u70b9\u7279\u5f81\u6ce8\u5165\u8282\u70b9\u4e2d\u3002</p> <p>\u5047\u8bbe\u56fe\\(\\mathcal{G}=(\\mathcal{V},\\epsilon)\\)\uff0c\u5176\u4e2d\\(\\mathcal{V}=\\{ v_1,...,v_n \\}\\)\u4ee3\u8868\u8282\u70b9\u96c6\uff0c\\(\\epsilon\\)\u4ee3\u8868\u56fe\u4e2d\u8282\u70b9\u7684\u8fde\u63a5\u5173\u7cfb\uff0c\u8282\u70b9\u7279\u5f81\\(\\mathcal{X} = \\{ x_v |\\ \\forall v \\in \\mathcal{V} \\}\\)\uff0c\u8fb9\u4e5f\u53ef\u4ee5\u5b58\u5728\u7279\u5f81\\(\\mathcal{F}=\\{ f_{uv} | \\ \\forall e_{uv} \\in \\epsilon \\}\\)\u3002\u5bf9\u4e8e\u8fed\u4ee3k\u5c42\u7684\u5e38\u89c1GNN\u7f51\u7edc\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> \\[ \\mathbf{m}_u^{(k)}=\\mathbf{MSG}^{(k)}(h_u^{(k-1)})\\\\h_v^{(k)}=\\mathbf{AGG}^{(k)}(\\{\\mathbf{m}_u^{}(k),u\\in\\mathcal{N}(v)\\},h_v^{(k-1)}) \\] <p>ID-GNN \u7684\u4e24\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff1a</p> <ul> <li> <p>\u5f52\u7eb3\u8eab\u4efd\u7740\u8272\uff0c\u5c06\u8eab\u4efd\u4fe1\u606f\u6ce8\u5165\u5230\u6bcf\u4e2a\u8282\u70b9</p> </li> <li> <p>\u5f02\u6784\u6d88\u606f\u4f20\u9012\uff0c\u5728\u6d88\u606f\u4f20\u9012\u4e2d\u4f7f\u7528\u8eab\u4efd\u4fe1\u606f</p> </li> </ul> <p></p> <p>\u6211\u4eec\u5148\u62bd\u53d6\u4ee5\u8282\u70b9 \\(v\\) \u4e3a\u4e2d\u5fc3\u8fdb\u884cK-hop\u5f97\u5230\u7684Ego-Networks \\(\\mathcal{G}^{(K)}_v\\)\uff0c\uff0c\u5176\u8282\u70b9\u5206\u4e3a\u4e24\u7c7b\uff1a\u7740\u8272\u8282\u70b9\u4e0e\u975e\u7740\u8272\u8282\u70b9\u3002\u7136\u540e\u8fdb\u884c\u591a\u8f6e\u7684\u5f02\u6784\u6d88\u606f\u4f20\u9012\uff0c\u5728\u81ea\u6211\u7f51\u7edc\u7684\u4e2d\u5fc3\u8282\u70b9\u4e0a\u5e94\u7528\u4e0e\u5176\u4ed6\u8282\u70b9\u4e0d\u540c\u7684\u53c2\u6570\u3002</p> <p>\u5373\u6bcf\u4e2a\u8282\u70b9\u5f62\u6210\u4e86\u4e00\u4e2a\u4ee5\u8fd9\u4e2a\u8282\u70b9\u4e3a\u4e2d\u5fc3\u7684ego network(subgraphs)\uff0c\u5c06batch graph\u7684\u65b9\u5f0f\u8f6c\u5316\u4e3a\u4e00\u4e2abig batch graph\u3002</p> <p>ID-GNN\u7684\u60f3\u6cd5\u5f88\u7b80\u5355, \u5c31\u662f\u4e3a\u5bf9ego\u8282\u70b9\u548c\u5176\u5b83\u8282\u70b9\u91c7\u53d6\u4e0d\u540c\u7684\u6d88\u606f\u4f20\u64ad\u65b9\u5f0f:</p> \\[ \\mathbf{m}_{su}^{(k)}=\\mathbf{MSG}_{1[s=v]}^{(k)}(h_s^{(k-1)}, f_{su})\\\\h_u^{(k)}=\\mathbf{AGG}^{(k)}(\\{\\mathbf{m}_{su}^{(k)},s \\in \\mathcal{N}(u)\\},h_u^{(k-1)}) \\] <p>\u5f02\u6784\u6d88\u606f\u4f20\u9012\u4f7f\u7528\u4e86\u4e24\u79cd\\(MSG^{(k)}\\)\uff0c\\(MSG_1^{(k)}\\)\u7528\u4e8e\u7740\u8272\u8282\u70b9\u7684\u5d4c\u5165\uff0c\\(MSG_0^{(k)}\\)\u7528\u4e8e\u975e\u7740\u8272\u8282\u70b9\u7684\u5d4c\u5165\uff0c\\(1[s=v]\\)\u4e3a\\(MSG\\)\u7684\u793a\u6027\u51fd\u6570\u3002\u5982\u679c\u5f53\u524d\u8282\u70b9\u662f\u67d0\u4e2aego network\u7684\u4e2d\u5fc3\u70b9(seed node)\uff0c\u5219\u8fd9\u4e2a\u8282\u70b9\u4f1a\u989d\u5916\u8fdb\u884c\u4e00\u6b21<code>Linear</code>\uff0c\u7136\u540e\u548c\u6309\u7167\u6b63\u5e38\u8ba1\u7b97\u51fa\u6765\u7684GNN\u7684outputs\uff0c\u505a\u4e00\u4e2aseed node\u4e0a\u7684\u76f8\u52a0\u3002</p> <p><code>ID-GNN-Fast</code>\u901a\u8fc7\u6e90\u81ea\u7ed9\u5b9a\u8282\u70b9\u7684\u5468\u671f\u8ba1\u6570\uff08cycle counts\uff09\u6ce8\u5165\u8eab\u4efd\u4fe1\u606f\u4f5c\u4e3a\u589e\u5f3a\u8282\u70b9\u7279\u5f81\u3002cycle counts\u901a\u8fc7\u5bf9\u8ba1\u7b97\u56fe\u7684\u6bcf\u4e00\u5c42\u4e2d\u7684\u7740\u8272\u8282\u70b9\u8fdb\u884c\u8ba1\u6570\u6765\u6355\u83b7\u8282\u70b9\u8eab\u4efd\u4fe1\u606f\\(x^+_v \\in \\mathcal{R}\\)\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u56fe\u7684\u90bb\u63a5\u77e9\u9635\u7684\u5e42\u6765\u6709\u6548\u5730\u8ba1\u7b97\\(x^+_v [k] = Diag (A^k) [v]\\)\uff0c\u6700\u7ec8\u901a\u8fc7\u7ea7\u8054\u64cd\u4f5c\u5b8c\u6210\u589e\u5f3a\\(x_v = Concat(x_v, x^+_v)\\)\u3002</p> <p></p> <p>\u5982\u4e0a\u56fe\uff0c\u5b9e\u9645\u4e0a\u6211\u4eec\u5bf9\u4e2d\u5fc3\u8282\u70b9\u989d\u5916\u8fdb\u884c\u4e86\u4e00\u6b21nn.linear\u5e76\u5c06\u4e24\u6b21\u53d8\u6362\u7684\u7ed3\u679c\u6c42\u548c\uff0c\u4f7f\u5f97\u4e8c\u8005\u7684\u8ba1\u7b97\u7ed3\u6784\u4e0d\u540c\uff0c\u4f7f\u5f97ID-GNN\u5177\u6709\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u80fd\u533a\u5206\u51fa\u4e0d\u540c\u7ed3\u6784\u7684\u6b63\u5219\u56fe\u3002</p> <p></p> <p>Graph Neural Networks are More Powerful than We Think</p> <p>Counting Graph Substructures with Graph Neural Networks</p> <p></p> <p></p> <p></p> <p></p> <p>To maintain inductive capability the final output:</p> \\[ \\mathbf{y}= \\mathbb{E}[y^{(m)}] =\\frac{1}{M} \\sum_{m=1}^M \\mathbf{y}^{(m)} \\] <p>computes the closed loops of a graph:</p> \\[ C^{(0)} = [diag(A^0), diag(A^1), ..., diag(A^{D-1})] \\in \\mathbb{N}_0^{N \\times D} \\] <p>\u5df2\u77e5\uff0c\u90bb\u63a5\u77e9\u9635\\(A\\)\u8868\u793a\u56fe\u7684\u8fde\u63a5\u5173\u7cfb\uff0c\u5176\u4e2d\\(A[i][j]=1\\)\u8868\u793a\u8282\u70b9i\u5230j\u6709\u4e00\u6761\u8fb9\u3002\u800c\u5bf9\u4e8e\u90bb\u63a5\u77e9\u9635\u7684\u5e42\u6b21\\(A^k\\)\uff0c\\(A^k[i][j]\\)\u8868\u793a\u4ece\u8282\u70b9i\u51fa\u53d1\uff0c\u7ecf\u8fc7k\u6b65\u5230\u8fbej\u7684\u8def\u5f84\u6570\u91cf\u3002\u800c\\(diag(A^k)\\)\u63d0\u53d6\\(A^k\\)\u7684\u5bf9\u89d2\u7ebf\u5143\u7d20\uff0c\u5373\\(diag(A^k)[i] = A^k [i][i]\\)\uff0c\u8868\u793a\u4ece\u8282\u70b9i\u51fa\u53d1\uff0c\u7ecf\u8fc7k\u6b65\u56de\u5230\u81ea\u8eab\u7684\u95ed\u5408\u8def\u5f84\u6570(cycle conut)\u3002</p>"},{"location":"python/cs224w-notebook/chapter6/#position-aware-gnns","title":"Position-aware GNNs","text":"<p>Position-aware Graph Neural Networks</p> <p></p> <p>We randomly denote an Anchor node(coordinate axis), then represent the position of each node as the distance to the anchor node, which are different.</p> <p></p> <p>Observation: picking more anchors can better characterize node position in different regions of the graph</p> <p></p> <p>Meanwhile, we generalize anchor from a single node to a set of nodes</p> <p>Observation: Large anchor-sets can sometimes provide more precise position estimate</p> <p></p> <p>P-GNN follows the theory of Bourgain theorem:</p> <p>We embed the metric space \\((V,d)\\) into the Euclidean space \\(\\mathbb{R}^k\\) such that the original distance metric is preserved.(\\(||z_u - z_v||_2\\) is close to the original distance metric \\(d(u,v)\\))</p> \\[ f(v) = \\Big( d_{min}(v, S_{1,1}), d_{min}(v, S_{1,2}),..., d_{min}(v, S_{\\log{n},c\\log{n}})  \\Big) \\in \\mathbb{R}^{c\\log^2{n}} \\] <p>\u5176\u4e2d\uff0c\\(c\\)\u4e3a\u5e38\u6570\uff0c\\(S_{i,j} \\subset V\\)\u4ee5\\(\\frac{1}{2^i}\\)\u7684\u6982\u7387\u4ece\u8282\u70b9\u96c6\\(V\\)\u4e2d\u9009\u62e9\uff0c\\(d_{min}(v, S_{i,j}) \u2261 \\min_{u \\in S_{i,j}} d(v,u)\\)</p> <p></p> <p></p> <p></p> <p>\u5982\u679c\u4e24\u4e2a\u8282\u70b9\u7684\u5d4c\u5165\u53ef\u7528\u4e8e\uff08\u8fd1\u4f3c\uff09\u6062\u590d\u5b83\u4eec\u5728\u7f51\u7edc\u4e2d\u7684\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\uff0c\u6211\u4eec\u79f0\u8282\u70b9\u5d4c\u5165\u4e3a\u4f4d\u7f6e\u611f\u77e5(position-aware)\u3002</p> <p>\\(Def.\\) \u8282\u70b9\u5d4c\u5165\\(z_i = f_p (v_i) , \\ \\forall v_i \\in \\mathcal{V}\\)\uff0c\u82e5\u5b58\u5728\\(g_p (\\cdot, \\cdot)\\)\u4f7f\u5f97\\(d_{sp}(v_i,v_j)=g_p(z_i,z_j)\\)\uff08\\(d_{sp}(\\cdot, \\cdot)\\)\u662f\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\uff09\uff0c\u5219\u8282\u70b9\u5d4c\u5165\u4e3aposition-aware\u3002</p> <p><code>P-GNN</code>\u5305\u542b\u4ee5\u4e0b\u5173\u952e\u7ec4\u4ef6\uff1a</p> <ul> <li> <p>k\u4e2a\u5927\u5c0f\u4e0d\u540c\u7684\u951a\u70b9\u96c6\\(S_i\\)</p> </li> <li> <p>\u5c06\u4e24\u4e2a\u8282\u70b9\u7279\u5f81\u4fe1\u606f\u4e0e\u5176\u7f51\u7edc\u8ddd\u79bb\u76f8\u7ed3\u5408\u7684\u6d88\u606f\u8ba1\u7b97\u51fd\u6570\\(F\\)</p> </li> <li> <p>\u951a\u70b9\u96c6\u6d88\u606f\u7684\u77e9\u9635\\(M\\)\uff0c\u5176\u4e2d\u6bcf\u4e00\u884c\\(i\\)\u662f\\(F\\)\u8ba1\u7b97\u7684\u951a\u96c6\u6d88\u606f\\(\\mathcal{M_i}\\)</p> </li> <li> <p>\u53ef\u8bad\u7ec3\u805a\u5408\u51fd\u6570\\({AGG}_M\u3001{AGG}_S\\)\uff0c\u805a\u5408\u951a\u70b9\u96c6\u4e2d\u8282\u70b9\u7684\u7279\u5f81\u4fe1\u606f</p> </li> <li> <p>\u5c06\u6d88\u606f\u77e9\u9635\\(M\\)\u6295\u5f71\u5230\u4f4e\u7ef4\u5d4c\u5165\u7a7a\u95f4\u7684\u53ef\u8bad\u7ec3\u5411\u91cf\\(w\\)</p> </li> </ul> <p></p> <p>$Def. $ \u7ed9\u5b9a\u4e24\u4e2a\u5ea6\u91cf\u7a7a\u95f4\\((\\mathcal{V},d)\\)\u548c\\((\\mathcal{Z},d')\\)\u4ee5\u53ca\u6620\u5c04\u51fd\u6570\\(f:\\mathcal{V} \\rightarrow \\mathcal{Z}\\)\uff0c\u82e5\\(\\forall u,v \\in \\mathcal{V}, \\ \\frac{1}{\\alpha} d(u,v) \\leq d' (f(u),f(v)) \\leq d(u,v)\\)\uff0c\u5219\u79f0\\(f\\)\u7684\u5931\u771f\u5ea6\u4e3a\\(\\alpha\\)\u3002</p> <p>\\(Bourgain \\ Theorem:\\) \u5bf9\u4e8e\u4efb\u610f\u6709\u9650\u5ea6\u91cf\u7a7a\u95f4\\((\\mathcal{V},d)\\)(\\(|\\mathcal{V}|=n\\))\uff0c\u5b58\u5728\u4e00\u4e2a\u5d4c\u5165\u6620\u5c04\u5c06\u5176\u6620\u5c04\u5230\\(\\mathcal{R}^k\\)\u4e2d\uff0c\u5176\u4e2d\\(k = O(\\log^2 n)\\)\uff0c\u5931\u771f\u5ea6\u4e3a\\(O(\\log n)\\)\u3002</p> <p>\u8be5\u5b9a\u7406\u4e3a\u9ad8\u7ef4\u6216\u590d\u6742\u7ed3\u6784\u7684\u5ea6\u91cf\u7a7a\u95f4\u63d0\u4f9b\u4e86\u4f4e\u7ef4\u5d4c\u5165\u7684\u7406\u8bba\u4fdd\u8bc1\u3002</p> <p>\u5bf9\u4e8e\u5ea6\u91cf\u7a7a\u95f4\\((\\mathcal{V},d)\\)\uff0c\u8bbe\\(k=clog^2n\uff0cS_{i,j} \\subset \\mathcal{V},\\ i=1,2,...,\\log{n}, \\ j=1,2,...,c\\log{n}\uff0cd(v,S_{i,j})=\\min_{u \\in S_{i,j}} d(v,u)\\)\u3002\u672c\u6587\u63d0\u51fa\u7684\u4e00\u79cd\u5d4c\u5165\u65b9\u6cd5\u5b9a\u4e49\u4e3a\uff1a</p> \\[ f(v) = \\Big( \\frac{d(v, S_{1,1})}{k}, \\frac{d(v, S_{1,2})}{k},...,\\frac{d(v, S_{\\log{n},c\\log{n}})}{k} \\Big)  \\] <ul> <li> <p>\u9700\u8981\u91c7\u6837\\(O(\\log^2{n})\\)\u4e2a\u951a\u70b9\u96c6\u6765\u4fdd\u8bc1\u4f4e\u5931\u771f\u5d4c\u5165\uff08low distortion embedding\uff09</p> </li> <li> <p>\u951a\u96c6\u7684\u5927\u5c0f\u5448\u6307\u6570\u5206\u5e03\uff08\\(\\frac{1}{2^i}\\)\uff09</p> </li> </ul> <p>Message Computation Function \\(F\\)\uff1a\u6d88\u606f\u8ba1\u7b97\u51fd\u6570\\(F(v,u,\\mathbf{h}_v,\\mathbf{h}_u)\\)\u8003\u8651\u4f4d\u7f6e\u76f8\u4f3c\u5ea6\u4ee5\u53ca\u7279\u5f81\u4fe1\u606f</p> <p>\u7531\u4e8e\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\u7684\u8ba1\u7b97\u5177\u6709\\(O(|\\mathcal{V}|^3)\\)\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6211\u4eec\u63d0\u51fa\u4ee5\u4e0b\\(q\\)-hop\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\uff1a</p> \\[ d^q_{sp} (v,u) =  \\begin{cases} d_{sp}(v,u), \\ if \\ d_{sp}(v,u) \\leq q \\\\ \\infty, \\ otherwise \\end{cases} \\] <p>\u820d\u53bb\u8fc7\u8fdc\u7684\u8282\u70b9\uff0c\u5e76\u4e141-hop\u7684\u8282\u70b9\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u90bb\u63a5\u77e9\u9635\u5f97\u5230</p> <p>\u6211\u4eec\u5c06\u8ddd\u79bb\u5b9a\u4e49\u4e3a\uff1a</p> \\[ s(v,u) = \\frac{1}{d^q_{sp}(v,u) + 1} \\in (0,1) \\] <p>\u6700\u540e\u5c06\u7279\u5f81\u4fe1\u606f\u4e0e\u4f4d\u7f6e\u4fe1\u606f\u7ed3\u5408\uff1a</p> \\[ F(v,u,\\mathbf{h}_v,\\mathbf{h}_u) = s(v,u) \\cdot \\text{Concat}(h_v) \\]"},{"location":"python/cs224w-notebook/chapter6/#graph-transformers","title":"Graph Transformers","text":"<p>chapter8 \u5f85\u8865</p>"},{"location":"python/cs224w-notebook/chapter6/#heterogeneous-graphs\u5f02\u6784\u56fe","title":"Heterogeneous graphs(\u5f02\u6784\u56fe)","text":"<p>\u5f02\u8d28\u56fe(heterogeneous graph\uff0c\u4e5f\u79f0\u5f02\u6784\u56fe)\uff0c\u53c8\u88ab\u79f0\u4e3a\u5f02\u8d28\u4fe1\u606f\u7f51\u7edc(heterogeneous information network\uff0c\u4e5f\u79f0\u5f02\u6784\u4fe1\u606f\u7f51\u7edc)\u3002\u533a\u522b\u4e8e\u540c\u8d28\u56fe(homogeneous graph)\uff0c\u5b83\u662f\u4e00\u79cd\u5177\u6709\u591a\u79cd\u8282\u70b9\u7c7b\u578b\u6216\u591a\u79cd\u8fb9\u7c7b\u578b\u7684\u56fe\u6570\u636e\u7ed3\u6784( graphs with multiple nodes or edge types)\uff0c\u7528\u4e8e\u523b\u753b\u590d\u6742\u5f02\u8d28\u5bf9\u8c61\u53ca\u5176\u4ea4\u4e92\uff0c\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u56fe\u6570\u636e\u6316\u6398\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5efa\u6a21\u5de5\u5177\u548c\u5206\u6790\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5f02\u8d28\u56fe\u6570\u636e\u4e5f\u662f\u4e00\u79cd\u5e7f\u6cdb\u5b58\u5728\u7684\u6570\u636e\u7c7b\u578b\uff0c\u4f8b\u5982\u77e5\u8bc6\u56fe\u8c31\uff0c\u793e\u4ea4\u7f51\u7edc\u6570\u636e\u3002</p> <ul> <li> <p>Relational GCNs</p> </li> <li> <p>Heterogeneous Graph Transformer</p> </li> <li> <p>Design space for heterogeneous GNNs</p> </li> </ul> <p></p> <p></p> <p>A heterogeneous graph can be defined as :</p> \\[ \\mathbf{G} = \\{\\mathbf{V}, \\mathbf{E}, \\tau,\\phi \\} \\] <ul> <li> <p>Node with node types: \\(v \\in \\mathbf{V}\\)</p> </li> <li> <p>Node types for node: \\(v:\\tau (v)\\)</p> </li> <li> <p>Edge with edge types: \\((u,v) \\in \\mathbf{E}\\)</p> </li> <li> <p>Edge types: \\((u,v): \\phi (u,v)\\)</p> </li> </ul> <p>heterogeneous graph's relation type for edge \\(e\\) is a tuple\uff1a\\(r(u,v) = (\\tau(u,v), \\phi(u,v), \\tau(v))\\)</p> <p></p>"},{"location":"python/cs224w-notebook/chapter6/#relation-gcnrgcn","title":"Relation GCN(RGCN)","text":"<p>How do we extend GCNmodel to handle heterogeneous graphs?</p> <p></p> \\[ h^{(l+1)}_v = \\sigma( \\sum_{r \\in R} \\sum_{u \\in N^r_v} \\frac{1}{c_{v,r}} W_r^{(l)} h^{(l)}_u + W_0^{(l)} h^{(l)}_v ) \\] <ul> <li> <p>\\(c_{v,r}=|N^r_v|\\) is the number of relations of \\(v\\), for normalization.</p> </li> <li> <p>Each neighbor of a given relation:</p> </li> </ul> \\[ m_{u,r}^{(l)} = \\frac{1}{c_{v,r}} W_r^{(l)} h^{(l)}_u \\] <ul> <li>Self-loop</li> </ul> \\[ m_{v}^{(l)} = W_0^{(l)} h^{(l)}_v \\] <ul> <li>Aggregation:</li> </ul> \\[ h^{(l+1)}_v = \\sigma(Sum(\\{{m_{u,r}^{(l)} \uff0c u \\in N^r_v}\\}) \u222a \\{m_{v}^{(l)}\\}) \\] <p></p>"},{"location":"python/cs224w-notebook/chapter6/#block-diagonal-matrix\u5206\u5757\u5bf9\u89d2\u77e9\u9635","title":"Block Diagonal Matrix\uff08\u5206\u5757\u5bf9\u89d2\u77e9\u9635\uff09","text":""},{"location":"python/cs224w-notebook/chapter6/#basis-learning\u5b66\u4e60\u57fa\u77e9\u9635","title":"Basis Learning\uff08\u5b66\u4e60\u57fa\u77e9\u9635\uff09","text":""},{"location":"python/cs224w-notebook/chapter6/#rgcn-for-link-prediction","title":"RGCN for Link Prediction","text":"<p>Firstly, assume \\((E, r_3, A)\\) is training supervision edge, all the other edges are training message edges</p> <p></p> <ul> <li> <p>Use RGCN to score the training supervision edge \\((E, r_3, A)\\)</p> </li> <li> <p>Create a negative edge by perturbing the supervision edge , e.g. \\((E, r_3, B)\\), \\((E, r_3, D)\\)</p> </li> </ul> <p>Note that negative edges should not belong to training message edges or training supervision edges.</p> <ul> <li> <p>Use GNN model to score negative edge</p> </li> <li> <p>Optimize a standatd CrossEntropy loss </p> </li> <li> <p>Maximize the score of training supervision edge</p> </li> <li> <p>Minimize the score of negative edges</p> </li> </ul> \\[ \\mathcal{l} = -log \\sigma (f_{r3} (h_E,h_A)) - log (1-\\sigma(f_{r3} (h_E-h_B))) \\] <p></p> <p></p> <p>Hits@k:\u4e00\u4e2a\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u6027\u80fd\u7684\u6307\u6807\uff0c\u5e38\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3001\u641c\u7d22\u5f15\u64ce\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7b49\u573a\u666f.\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u9884\u6d4b\u5217\u8868\uff0c\u5982\u679c\u771f\u5b9e\u7684\u5143\u7d20\u51fa\u73b0\u5728\u5217\u8868\u7684\u524d k \u4e2a\u4f4d\u7f6e\u4e2d\uff0c\u5219\u8ba4\u4e3a\u662f\u4e00\u4e2a\u6210\u529f\u7684\u9884\u6d4b\uff08Hit\uff09\u3002Hits@k \u6307\u6807\u8ba1\u7b97\u7684\u662f\u6240\u6709\u6210\u529f\u7684\u9884\u6d4b\u5360\u603b\u9884\u6d4b\u6b21\u6570\u7684\u6bd4\u4f8b\u3002</p> <p>Reciprocal Rank (RR):\u4e00\u4e2a\u8861\u91cf\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u6027\u80fd\u7684\u6307\u6807\uff0c\u5b83\u7279\u522b\u5173\u6ce8\u6700\u76f8\u5173\u7ed3\u679c\u7684\u6392\u540d\u3002\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u641c\u7d22\u5f15\u64ce\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u9886\u57df\u4e2d\uff0c\u8fd9\u4e2a\u6307\u6807\u7528\u6765\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002\u5728\u7ed9\u5b9a\u7684\u67e5\u8be2\u4e2d\uff0c\u53d6\u6700\u76f8\u5173\u7ed3\u679c\u7684\u6392\u540d\u7684\u5012\u6570\u3002\u5982\u679c\u6700\u76f8\u5173\u7684\u7ed3\u679c\u6392\u540d\u8d8a\u9760\u524d\uff0c\u5176\u5012\u6570\uff08\u5373 Reciprocal Rank\uff09\u5c31\u8d8a\u5927\uff0c\u8868\u793a\u7cfb\u7edf\u7684\u6027\u80fd\u8d8a\u597d\u3002</p> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter6/#understanding-rgcn\u5173\u7cfb\u56fe\u5377\u79ef","title":"Understanding RGCN(\u5173\u7cfb\u56fe\u5377\u79ef)","text":"<p>\u8bba\u6587\uff1aModeling Relational Data with Graph Convolutional Networks</p>"},{"location":"python/cs224w-notebook/chapter7/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p>"},{"location":"python/cs224w-notebook/chapter7/#heterogeneous-message","title":"Heterogeneous Message","text":"<p>Message function\uff1a \\(\\mathbf{m}_u^{(l)} = MSG_r^{(l)} (\\mathbf{h}_u^{(l-1)}), \\ r=(u,e,v)\\)</p> <p>Observation: A node could receive multiple types of messages. Num of message type = Num of relation type. </p> <p>\\(r=(u,e,v)\\) is the  relation type between node \\(u\\) that sends the message, edge type \\(e\\) , and node \\(v\\) that receive the message.</p>"},{"location":"python/cs224w-notebook/chapter7/#heterogeneous-aggregation","title":"Heterogeneous Aggregation","text":"<p>Observation: Each node could receive multiple types of messages from its neighbors, and multiple neighbors may belong to each message type.</p> \\[ h_v^{(l)} = AGG^{(l)}_{all} \\Big( AGG^{(l)}_r (\\{ \\mathbf{m}^{(l)}_u, u \\in N_r(v)\\}) \\Big) \\] <ul> <li> <p>\\(AGG^{(l)}_r\\)\uff1aaggregate the messages that belongs to the relation type</p> </li> <li> <p>\\(AGG^{(l)}_{all}\\)\uff1aaggregate across the edge types</p> </li> </ul> <p></p>"},{"location":"python/cs224w-notebook/chapter7/#heterogeneous-graphs-transformerhgt","title":"Heterogeneous Graphs Transformer(HGT)","text":"<p>Innovation: Decompose heterogeneous graph to Node-type and edge-type dependent attention mechanism</p> <p></p> \\[ \\begin{aligned} AttenHead^i (s,e,t) =&amp; (K^i(s) W^{Att}_{\\phi(e)} Q^i(t)^T) \\\\ K^i(s) = KLinear^i_{\\tau(s)} &amp;(H^{(l-1)[s]}) \\\\ Q^i(t) = QLinear^i_{\\tau(t)} &amp;(H^{(l-1)[t]}) \\\\ \\end{aligned} \\] <p>Each relation \\((Type(s), Relation(e), Type(t))\\) has a distinct set of projection weights</p> <p></p> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter7/#understanding-heterogeneous-graph-transformer","title":"Understanding Heterogeneous Graph Transformer","text":"<p>\u9605\u8bfb\u5730\u5740\uff1aHeterogeneous Graph Transformer</p> <p></p> <p>\u56de\u987eGAT\uff0c\u5176\u5047\u8bbe\u6240\u6709\u7684\u8282\u70b9\u62e5\u6709\u76f8\u540c\u7684\u7279\u5f81\u5206\u5e03\uff08\u4f7f\u7528\u4e86\u540c\u4e00\u4e2a\u6743\u91cd\u77e9\u9635\\(W\\)\uff09\uff0c\u4f46\u5bf9\u4e8e\u5f02\u8d28\u56fe\u6765\u8bf4\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u8282\u70b9\u62e5\u6709\u4e0d\u540c\u7684\u7279\u5f81\u5206\u5e03\uff0c\u56e0\u6b64\u9700\u8981\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684\u8282\u70b9\u4f7f\u7528\u4e0d\u540c\u7684\u6743\u91cd\u77e9\u9635\\(W\\)\u3002</p> <p>\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u8bbe\u8ba1\u4e86<code>Heterogeneous Mutual Attention</code>\u673a\u5236\u3002\u8bbe\u5b9a\u4e00\u4e2a\u76ee\u6807\u8282\u70b9\\(t\\)\uff0c\u4ee5\u53ca\u5176\u5c5e\u4e8e\u4e0d\u540c\u5206\u5e03\u7684\u6240\u6709\u90bb\u5c45\\(s \\in N(t)\\)\uff0c\u6211\u4eec\u5e0c\u671b\u6839\u636e\u8282\u70b9\u4e4b\u95f4\u7684\u5143\u5173\u7cfb\u6765\u8ba1\u7b97\u76f8\u4e92\u6ce8\u610f\u529b\\((\\tau(s), \\phi(e), \\tau(t))\\)\u4e09\u5143\u7ec4\u3002</p> \\[ \\begin{aligned} \\text{Attention}_{HGT} (s,e,t)&amp;= Softmax_{\\forall s \\in N(t)} \\Big( ||_{i\\in [1,h]} \\text{Att-head}^i (s,e,t) \\Big) \\\\ \\text{Att-head}^i (s,e,t) &amp;= (K^i(s) W^{Att}_{\\phi(e)} Q^i(t)^T) \\cdot \\frac{\\mu_{&lt;\\tau(s), \\phi(e), \\tau(t)&gt;}}{\\sqrt{d}} \\\\ K^i(s) &amp;= \\text{K-Linear}^i_{\\tau(s)} &amp;(H^{(l-1)[s]}) \\\\ Q^i(t) &amp;= \\text{Q-Linear}^i_{\\tau(t)} &amp;(H^{(l-1)[t]}) \\\\ \\end{aligned} \\] <p>\u5bf9\u4e8e\u7b2ci\u4e2a\u6ce8\u610f\u529b\u5934\\(\\text{Att-head}^i (s,e,t)\\)\uff0c\u6211\u4eec\u4f7f\u7528\u7ebf\u6027\u6295\u5f71\\(\\text{K-Linear}_{\\tau(s)}^i\\)\u5c06\\(\\tau(s)\\)\u7c7b\u578b\u7684\u6e90\u8282\u70b9\\(s\\)\u6295\u5f71\u5230\\(i\\)-th \\(Key\\)\u5411\u91cf\\(K^i(s)\\)\uff1a\\(\\mathbb{R}^d \\rightarrow \\mathbb{R}^{\\frac{d}{h}}\\)\uff0c\u5176\u4e2d\\(h\\)\u662f\u6ce8\u610f\u529b\u5934\u7684\u6570\u91cf\uff0c\\(\\frac{d}{h}\\)\u662f\u6bcf\u4e2a\u5934\u7684\u5411\u91cf\u8868\u793a\u3002</p> <p>\u6bcf\u79cd\u7c7b\u578b\\(\\tau(s)\\)\u7684\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684\u7ebf\u6027\u6295\u5f71\\(\\text{Linear}_{\\tau(s)}^i\\)\u6765\u6700\u5927\u9650\u5ea6\u5730\u5efa\u6a21\u5206\u5e03\u5dee\u5f02\u3002</p> <p>\u5f02\u6784\u56fe\u7684\u4e00\u4e2a\u72ec\u7279\u7279\u5f81\u662f\u8282\u70b9\u7c7b\u578b\u5bf9\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u4e0d\u540c\u7684\u8fb9\u5173\u7cfb\uff0c\u56e0\u6b64\u6211\u4eec\u4e3a\u6bcf\u4e2a\u8fb9\u7c7b\u578b\\(\\phi(e)\\)\u5b9a\u4e49\u4e00\u4e2a\u6295\u5f71\u77e9\u9635\\(W^{Att}_{\\phi(e)} \\in \\mathbb{R}^{\\frac{d}{h} \\times \\frac{d}{h}}\\)\uff0c</p> <p>\u5e76\u4e14\uff0c\u7531\u4e8e\u5e76\u975e\u6240\u6709\u7684\u5173\u7cfb\u8fde\u63a5\u5bf9\u76ee\u6807\u8282\u70b9\u7684\u8d21\u732e\u76f8\u540c\uff0c\u56e0\u6b64\u6211\u4eec\u6dfb\u52a0\u4e00\u4e2a\u5148\u9a8c\u5f20\u91cf\uff08prior tensor\uff09\\(\\mu \\in \\mathbb{R}^{|\\mathcal{A}| \\times |\\mathcal{R}| \\times |\\mathcal{A}|}\\)\uff0c\u6765\u8868\u793a\u6bcf\u4e2a\u5143\u5173\u7cfb\u4e09\u5143\u7ec4\u7684\u4e00\u822c\u610f\u4e49\uff0c\u4f5c\u4e3a\u5bf9\u6ce8\u610f\u529b\u7684\u81ea\u9002\u5e94\u7f29\u653e\u3002</p> <p>\u6700\u540e\uff0c\u6211\u4eec\u5c06\\(h\\)\u4e2a\u6ce8\u610f\u529b\u5934\u8fde\u63a5\u5728\u4e00\u8d77\u4ee5\u83b7\u5f97\u6bcf\u4e2a\u8282\u70b9\u5bf9\u7684\u6ce8\u610f\u529b\u5411\u91cf\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u76ee\u6807\u8282\u70b9\\(t\\)\uff0c\u6211\u4eec\u4ece\u5176\u90bb\u5c45\\(N (t)\\)\u4e2d\u6536\u96c6\u6240\u6709\u6ce8\u610f\u529b\u5411\u91cf\u5e76\u8fdb\u884csoftmax\uff0c\u6709\\(\\sum_{\\forall s \\in N(t)} \\text{Attention}_{HGT} (s,e,t) = \\mathbf{1}_{h \\times 1}\\)</p> \\[ \\text{Message}_{HGT} (s,e,t) = ||_{i\\in [1,h]} \\text{MSG-head}^i (s,e,t) \\\\ \\text{MSG-head}^i (s,e,t) = \\text{M-Linear}^i_{\\tau(t)} (H^{(l-1)}[s]) W^{MSG}_{\\phi(e)} \\] <p>\u5176\u4e2d\\(\\text{M-Linear}^i_{\\tau(t)}: \\ \\mathbb{R}^d \\rightarrow \\mathbb{R}^{\\frac{d}{h}}\\)\u662f\u76ee\u6807\u8282\u70b9\\(t\\)\u7684\u7ebf\u6027\u6295\u5f71\uff0c\\(W^{MSG}_{\\phi(e)} \\in \\mathbb{R}^{\\frac{d}{h} \\times \\frac{d}{h}}\\)\u662f\u8fb9\u7c7b\u578b\\(\\phi(e)\\)\u7684\u6295\u5f71\u77e9\u9635\u3002\u6700\u540e\u8fde\u63a5\u6240\u6709\\(h\\)\u4e2a\u6d88\u606f\u5934\u4ee5\u83b7\u5f97\u6bcf\u4e2a\u8282\u70b9\u5bf9\u7684\u6d88\u606f\\(\\text{Message}_{HGT}(s, e, t)\\)\u3002</p> \\[ \\tilde{H}^{(l)}[t] = \\mathop{\\oplus}\\limits_{\\forall s \\in N(t)} (\\text{Attention}_{HGT} (s,e,t) \\cdot \\text{Message}_{HGT} (s,e,t)) \\] <p>\u8fd9\u5c06\u4fe1\u606f\u805a\u5408\u5230\u6765\u81ea\u4e0d\u540c\u7279\u5f81\u5206\u5e03\u7684\u6240\u6709\u90bb\u5c45\uff08\u6e90\u8282\u70b9\uff09\u7684\u76ee\u6807\u8282\u70b9\\(t\\)\u3002</p> \\[ H^{(l)}[t] = \\text{A-Linear}_{\\tau(t)} (\\sigma(\\tilde{H}^{(l)}[t])) + H^{(l-1)}[t] \\] <p>Relative Temporal Encoding</p> <p></p> <p>\u672c\u6587\u63d0\u51fa\u4e86\u76f8\u5bf9\u65f6\u95f4\u7f16\u7801 (RTE) \u673a\u5236\u6765\u6a21\u62df\u5f02\u6784\u56fe\u4e2d\u7684\u52a8\u6001\u4f9d\u8d56\u5173\u7cfb\u3002RTE\u53d7\u5230 Transformer\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u7684\u542f\u53d1\u3002\u7ed9\u5b9a\u4e00\u4e2a\u6e90\u8282\u70b9\\(s\\)\u548c\u4e00\u4e2a\u76ee\u6807\u8282\u70b9\\(t\\)\uff0c\u4ee5\u53ca\u54cd\u5e94\u7684\u65f6\u95f4\u6233\\(T(s)\\)\u548c\\(T(t)\\)\uff0c\u6211\u4eec\u5c06\u76f8\u5bf9\u65f6\u95f4\u95f4\u9694\\(\\Delta T(t,s) = T(t) - T(s)\\)\u8868\u793a\u4e3a\u83b7\u5f97\u76f8\u5bf9\u65f6\u95f4\u7f16\u7801\\(\\text{RTE}(\\Delta T(t,s))\\)\u7684\u7d22\u5f15\u3002</p> \\[ \\begin{aligned} Base (\\Delta T(t,s), 2i) &amp;= \\sin{(\\Delta T(t,s) / 10000^{2i/d})} \\\\ Base (\\Delta T(t,s), 2i+1) &amp;= \\cos{(\\Delta T(t,s) / 1 0000^{(2i+1)/d})} \\\\ RTE(\\Delta T(t,s)) &amp;= \\text{T-Linear} (Base(\\Delta T_{t,s})) \\\\ \\end{aligned} \\] <p>\u6700\u540e\uff0c\u5c06\u76f8\u5bf9\u4e8e\u76ee\u6807\u8282\u70b9 t \u7684\u65f6\u95f4\u7f16\u7801\u6dfb\u52a0\u5230\u6e90\u8282\u70b9 s \u7684\u8868\u793a\u4e2d:</p> \\[ \\tilde{H}^{(l-1)}[t] = H^{(l-1)}[t] + RTE(\\Delta T(t,s)) \\]"},{"location":"python/cs224w-notebook/chapter8/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p>"},{"location":"python/cs224w-notebook/chapter8/#knowledge-graphs","title":"Knowledge Graphs","text":""},{"location":"python/cs224w-notebook/chapter8/#_1","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":""},{"location":"python/cs224w-notebook/chapter8/#transe","title":"TransE","text":"<p>For a triple \\((h,r,t)\\),let \\(h,r,t \\in \\mathbb{R}^d\\) be embedding vectors.</p> <p>TransE: \\(h + r \u2248 t\\) if the given link exists else \\(h + r \u2260 t\\).</p> <p>Entity scoring func:</p> \\[ f_r(h,t) =  -|| h + r - t || \\] <p></p> <p></p> <p>\u5bf9\u6bd4\u635f\u5931(Contrastive loss)\uff1a\u5bf9\u6709\u6548\u7684\u4e09\u5143\u7ec4\u652f\u6301\u8f83\u4f4e\u7684\u8ddd\u79bb\uff08\u6216\u8f83\u9ad8\u7684\u5206\u6570\uff09\uff0c\u5bf9\u635f\u574f\u7684\u4e09\u5143\u7ec4\u5219\u652f\u6301\u8f83\u9ad8\u7684\u8ddd\u79bb\uff08\u6216\u8005\u8f83\u4f4e\u7684\u5206\u6570\uff09</p>"},{"location":"python/cs224w-notebook/chapter8/#connectivity-patterns-in-kg","title":"Connectivity Patterns in KG","text":"<ul> <li> <p>Symmetry:  If the edge \\((h,\"Roommate\",t)\\) exists in KG, then the edge \\((t,\"Roommate\",h)\\) should also exist.</p> </li> <li> <p>Inverse relation :  If the edge \\((h,\"Advisor\",t)\\) exists in KG, then the edge \\((t, \"Advisee\",h)\\) should also exist.</p> </li> </ul> <p>Are TransE expressive enough to capture these patterns?</p> <p> </p>"},{"location":"python/cs224w-notebook/chapter8/#transr","title":"TransR","text":"<p>TransE models translation of any relation in the same embedding space.</p> <p>TransR: model entities as vectors in the entity space \\(\\mathbb{R}^d\\) and model each relation as vector in relation space \\(\\mathbf{r} \\in \\mathbb{R}^{k}\\) with \\(\\mathbf{M}_r \\in \\mathbb{R}^{k \\times d}\\) as the projection matrix.</p> <p>\\(h_{k} = M_r h, t_k=M_r t\\)</p> <p>scoring func: $$  f_r(h,t) = -|| h_k + r - t_k || $$</p> <p> </p> <ul> <li>DistMult</li> </ul> <p>Entities and relations are vectros in \\(\\mathbb{R}^k\\)</p> <p>Score func:</p> \\[ f_r(h,t)= \\sum_i \\mathbf{h}_i \\cdot \\mathbf{r}_i \\cdot \\mathbf{t}_i \\] <p></p> <p>Intuition of the score function: Can be viewed as a cosine similarity between \\(\\mathbf{h} \\cdot \\mathbf{r}\\) and \\(\\mathbf{t}\\)</p> <p></p> <p></p> <p></p> <p></p> <p></p> <ul> <li>ComplEx</li> </ul> <p>model entities and relations as complex vectors in \\(\\mathbb{C}^k\\)</p> <p></p> <p>Score func:</p> \\[ f_r(h,t) = \\text{Re}(\\sum_i \\mathbf{h}_i \\cdot \\mathbf{r}_i \\cdot \\bar{\\mathbf{t}_i} ) \\] <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"python/cs224w-notebook/chapter9/","title":"\ud83d\udee3[Deep Learning]Stanford CS224w:Machine Learning with Graphs","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttp://web.stanford.edu/class/cs224w/</p> <p>\ud83d\udc40\u4e00\u4e9b\u8d44\u6e90:  B\u7ad9\u7cbe\u8bb2\uff1ahttps://www.bilibili.com/video/BV1pR4y1S7GA/?spm_id_from=333.337.search-card.all.click&amp;vd_source=280e4970f2995a05fdeab972a42bfdd0</p> <p>https://github.com/TommyZihao/zihao_course/tree/main/CS224W</p> <p>Slides: http://web.stanford.edu/class/cs224w/slides</p> <p></p>"},{"location":"python/cs224w-notebook/chapter9/#reasoning-over-kgs","title":"Reasoning over KGs","text":"<p>Goal: How to perform multi-hop reasoning over KGs?</p> <p></p> <p></p> <ul> <li>Path Querise</li> </ul> <p>An N-hop path query q can be represented by</p> \\[ q = (v_a ,(r_1,...,r_n)) \\] <p></p> <p>Question: What proteins are associated with adverse events caused by Fulvestrant?</p> <ul> <li> <p>\\(v_a\\) is : <code>e:Fulvestrant</code></p> </li> <li> <p>\\((r_1,r_2)\\) is <code>(r:Causes, r:Assoc)</code></p> </li> <li> <p>Query: <code>(e:Fulvestrant,(r:Causes,r:Assoc))</code></p> </li> </ul> <p></p> <p>Because the completed(probabilistic) KGs is a dense graph, we need a way to answer path-based queries over an incomplete knowledge graph</p> <p>We want our approach to implicitly impute and account for the incomplete KG</p> <p>Task: Predictive queries</p> <ul> <li>Want to be able to answer arbitrary queries while implicitly imputing for the missing information</li> </ul> <p>Key Idea:Embed queries</p> <ul> <li> <p>Generalize TransE to multi-hop reasoning.</p> </li> <li> <p>Query embedding: \\(q = h + r\\)</p> </li> <li> <p>Goal: query embedding \\(q\\) should be close to the answer embedding \\(t\\)</p> </li> </ul> \\[ f_q(t) = -||q - t|| \\] <p></p> <p></p> <p></p> <p>Since TransE can naturally handle compositional relations, it can handle path queries by translating in the latent space for multiple hops using addition of relation embeddings.(DistMult / ComplEx can't)</p> <ul> <li>Conjunctive Queries</li> </ul> <p>Conjunctive Queries: What are drugs that cause Short of Breath and treat diseases associated with protein ESR2?</p> <p><code>(e:ESR2, (r:Assoc, r:TreatedBy)), (e:Short of Breath, (r:CausedBy))</code></p> <p></p> <p></p> <p>How can we use embeddings to implicitly impute the missing edges?</p> <p></p> <ul> <li>Query2Box</li> </ul> <p></p> <p></p> <p>Projection Operator \\(\\mathcal{P}\\):</p> <p>Intuition:Take the current box as input and use the relation embedding to project and expand the box</p> \\[ Cen(q') = Cen(q) + Cen(r) \\] \\[ Off(q') = Off(q) + Off(r) \\] <p></p> <p></p> <p>How do we take intersection of boxes?</p> <p></p> <p></p> <p>How do we define the score function \\(f_q(v)\\) (negative distance) ?</p> \\[ d_{box}(\\mathbf{q},\\mathbf{v}) = d_{out}(\\mathbf{q},\\mathbf{v}) + \\alpha \\cdot d_{in}(\\mathbf{q},\\mathbf{v}) \\] <p>where \\(0&lt; \\alpha &lt;1\\)</p> <p>Intuition: if the point is enclosed in the box, the distance should be downweighted.</p> <p>\\(\\alpha\\)\u8ba9\u5e94\u8be5\u5728box\u5185\u90e8\u7684\u70b9\u66f4\u52a0\u9760\u8fd1box\u4e2d\u5fc3\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u3002</p> \\[ f_q(v) = -d_{box}(\\mathbf{q},\\mathbf{v}) \\] <p></p>"},{"location":"python/cs224w-notebook/chapter9/#and-or-queries-union-operation","title":"AND-OR queries (union operation)","text":"<p>E.g.: What drug can treat breast cancer or lung cancer?</p> <p>AND-OR queries: Conjunctive queries + disjunction, called  Existential Positive First-order (EPFO) queries.</p> <p>\u95ee\u9898\uff1a\u5728box\u7684\u8868\u793a\u5f62\u5f0f\u4e0b\uff0c\u4f7f\u7528union\u64cd\u4f5c\u7684\u8bdd\uff0c\u90a3\u4e48\u6211\u4eec\u7684\u5d4c\u5165\u7a7a\u95f4\u7ef4\u5ea6\u4f1a\u5f88\u5927\uff0c\u8282\u70b9\u8d8a\u591a\u7ef4\u5ea6\u8d8a\u5927\u3002\u800c\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u8282\u70b9\u975e\u5e38\u591a\u3002</p> <p></p> <p></p> <p>\u56e0\u6b64\u6211\u4eec\u5f88\u96be\u5728\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\u5d4c\u5165AND-OR\u67e5\u8be2</p> <p>Key idea: take all unions out and only do union at the last step</p> <p></p> <p>Logically, any AND-OR query can be expressed as a disjunction of conjunctive queries.</p> <p>Distance between entity embedding and a DNF \\(q=q_1 \\cup q_2 \\cup... \\cup q_m\\) is defined as:</p> \\[ d_{box}(\\mathbf{q},\\mathbf{v}) = \\min( d_{box}(\\mathbf{q_1},\\mathbf{v}),..., d_{box}(\\mathbf{q_m},\\mathbf{v})) \\] <p></p>"},{"location":"python/cs224w-notebook/chapter9/#training-query2box","title":"Training Query2Box","text":"<ul> <li> <p>Training:</p> </li> <li> <p>Smaple a query \\(q\\) from the training graph \\(G_{train}\\), answer \\(v \\in [q]_{G_{train}}\\),and a negative sample \\(v' \\notin [q]_{G_{train}}\\)</p> </li> </ul> <p>Negative sample: Entity of same type as \\(v\\) but not answer to \\(q\\)</p> <ol> <li> <p>Embed the query \\(\\mathbf{q}\\)</p> </li> <li> <p>Calculate the score \\(f_q(v)\\) and \\(f_q(v')\\).</p> </li> <li> <p>Optimize the loss \\(\\mathcal{l}\\) to maximize \\(f_q(v)\\) while minimize \\(f_q(v')\\).</p> </li> </ol> \\[ \\mathcal{l} = -log \\sigma(f_q(v)) - log(1-\\sigma(f_q(v')))  \\]"},{"location":"python/cs224w-notebook/chapter9/#query-template","title":"Query Template","text":""},{"location":"python/cs224w-notebook/chapter9/#a-simple-example","title":"A Simple Example","text":"<p>We use t-SNE to reduce the dimension of the embedding space to 2D for visualization.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>TP (True Positive)\uff1a\u5b9e\u9645\u4e3a\u6b63\uff0c\u9884\u6d4b\u4e5f\u4e3a\u6b63\u3002TN (True Negative)\uff1a\u5b9e\u9645\u4e3a\u8d1f\uff0c\u9884\u6d4b\u4e5f\u4e3a\u8d1f\u3002FP (False Positive)\uff1a\u5b9e\u9645\u4e3a\u8d1f\uff0c\u4f46\u9884\u6d4b\u4e3a\u6b63\u3002\u4e5f\u88ab\u79f0\u4e3a I \u7c7b\u9519\u8bef\u6216\u201c\u5047\u9633\u6027\u201d\u3002FN (False Negative)\uff1a\u5b9e\u9645\u4e3a\u6b63\uff0c\u4f46\u9884\u6d4b\u4e3a\u8d1f\u3002\u4e5f\u88ab\u79f0\u4e3a II \u7c7b\u9519\u8bef\u6216\u201c\u5047\u9634\u6027\u201d\u3002</p>"},{"location":"python/cs224w-notebook/chapter9/#understanding-query2box","title":"Understanding Query2Box","text":""},{"location":"python/cs231n-notebook/chapter1/","title":"\ud83d\udee3Stanford CS231n:Deep Learning for Computer Vision","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttps://cs231n.stanford.edu/</p> <p>2024\u7248PPT: https://cs231n.stanford.edu/slides/2024/</p> <p></p>"},{"location":"python/cs231n-notebook/chapter1/#nearest-neighbor-classifier","title":"Nearest Neighbor Classifier","text":"<p>K\u90bb\u8fd1\u7b97\u6cd5\u662f\u4e00\u79cd\u7b80\u5355\u7684\u60f0\u6027\u7b97\u6cd5\uff08\u4e0d\u9700\u8981\u8bad\u7ec3\uff0c\u975e\u53c2\u6570\uff0c\u4e0d\u5b66\u4e60\u7279\u5f81\uff09\uff0c\u53ef\u76f4\u63a5\u8fdb\u884c\u9884\u6d4b\u8ba1\u7b97\u3002</p> <p></p> <p>\u8861\u91cf\u4e24\u4e2a\u6837\u672c\u7684\u8ddd\u79bb:</p> <ul> <li>\\(L_1\\) distance: \\(d_1(I_1, I_2) = \\sum_{p} |I_1^p - I_2^p|\\)</li> </ul> <p></p> <ul> <li>\\(L_2\\) distance: \\(d_2(I_1, I_2) = \\sqrt{\\sum_{p} (I_1^p - I_2^p)^2}\\)</li> </ul> <p><code>distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))</code></p> <ul> <li>K\u90bb\u8fd1\u5b9e\u73b0\uff1a</li> </ul> <pre><code>import numpy as np\n\n# input x.shape [num_examples, y, x]\n\n#  [ [[1, 2, 3],\n#     [2, 3, 4],\n#     [4, 5, 6]],\n#\n#    [[1, 2, 3],\n#     [2, 3, 4],\n#     [4, 5, 6]] ]\n\nclass KNearestNeighbor:\n    def __init__(self, k):\n        super().__init__()\n        self.k = k\n\n    def train(self, X, Y):\n        self.X_train = X\n        self.y_train = Y\n\n    def distance(self, x_1, x_2):\n        dis = np.sum(np.abs(x_1 - x_2), axis=1)\n        return np.sum(dis, axis=1)\n\n    def predict(self, X):\n        num_examples = X.shape[0]\n        y_predict = np.zeros(num_examples, dtype=self.y_train.dtype)\n        for i in range(num_examples):\n            Distance = self.distance(self.X_train, X[i,:])\n            sorted_indices = np.argsort(Distance)\n            k_nearest_labels = self.y_train[sorted_indices]\n            y_predict[i] = collections.Counter(k_nearest_labels[0:self.k]).most_common(1)[0][0]\n\n        return y_predict\n</code></pre> <p>\\(L_1\\)\u548c\\(L_2\\)\u6bd4\u8f83</p> <p> \u5728 \\(L_1\\) \u8ddd\u79bb\u66f4\u4f9d\u8d56\u4e8e\u5750\u6807\u8f74\u7684\u9009\u5b9a(\u65cb\u8f6c\u89d2\u5ea6)\uff0c\u5750\u6807\u8f74\u9009\u62e9\u4e0d\u540c \\(L_1\\) \u8ddd\u79bb\u4e5f\u4f1a\u8ddf\u7740\u53d8\u5316\uff0c\u5224\u5b9a\u7684\u6570\u636e\u5f52\u7c7b\u7684\u8fb9\u754c\u4f1a\u66f4\u8d8b\u5411\u4e8e\u8d34\u8fd1\u5750\u6807\u7cfb\u7684\u8f74\u6765\u5206\u5272\u6240\u5c5e\u533a\u57df\uff0c\u800c \\(L_2\\) \u7684\u8bdd\u76f8\u5bf9\u6765\u8bf4\u4e0e\u5750\u6807\u7cfb\u7684\u5173\u8054\u5ea6\u6ca1\u90a3\u4e48\u5927\uff0c\u4f1a\u5f62\u6210\u4e00\u4e2a\u5706\uff0c\u4e0d\u8ddf\u968f\u5750\u6807\u8f74\u53d8\u5316\u3002 <ul> <li>\u5728\u9762\u5bf9\u4e24\u4e2a\u5411\u91cf\u4e4b\u95f4\u7684\u5dee\u5f02\u65f6\uff0c\\(L_2\\) \u6bd4 \\(L_1\\) \u66f4\u52a0\u4e0d\u80fd\u5bb9\u5fcd\u8fd9\u4e9b\u5dee\u5f02\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u76f8\u5bf9\u4e8e1\u4e2a\u5de8\u5927\u7684\u5dee\u5f02\uff0c\\(L_2\\) \u8ddd\u79bb\u66f4\u503e\u5411\u4e8e\u63a5\u53d7\u591a\u4e2a\u4e2d\u7b49\u7a0b\u5ea6\u7684\u5dee\u5f02 </li> </ul> <p></p> <p>\u7531\u4e0a\u51e0\u5f20\u88ab\u523b\u610f\u5904\u7406\u7684\u56fe(\u4fdd\u8bc1\u4e86\u5176\u4e0e\u539f\u56fe\u7684\\(L_2\\)distance\u76f8\u540c)\u53ef\u77e5\uff0c\u4f7f\u7528\u50cf\u7d20\u5dee\u5f02\uff08\u4e24\u4e2a\u50cf\u7d20\u503c\u4e4b\u5dee\uff09\u6765\u6bd4\u8f83\u56fe\u50cf\u662f\u8fdc\u8fdc\u4e0d\u591f\u7684\uff0c\u611f\u5b98\u4e0a\u4e0d\u540c\u7684\u4e24\u5f20\u56fe\u7247\uff0c\u53ef\u80fd\u6709\u76f8\u540c\u7684\\(L_2\\)distance\uff0c\u4e0e\u56fe\u50cf\u7684\u8bed\u4e49\u5185\u5bb9\u5173\u8054\u4e0d\u5927\u3002</p>"},{"location":"python/cs231n-notebook/chapter1/#linear-classifier","title":"Linear Classifier","text":"<p>KNN \u6a21\u578b\u4e2d\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u4f7f\u7528\u4efb\u4f55\u53c2\u6570\uff0c\u53ea\u662f\u5355\u7eaf\u7684\u628a\u8bad\u7ec3\u6570\u636e\u5b58\u50a8\u8d77\u6765\uff0c\u800c\u4e0e\u4e4b\u76f8\u5bf9\u7684\u662f\u53c2\u6570\u6a21\u578b\uff0c\u6700\u7b80\u5355\u7684\u53c2\u6570\u6a21\u578b\u662f\u7ebf\u6027\u5206\u7c7b\u6a21\u578b\uff08Linear classifier\uff09:</p> \\[     f(x_i,W,b) = W x_i + b     \\] <p></p> <p>\u5b9e\u9645\u4e0a\uff0c\u4e0a\u56fe\u53c2\u6570\u77e9\u9635\\(W\\)\u76f8\u5f53\u4e8e\u662f\u4e09\u4e2a\u5206\u7c7b\u5668\u7684\u7ec4\u5408\uff0c\\(W\\)\u7684\u6bcf\u4e00\u884c\u90fd\u662f\u4e00\u4e2a\u5206\u7c7b\u5668\uff0c\u5206\u522b\u5bf9\u5e94'\u732b'\u3001'\u72d7'\u3001'\u8239'</p> <p>\u5c06\u7ebf\u6027\u5206\u7c7b\u5668\u770b\u505a\u6a21\u677f\u5339\u914d</p> <p> \u628a\u6743\u91cd\\(W\\)\u7684\u6bcf\u4e00\u884c\u770b\u4f5c\u4e00\u4e2a\u5206\u7c7b\u7684\u6a21\u677f\uff0c\u4e00\u5f20\u56fe\u50cf\u5bf9\u5e94\u4e0d\u540c\u5206\u7c7b\u7684\u5f97\u5206\uff0c\u662f\u901a\u8fc7\u4f7f\u7528\u5185\u79ef\uff08\u4e5f\u53eb\u70b9\u79ef\uff09\u6765\u6bd4\u8f83\u56fe\u50cf\u548c\u6a21\u677f\uff0c\u7136\u540e\u627e\u5230\u548c\u54ea\u4e2a\u6a21\u677f\u6700\u76f8\u4f3c\u3002 <p>\u53ef\u4ee5\u770b\u5230\uff1a</p> <p></p> <p></p> <p>\u5c06\u56fe\u50cf\u770b\u505a\u9ad8\u7ef4\u7a7a\u95f4\u7684\u70b9</p> <p> \u628a\u56fe\u50cf\u770b\u4f5c\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u4e2a\u70b9\uff0c\u7ebf\u6027\u5206\u7c7b\u5668\u5bf9\u6574\u4e2a\u7a7a\u95f4\u8fdb\u884c\u5206\u5272\uff0c\u5bf9\u5e94\u4e00\u4e2a\u4e2a\u7c7b\u522b <p></p> <p></p>"},{"location":"python/cs231n-notebook/chapter1/#loss-function","title":"Loss Function","text":"<p>\u5bf9\u4e8e\u6709N\u4e2a\u8bad\u7ec3\u6837\u672c\u5bf9\u5e94N\u4e2a\u6807\u7b7e\u7684\u8bad\u7ec3\u96c6\u6570\u636e\\((x_i, y_i)\\)\uff0c\u635f\u5931\u51fd\u6570\u5b9a\u4e49\u4e3a:</p> \\[     L =  \\frac{1}{N} \\sum_{i=1}^{N} L_i(f(x_i, W), y_i)   \\] <ul> <li> <p>\u591a\u7c7b\u652f\u6301\u5411\u91cf\u673a\u635f\u5931 (Multiclass Support Vector Machine Loss)</p> <p><code>SVM</code>\u7684\u635f\u5931\u51fd\u6570\u60f3\u8981 <code>SVM</code>\u5728\u6b63\u786e\u5206\u7c7b\u4e0a\u7684\u5f97\u5206\u59cb\u7ec8\u6bd4\u4e0d\u6b63\u786e\u5206\u7c7b\u4e0a\u7684\u5f97\u5206\u9ad8\u51fa\u4e00\u4e2a\u8fb9\u754c\u503c \\(\\Delta\\)</p> <p></p> <p>\u5bf9\u4e8e\u4e0a\u8ff0\u7b2c1\u5f20\u56fe\u7247\u300c\u5c0f\u732b\u300d\u6765\u8bf4\uff0c\u8bbe\\(\\Delta\\)\u4e3a1\uff0c\u5219\uff1a</p> \\[     L_1 = max(0, 5.1-3.2+1) + max(0, -1.7-3.2+1) = 2.9+0 = 2.9     \\] <p>\u5373\uff1a<code>SVM</code>\u635f\u5931\u51fd\u6570\u4e0d\u4ec5\u5e0c\u671b\u6b63\u786e\u5206\u7c7b\u7684\u5206\u6570\u6bd4\u5176\u4f59\u5206\u7c7b\u9ad8\uff0c\u800c\u4e14\u5e0c\u671b\u4fdd\u6301\u9ad8\u51fa\u4e00\u4e2a\u8ddd\u79bb\\(\\Delta\\)</p> <pre><code># x:\u5f97\u5206\u77e9\u9635 [[3.2, 5.1, -1.7],\n#            [1.3, 4.9, 2,0],\n#            [2.2, 2.5, -3.1],\n#            ..]\n#y:\u771f\u5b9e\u6807\u7b7e [0, 1, 2]\n\nclass MulticlassSVMLoss:\ndef __init__(self, x, y, delta):\n    self.x_train = x\n    self.y_train = y\n    self.delta = delta\ndef train(self):\n    x_true = np.array([self.x_train[k][self.y_train[k]] for k in range(len(self.y_train))])\n    margins = np.maximum(self.x_train - x_true.reshape(-1,1) + self.delta, 0)\n    loss = np.sum(margins,axis=1) - 1\n\n    return np.sum(loss) / len(self.y_train)\n</code></pre> <p>\u82e5\u4f7f\u7528\u7684\u662f\u5e73\u65b9\u635f\u5931SVM\uff1a\\(max(0,(s_j - s_{y_j} + 1)^2)\\)\uff0c\u5219\u635f\u5931\u51fd\u6570\u4f1a\u66f4\u5f3a\u70c8\u5730\u60e9\u7f5a\u8fc7\u754c\u7684\u8fb9\u754c\u503c\u3002</p> </li> <li> <p>Softmax classifier</p> <p></p> \\[     L = \\frac{1}{N} \\sum_i [-log(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}})] + \\lambda R(W)    \\] <p>\u5b9e\u9645\u4ee3\u7801\u7f16\u5199\u4e2d\uff0c\u7531\u4e8e\u6307\u6570\u5f62\u5f0f\u7684\u5b58\u5728\uff0c\u5982\u679c\u5f97\u5206\u5f88\u9ad8\uff0c\u4f1a\u5f97\u5230\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u6570\uff08\u6307\u6570\u6570\u503c\u7206\u70b8\uff09\u3002\u9664\u4ee5\u5927\u6570\u503c\u53ef\u80fd\u5bfc\u81f4\u6570\u503c\u8ba1\u7b97\u7684\u4e0d\u7a33\u5b9a\uff0c\u6240\u4ee5\u5b66\u4f1a\u4f7f\u7528\u5f52\u4e00\u5316\u6280\u5de7\u975e\u5e38\u91cd\u8981\u3002\u5982\u679c\u5728\u5206\u5f0f\u7684\u5206\u5b50\u548c\u5206\u6bcd\u90fd\u4e58\u4ee5\u4e00\u4e2a\u5e38\u6570\\(C\\)\uff0c\u5c31\u80fd\u5f97\u5230\u4e00\u4e2a\u4ece\u6570\u5b66\u4e0a\u7b49\u4ef7\u7684\u516c\u5f0f\uff1a</p> \\[     \\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}} = \\frac{Ce^{s_{y_i}}}{C\\sum_j e^{s_j}} = \\frac{e^{s_{y_i}+logC}}{\\sum_j e^{s_j+logC}} \\] <p>\u901a\u5e38\u5c06\\(C\\)\u8bbe\u4e3a\uff1a\\(logC = -\\max{s_j}\\)\uff0c\u901a\u8fc7\u5c06\u6570\u503c\u8fdb\u884c\u5e73\u79fb\uff0c\u4f7f\u5f97\u6700\u5927\u503c\u4e3a0</p> <pre><code>s = np.array([123, 456, 789]) # \u4f8b\u5b50\u4e2d\u67093\u4e2a\u5206\u7c7b\uff0c\u6bcf\u4e2a\u8bc4\u5206\u7684\u6570\u503c\u90fd\u5f88\u5927\np = np.exp(s) / np.sum(np.exp(s)) # \u4e0d\u597d\uff1a\u6570\u503c\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u6570\u503c\u7206\u70b8\n# \u90a3\u4e48\u5c06f\u4e2d\u7684\u503c\u5e73\u79fb\u5230\u6700\u5927\u503c\u4e3a0\uff1a\ns -= np.max(s) # s\u53d8\u6210 [-666, -333, 0]\np = np.exp(s) / np.sum(np.exp(s)) # \u73b0\u5728\u53ef\u4ee5\u4e86\uff0c\u5c06\u7ed9\u51fa\u6b63\u786e\u7ed3\u679c\n</code></pre> <p> <code>Softmax</code> \u548c <code>SVM</code> \u6bd4\u8f83</p> <p> - \u76f8\u5bf9\u4e8e <code>Softmax</code> \u5206\u7c7b\u5668\uff0c<code>SVM</code> \u66f4\u52a0 \u300c\u5c40\u90e8\u76ee\u6807\u5316\uff08local objective\uff09\u300d\uff0c\u53ea\u8981\u770b\u5230\u6b63\u786e\u5206\u7c7b\u76f8\u8f83\u4e8e\u4e0d\u6b63\u786e\u5206\u7c7b\uff0c\u5df2\u7ecf\u5f97\u5230\u4e86\u6bd4\u8fb9\u754c\u503c\u8fd8\u8981\u9ad8\u7684\u5206\u6570\uff0c\u5b83\u5c31\u4f1a\u8ba4\u4e3a\u635f\u5931\u503c\u662f \u516c\u5f0f\uff0c\u5bf9\u4e8e\u6570\u5b57\u4e2a\u4f53\u7684\u7ec6\u8282\u662f\u4e0d\u5173\u5fc3\u7684\u3002 <ul> <li><code>Softmax</code> \u5206\u7c7b\u5668\u5bf9\u4e8e\u5206\u6570\u662f\u6c38\u4e0d\u6ee1\u8db3\u7684\uff1a\u6b63\u786e\u5206\u7c7b\u603b\u80fd\u5f97\u5230\u66f4\u9ad8\u7684\u6982\u7387\uff0c\u9519\u8bef\u5206\u7c7b\u603b\u80fd\u5f97\u5230\u66f4\u4f4e\u7684\u6982\u7387\uff0c\u635f\u5931\u503c\u603b\u662f\u80fd\u591f\u66f4\u5c0f\u3002 </li> </ul>"},{"location":"python/cs231n-notebook/chapter2/","title":"\ud83d\udee3Stanford CS231n:Deep Learning for Computer Vision","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttps://cs231n.stanford.edu/</p> <p>2024\u7248PPT: https://cs231n.stanford.edu/slides/2024/</p> <p></p>"},{"location":"python/cs231n-notebook/chapter2/#optimization","title":"Optimization","text":"<p>SGD\u7684\u95ee\u9898</p> <p> <p></p> <ul> <li> <p>\u5982\u679c\u635f\u5931\u51fd\u6570\u5728\u4e00\u4e2a\u53c2\u6570\u65b9\u5411\u4e0b\u964d\u7684\u5feb\u53e6\u4e00\u4e2a\u65b9\u5411\u4e0b\u964d\u7684\u6162\uff0c\u8fd9\u6837\u4f1a\u5bfc\u81f4\u300c\u4e4b\u5b57\u5f62\u300d\u4e0b\u964d\u5230\u6700\u4f4e\u70b9\uff0c\u8fd9\u4e2a\u73b0\u8c61\u5728\u9ad8\u7ef4\u4e2d\u5f88\u666e\u904d\u3002</p> </li> <li> <p>\u5982\u679c\u635f\u5931\u51fd\u6570\u6709\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u978d\u70b9\u65f6\uff0c\u6b64\u65f6\u7684\u68af\u5ea6\u4e3a\u516c\u5f0f\uff0c\u53c2\u6570\u66f4\u65b0\u4f1a\u5361\u4f4f\uff0c\u6216\u5728\u6781\u5c0f\u503c\u9644\u8fd1\u9707\u8361\u3002</p> </li> <li> <p>SGD\u5177\u6709\u968f\u673a\u6027\uff0c\u6211\u4eec\u7684\u68af\u5ea6\u6765\u81ea\u5c0f\u6279\u91cf\u6570\u636e\uff0c\u53ef\u80fd\u4f1a\u6709\u566a\u58f0\uff0c\u8fd9\u6837\u68af\u5ea6\u4e0b\u964d\u7684\u8def\u7ebf\u4f1a\u5f88\u66f2\u6298\uff0c\u6536\u655b\u7684\u6162\u3002</p> </li> </ul> <p> </p> <ul> <li> <p>Momentum</p> <p>\u5728SGD\u4e2d\uff0c\u5f53\u524d\u4f4d\u7f6e\u7684\u68af\u5ea6\u76f4\u63a5\u5f71\u54cd\u66f4\u65b0\u65b9\u5411\uff08\u5728\u68af\u5ea6\u4e3a0\u7684\u5730\u65b9\uff0c\u4f4d\u7f6e\u66f4\u65b0\u53ef\u80fd\u5361\u4f4f\uff09\uff0c\u800cMomentum\u5c06\u68af\u5ea6\u770b\u4f5c\u4f5c\u7528\u529b,\u5f71\u54cd\u7684\u662f\u901f\u5ea6\uff0c\u5373\u5f15\u5165\u4e86\u52a8\u91cf\u9879\uff0c\u4f7f\u5f97\u66f4\u65b0\u65b9\u5411\u66f4\u52a0\u5e73\u6ed1\uff08\u5f53\u68af\u5ea6\u4e3a0\u65f6\uff0c\u901f\u5ea6\u4ecd\u7136\u5b58\u5728\uff09\u3002</p> <p>\u5177\u4f53\u6765\u8bf4\uff0cMomentum\u5c06\u5f53\u524d\u4f4d\u7f6e\u7684\u68af\u5ea6\u4e0e\u4e4b\u524d\u4f4d\u7f6e\u7684\u68af\u5ea6\u8fdb\u884c\u52a0\u6743\u5e73\u5747\uff0c\u4fdd\u6301\u4e86\u8d28\u70b9\u7684\u4e00\u5b9a\u8fd0\u52a8\u8d8b\u52bf\uff0c\u4ece\u800c\u4f7f\u5f97\u66f4\u65b0\u65b9\u5411\u66f4\u52a0\u7a33\u5b9a\uff0c\u4f7f\u5f97\u8d28\u70b9\u80fd\u591f\u51b2\u51fa\u978d\u70b9/\u5c40\u90e8\u6700\u4f18\u70b9,\u7ee7\u7eed\u66f4\u65b0\u53c2\u6570\u3002</p> <p></p> <p><pre><code>while True:\n    dx = compute_gradient(x)\n    v = mu * v - learning_rate * dx\n    x += v\n</code></pre> - <code>mu</code>\uff1a\u8868\u793a\u8870\u51cf\u7cfb\u6570\uff0c\u662f\u5bf9\u8fc7\u53bb\u8d8b\u52bf\u7684\u4fdd\u7559/\u79ef\u7d2f</p> <p></p> </li> <li> <p>Nesterov Accelerated Gradient (NAG)</p> <p><code>NAG</code>\u662f<code>Momentum</code>\u7684\u6539\u8fdb\u7248\uff0c\u65e2\u7136\u6211\u4eec\u77e5\u9053\u52a8\u91cf\u5c06\u4f1a\u5c06\u8d28\u70b9\u5e26\u5230\u4e00\u4e2a\u65b0\u7684\u4f4d\u7f6e\uff08\u5373\u5411\u524d\u770b\uff09\uff0c\u6211\u4eec\u5c31\u4e0d\u8981\u5728\u539f\u6765\u7684\u4f4d\u7f6e\u8ba1\u7b97\u68af\u5ea6\u4e86\uff0c\u5728\u8fd9\u4e2a\u300c\u5411\u524d\u770b\u300d\u7684\u5730\u65b9\u8ba1\u7b97\u68af\u5ea6\uff0c\u66f4\u65b0\u53c2\u6570\u3002</p> <p><code>NAG</code>\u4e0d\u4ec5\u53ef\u4ee5\u50cf\u52a8\u91cf\u6cd5\u4e00\u6837\u8003\u8651\u5386\u53f2\u8fed\u4ee3\u6b65\u9aa4\u7684\u68af\u5ea6\u4fe1\u606f\uff0c\u4e5f\u53ef\u4ee5\u8d85\u524d\u53c2\u8003\u672a\u6765\u7684\u68af\u5ea6\u4fe1\u606f</p> <p></p> <pre><code>while True:\n    pre_v = v\n    dW =  compute_gradient(W, X_train, y_train)\n    v = rho * v - learning_rate * dW\n    W += -rho * pre_v + (1 + rho) * v\n</code></pre> <p></p> </li> <li> <p>AdaGrad</p> <p>AdaGrad\u662f\u68af\u5ea6\u4e0b\u964d\u7684\u53e6\u4e00\u79cd\u6539\u8fdb\uff0c\u5b83\u901a\u8fc7\u7d2f\u79ef\u68af\u5ea6\u7684\u5e73\u65b9\u6765\u8c03\u6574\u5b66\u4e60\u7387\uff0c\u4f7f\u5f97\u5b66\u4e60\u7387\u5728\u68af\u5ea6\u8f83\u5927\u7684\u5730\u65b9\u51cf\u5c0f\uff0c\u5728\u68af\u5ea6\u8f83\u5c0f\u7684\u5730\u65b9\u589e\u5927\u3002\u5177\u4f53\u6765\u8bf4\uff0cAdaGrad\u5c06\u68af\u5ea6\u5e73\u65b9\u7684\u7d2f\u79ef\u548c\u4f5c\u4e3a\u5b66\u4e60\u7387\u7684\u5206\u6bcd\uff0c\u4f7f\u5f97\u5b66\u4e60\u7387\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\u800c\u51cf\u5c0f\u3002</p> <pre><code>while True:\n    dW = compute_gradient(W)\n    grad_squared += dW * dW\n    W -= learning_rate * dW / (np.sqrt(grad_squared) + eps)\n</code></pre> <p>\u4f46\u662f\uff0c\u5982\u679c\u68af\u5ea6\u7d2f\u52a0\u7684\u5f88\u5927\uff0c\u5b66\u4e60\u7387\u5c31\u4f1a\u53d8\u5f97\u975e\u5e38\u5c0f\uff0c\u5c31\u4f1a\u9677\u5728\u5c40\u90e8\u6781\u5c0f\u503c\u70b9\u6216\u63d0\u524d\u505c</p> </li> <li> <p>RMSProp: \u201cLeaky AdaGrad\u201d</p> <p>RMSProp\u7b97\u6cd5\u5728AdaGrad\u57fa\u7840\u4e0a\u5f15\u5165\u4e86\u8870\u51cf\u56e0\u5b50\uff0cRMSProp\u5728\u68af\u5ea6\u7d2f\u79ef\u7684\u65f6\u5019\uff0c\u4f1a\u5bf9\u300c\u8fc7\u53bb\u300d\u4e0e\u300c\u73b0\u5728\u300d\u505a\u4e00\u4e2a\u5e73\u8861\uff0c\u901a\u8fc7\u8d85\u53c2\u6570 <code>decay_rate</code> \u8c03\u8282\u8870\u51cf\u91cf</p> <pre><code>while True:\n    dW = compute_gradient(W)\n    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dW * dW\n    W -= learning_rate * dW / (np.sqrt(grad_squared) + eps)\n</code></pre> <p></p> </li> <li> <p>Adam</p> <p>\u52a8\u91cf\u66f4\u65b0\u5728SGD\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u4e00\u9636\u52a8\u91cf\uff0cAdaGrad\u548cRMSProp\u5728SGD\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u4e8c\u9636\u52a8\u91cf\u3002\u628a\u4e00\u9636\u52a8\u91cf\u548c\u4e8c\u9636\u52a8\u91cf\u7ed3\u5408\u8d77\u6765\uff0c\u5c31\u5f97\u5230\u4e86Adam\u4f18\u5316\u7b97\u6cd5\uff1aAdaptive + Momentum\u3002</p> <p>\u5b58\u5728\u7684\u95ee\u9898\uff1a\u7b2c\u4e00\u6b65\u4e2d<code>second_monent</code>\u53ef\u80fd\u4f1a\u6bd4\u8f83\u5c0f\uff0c\u8fd9\u6837\u5c31\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u7387\u975e\u5e38\u5927\uff0c\u6240\u4ee5<code>Adam</code>\u4e2d\u9700\u8981\u52a0\u5165\u504f\u7f6e\u3002</p> <p></p> <pre><code>eps = 1e-8\nfirst_moment = 0  # \u7b2c\u4e00\u52a8\u91cf\uff0c\u7528\u4e8e\u7d2f\u79ef\u68af\u5ea6\uff0c\u52a0\u901f\u8bad\u7ec3\nsecond_moment = 0  # \u7b2c\u4e8c\u52a8\u91cf\uff0c\u7528\u4e8e\u7d2f\u79ef\u68af\u5ea6\u5e73\u65b9\uff0c\u81ea\u52a8\u8c03\u6574\u5b66\u4e60\u7387\nfor t in range(1, num_iterations+1):\n    dW = compute_gradient(W)\n    first_moment = beta1 * first_moment + (1 - beta1) * dW  # Momentum\n    second_moment = beta2 * second_moment + (1 - beta2) * dW * dW  # AdaGrad / RMSProp\n\n    first_unbias = first_moment / (1 - beta1 ** t)  \n    # \u52a0\u5165\u504f\u7f6e\uff0c\u968f\u6b21\u6570\u51cf\u5c0f\uff0c\u9632\u6b62\u521d\u59cb\u503c\u8fc7\u5c0f\n    second_unbias = second_moment / (1 - beta2 ** t)\n\n    W -= learning_rate * first_unbias / (np.sqrt(second_unbias) + eps)\n</code></pre> <p>\u8bba\u6587\u4e2d\u63a8\u8350\u7684\u53c2\u6570\u503c <code>eps=1e-8, beta1=0.9, beta2=0.999, learning_rate = 1e-3\u62165e-4</code>\uff0c\u5bf9\u5927\u591a\u6570\u6a21\u578b\u6548\u679c\u90fd\u4e0d\u9519\u3002</p> </li> <li> <p>AdamW: Adam Variant with Weight Decay</p> <p><code>AdamW</code>\u662f<code>Adam</code>\u7684\u53d8\u4f53\uff0c\u5b83\u5c06\u6743\u91cd\u8870\u51cf\uff08<code>weight decay</code>\uff09\u6dfb\u52a0\u5230<code>Adam</code>\u7684\u66f4\u65b0\u89c4\u5219\u4e2d\u3002\u6743\u91cd\u8870\u51cf\u662f\u4e00\u79cd\u6b63\u5219\u5316\u6280\u672f\uff0c\u7528\u4e8e\u9632\u6b62\u6a21\u578b\u8fc7\u62df\u5408\u3002</p> <p>\u7b80\u5355\u6765\u8bf4\uff0cAdamW\u5c31\u662fAdam\u4f18\u5316\u5668\u52a0\u4e0aL2\u6b63\u5219\uff0c\u6765\u9650\u5236\u53c2\u6570\u503c\u4e0d\u53ef\u592a\u5927\u3002\u539f\u5148Adam\u7684\u5b9e\u73b0\u4e2d\u5982\u679c\u91c7\u7528\u4e86L2\u6743\u91cd\u8870\u51cf\uff0c\u5219\u76f8\u5e94\u7684\u6743\u91cd\u8870\u51cf\u9879\u4f1a\u88ab\u76f4\u63a5\u52a0\u5728loss\u91cc\uff0c\u4ece\u800c\u5bfc\u81f4\u52a8\u91cf\u7684\u4e00\u9636\u4e0e\u4e8c\u9636\u6ed1\u52a8\u5e73\u5747\u5747\u8003\u8651\u4e86\u8be5\u6743\u91cd\u8870\u51cf\u9879,\u800c\u8fd9\u5f71\u54cd\u4e86Adam\u7684\u4f18\u5316\u6548\u679c\uff0c\u800c\u5c06\u6743\u91cd\u8870\u51cf\u4e0e\u68af\u5ea6\u7684\u8ba1\u7b97\u8fdb\u884c\u89e3\u8026\u80fd\u591f\u663e\u8457\u63d0\u5347Adam\u7684\u6548\u679c</p> </li> </ul>"},{"location":"python/cs231n-notebook/chapter2/#regularization","title":"Regularization","text":""},{"location":"python/cs231n-notebook/chapter3/","title":"\ud83d\udee3Stanford CS231n:Deep Learning for Computer Vision","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttps://cs231n.stanford.edu/</p> <p>2024\u7248PPT: https://cs231n.stanford.edu/slides/2024/</p> <p></p>"},{"location":"python/cs231n-notebook/chapter3/#\u524d\u5411\u4f20\u64ad\u4e0e\u53cd\u5411\u4f20\u64ad","title":"\u524d\u5411\u4f20\u64ad\u4e0e\u53cd\u5411\u4f20\u64ad","text":""},{"location":"python/cs231n-notebook/chapter3/#\u5411\u91cf\u5f62\u5f0f\u7684\u53cd\u5411\u4f20\u64ad","title":"\u5411\u91cf\u5f62\u5f0f\u7684\u53cd\u5411\u4f20\u64ad","text":"<p>\u8fd9\u4e2a\\(max\\)\u51fd\u6570\u5bf9\u8f93\u5165\u5411\u91cf\\(x\\)\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u548c0\u6bd4\u8f83\u8f93\u51fa\u6700\u5927\u503c\uff0c\u56e0\u6b64\u8f93\u51fa\u5411\u91cf\u7684\u7ef4\u5ea6\u4e0d\u53d8\u3002\u6b64\u65f6\u7684\u68af\u5ea6\u662f\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u5373\u8f93\u51fa\u7684\u6bcf\u4e2a\u5143\u7d20\u5bf9\u8f93\u5165\u7684\u6bcf\u4e2a\u5143\u7d20\u6c42\u504f\u5bfc\u7ec4\u6210\u7684\u77e9\u9635\u3002</p> <p>\u5047\u5982\u8f93\u5165\\(x\\)\u662fn\u7ef4\u7684\u5411\u91cf\uff0c\u8f93\u51fa\\(y\\)\u662fm\u7ef4\u7684\u5411\u91cf\uff0c\u5219\\(y_1,y_2,...,y_n\\)\u90fd\u662f \\(x_1 ~ x_n\\)\u7684\u51fd\u6570\uff0c\u5f97\u5230\u7684\u96c5\u514b\u6bd4\u77e9\u9635\u5982\u4e0b\u6240\u793a\uff1a</p> \\[     \\begin{equation*}     \\begin{bmatrix}         \\dfrac{\\partial y_1}{\\partial x_1} &amp; \\dfrac{\\partial y_1}{\\partial x_2}  &amp; \\cdots &amp; \\dfrac{\\partial y_1}{\\partial x_n}\\\\          \\dfrac{\\partial y_2}{\\partial x_1} &amp; \\dfrac{\\partial y_2}{\\partial x_2}  &amp; \\cdots &amp; \\dfrac{\\partial y_2}{\\partial x_n}\\\\         \\vdots                             &amp; \\vdots                              &amp; \\vdots &amp; \\vdots                            \\\\         \\dfrac{\\partial y_m}{\\partial x_1} &amp; \\dfrac{\\partial y_m}{\\partial x_2}  &amp; \\cdots &amp; \\dfrac{\\partial y_m}{\\partial x_n}     \\end{bmatrix}     \\end{equation*} \\] <p>\u6839\u636e\u4e0a\u56fe\uff1a</p> <p></p> <p></p>"},{"location":"python/cs231n-notebook/chapter3/#\u7406\u89e3\u795e\u7ecf\u7f51\u7edc","title":"\u7406\u89e3\u795e\u7ecf\u7f51\u7edc","text":"<p>\u5168\u8fde\u63a5\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u79cd\u7406\u89e3\u662f\uff1a</p> <ul> <li>\u5b83\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7531\u4e00\u7cfb\u5217\u51fd\u6570\u7ec4\u6210\u7684\u51fd\u6570\u65cf\uff0c\u7f51\u7edc\u7684\u6743\u91cd\u5c31\u662f\u6bcf\u4e2a\u51fd\u6570\u7684\u53c2\u6570\u3002</li> </ul> <p>\u62e5\u6709\u81f3\u5c11\u4e00\u4e2a\u9690\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u662f\u4e00\u4e2a\u901a\u7528\u7684\u8fd1\u4f3c\u5668\uff0c\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u8fd1\u4f3c\u4efb\u4f55\u8fde\u7eed\u51fd\u6570\u3002</p> <p>\u867d\u7136\u4e00\u4e2a2\u5c42\u7f51\u7edc\u5728\u6570\u5b66\u7406\u8bba\u4e0a\u80fd\u5b8c\u7f8e\u5730\u8fd1\u4f3c\u6240\u6709\u8fde\u7eed\u51fd\u6570\uff0c\u4f46\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u6548\u679c\u76f8\u5bf9\u8f83\u5dee\u3002\u867d\u7136\u5728\u7406\u8bba\u4e0a\u6df1\u5c42\u7f51\u7edc\uff08\u4f7f\u7528\u4e86\u591a\u4e2a\u9690\u5c42\uff09\u548c\u5355\u5c42\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u662f\u4e00\u6837\u7684\uff0c\u4f46\u662f\u5c31\u5b9e\u8df5\u7ecf\u9a8c\u800c\u8a00\uff0c\u6df1\u5ea6\u7f51\u7edc\u6548\u679c\u6bd4\u5355\u5c42\u7f51\u7edc\u597d\u3002</p> <p>\u5bf9\u4e8e\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u800c\u8a00\uff0c\u5728\u5b9e\u8df5\u4e2d3\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u4f1a\u6bd42\u5c42\u7684\u8868\u73b0\u597d\uff0c\u7136\u800c\u7ee7\u7eed\u52a0\u6df1\uff08\u505a\u52304\uff0c5\uff0c6\u5c42\uff09\u5f88\u5c11\u6709\u592a\u5927\u5e2e\u52a9\u3002\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u60c5\u51b5\u5374\u4e0d\u540c\uff0c\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u826f\u597d\u7684\u8bc6\u522b\u7cfb\u7edf\u6765\u8bf4\uff0c\u6df1\u5ea6\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u56e0\u7d20\uff08\u6bd4\u5982\u5f53\u4eca\u6548\u679c\u597d\u7684CNN\u90fd\u6709\u51e0\u5341\u4e0a\u767e\u5c42\uff09\u3002\u5bf9\u4e8e\u8be5\u73b0\u8c61\u7684\u4e00\u79cd\u89e3\u91ca\u89c2\u70b9\u662f\uff1a\u56e0\u4e3a\u56fe\u50cf\u62e5\u6709\u5c42\u6b21\u5316\u7ed3\u6784\uff08\u6bd4\u5982\u8138\u662f\u7531\u773c\u775b\u7b49\u7ec4\u6210\uff0c\u773c\u775b\u53c8\u662f\u7531\u8fb9\u7f18\u7ec4\u6210\uff09\uff0c\u6240\u4ee5\u591a\u5c42\u5904\u7406\u5bf9\u4e8e\u8fd9\u79cd\u6570\u636e\u5c31\u6709\u76f4\u89c2\u610f\u4e49\u3002</p>"},{"location":"python/cs231n-notebook/chapter4/","title":"\ud83d\udee3Stanford CS231n:Deep Learning for Computer Vision","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttps://cs231n.stanford.edu/</p> <p>2024\u7248PPT: https://cs231n.stanford.edu/slides/2024/</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"python/cs231n-notebook/chapter4/#data-preprocessing","title":"Data Preprocessing","text":"<p>\u5f52\u4e00\u5316\uff08Normalization\uff09\u4e0e\u51cf\u5747\u503c\uff08Mean Subtraction\uff09\u662f\u6700\u5e38\u7528\u7684\u4e24\u79cd\u6570\u636e\u9884\u5904\u7406\u65b9\u6cd5\uff0c</p>"},{"location":"python/cs231n-notebook/chapter4/#weight-initialization","title":"Weight Initialization","text":"<ul> <li> <p>\u5168\u96f6\u521d\u59cb\u5316</p> <p>\u8fd9\u79cd\u505a\u6cd5\u662f\u9519\u8bef\u7684\u3002 \u56e0\u4e3a\u5982\u679c\u7f51\u7edc\u4e2d\u7684\u6bcf\u4e2a\u795e\u7ecf\u5143\u90fd\u8ba1\u7b97\u51fa\u540c\u6837\u7684\u8f93\u51fa\uff0c\u7136\u540e\u5b83\u4eec\u5c31\u4f1a\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u8ba1\u7b97\u51fa\u540c\u6837\u7684\u68af\u5ea6\uff0c\u4ece\u800c\u8fdb\u884c\u540c\u6837\u7684\u53c2\u6570\u66f4\u65b0\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u5982\u679c\u6743\u91cd\u88ab\u521d\u59cb\u5316\u4e3a\u540c\u6837\u7684\u503c\uff0c\u795e\u7ecf\u5143\u4e4b\u95f4\u5c31\u5931\u53bb\u4e86\u4e0d\u5bf9\u79f0\u6027\u7684\u6e90\u5934\u3002</p> <p>(\u8981\u6253\u7834\u53c2\u6570\u5bf9\u79f0\u7684\u95ee\u9898)</p> </li> <li> <p>\u5c0f\u968f\u673a\u6570\u521d\u59cb\u5316</p> <p><code>w = 0.01 * np.random.rangn(D, H)</code>(\u57fa\u4e8e\u96f6\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\u6765\u751f\u6210\u968f\u673a\u6570\u7684)\uff0c\u4e0b\u56fe\u5c55\u793a\u5c0f\u968f\u673a\u6570\u521d\u59cb\u5316\u5728\u4e0d\u540c\u5c42\u4e2d\u53c2\u6570\u7684\u6982\u7387\u5206\u5e03\uff1a</p> <p></p> <p>\u4e0a\u56fe\u53ef\u4ee5\u770b\u5230\u53ea\u6709\u7b2c\u4e00\u5c42\u7684\u8f93\u51fa\u5747\u503c\u65b9\u5dee\u6bd4\u8f83\u597d\uff0c\u8f93\u51fa\u63a5\u8fd1\u9ad8\u65af\u5206\u5e03\uff0c\u540e\u51e0\u5c42\u5747\u503c\u65b9\u5dee\u57fa\u672c\u4e3a0\uff0c\u8fd9\u6837\u5bfc\u81f4\u7684\u540e\u679c\u662f\u6b63\u5411\u4f20\u64ad\u7684\u6fc0\u6d3b\u503c\u57fa\u672c\u4e3a0\uff0c\u53cd\u5411\u4f20\u64ad\u65f6\u5c31\u4f1a\u8ba1\u7b97\u51fa\u975e\u5e38\u5c0f\u7684\u68af\u5ea6\uff08\u56e0\u6743\u91cd\u7684\u68af\u5ea6\u5c31\u662f\u5c42\u7684\u8f93\u5165\uff0c\u8f93\u5165\u63a5\u8fd10\uff0c\u68af\u5ea6\u63a5\u8fd10 \uff09\uff0c\u53c2\u6570\u57fa\u672c\u4e0d\u4f1a\u66f4\u65b0\u3002\uff08\u68af\u5ea6\u6d88\u5931\uff09</p> </li> <li> <p>\u5927\u968f\u673a\u6570\u521d\u59cb\u5316</p> <p><code>w = 1 * np.random.rangn(D, H)</code>\uff0c\u5018\u82e5\u4f7f\u7528\u7684\u662ftanh\u51fd\u6570</p> <p></p> <p>\u5982\u4e0a\u56fe\uff0c\u6240\u6709\u795e\u7ecf\u5143\u90fd\u4f1a\u9971\u548c\uff0c\u8f93\u51fa\u4e3a\u00b11\uff0c\u68af\u5ea6\u4e3a0</p> </li> <li> <p>Xavier/He\u521d\u59cb\u5316\uff08\u9488\u5bf9sigmoid &amp; tanh\uff09</p> <p><code>W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in + fan_out)</code></p> <p></p> <p>\u4fdd\u8bc1\u4e86\u7f51\u7edc\u4e2d\u6240\u6709\u795e\u7ecf\u5143\u8d77\u59cb\u65f6\u6709\u8fd1\u4f3c\u540c\u6837\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u56fe\u4e0a\u53ef\u4ee5\u770b\u51fa\u540e\u9762\u51e0\u5c42\u7684\u8f93\u5165\u8f93\u51fa\u53c2\u6570\u5206\u5e03\u4fdd\u6301\u63a5\u8fd1\u9ad8\u65af\u5206\u5e03\u3002</p> </li> <li> <p>Kaiming / MSRA \u521d\u59cb\u5316\uff08\u9488\u5bf9relu\uff09</p> <p>\u5f53\u4f7f\u7528ReLU\u6fc0\u6d3b\u51fd\u6570\u65f6\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u6bcf\u5c42\u4f1a\u6d88\u9664\u4e00\u534a\u7684\u795e\u7ecf\u5143\uff08\u7f6e0\uff09\uff0c\u7ed3\u679c\u4f1a\u4f7f\u65b9\u5dee\u6bcf\u6b21\u51cf\u534a\uff0c\u4f1a\u6709\u8d8a\u6765\u8d8a\u591a\u7684\u795e\u7ecf\u5143\u5931\u6d3b\uff0c\u8f93\u51fa\u4e3a0\u7684\u795e\u7ecf\u5143\u8d8a\u6765\u8d8a\u591a\u3002</p> <p></p> <p><code>W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)</code>\uff0c\u56e0\u4e3a\u6bcf\u6b21\u6709\u4e00\u534a\u7684\u795e\u7ecf\u5143\u5931\u6d3b\uff0c\u6821\u51c6\u65f6\u96642\u5373\u53ef\uff0c\u8fd9\u6837\u5f97\u5230\u7684\u7ed3\u679c\u4f1a\u6bd4\u8f83\u597d\u3002</p> <p></p> </li> </ul>"},{"location":"python/cs231n-notebook/chapter4/#batch-normalization","title":"Batch Normalization","text":"\\[     \\hat{x}^{(k)} = \\frac{x^{(k)} - E(x^{(k)})}{\\sqrt{Var(x^{(k)})}}     \\] <p>\u5f53\u4e00\u5f20\u7ecf\u8fc7\u6807\u51c6\u5316\u5904\u7406\u7684\u56fe\u7247\u7ecf\u8fc7\u5377\u79ef\u64cd\u4f5c\u4e4b\u540e\uff0c\u5f97\u5230\u7684\u5206\u5e03\u5c31\u4e0d\u518d\u50cf\u4e00\u5f00\u59cb\u90a3\u6837\u7684\u6807\u51c6\u5206\u5e03\uff0c\u4e00\u65e6\u6bcf\u6279\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\u5404\u4e0d\u76f8\u540c(batch \u68af\u5ea6\u4e0b\u964d)\uff0c\u90a3\u4e48\u7f51\u7edc\u5c31\u8981\u5728\u6bcf\u6b21\u8fed\u4ee3\u90fd\u53bb\u5b66\u4e60\u9002\u5e94\u4e0d\u540c\u7684\u5206\u5e03\uff0c\u8fd9\u6837\u5c06\u4f1a\u5927\u5927\u964d\u4f4e\u7f51\u7edc\u7684\u8bad\u7ec3\u901f\u5ea6</p> <p><code>Batch Normalization</code>\u65b9\u6cd5\u4e00\u5b9a\u7a0b\u5ea6\u89e3\u51b3\u4e86\u5982\u4f55\u5408\u7406\u521d\u59cb\u5316\u795e\u7ecf\u7f51\u7edc\u8fd9\u4e2a\u68d8\u624b\u95ee\u9898\uff0c\u5176\u505a\u6cd5\u662f\u8ba9\u6fc0\u6d3b\u6570\u636e\u5728\u8bad\u7ec3\u5f00\u59cb\u524d\u901a\u8fc7\u4e00\u4e2a\u7f51\u7edc\uff0c\u7f51\u7edc\u5904\u7406\u6570\u636e\u4f7f\u5176\u670d\u4ece\u6807\u51c6\u9ad8\u65af\u5206\u5e03\uff0c\u5373\u8ba9\u6bcf\u4e2a\u9690\u5c42\u8282\u70b9\u7684\u6fc0\u6d3b\u8f93\u5165\u5206\u5e03\u56fa\u5b9a\u4e0b\u6765</p> <p>\u5728\u5b9e\u73b0\u5c42\u9762\uff0c\u5e94\u7528\u8fd9\u4e2a\u6280\u5de7\u901a\u5e38\u610f\u5473\u7740\u5168\u8fde\u63a5\u5c42\u6216\u5377\u79ef\u5c42\u4e0e\u6fc0\u6d3b\u51fd\u6570\u4e4b\u95f4\u6dfb\u52a0\u4e00\u4e2a<code>BatchNorm</code>\u5c42\u3002\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u4f7f\u7528\u6279\u91cf\u5f52\u4e00\u5316\u5df2\u7ecf\u53d8\u5f97\u975e\u5e38\u5e38\u89c1\uff0c\u5728\u5b9e\u8df5\u4e2d\u4f7f\u7528\u4e86\u6279\u91cf\u5f52\u4e00\u5316\u7684\u7f51\u7edc\u5bf9\u4e8e\u4e0d\u597d\u7684\u521d\u59cb\u503c\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002</p> <p>\u5047\u8bbe\u6709N\u4e2a\u6837\u672c\u6570\u636e\u7684\u5c0f\u6279\u91cf\u8f93\u5165\uff0c\u6bcf\u4e2a\u8f93\u5165\\(x\\)\u6709\\(D\\)\u7ef4\uff0c\u5373\\(x = (x^{(1)}...x^{(d)})\\),\u5bf9\u6570\u636e\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff1a</p> <p></p> <p>\u5f15\u5165\u7f29\u653e\u4e0e\u5e73\u79fb\u53c2\u6570\uff0c\u8fd9\u4e24\u4e2a\u53c2\u6570\u53ef\u4ee5\u5728\u7f51\u7edc\u4e2d\u5b66\u4e60\uff0c\u5e76\u4e14\u80fd\u5b9e\u73b0\u6211\u4eec\u60f3\u8981\u7684\u6548\u679c\uff1a</p> \\[     y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}       \\] <p>BatchNorm\u7684\u5de5\u4f5c\u6d41\uff1a</p> <p></p> <p></p> <p><code>BatchNorm</code>\u7684\u4f18\u52bf:</p> <p></p> <ul> <li> <p>\u6539\u5584\u901a\u8fc7\u7f51\u7edc\u7684\u68af\u5ea6\u6d41(gradient flow)</p> </li> <li> <p>\u5177\u6709\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\uff1a\u5141\u8bb8\u66f4\u5927\u7684\u5b66\u4e60\u901f\u7387\u8303\u56f4\u3001\u51cf\u5c11\u5bf9\u521d\u59cb\u5316\u7684\u4f9d\u8d56</p> </li> <li> <p>\u52a0\u5feb\u5b66\u4e60\u901f\u7387\u8870\u51cf\uff0c\u66f4\u5bb9\u6613\u8bad\u7ec3</p> </li> <li> <p>\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u79cd\u6b63\u5219\u65b9\u5f0f\uff0c\u5728\u539f\u59cb\u8f93\u5165\\(X\\)\u4e0a\u6296\u52a8</p> </li> <li> <p>\u53ef\u4ee5\u4e0d\u4f7f\u7528Dropout\uff0c\u52a0\u5feb\u8bad\u7ec3</p> </li> <li> <p>\u6d4b\u8bd5\u65f6\u4e0d\u4f7f\u7528\u5c0f\u6279\u91cf\u4e2d\u8ba1\u7b97\u7684\u5747\u503c\u548c\u65b9\u5dee\uff0c\u76f8\u53cd\uff0c\u4f7f\u7528\u8bad\u7ec3\u671f\u95f4\u6fc0\u6d3b\u6570\u636e\u7684\u4e00\u4e2a\u56fa\u5b9a\u7684\u7ecf\u9a8c\u5747\u503c\uff0c\u4f8b\u5982\u53ef\u4ee5\u4f7f\u7528\u5728\u8bad\u7ec3\u671f\u95f4\u7684\u5e73\u5747\u503c\u4f5c\u4e3a\u4f30\u8ba1\u3002</p> </li> </ul> <p></p> <p><code>BatchNorm</code>\u9700\u8981\u6ce8\u610f\u7684\u95ee\u9898:</p> <ul> <li> <p>\u8bad\u7ec3\u65f6\u5c06traning\u53c2\u6570\u8bbe\u7f6e\u4e3aTrue\uff0c\u5728\u9a8c\u8bc1\u65f6\u5c06trainning\u53c2\u6570\u8bbe\u7f6e\u4e3aFalse\u3002\u5728pytorch\u4e2d\u53ef\u901a\u8fc7\u521b\u5efa\u6a21\u578b\u7684model.train()\u548cmodel.eval()\u65b9\u6cd5\u63a7\u5236.</p> </li> <li> <p>batch_size\u5c3d\u53ef\u80fd\u8bbe\u7f6e\u5927\u70b9\uff0c\u8bbe\u7f6e\u5c0f\u540e\u8868\u73b0\u53ef\u80fd\u5f88\u7cdf\u7cd5\uff0c\u8bbe\u7f6e\u7684\u8d8a\u5927\u6c42\u7684\u5747\u503c\u548c\u65b9\u5dee\u8d8a\u63a5\u8fd1\u6574\u4e2a\u8bad\u7ec3\u96c6\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002</p> <p>\u7814\u7a76\u8868\u660e\u5bf9\u4e8eResNet\u7c7b\u6a21\u578b\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0cbatch\u4ece16\u964d\u4f4e\u52308\u65f6\u5f00\u59cb\u6709\u975e\u5e38\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\u3002\u6240\u4ee5BN\u4e0d\u9002\u5e94\u4e8e\u5f53\u8bad\u7ec3\u8d44\u6e90\u6709\u9650\u800c\u65e0\u6cd5\u5e94\u7528\u8f83\u5927\u7684batch\u7684\u573a\u666f\u3002</p> </li> <li> <p>\u5efa\u8bae\u5c06bn\u5c42\u653e\u5728\u5377\u79ef\u5c42\uff08Conv\uff09\u548c\u6fc0\u6d3b\u5c42\uff08\u4f8b\u5982Relu\uff09\u4e4b\u95f4\uff0c\u4e14\u5377\u79ef\u5c42\u4e0d\u8981\u4f7f\u7528\u504f\u7f6ebias\uff0c\u56e0\u4e3a\u6ca1\u6709\u7528</p> </li> </ul>"},{"location":"python/cs231n-notebook/chapter4/#--layer-normalization","title":"- Layer Normalization","text":"<p>\u4e8b\u5b9e\u8bc1\u660e\uff0c\u6279\u91cf\u5f52\u4e00\u5316\u80fd\u4f7f\u7f51\u7edc\u66f4\u5bb9\u6613\u8bad\u7ec3\uff0c\u4f46\u662f\u5bf9\u6279\u91cf\u7684\u5927\u5c0f\u6709\u4f9d\u8d56\u6027\uff0c\u6279\u91cf\u592a\u5c0f\u6548\u679c\u4e0d\u597d\uff0c\u6279\u91cf\u592a\u5927\u53c8\u53d7\u5230\u786c\u4ef6\u7684\u9650\u5236\u3002\u6240\u4ee5\u5728\u5bf9\u8f93\u5165\u6279\u91cf\u5927\u5c0f\u5177\u6709\u4e0a\u9650\u7684\u590d\u6742\u7f51\u7edc\u4e2d\u4e0d\u592a\u6709\u7528\u3002\u5bf9\u4e8eRNN\u8fd9\u7c7b\u65f6\u5e8f\u7f51\u7edc\uff0c\u65f6\u5e8f\u7684\u957f\u5ea6\u5e76\u4e0d\u662f\u4e00\u4e2a\u5b9a\u503c\uff0c\u5f88\u96be\u53bb\u4f7f\u7528BN</p> <p>\u5c42\u5f52\u4e00\u5316(ayer Normalization)\u4e0d\u518d\u5bf9\u8fd9\u4e2a\u5c0f\u6279\u91cf\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u800c\u662f\u5bf9\u7279\u5f81\u5411\u91cf\u8fdb\u884c\u5f52\u4e00\u5316\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u5f53\u4f7f\u7528\u5c42\u5f52\u4e00\u5316\u65f6\uff0c\u57fa\u4e8e\u8be5\u7279\u5f81\u5411\u91cf\u5185\u7684\u6240\u6709\u9879\u7684\u603b\u548c\u6765\u5f52\u4e00\u5316\u5bf9\u5e94\u4e8e\u5355\u4e2a\u6570\u636e\u70b9\u3002\uff08\u65e0\u9700\u6279\u8bad\u7ec3\uff0c\u5176\u5728\u5355\u4e2a\u6837\u672c\u5185\u90e8\u5c31\u80fd\u5f52\u4e00\u5316\uff09</p> <p></p> <p></p> <p>\u5bf9\u4e8eLN\u4e0eBN\u800c\u8a00\uff0cBN \u53d6\u7684\u662f\u4e0d\u540c\u6837\u672c\u7684\u540c\u4e00\u4e2a\u7279\u5f81\uff0c\u800c LN \u53d6\u7684\u662f\u540c\u4e00\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u7279\u5f81</p>"},{"location":"python/cs231n-notebook/chapter4/#--instance-normalization\u5b9e\u4f8b\u5f52\u4e00\u5316","title":"- Instance Normalization(\u5b9e\u4f8b\u5f52\u4e00\u5316)","text":"<p><code>Instance normalization</code>\u662f\u4e00\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5b83\u5c06\u6bcf\u4e2a\u6837\u672c\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\u5f52\u4e00\u5316\u5230\u7279\u5b9a\u7684\u503c(\u5373<code>batch_size = 1</code>)\u3002\u8fd9\u79cd\u65b9\u6cd5\u4fdd\u6301\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u72ec\u7acb\u6027\uff0c\u9002\u7528\u4e8e\u98ce\u683c\u8fc1\u79fb\u3001GAN\u7b49\u9700\u8981\u4fdd\u6301\u5b9e\u4f8b\u72ec\u7acb\u6027\u7684\u4efb\u52a1</p>"},{"location":"python/cs231n-notebook/chapter4/#--group-normalization-\u5206\u7ec4\u5f52\u4e00\u5316","title":"- Group Normalization (\u5206\u7ec4\u5f52\u4e00\u5316)","text":"<p><code>Group Normalization\uff08GN\uff09</code>\u662f\u9488\u5bf9<code>Batch Normalization</code>\u5728<code>batch_size</code>\u8f83\u5c0f\u65f6\u9519\u8bef\u7387\u8f83\u9ad8\u800c\u63d0\u51fa\u7684\u6539\u8fdb\u7b97\u6cd5(\u6279\u6b21\u8f83\u5c0f\u65f6\uff0c\u8ba1\u7b97\u7684\u7edf\u8ba1\u5747\u503c\u548c\u65b9\u5dee\u4e0d\u591f\u7a33\u5b9a)\uff0c\u56e0\u4e3aBN\u5c42\u7684\u8ba1\u7b97\u7ed3\u679c\u4f9d\u8d56\u5f53\u524d<code>batch</code>\u7684\u6570\u636e\uff0c\u5f53<code>batch_size</code>\u8f83\u5c0f\u65f6\uff08\u6bd4\u59822\u30014\u8fd9\u6837\uff09\uff0c\u8be5<code>batch</code>\u6570\u636e\u7684\u5747\u503c\u548c\u65b9\u5dee\u7684\u4ee3\u8868\u6027\u8f83\u5dee\uff0c\u56e0\u6b64\u5bf9\u6700\u540e\u7684\u7ed3\u679c\u5f71\u54cd\u4e5f\u8f83\u5927\u3002</p> <p><code>GN</code>\u73b0\u5c06\u6240\u6709\u7684\u8f93\u5165\u901a\u9053\u5206\u4e3aG\u4e2a\u5c0f\u7ec4\uff0c\u7136\u540e\u5bf9\u6bcf\u4e2a\u5c0f\u7ec4\u5206\u522b\u505a\u5f52\u4e00\u5316\uff08\u5f53<code>G=1</code>\u662f\uff0c\u5373\u4e3a<code>LN</code>\uff09</p> <p></p>"},{"location":"python/cs231n-notebook/chapter5/","title":"\ud83d\udee3Stanford CS231n:Deep Learning for Computer Vision","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttps://cs231n.stanford.edu/</p> <p>2024\u7248PPT: https://cs231n.stanford.edu/slides/2024/</p> <p></p>"},{"location":"python/cs231n-notebook/chapter5/#3x3\u5c0f\u5377\u79ef\u6838","title":"3x3\u5c0f\u5377\u79ef\u6838","text":"<p>\u5728VGG\u4e2d\u7b2c\u4e00\u6b21\u4f7f\u7528\uff0c\u6709\u8bb8\u591a\u4f18\u52bf\uff1a</p> <ul> <li> <p>\u591a\u4e2a3x3\u5377\u79ef\u6838\u53ef\u4ee5\u6bd4\u4e00\u4e2a\u5927\u5377\u79ef\u6838\u6709\u66f4\u591a\u7684\u975e\u7ebf\u6027\uff0c\u66f4\u597d\u5730\u6355\u6349\u5c40\u90e8\u6a21\u5f0f\u3002</p> </li> <li> <p>3x3\u5377\u79ef\u6838\u53ef\u4ee5\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002</p> <p>\u5047\u8bbe\u5377\u79ef\u5c42\u7684\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7279\u5f81\u56fe\u5927\u5c0f\u76f8\u540c\u4e3a\\(C\\)\uff0c\u5219\u4e09\u4e2a3x3\u5377\u79ef\u5c42\u53c2\u6570\u4e2a\u6570\u4e3a\uff1a3(33\\(C\\)\\(C\\))=27\\(C^2\\)\uff0c\u4e00\u4e2a\u611f\u53d7\u91ce\u76f8\u540c\u76847x7\u5377\u79ef\u5c42\u7684\u53c2\u6570\u4e3a77\\(C\\)*\\(C\\)=49\\(C^2\\)\u3002</p> <p>\u663e\u7136\u591a\u4e2a3x3\u5377\u79ef\u6838\u7684\u53c2\u6570\u66f4\u5c11\uff0c\u5e76\u4e14\u4e2d\u95f4\u5c42\u5177\u5907\u66f4\u591a\u7684\u975e\u7ebf\u6027\u3002</p> </li> </ul>"},{"location":"python/cs231n-notebook/chapter5/#inception\u5757","title":"Inception\u5757\uff1a","text":"<p>\u300cInception\u300d\u6a21\u5757\u662f\u4e00\u79cd\u8bbe\u8ba1\u7684\u6bd4\u8f83\u597d\u7684\u5c40\u57df\u7f51\u62d3\u6251\u7ed3\u6784\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6a21\u5757\u5806\u53e0\u5728\u4e00\u8d77\u3002\u8fd9\u79cd\u62d3\u6251\u7ed3\u6784\u5bf9\u6765\u81ea\u524d\u4e00\u5c42\u7684\u8f93\u5165\uff0c\u5e76\u884c\u5e94\u7528\u591a\u79cd\u4e0d\u540c\u7684\u6ee4\u6ce2\u64cd\u4f5c\uff0c\u7136\u540e\u5c06\u6240\u6709\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u5728\u6df1\u5ea6\u4e0a\u4e32\u8054\u5728\u4e00\u8d77\u3002</p> <p></p> <p>\u4e0a\u8ff0\u7684inception\u5757\u4f1a\u5bfc\u81f4\u8f93\u51fa\u7684\u7f51\u7edc\u6df1\u5ea6 \u589e\u52a0\uff0c\u56e0\u6b64\u4f5c\u8005\u53c8\u5f15\u5165\u4e86\u300c\u964d\u7ef4\u300d\u64cd\u4f5c\uff0c\u5373\u5bf91x1\u5377\u79ef\u6838\u7684\u8f93\u51fa\u8fdb\u884c\u964d\u7ef4\uff0c\u5728\u4fdd\u7559\u539f\u8f93\u5165\u7a7a\u95f4\u5c3a\u5bf8\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002</p> <p></p>"},{"location":"python/cs231n-notebook/chapter5/#residual-learning","title":"Residual Learning","text":"<p>ResNet\u901a\u8fc7\u4f7f\u7528\u591a\u4e2a\u6709\u53c2\u5c42\u6765\u5b66\u4e60\u8f93\u5165\u4e0e\u8f93\u5165\u8f93\u51fa\u4e4b\u95f4\u7684\u6b8b\u5dee\u6620\u5c04\uff08 residual mapping \uff09 \uff0c\u800c\u975e\u50cf\u4e00\u822cCNN\u7f51\u7edc\uff08\u5982AlexNet/VGG\u7b49\uff09\u90a3\u6837\u4f7f\u7528\u6709\u53c2\u5c42\u6765\u76f4\u63a5\u5b66\u4e60\u8f93\u5165\u8f93\u51fa\u4e4b\u95f4\u7684\u5e95\u5c42\u6620\u5c04\uff08underlying mapping\uff09\u3002</p> <p></p> <p>ResNet\u7684\u5b9e\u9645\u8bad\u7ec3\u7684\u4e00\u4e9b\u7ec6\u8282\u5982\u4e0b\uff1a</p> <ul> <li> <p>\u6bcf\u4e2a CONV \u5c42\u540e\u4f7f\u7528<code>BatchNorm</code></p> </li> <li> <p>\u6743\u91cd\u4f7f\u7528<code>Kaiming</code>\u521d\u59cb\u5316</p> </li> <li> <p>\u66f4\u65b0\u65b9\u5f0f\u4f7f\u7528<code>SGD + Momentum (0.9)</code></p> </li> <li> <p>\u5b66\u4e60\u7387\u4e3a 0.1, \u9a8c\u8bc1\u9519\u8bef\u7387\u4e0d\u53d8\u65f6\u9664 10</p> </li> <li> <p><code>Mini-batch size</code> \u4e3a 256</p> </li> <li> <p>\u6743\u91cd\u8870\u51cf\u662f <code>1e-5</code></p> </li> <li>\u672a\u4f7f\u7528<code>dropout</code></li> </ul>"},{"location":"python/cs231n-notebook/chapter5/#resnet\u7684\u53d8\u4f53","title":"ResNet\u7684\u53d8\u4f53","text":"<ul> <li> <p>Identity Mappings in Deep Residual Networks</p> <p>\u6539\u8fdb\u4e86\u6b8b\u5dee\u5757\u8bbe\u8ba1\uff0c\u521b\u5efa\u66f4\u76f4\u63a5\u7684\u8def\u5f84\uff08\u5c06\u6fc0\u6d3b\u51fd\u6570\u79fb\u52a8\u5230\u6b8b\u5dee\u7684\u6620\u5c04\u8def\u5f84\uff09\uff0c\u4ee5\u4fbf\u5728\u6574\u4e2a\u7f51\u7edc\u4e2d\u4f20\u64ad\u4fe1\u606f</p> <p>[BatchNorm - ReLU - Conv - BatchNorm - ReLU -Conv]</p> <p></p> </li> <li> <p>ResNeXt</p> <p>ResNeXt\u5728ResNet\u7684\u57fa\u7840\u4e0a\uff0c\u5c06\u6b8b\u5dee\u5757\u4e2d\u76841x1\u5377\u79ef\u6838\u66ff\u6362\u4e3a1x1\u5377\u79ef\u6838+3x3\u5377\u79ef\u6838+1x1\u5377\u79ef\u6838\u7684\u6a21\u5757\uff0c\u5e76\u4e14\u5f15\u5165\u4e86\u5206\u7ec4\u5377\u79ef\uff08\u4e0e Inception \u6a21\u5757\u76f8\u4f3c\uff09\uff0c\u901a\u8fc7\u591a\u4e2a\u5e73\u884c\u8def\u5f84\u589e\u52a0\u6b8b\u5dee\u5757\u7684\u5bbd\u5ea6\uff08cardinality\uff09\u3002</p> <p></p> </li> </ul>"},{"location":"python/cs231n-notebook/chapter5/#squeezenet","title":"SqueezeNet","text":"<p>\u4f5c\u4e3a\u4e00\u4e2a\u8f7b\u91cf\u5316\u7f51\u7edc\uff0cSqueezeNet\u62e5\u6709\u4e0e AlexNet \u76f8\u540c\u7684\u7cbe\u5ea6\uff0c\u4f46\u53ea\u7528\u4e86 AlexNet 1/50 \u7684\u53c2\u6570\u91cf\u3002</p> <ul> <li> <p>Fire Module     SqueezeNet\u91c7\u7528\u4e86\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u5377\u79ef\u65b9\u5f0f\uff0c\u63d0\u51fa<code>fire module</code>\uff1b<code>fire module</code> \u5305\u542b\u4e24\u90e8\u5206\uff1a<code>squeeze</code> +<code>expand</code> \u3002\u5728\u4fdd\u8bc1\u540c\u7b49\u7ea7\u522b\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u7528\u66f4\u5c11\u53c2\u6570\u7684 CNN\u7ed3\u6784</p> <p>\u538b\u7f29\u7b56\u7565\uff1a</p> <ul> <li> <p>\u4f7f\u7528 1 x 1 \u5377\u79ef\u6ee4\u6ce2\u5668\u4ee3\u66ff 3 x 3 \u5377\u79ef \uff08\u53c2\u6570\u91cf\u5c119\u500d\uff09\uff1b</p> </li> <li> <p>\u4f7f\u75283x3\u4e2a\u6ee4\u6ce2\u5668\u51cf\u5c11\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\uff0c\u5229\u7528<code>squeeze layers</code>\u5b9e\u73b0\uff1b</p> </li> <li> <p>\u5728\u7f51\u7edc\u540e\u671f\u8fdb\u884c\u4e0b\u91c7\u6837\u64cd\u4f5c\uff0c\u53ef\u4ee5\u4f7f\u5377\u79ef\u5c42\u6709\u66f4\u5927\u7684\u6fc0\u6d3b\u7279\u5f81\u56fe\u3002</p> </li> </ul> <p><code>squeeze</code>\uff1a\u53ea\u6709 1 x 1 \u5377\u79ef\u6ee4\u6ce2\u5668\uff0c\u5bf9 feature map\u7684\u7ef4\u6570\u8fdb\u884c\u538b\u7f29\uff0c\u4ece\u800c\u8fbe\u5230\u51cf\u5c11\u6743\u503c\u53c2\u6570\u7684\u76ee\u7684\uff1b</p> <p><code>expand</code>\uff1a\u6df7\u5408\u6709 1 x 1 \u548c 3 x 3 \u5377\u79ef\u6ee4\u6ce2\u5668\uff1b</p> <p></p> <p>\u5177\u4f53\u64cd\u4f5c\u60c5\u51b5\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <p></p> </li> </ul> <p> \uff08 SqueezeNet - \u5e26\u7b80\u5355\u65c1\u8def\u7684 SqueezeNet - \u5e26\u590d\u6742\u65c1\u8def\u7684 SqueezeNet \uff09</p> <p></p>"},{"location":"python/cs231n-notebook/chapter5/#xception","title":"Xception","text":"<p>Xception\u662fInception V3\u7684\u6539\u8fdb\u7248\uff0c\u4e3b\u8981\u6539\u8fdb\u70b9\u5982\u4e0b\uff1a</p> <ul> <li> <p>\u4f7f\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff08depthwise separable convolution\uff09\u4ee3Inception\u6a21\u5757\u3002</p> </li> <li> <p>\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5c06\u6807\u51c6\u7684\u5377\u79ef\u64cd\u4f5c\u5206\u89e3\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u64cd\u4f5c\uff1a\u6df1\u5ea6\u5377\u79ef\uff08depthwise convolution\uff09\u548c\u9010\u70b9\u5377\u79ef\uff08pointwise convolution\uff09\u3002</p> <p>\u6df1\u5ea6\u5377\u79ef\u5bf9\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u5355\u72ec\u8fdb\u884c\u5377\u79ef\u64cd\u4f5c</p> <p></p> <p>\u800c\u9010\u70b9\u5377\u79ef\u5219\u5c06\u6240\u6709\u8f93\u5165\u901a\u9053\u7684\u5377\u79ef\u7ed3\u679c\u8fdb\u884c\u7ebf\u6027\u7ec4\u5408\u3002\u8fd9\u79cd\u5206\u89e3\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u548c\u53c2\u6570\u6570\u91cf\u3002</p> <p></p> </li> </ul> <p></p> <p>\u5728\u4f20\u7edf\u7684\u5377\u79ef\u7f51\u7edc\u4e2d\uff0c\u5377\u79ef\u5c42\u4f1a\u540c\u65f6\u5bfb\u627e\u8de8\u7a7a\u95f4\u548c\u8de8\u6df1\u5ea6\u7684\u76f8\u5173\u6027(\u5982\u4e0b\u56fe):</p> <p></p> <p>\u8fc7\u6ee4\u5668\u540c\u65f6\u8003\u8651\u4e86\u4e00\u4e2a\u7a7a\u95f4\u7ef4\u5ea6\uff08\u6bcf\u4e2a 2\u00d72 \u7684\u5f69\u8272\u65b9\u5757\uff09\u548c\u4e00\u4e2a\u8de8\u901a\u9053\u6216\u300c\u6df1\u5ea6\u300d\u7ef4\u5ea6\uff084 \u4e2a\u65b9\u5757\u7684\u5806\u53e0\uff09\u3002\u5728\u8f93\u5165\u56fe\u50cf\u7684\u8f93\u5165\u5c42\uff0c\u8fd9\u5c31\u76f8\u5f53\u4e8e\u4e00\u4e2a\u5728\u6240\u6709 3 \u4e2a RGB \u901a\u9053\u4e0a\u67e5\u770b\u4e00\u4e2a2\u00d72\u50cf\u7d20\u5757\u7684\u5377\u79ef\u8fc7\u6ee4\u5668\u3002</p> <p>\u5728 Inception \u4e2d\uff0c\u6211\u4eec\u5f00\u59cb\u5c06\u4e24\u8005\u7a0d\u5fae\u5206\u5f00\u3002\u6211\u4eec\u4f7f\u7528 1\u00d71 \u7684\u5377\u79ef\u5c06\u539f\u59cb\u8f93\u5165\u6295\u5c04\u5230\u591a\u4e2a\u5206\u5f00\u7684\u66f4\u5c0f\u7684\u8f93\u5165\u7a7a\u95f4\uff0c\u800c\u4e14\u5bf9\u4e8e\u5176\u4e2d\u7684\u6bcf\u4e2a\u8f93\u5165\u7a7a\u95f4\uff0c\u6211\u4eec\u90fd\u4f7f\u7528\u4e00\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u8fc7\u6ee4\u5668\u6765\u5bf9\u8fd9\u4e9b\u6570\u636e\u7684\u66f4\u5c0f\u7684 3D \u6a21\u5757\u6267\u884c\u53d8\u6362\u3002</p> <p>Xception \u66f4\u8fdb\u4e00\u6b65\u3002\u4e0d\u518d\u53ea\u662f\u5c06\u8f93\u5165\u6570\u636e\u5206\u5272\u6210\u51e0\u4e2a\u538b\u7f29\u7684\u6570\u636e\u5757\uff0c\u800c\u662f\u4e3a\u6bcf\u4e2a\u8f93\u51fa\u901a\u9053\u5355\u72ec\u6620\u5c04\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u7136\u540e\u518d\u6267\u884c 1\u00d71 \u7684\u6df1\u5ea6\u65b9\u9762\u7684\u5377\u79ef\u6765\u83b7\u53d6\u8de8\u901a\u9053\u7684\u76f8\u5173\u6027\u3002</p> <p></p> <p>https://cloud.tencent.com/developer/article/1119273</p>"},{"location":"python/cs231n-notebook/chapter5/#shufflenet","title":"ShuffleNet","text":"<p>ShuffleNet\u7684\u52a8\u673a\u5728\u4e8e\u5927\u91cf\u7684 \u516c\u5f0f \u5377\u79ef\u4f1a\u8017\u8d39\u5f88\u591a\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c<code>Group Conv</code>\u96be\u4ee5\u5b9e\u73b0\u4e0d\u540c\u5206\u7ec4\u4e4b\u95f4\u7684\u4fe1\u606f\u4ea4\u6d41\u3002</p> <ul> <li> <p>\u5206\u7ec4\u5377\u79ef\uff08group conv\uff09</p> <p>\u5bf9\u8f93\u5165\u5c42\u7684\u4e0d\u540c\u7279\u5f81\u56fe\u8fdb\u884c\u5206\u7ec4\uff0c\u518d\u4f7f\u7528\u4e0d\u540c\u7684\u5377\u79ef\u6838\u5bf9\u4e0d\u540c\u7ec4\u7684\u7279\u5f81\u56fe\u8fdb\u884c\u5377\u79ef\uff0c\u901a\u8fc7\u5206\u7ec4\u964d\u4f4e\u5377\u79ef\u7684\u8ba1\u7b97\u91cf</p> <p>\u5047\u8bbe\u8f93\u5165\u901a\u9053\u4e3a\\(C_i\\)\uff0c\u8f93\u51fa\u901a\u9053\u4e3a\\(C_o\\)\uff0c\u5206\u7ec4\u6570\u76ee\u4e3a\\(g\\)\uff0cGroup Conv\u7684\u64cd\u4f5c\u5982\u4e0b\uff1a</p> <ul> <li> <p>\u5c06\u8f93\u5165\u7279\u5f81\u56fe\u6cbf\u7740\u901a\u9053\u5206\u4e3a\\(g\\)\u7ec4\uff0c\u6bcf\u4e00\u7ec4\u7684\u901a\u9053\u6570\u76ee\u4e3a\\(C_i / g\\)\u3002</p> </li> <li> <p>\u4f7f\u7528\\(g\\)\u4e2a\u4e0d\u540c\u7684\u5377\u79ef\u6838\uff0c\u6bcf\u4e00\u4e2a\u5377\u79ef\u6838\u7684\u6ee4\u6ce2\u5668\u6570\u91cf\u4e3a\\(C_o / g\\)\u3002</p> </li> <li> <p>\u4f7f\u7528\u8fd9\\(g\\)\u4e2a\u4e0d\u540c\u7684\u5377\u79ef\u6838\uff0c\u5bf9\\(g\\)\u7ec4\u7279\u5f81\u56fe\u5206\u522b\u8fdb\u884c\u5377\u79ef\uff0c\u5f97\u5230\\(g\\)\u7ec4\u8f93\u51fa\u7279\u5f81\u56fe\uff0c\u6bcf\u4e00\u7ec4\u7684\u901a\u9053\u6570\u4e3a\\(C_o / g\\)\u3002</p> </li> <li> <p>\u5c06\u8fd9\\(g\\)\u7ec4\u7684\u8f93\u51fa\u7279\u5f81\u56fe\u7ed3\u5408\uff0c\u5f97\u5230\u6700\u7ec8\u7684\\(C_o\\)\u901a\u9053\u7684\u8f93\u51fa\u7279\u5f81\u56fe\u3002</p> </li> </ul> </li> <li> <p>\u901a\u9053\u6d17\u724c(channel Shuffle)</p> <p><code>Group Conv</code>\u7684\u4e00\u4e2a\u7f3a\u70b9\u5728\u4e8e\u4e0d\u540c\u7ec4\u4e4b\u95f4\u96be\u4ee5\u5b9e\u73b0\u901a\u4fe1,\u800c\u5bf9<code>Group Conv</code>\u4e4b\u540e\u7684\u7279\u5f81\u56fe\u6cbf\u7740\u901a\u9053\u7ef4\u5ea6\u8fdb\u884c\u91cd\u7ec4\uff0c\u8fd9\u6837\u4fe1\u606f\u5c31\u53ef\u4ee5\u5728\u4e0d\u540c\u7ec4\u4e4b\u95f4\u6d41\u8f6c\u3002</p> <p></p> </li> </ul> <p>ShuffleNet Unit</p> <p> \u57fa\u4e8e\u6b8b\u5dee\u5757\uff08residual block\uff09\u548c \u901a\u9053\u6d17\u724c\uff08channel shuffle\uff09\u8bbe\u8ba1\u7684<code>ShuffleNet Unit</code>: <p> </p>"},{"location":"python/cs231n-notebook/chapter5/#shufflenet-v2","title":"ShuffleNet V2","text":"<p>\u4e8b\u5b9e\u4e0aShuffleNet\u5b58\u5728\u8f83\u5927\u7684\u7f3a\u70b9\uff1aChannel Shuffle \u64cd\u4f5c\u8f83\u4e3a\u8017\u65f6\uff0c\u5bfc\u81f4 ShuffleNet \u7684\u5b9e\u9645\u8fd0\u884c\u901f\u5ea6\u6ca1\u6709\u90a3\u4e48\u7406\u60f3</p> <p>\u8f7b\u91cf\u7ea7CNN\u7f51\u7edc\u9ad8\u6548\u8bbe\u8ba1\u51c6\u5219-ShuffleNet v2</p>"},{"location":"python/cs231n-notebook/chapter5/#mobilenet","title":"MobileNet","text":"<p><code>MobileNet</code>\u662f\u4e13\u7528\u4e8e\u79fb\u52a8\u548c\u5d4c\u5165\u5f0f\u89c6\u89c9\u5e94\u7528\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u662f\u57fa\u4e8e\u4e00\u4e2a\u6d41\u7ebf\u578b\u7684\u67b6\u6784\uff0c\u4f7f\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u7684\u5377\u79ef\u6765\u6784\u5efa\u8f7b\u91cf\u7ea7\u7684\u6df1\u5c42\u795e\u7ecf\u7f51\u7edc\u3002</p> <p><code>MobileNet V1</code>\u7684\u6838\u5fc3\u662f\u5c06\u5377\u79ef\u62c6\u5206\u6210 <code>Depthwise Conv</code> \u548c <code>Pointwise Conv</code> \u4e24\u90e8\u5206</p> <ul> <li> <p>\u666e\u901a\u7f51\u7edc\uff08\u4ee5VGG\u4e3a\u4f8b\uff09 \uff1a<code>3x3 Conv BN ReLU</code></p> </li> <li> <p><code>Mobilenet</code>\u57fa\u7840\u6a21\u5757\uff1a<code>3x3 Depthwise Conv BN ReLU</code> \u548c <code>3x3 Pointwise Conv BN ReLU</code></p> </li> </ul> <p></p> <p>V1\u7f3a\u70b9:</p> <ul> <li> <p>ReLU\u6fc0\u6d3b\u51fd\u6570\u7528\u5728\u4f4e\u7ef4\u7279\u5f81\u56fe\u4e0a\uff0c\u4f1a\u7834\u574f\u7279\u5f81\u3002</p> </li> <li> <p>ReLU\u8f93\u51fa\u4e3a0\u65f6\u5bfc\u81f4\u7279\u5f81\u9000\u5316\u3002\u7528\u6b8b\u5dee\u8fde\u63a5\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002</p> </li> <li> <p>\u7ed3\u6784\u8fc7\u4e8e\u7b80\u5355\uff0c\u6ca1\u6709\u590d\u7528\u56fe\u50cf\u7279\u5f81\uff0c\u5373\u6ca1\u6709Concat\u6216Add\u7b49\u64cd\u4f5c\u8fdb\u884c\u7279\u5f81\u878d\u5408</p> </li> </ul>"},{"location":"python/cs231n-notebook/chapter5/#mobilenet-v2","title":"MobileNet V2","text":"<p><code>MobileNet V2</code>\u9488\u5bf9<code>MobileNet</code>\u7684\u4e0a\u8ff02\u4e2a\u95ee\u9898\uff0c\u5f15\u5165\u4e86<code>Inverted Residual</code>\u548c<code>Linear Bottleneck</code>\u5bf9\u5176\u8fdb\u884c\u6539\u9020\uff0c\u7f51\u7edc\u4e3a\u5168\u5377\u79ef\uff0c\u4f7f\u7528<code>ReLU6</code>\uff08\u6700\u9ad8\u8f93\u51fa\u4e3a6\uff09\u6fc0\u6d3b\u51fd\u6570\u3002</p> \\[     y = ReLU6(x) = min(max(x,0),6)     \\] <ul> <li> <p>Manifold of interest</p> <p><code>Manifold of interest</code>\u662f\u6307\u5728\u7279\u5f81\u7a7a\u95f4(\u7279\u5f81\u56fe)\u4e2d\uff0c\u4e0e\u7279\u5b9a\u4efb\u52a1\u6216\u6982\u5ff5\u76f8\u5173\u7684\u6570\u636e\u6837\u672c\u7684\u805a\u96c6\u533a\u57df\u3002</p> <p>\u56e0\u6b64\u6709\u4e00\u79cd\u76f4\u89c9\uff0c\u53ef\u4ee5\u901a\u8fc7\u51cf\u5c0f\u5377\u79ef\u5c42\u7684\u7ef4\u6570\u6765\u51cf\u5c0f\u7279\u5f81\u7a7a\u95f4\u7684\u7ef4\u6570\uff0c\u56e0\u4e3a<code>Manifold of interest</code>\u53ea\u5360\u7279\u5f81\u7a7a\u95f4\u7684\u4e00\u90e8\u5206\uff0c\u5e0c\u671b\u5c3d\u53ef\u80fd\u7684\u51cf\u5c11\u5176\u4f59\u65e0\u5173\u7684\u7279\u5f81\u7a7a\u95f4\u3002</p> <p>\u5f53\u7279\u5f81\u7a7a\u95f4\u7ecf\u8fc7\u975e\u7ebf\u6027\u53d8\u6362<code>ReLU</code>\u6fc0\u6d3b\u4f1a\u5bfc\u81f4\u4e3a\u8d1f\u7684\u8f93\u5165\u5168\u90e8\u53d8\u4e3a\u96f6\uff0c\u5bfc\u81f4\u5931\u53bb\u4fdd\u5b58\u7684\u4fe1\u606f\uff0c\u5f53ReLU\u4f7f\u5f97\u67d0\u4e00\u4e2a\u8f93\u5165\u901a\u9053\u5d29\u584c\u65f6(\u8fd9\u4e2a\u901a\u9053\u4e2d\u6709\u5f88\u591a\u7279\u5f81\u50cf\u7d20\u503c\u90fd\u53d8\u6210\u4e86\u8d1f\u503c)\uff0c\u5c31\u4f1a\u4f7f\u5f53\u524d\u901a\u9053\u4e22\u5931\u5f88\u591a\u4fe1\u606f</p> <p>\u4f46\u662f\u5982\u679c\u6709\u5f88\u591a\u5377\u79ef\u6838\uff0c\u4e5f\u5c31\u662f\u751f\u6210\u4e86\u5f88\u591a\u901a\u9053\uff0c\u90a3\u4e48\u5728\u5f53\u524d\u901a\u9053\u4e22\u5931\u7684\u4fe1\u606f\u5c31\u4f1a\u6709\u53ef\u80fd\u5728\u5176\u4ed6\u901a\u9053\u627e\u56de\u6765\uff0c\u4e0b\u56fe\u5c55\u793a\u4e86\u5d4c\u5165\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4f4e\u7ef4\u5174\u8da3\u6d41\u5f62\u7ecf\u8fc7<code>ReLU</code>\u6fc0\u6d3b\u7684\u60c5\u51b5\u3002</p> <p></p> </li> </ul> <p>MobileNet\u4f5c\u8005\u7ed9\u51fa\u4e86\u8bc1\u660e\uff1a\u5982\u679c\u8f93\u5165\u6d41\u5f62\u5d4c\u5165\u7684\u7279\u5f81\u7a7a\u95f4\u7ef4\u5ea6\u8db3\u591f\u9ad8\uff0c\u90a3\u4e48ReLU\u5c31\u53ef\u4ee5\u5728\u4fdd\u7559\u4fe1\u606f\u7684\u540c\u65f6\u5b8c\u6210non-linearity\u7684\u672c\u804c\u5de5\u4f5c\u2014\u2014\u63d0\u9ad8\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff08\u5728\u5fc5\u8981\u7684ReLU\u4e4b\u524d\uff0c\u63d0\u9ad8\u5377\u79ef\u5c42\u8f93\u51fatensor\u7684\u7ef4\u5ea6\u3002\uff09</p> <ul> <li> <p><code>Inverted Residual</code>(\u5012\u6b8b\u5dee\u7ed3\u6784)</p> <p>\u6df1\u5ea6\u5377\u79ef\u5c42\u63d0\u53d6\u7279\u5f81\u9650\u5236\u4e8e\u8f93\u5165\u7279\u5f81\u7ef4\u5ea6\uff0c\u82e5\u91c7\u7528\u666e\u901a\u6b8b\u5dee\u5757\u4f1a\u5c06\u8f93\u5165\u7279\u5f81\u56fe\u538b\u7f29\uff0c\u6df1\u5ea6\u5377\u79ef\u63d0\u53d6\u7684\u7279\u5f81\u4f1a\u66f4\u5c11\uff0cMobileNets_V2\u5c06\u8f93\u5165\u7279\u5f81\u56fe\u6269\u5f20\uff0c\u4e30\u5bcc\u7279\u5f81\u6570\u91cf\uff0c\u8fdb\u800c\u63d0\u9ad8\u7cbe\u5ea6\u3002</p> <p>\u5148\u4f7f\u75281x1\u5377\u79ef\u63d0\u5347\u901a\u9053\u6570\u91cf\uff0c\u7136\u540e\u4f7f\u75283x3\u5377\u79ef\u63d0\u53d6\u7279\u5f81\uff0c\u4e4b\u540e\u4f7f\u75281x1\u5377\u79ef\u964d\u4f4e\u901a\u9053\u6570\u91cf\uff0c\u6700\u540e\u52a0\u4e0a\u6b8b\u5dee\u8fde\u63a5\u3002\u6574\u4e2a\u8fc7\u7a0b\u662f\u300c\u6269\u5f20-\u5377\u79ef-\u538b\u7f29\u300d\u3002(\u4e0e\u4f20\u7edf\u6b8b\u5dee\u76f8\u53cd)</p> <p></p> <p></p> </li> <li> <p><code>Linear Bottleneck</code>(\u7ebf\u6027\u74f6\u9888\u5c42)</p> <p>\u7ebf\u6027\u74f6\u9888\u5c42\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u901a\u8fc7\u964d\u4f4e\u7ef4\u5ea6\u6765\u63d0\u53d6\u6570\u636e\u7684\u4e3b\u8981\u7279\u5f81\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u91cf\u548c\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u5165\u6570\u636e\u7684\u91cd\u8981\u4fe1\u606f\uff0c\u901a\u5e38\u7531\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\u64cd\u4f5c\u7ec4\u6210\uff0c\u4f8b\u5982\u5168\u8fde\u63a5\u5c42\u6216\u5377\u79ef\u5c42\uff0c\u5176\u8f93\u51fa\u7ef4\u5ea6\u8fdc\u5c0f\u4e8e\u8f93\u5165\u7ef4\u5ea6\uff0c\u5e76\u4e14\u4e0d\u5f15\u5165\u975e\u7ebf\u6027\u53d8\u6362\u3002    </p> <p><code>MobileNet V2</code> \u5728 <code>Depthwise Conv</code> \u7684\u524d\u9762\u52a0\u4e86\u4e00\u4e2a 1x1\u5377\u79ef\uff0c\u4f7f\u7528 <code>ReLU6</code> \u4ee3\u66ff <code>ReLU</code>\uff0c\u4e14\u53bb\u6389\u4e86\u7b2c\u4e8c\u4e2a1x1\u5377\u79ef\u7684\u6fc0\u6d3b\u51fd\u6570\uff08\u5373\u4f7f\u7528\u7ebf\u6027\u7684\u6fc0\u6d3b\u51fd\u6570\uff09\uff0c\u9632\u6b62 <code>ReLU</code> \u5bf9\u7279\u5f81\u7684\u7834\u574f\u3002</p> <p></p> </li> </ul> <p></p>"},{"location":"python/cs231n-notebook/chapter5/#--mobilenet-v3","title":"- MobileNet V3","text":"<p>\u5728 <code>MobileNet V2</code> \u7684\u57fa\u7840\u4e0a\uff0c\u53c8\u63d0\u51fa\u4e86<code>MobileNet V3</code>\uff0c\u5b83\u7684\u4f18\u5316\u4e4b\u5904\u5305\u62ec\uff1a\u5f15\u5165\u4e86 SE\u3001\u5c3e\u90e8\u7ed3\u6784\u6539\u8fdb\u3001\u901a\u9053\u6570\u76ee\u8c03\u6574\u3001h-swish \u6fc0\u6d3b\u51fd\u6570\u5e94\u7528\uff0cNAS \u7f51\u7edc\u7ed3\u6784\u641c\u7d22\u7b49</p> <p><code>MobileNetV3</code> \u6709\u4e24\u4e2a\u7248\u672c\uff0c<code>MobileNetV3-Small</code> \u4e0e <code>MobileNetV3-Large</code> \u5206\u522b\u5bf9\u5e94\u5bf9\u8ba1\u7b97\u548c\u5b58\u50a8\u8981\u6c42\u4f4e\u548c\u9ad8\u7684\u7248\u672c\u3002</p> <ul> <li> <p>SE(squeeze and excitation)\u7ed3\u6784</p> <p>SE \u7ed3\u6784\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7528\u4e8e\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f15\u5165\u901a\u9053\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u3002</p> <p>\u5176\u6838\u5fc3\u601d\u60f3\u662f\u4e0d\u540c\u901a\u9053\u7684\u6743\u91cd\u5e94\u8be5\u81ea\u9002\u5e94\u5206\u914d\uff0c\u7531\u7f51\u7edc\u81ea\u5df1\u5b66\u4e60\u51fa\u6765\u7684\uff0c\u800c\u4e0d\u662f\u50cfInception net\u4e00\u6837\u7559\u4e0b\u8fc7\u591a\u4eba\u5de5\u5e72\u9884\u7684\u75d5\u8ff9\u3002</p> <p></p> <p><code>MobileNet V3</code>\u5728 <code>bottleneck</code>\u4e2d\u5f15\u5165\u4e86 SE \u7ed3\u6784\uff0c\u653e\u5728 <code>Depthwise Conv</code> \u4e4b\u540e\uff0c\u5e76\u4e14\u5c06<code>Expansion Layer</code>\u7684\u901a\u9053\u6570\u76ee\u53d8\u4e3a\u539f\u6765\u7684 \u00bc\uff0c\u5728\u63d0\u5347\u7cbe\u5ea6\u7684\u540c\u65f6\u57fa\u672c\u4e0d\u589e\u52a0\u65f6\u95f4\u6d88\u8017\u3002  </p> <p></p> <p>Excitation\uff08\u81ea\u9002\u5e94\u91cd\u65b0\u6821\u6b63\uff09\u7528\u6765\u5168\u9762\u6355\u83b7\u901a\u9053\u4f9d\u8d56\u6027\uff0c\u4f5c\u8005\u91c7\u7528\u4e86\u4e24\u5c42\u5168\u8fde\u63a5\u6784\u6210\u7684\u95e8\u673a\u5236\uff0c\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u628a\\(C\\)\u4e2a\u901a\u9053\u538b\u7f29\u6210\u4e86\\(C/r\\)\u4e2a\u901a\u9053\u6765\u964d\u4f4e\u8ba1\u7b97\u91cf\uff0c\u518d\u901a\u8fc7\u4e00\u4e2a<code>ReLU</code>\u975e\u7ebf\u6027\u6fc0\u6d3b\u5c42\uff0c\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u5c06\u901a\u9053\u6570\u6062\u590d\u56de\u4e3a\\(C\\)\u4e2a\u901a\u9053\uff0c\u518d\u901a\u8fc7h-swish\u6fc0\u6d3b\u5f97\u5230\u6743\u91cds\uff0c\u6700\u540e\u5f97\u5230\u7684\u8fd9\u4e2as\u7684\u7ef4\u5ea6\u662f1\u00d71\u00d7C\uff0c\u5b83\u662f\u7528\u6765\u523b\u753b\u7279\u5f81\u56feU\u4e2d\\(C\\)\u4e2a<code>feature map</code>\u7684\u6743\u91cd\u3002</p> </li> </ul> <p></p> <ul> <li>hardswish\u6fc0\u6d3b\u51fd\u6570</li> </ul> \\[     Swish(x) = x * sigmoid(x) \\] <p><code>swish</code>\u6fc0\u6d3b\u51fd\u6570\u5176\u5177\u5907\u65e0\u4e0a\u754c\u6709\u4e0b\u754c\u3001\u5e73\u6ed1\u4ee5\u53ca\u975e\u5355\u8c03\u7684\u7279\u6027\uff0c\u5e76\u4e14\u5728\u6df1\u5c42\u6a21\u578b\u4e0a\u7684\u6548\u679c\u4f18\u4e8eReLU\u3002\u867d\u7136\u63d0\u9ad8\u4e86\u7cbe\u5ea6\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u8ba1\u7b97\u6210\u672c\u7684\u589e\u52a0</p> <p>\u82e5\u628a<code>swish</code>\u4e2d\u7684<code>sigmoid</code>\u66ff\u6362\u6210<code>ReLU6</code>\uff0c\u4fbf\u662f<code>MobileNet V3</code>\u4e2d\u4f7f\u7528\u7684<code>h-swish</code>\u3002</p> \\[     h-Swish(x) = x \\frac{ReLU6(x+3)}{6}     \\] <p></p> <p>\u8ba1\u7b97\u4e0d\u4ec5\u65b9\u4fbf\uff0c\u56fe\u50cf\u4e0a\u903c\u8fd1<code>swish</code>\uff0c\u4e14<code>h-swish</code>\u80fd\u5728\u7279\u5b9a\u6a21\u5f0f\u4e0b\u6d88\u9664\u7531\u4e8e\u8fd1\u4f3csigmoid\u7684\u4e0d\u540c\u5b9e\u73b0\u800c\u5e26\u6765\u7684\u6f5c\u5728\u6570\u503c\u7cbe\u5ea6\u635f\u5931\u3002</p> <ul> <li> <p>\u5c3e\u90e8\u7ed3\u6784\u6539\u8fdb</p> <p></p> <ul> <li> <p>\u5c061x1\u5377\u79ef\u79fb\u52a8\u5230 avg pooling \u540e\u9762\uff0c\u964d\u4f4e\u8ba1\u7b97\u91cf\u3002</p> </li> <li> <p>\u53bb\u6389\u4e86\u5c3e\u90e8\u7ed3\u6784\u4e2d\u300c\u6269\u5f20-\u5377\u79ef-\u538b\u7f29\u300d\u4e2d\u76843x3 \u5377\u79ef\u4ee5\u53ca\u5176\u540e\u9762\u7684 1x1 \u5377\u79ef\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u7cbe\u5ea6\u6ca1\u6709\u635f\u5931\u3002</p> </li> </ul> </li> </ul>"},{"location":"python/cs231n-notebook/chapter6/","title":"\ud83d\udee3Stanford CS231n:Deep Learning for Computer Vision","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttps://cs231n.stanford.edu/</p> <p>2024\u7248PPT: https://cs231n.stanford.edu/slides/2024/</p> <p></p>"},{"location":"python/cs231n-notebook/chapter6/#recurrent-neural-networks","title":"Recurrent Neural Networks","text":"<p>\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08Recurrent Neural Networks\uff09\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u5bf9\u5e8f\u5217\u6570\u636e\u505a\u5f88\u597d\u7684\u5efa\u6a21\uff0cRNN\u5f88\u7075\u6d3b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8f93\u5165\u548c\u8f93\u51fa\u7684\u4e0d\u540c\u7c7b\u578b:</p> <p></p> <p></p> <p>\u6ce8\u610f\uff1a\u8ba1\u7b97\\(h_t\\)\u7684\u6bcf\u4e00\u6b65\u90fd\u4f7f\u7528\u7684\u76f8\u540c\u7684\u53c2\u6570\\(W\\)\uff0c\u53c2\u6570\u51fd\u6570\\(f\\)\u4e5f\u662f\u5b8c\u5168\u76f8\u540c\u7684\u3002\u8fd9\u6837\u5728\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\u65f6\uff0c\u9700\u8981\u5c06\u6bcf\u4e00\u4e2a\u65f6\u523b\u7684\u68af\u5ea6\u7d2f\u52a0\u8d77\u6765\u5f97\u5230\u6700\u7ec8\\(W\\)\u7684\u68af\u5ea6\u3002</p>"},{"location":"python/cs231n-notebook/chapter6/#\u622a\u65ad\u53cd\u5411\u4f20\u64adtruncated-backpropagation","title":"\u622a\u65ad\u53cd\u5411\u4f20\u64ad\uff08Truncated Backpropagation\uff09","text":"<p>\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u9700\u8981\u904d\u5386\u6574\u4e2a\u5e8f\u5217\u7d2f\u52a0\u8ba1\u7b97\u635f\u5931\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u4e5f\u9700\u8981\u904d\u5386\u6574\u4e2a\u5e8f\u5217\u6765\u8ba1\u7b97\u68af\u5ea6\u3002\u6211\u4eec\u53ef\u4ee5\u60f3\u8c61\u4e00\u4e0b\uff0c\u5982\u679c\u6211\u4eec\u7684\u8bed\u6599\u5e93\u975e\u5e38\u5927\uff08\u4f8b\u5982\u7ef4\u57fa\u767e\u79d1\u4e2d\u6240\u6709\u6587\u672c\uff09\uff0c\u90a3\u4e48\u65f6\u95f4\u82b1\u8d39\u4ee5\u53ca\u5185\u5b58\u5360\u7528\u90fd\u662f\u5de8\u5927\u7684</p> <p></p> <p>\u56e0\u6b64\u6211\u4eec\u4f7f\u7528<code>Truncated Backpropagation</code>\uff0c \u4e0d\u518d\u4f7f\u7528\u6574\u4e2a\u5e8f\u5217\u8ba1\u7b97\u635f\u5931\uff0c\u800c\u662f\u4f7f\u7528\u5e8f\u5217\u7684\u4e00\u4e2a\u5757\uff08\u5982100\u4e2a\u65f6\u95f4\u6b65\uff09\u7684\u635f\u5931\u503c\uff0c\u7136\u540e\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\u3002</p>"},{"location":"python/cs231n-notebook/chapter6/#\u56fe\u7247\u63cf\u8ff0image-captioning","title":"\u56fe\u7247\u63cf\u8ff0\uff08Image Captioning\uff09","text":"<p>\u6211\u4eec\u628a\u6d4b\u8bd5\u56fe\u50cf\u8f93\u5165\u5230\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7 CNN \u5f97\u5230\u6a21\u578b\u6700\u540e1\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u524d\u76841\u4e2a\u56fe\u50cf\u5411\u91cf\uff0c\u4f5c\u4e3a\u6574\u5f20\u56fe\u50cf\u7684\u5185\u5bb9\u8868\u5f81\u3002</p> <p>\u7136\u540e\u628a\u8fd9\u4e2a\u56fe\u50cf\u5411\u91cf\u4f5c\u4e3a RNN \u7684\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\uff0c\u8f93\u51fa\u662f\u5355\u8bcd\u7684\u5411\u91cf\uff0c\u901a\u8fc7 softmax \u5f97\u5230\u6bcf\u4e2a\u5355\u8bcd\u7684\u6982\u7387\u5206\u5e03\u3002</p> <p></p> <p>\u4e8b\u5b9e\u4e0a\uff0c\u5f53\u8f93\u5165\u56fe\u50cf\u4e0e\u8bad\u7ec3\u96c6\u5dee\u8ddd\u8f83\u5927\u65f6\uff0c\u8be5\u6a21\u578b\u6548\u679c\u8f83\u5dee</p>"},{"location":"python/cs231n-notebook/chapter6/#\u591a\u5c42rnnmultilayer-rnns","title":"\u591a\u5c42RNN\uff08Multilayer RNNs\uff09","text":"<p>Multilayer RNNs\u53ef\u4ee5\u770b\u4f5c\u662f\u591a\u4e2aRNN\u7684\u5806\u53e0\uff0c\u6bcf\u4e00\u5c42RNN\u7684\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u5c42RNN\u7684\u8f93\u5165\u3002</p> <p></p>"},{"location":"python/cs231n-notebook/chapter6/#rnn\u68af\u5ea6\u6d41gradient-flow-in-rnns","title":"RNN\u68af\u5ea6\u6d41(Gradient Flow in RNNs)","text":"\\[     \\begin{aligned}     h_t &amp;= tanh(W_{hh} h_{t-1} + W_{xh}x_t + b_h)  \\\\     &amp;= tanh(         \\begin{pmatrix}         W_{hh} \\quad  W_{xh}         \\end{pmatrix}         \\begin{pmatrix}         h_{t-1}   \\\\         x_t          \\end{pmatrix}     )       \\end{aligned}    \\] <p>\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\uff0c\u4ece\\(h_t\\)\u5230\\(h_{t-1}\\)\u9700\u8981\u4e58\u4ee5\\(W_{hh}^T\\),\u5373\u4ece\u6700\u540e\u4e00\u4e2a\u9690\u72b6\u6001\u4f20\u5230\u7b2c\u4e00\u4e2a\u9690\u72b6\u6001\uff0c\u4e2d\u95f4\u8981\u4e58\u5f88\u591a\u6b21\u540c\u4e00\u4e2a\u6743\u91cd\uff0c\u6240\u4ee5\u68af\u5ea6\u53ef\u80fd\u4f1a\u6d88\u5931\u6216\u8005\u7206\u70b8\u3002</p> <p>\u5e38\u4f7f\u7528\u68af\u5ea6\u622a\u65ad\uff08Gradient clipping\uff09\u6765\u7ed9\u68af\u5ea6\u8bbe\u7f6e\u4e00\u4e2a\u9608\u503c\uff0c\u5982\u679c\u68af\u5ea6\u7684\\(L_2\\)\u8303\u5f0f\u8d85\u8fc7\u8fd9\u4e2a\u9608\u503c\u5c31\u8981\u51cf\u5c0f\u68af\u5ea6\u3002</p>"},{"location":"python/cs231n-notebook/chapter6/#lstmlong-short-term-memory-","title":"LSTM\uff08Long Short Term Memory \uff09","text":"<p>LSTM\uff08\u957f\u77ed\u671f\u8bb0\u5fc6\uff09\u7f51\u7edc\u5c31\u662f\u7528\u6765\u89e3\u51b3\u300c\u68af\u5ea6\u7206\u70b8\u300d\u548c\u300c\u68af\u5ea6\u6d88\u5931\u300d\u95ee\u9898\u7684\uff0c\u4e0e\u5176\u5728\u8f93\u51fa\u4e0a\u9650\u5236\u68af\u5ea6\uff0cLSTM \u7684\u7f51\u7edc\u7ed3\u6784\u66f4\u52a0\u590d\u6742\u3002</p> <p></p> <ul> <li> <p>\\(i\\)\u662f\u8f93\u5165\u95e8\uff08Input gate\uff09 \uff0c\u8868\u793a\u6709\u591a\u5c11\u5185\u5bb9\u88ab\u5199\u5230\u5355\u5143\u72b6\u6001\uff1b      [     \\mathbf{I}t = sigmoid(\\mathbf{X}_t \\mathbf{W}} + \\mathbf{H{t-1} \\mathbf{W}_i)     ]} + \\mathbf{b</p> </li> <li> <p>\\(f\\)\u662f\u9057\u5fd8\u95e8\uff08Forget gate\uff09\uff0c\u8868\u793a\u5bf9\u4e4b\u524d\u7684\u5355\u5143\u72b6\u6001\u7684\u9057\u5fd8\u7a0b\u5ea6\uff1b</p> \\[ \\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)     \\] </li> <li> <p>\\(o\\)\u662f\u8f93\u51fa\u95e8\uff08Output gate\uff09 \uff0c\u8868\u793a\u5355\u5143\u72b6\u6001\u8f93\u51fa\u591a\u5c11\u7ed9\u9690\u72b6\u6001\uff1b</p> \\[ \\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o) \\] </li> <li> <p>\\(g\\)\u662fGate gate ? \uff0c\u63a7\u5236\u5199\u5165\u5230\u5355\u5143\u72b6\u6001\u7684\u4fe1\u606f\u3002</p> \\[ \\tilde{\\mathbf{G}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c) \\] </li> <li> <p>\u5355\u5143\u72b6\u6001\\(\\mathbf{C}_t\\)\uff1a</p> \\[ \\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{G}}_t \\] </li> <li> <p>\u9690\u72b6\u6001\\(\\mathbf{H}_t\\)\uff1a</p> \\[ \\mathbf{H}_t = \\mathbf{O}_t \\odot \\text{tanh}(\\mathbf{C}_t)     \\] </li> </ul> <p></p>"},{"location":"python/cs231n-notebook/chapter6/#lstm\u68af\u5ea6\u6d41","title":"LSTM\u68af\u5ea6\u6d41","text":"<p>\u4ece\\(c_t\\)\u5230\\(c_{t-1}\\)\u7684\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\uff0c\u53ea\u4f1a\u4e0e\\(f\\)\u8fdb\u884c\u9010\u5143\u7d20\u76f8\u4e58\uff0c\u4e0e\u4e58\\(W\\)\u76f8\u6bd4\u8981\u7b80\u5355\u5f88\u591a\u3002</p> <p>LSTM\u4e0d\u540c\u7684\u65f6\u95f4\u6b65\u7684\u9057\u5fd8\u95e8\\(f\\)\u7684\u503c\u90fd\u4e0d\u540c\uff0c\u4e0d\u50cf\u666e\u901a RNN \u6bcf\u6b21\u90fd\u4e58\u76f8\u540c\u7684\u6743\u91cd\u77e9\u9635\\(W\\)\uff0c\u8fd9\u6837\u5c31\u4e00\u5b9a\u7a0b\u5ea6\u907f\u514d\u68af\u5ea6\u7206\u70b8\u6216\u9510\u51cf\u3002</p> <p>\u8bad\u7ec3\u521d\u59cb\u65f6\uff0c\u4e00\u822c\u5c06\\(f\\)\u8bbe\u7f6e\u4e3a1\uff0c\u5373\\(c_t\\)\u4e0e\\(c_{t-1}\\)\u76f8\u540c\uff08\u8bb0\u4f4f\u5168\u90e8\uff09\uff0c\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\uff0c\\(f\\)\u9010\u6e10\u51cf\u5c0f\uff0c\\(c_t\\)\u4e0e\\(c_{t-1}\\)\u9010\u6e10\u4e0d\u540c\u3002</p>"},{"location":"python/cs231n-notebook/chapter7/","title":"\ud83d\udee3Stanford CS231n:Deep Learning for Computer Vision","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttps://cs231n.stanford.edu/</p> <p>2024\u7248PPT: https://cs231n.stanford.edu/slides/2024/</p> <p></p>"},{"location":"python/cs231n-notebook/chapter7/#sequence-to-sequence-with-rnns","title":"Sequence to Sequence with RNNs","text":"<p>\u5de6\u4fa7\u7684RNN\uff08Encoder\uff1a\\(h_t=f_w(x_t,h_{t-1})\\)\uff09\u5c06\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u603b\u7ed3\u62102\u4e2a\u5411\u91cf\uff08\\(s_0\\)\uff0c\\(c\\)\uff09\uff0c\\(s_0\\)\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u521d\u59cb\u72b6\u6001(initial decoder state,\u6216\u8005\u8bbe\u7f6e\u4e3a0)\uff0c\\(c\\)\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\uff08Context vector\uff0ctansfer encoded sequence information to the decoder\uff09\u3002</p> <p>\u53f3\u4fa7\u7684RNN(decoder)\u5c06\u8fd9\u4e2a\u5411\u91cf\u89e3\u7801\u6210\u8f93\u51fa\u5e8f\u5217\u3002</p> <ul> <li> <p>During Training:     \u5728\u8bad\u7ec3\u7f51\u7edc\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6b21\u4e0d\u4f7f\u7528\u4e0a\u4e00\u4e2astate\u7684\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u4e2astate\u7684\u8f93\u5165\uff0c\u800c\u662f\u76f4\u63a5\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u7684\u6807\u51c6\u7b54\u6848(ground truth)\u7684\u5bf9\u5e94\u4e0a\u4e00\u9879\u4f5c\u4e3a\u4e0b\u4e00\u4e2astate\u7684\u8f93\u5165,\u4e0d\u7ba1\u8f93\u51fa\u662f\u5426\u6b63\u786e\uff08teacher-forcing\uff09</p> <p></p> </li> <li> <p>During Test-time:</p> <p>\u6211\u4eec\u4ece\u8f93\u51fa\u4e2d\u8fdb\u884c\u62bd\u6837\uff0c\u76f4\u5230\u62bd\u4e2d<code>[STOP]</code></p> </li> </ul> <p>\u663e\u7136\uff0c\u8fd9\u4e2aSeq2seq\u6a21\u578b\u5e76\u4e0d\u9002\u7528\u4e8e\u957f\u6587\u672c\u4efb\u52a1 \uff0c\u56e0\u4e3a\u5982\u679c\u8f93\u5165\u5e8f\u5217\u8fc7\u957f\uff0c\u57fa\u4e8eRNN\u7684\u7f16\u7801\u5668\u6ca1\u6709\u80fd\u529b\u53bb\u6355\u6349\u8db3\u591f\u7684\u4fe1\u606f\uff0c\u5bfc\u81f4\u89e3\u7801\u5668\u65e0\u6cd5\u751f\u6210\u51c6\u786e\u7684\u8f93\u51fa\u3002\u5e76\u4e14\uff0c\u5e0c\u671b\u7528\u5355\u4e00\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\\(c\\)\u53bb\u603b\u7ed3\u6574\u4e2a\u957f\u5e8f\u5217\u4fe1\u606f\uff0c\u663e\u7136\u662f\u4e0d\u73b0\u5b9e\u7684\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u60f3\u8c61\u4e00\u79cd\u7b97\u6cd5\uff0c\u4e0d\u662f\u4f7f\u7528\u5355\u4e2a\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\\(c\\)\uff0c \u800c\u662f\u5728decoder\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4e2d\u8ba1\u7b97\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5411\u91cf\uff0c\u5373\u7ed9\u4e88decoder\u4e13\u6ce8\u4e8e\u8f93\u5165\u5e8f\u5217\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u9009\u62e9\u6216\u8005\u91cd\u5efa\u4e00\u4e2a\u65b0\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u7684\u80fd\u529b\u3002</p> <p></p> <p>\u5982\u4e0a\uff0c\u6211\u4eec\u7f16\u5199\u4e00\u4e2a\u5bf9\u9f50\u51fd\u6570\\(f_{att}\\)\uff08alignment function\uff0c\u901a\u5e38\u4e3aMLPs\uff09\uff0c\u5c06Encoder\u7684\u9690\u85cf\u72b6\u6001\u4e0e\\(s_0\\)\u8f93\u5165\u5f97\u5230alignment scores\uff08how much should we attend to each hidden state of encoder\uff09\uff0c\u7136\u540e\u4f7f\u7528softmax\u51fd\u6570\u5f52\u4e00\u5316\u5f97\u5230\u6743\u91cd\\(a_{t,i}\\)\uff08attention weights\uff09\u3002</p> <p>\u5f97\u5230\u6743\u91cd\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u52a0\u6743\u6c42\u548c\u5f97\u5230\u4e0a\u4e0b\u6587\u5411\u91cf\\(c_t\\)\uff0c\u5373\uff1a $$ c_t=\\sum_{i=1}^{T_x}a_{t,i}h_i $$</p> <p>\u5176\u4e2d\uff0c\\(a_{t,i}\\)\u8868\u793a\u7b2c\\(t\\)\u4e2adecoder\u7684\u9690\u85cf\u72b6\u6001\u5bf9\u7b2c\\(i\\)\u4e2aencoder\u7684\u9690\u85cf\u72b6\u6001\\(h_i\\)\u7684\u6ce8\u610f\u529b\u6743\u91cd\u3002</p> <p></p> <p>\u63a5\u4e0b\u6765\u91cd\u590d\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u5c06\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u72b6\u6001\\(s_1\\)\u4e0eEncoder\u7684\u5404\u4e2a\\(h_t\\)\u8f93\u5165\\(f_{att}\\),\u5f97\u5230\\(c_2\\)\uff0c\u4ee5\u6b64\u7c7b\u63a8\u3002</p> <p>\u56e0\u6b64\uff1a</p> <ul> <li> <p>\u8f93\u5165\u5e8f\u5217\u7684\u4fe1\u606f\u4f20\u9012\u4e0d\u4f1a\u53d7\u5355\u4e00\u4e0a\u4e0b\u6587\u5411\u91cf\u7684\u963b\u788d</p> </li> <li> <p>Decoder\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u90fd\u80fd\u591f\u201c\u67e5\u770b\u201d\u8f93\u5165\u5e8f\u5217\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u4ece\u800c\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\u5e8f\u5217\u3002</p> </li> </ul> <p>\u5bf9\u4e8e\u8ba1\u7b97\u5f97\u5230\u7684\u6982\u7387\u5206\u5e03\\(a_{t,i}\\)\u8fdb\u884c\u77e9\u9635\u53ef\u89c6\u5316\uff0c\u53ef\u4ee5\u770b\u5230decoder\u8f93\u51fa\u7684\u6bcf\u4e2a\u5355\u8bcd\u5173\u6ce8\u4e86\u8f93\u5165\u5e8f\u5217\u7684\u4e0d\u540c\u90e8\u5206\uff1a</p> <p></p> <p>\u6211\u4eec\u5c06\u4e24\u79cd\u8bed\u8a00\u7684\u5355\u8bcd\u8fdb\u884c\u5bf9\u5e94\uff0c\u53ef\u4ee5\u53d1\u73b0attention\u673a\u5236\u5f88\u597d\u5730\u6355\u6349\u5230\u4e86\u4e24\u79cd\u8bed\u8a00\u4e2d\u540c\u4e49\u5355\u8bcd\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff1a</p> <p></p> <p>\u4e8b\u5b9e\u4e0a\uff0cattention\u673a\u5236\u5e76\u4e0d\u5173\u5fc3\u8f93\u5165\u662f\u5426\u662f\u4e00\u4e2a\u987a\u5e8f\u5e8f\u5217\uff08ordered sequence \uff09\uff0c\u800c\u662f\u5bf9\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\u8fdb\u884c\u201c\u6ce8\u610f\u201d\u3002</p>"},{"location":"python/cs231n-notebook/chapter7/#image-captioning-with-rnns-and-attention","title":"Image Captioning with RNNs and Attention","text":""},{"location":"python/cs231n-notebook/chapter7/#general-attention-layer","title":"General Attention Layer","text":"<p>\u6211\u4eec\u5148\u5bf9image captioning\u4e2d\u7684attention\u673a\u5236\u8fdb\u884c\u603b\u7ed3\uff1a</p> <p>Input:</p> <ul> <li> <p>Features: \\(\\mathbf{z}\\) (shape\uff1aH x W x D)</p> </li> <li> <p>Query\uff1a\\(\\mathbf{h}\\) (shape\uff1aD)</p> </li> </ul> <p>\\(D\\)\u8868\u793a\u7279\u5f81\u56fe\u6570</p> <p>Operations:</p> <ul> <li> <p>Alignment func: \\(e_{i,j} = f_{att}(\\mathbf{z}_{i,j}, \\mathbf{h})\\) (shape: H x W)</p> </li> <li> <p>Attention weights: \\(\\mathbf{a} = softmax(\\mathbf{e})\\) (shape: H x W)</p> </li> </ul> <p>Outputs:</p> <ul> <li>Context vector: \\(c = \\sum_{i=1}^{H}\\sum_{j=1}^{W}a_{i,j}\\mathbf{z}_{i,j}\\) (shape: D)</li> </ul> <p></p> <p>\u524d\u9762\u6211\u4eec\u63d0\u53ca\u5230\uff0cattention\u673a\u5236\u4e0d\u5173\u6ce8\u8f93\u5165\u6570\u636e\u7684\u987a\u5e8f\uff0c\u56e0\u6b64\u6211\u4eec\u5c06input vectors\u62c9\u4f38\u6210\\(\\mathbf{x}\\)\uff08shape: N x D\uff09\uff0c\u5176\u4e2d\\(N = H \\times W\\)\u3002</p> <p>\u7406\u89e3\uff1a\u5c06\\(H \\times W\\)\u5c55\u5f00\u6210\\(N\\)\uff0c\u5373\u8f93\u5165\u7684\u4fe1\u606f\u5171\u6709\\(N\\)\u4e2a\u5411\u91cf\uff0c\u6bcf\u4e2a\u5411\u91cf\u7684\u7ef4\u5ea6\u4e3a\\(D\\)\u3002\u5982\u679c\u662f\u56fe\u50cf\u7684\u8bdd\uff0c\\(N\\)\u4e2a\u5411\u91cf\u7684\u5176\u4e2d\u4e00\u4e2a\u5bf9\u5e94\u539f\u56fe\u7247\u7684\u67d0\u4e00\u5757\uff08\u611f\u53d7\u91ce\uff09\uff1b\u5982\u679c\u662f\u6587\u672c\u5e8f\u5217\u7684\u8bdd\uff0c\\(N\\)\u4e2a\u5411\u91cf\u7684\u5176\u4e2d\u4e00\u4e2a\u5bf9\u5e94\u6587\u672c\u5e8f\u5217\u4e2d\u7684\u67d0\u4e2a\u8f93\u5165\u8bed\u53e5/\u5355\u8bcd\u3002\u5982\u4e0b\u56fe\uff0c\u8f93\u5165\u7684\u662f200\u4e2a\u5e8f\u5217\uff0c\u6bcf\u4e2a\u5e8f\u5217\u957f\u5ea6\u4e3a800</p> <p>\u5bf9\u4e8e\\(f_{att}\\)\u51fd\u6570\uff0c\u6211\u4eec\u5c06\u5176\u5b9a\u4e49\u4e3a\u70b9\u79ef\u64cd\u4f5c(dot product)\uff0c\u5373\uff1a $$ e_i = h \\cdot x_i $$</p> <p>\u4e5f\u53ef\u4ee5\u4f7f\u7528\u7f29\u653e\u70b9\u79ef(scaled dot product)\uff1a</p> \\[ e_i = \\frac{h \\cdot x_i}{\\sqrt{D}} \\] <p>\u6539\u7528scaled dot product\u7684\u7406\u7531\uff1a</p> <p>We suspect that for large values of \\(d_k\\), the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely samll gradients.</p> <p>\u5f53\u8f93\u5165\u4fe1\u606f\u7684\u7ef4\u6570\\(d\\)\u5f88\u5927\u65f6\uff0c\u70b9\u79ef\u6240\u5f97\uff08dot product\uff09\u7684\u503c\u7531\u5f88\u591a\u9879\u76f8\u52a0\u800c\u6210\uff0c\u901a\u5e38\u4f1a\u6709\u6bd4\u8f83\u5927\u7684\u65b9\u5dee\u3002</p> <p>\u5047\u8bbe\u4e0a\u8ff0\u7684\\(h\\)\u4e0e\\(x\\)(\u4e0a\u6587\u7684\\(x_i\\)\u5411\u91cf)\u76f8\u4e92\u72ec\u7acb\u4e14\u5747\u503c\u4e3a0\uff0c\u65b9\u5dee\u4e3a1</p> \\[ \\mathbf{E} [h_i] = \\mathbf{E} [x_i] = 0  \\] \\[ \\mathbf{Var} [h_i] = \\mathbf{E} [h_i^2] - (\\mathbf{E} (h_i))^2 = \\mathbf{E} [h_i^2] = 1\\\\ \\] \\[ \\mathbf{Var} [x_i] = \\mathbf{E} [x_i^2] - (\\mathbf{E} (x_i))^2 = \\mathbf{E} [x_i^2] = 1 \\] <p>\u56e0\u4e3a\\(h_i\\)\u4e0e\\(x_i\\)\u76f8\u4e92\u72ec\u7acb\uff0c\u6240\u4ee5\uff1a</p> \\[ \\mathbf{Cov}(h_i,x_i) = \\mathbf{E} [ (h_i-\\mathbf{E} [h_i]) (x_i-\\mathbf{E} [x_i]) ]  \\] \\[ = \\mathbf{E}[h_i x_i] - \\mathbf{E}[h_i] \\mathbf{E}[x_i]= 0 \\] <p>\u56e0\u6b64\uff1a\\(\\mathbf{E}[h_i x_i] = \\mathbf{E}[h_i] \\mathbf{E}[x_i]= 0\\)</p> <p>\u53ef\u5f97\uff1a</p> \\[ \\mathbf{Var} (h_i \\cdot x_i) = \\mathbf{E} [(h_i \\cdot x_i)^2] - (\\mathbf{E} [h_i \\cdot x_i])^2  \\] \\[ = \\mathbf{E} [(h_i \\cdot x_i)^2] = \\mathbf{E} [h_i^2] \\mathbf{E} [x_i^2]\\\\ \\] \\[ = \\mathbf{Var}(h_i) \\mathbf{Var}(x_i) = 1 \\] <p>\u7efc\u4e0a\uff1a</p> \\[ \\mathbf{Var}(h \\cdot x) = \\sum_{i=1}^D  \\mathbf{Var} (h_i \\cdot x_i) = D \\] <p>\u56e0\u6b64\uff0c\u5f53\\(d\\)\u5f88\u5927\u65f6\uff0c\\(h \\cdot x\\)\u65b9\u5dee\u7684\u503c\u4e5f\u4f1a\u53d8\u5927</p> <p>\u800c\u5bf9\u4e8esoftmax\u51fd\u6570\uff0c\u6709\uff1a</p> \\[ Softmax(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^N e^{x_j}} \\\\ \\] \\[ \\frac{\\partial S(x_i)}{\\partial x_i} = Softmax(x_i) (1 - Softmax(x_i)) \\] <p>\u56e0\u6b64\uff0c\u5f53\\(d\\)\u5f88\u5927\u65f6\uff0c\u5f97\u5230\u7684\\(x_i\\)\u53ef\u80fd\u51fa\u73b0\u6781\u5927/\u6781\u5c0f\u7684\u60c5\u51b5\uff0c\u5bfc\u81f4\u8ba1\u7b97\u7684\u68af\u5ea6\u503c\u4f1a\u8d8b\u8fd1\u4e8e0\uff0c\u5f15\u8d77\u68af\u5ea6\u6d88\u5931\u3002</p> <p>\u82e5\u4f7f\u7528\u7f29\u653e\u70b9\u79ef(scaled dot product)\uff0c\u5219\u53ef\u4ee5\u4f7f\u5f97\u65b9\u5dee\u7f29\u5c0f\u4e3a1\uff1a</p> \\[ \\mathbf{Var} (\\frac{h \\cdot x}{\\sqrt{D}}) = \\frac{1}{D} \\mathbf{Var}(h \\cdot x) = \\frac{1}{D} \\times D = 1 \\] <p>\u8fd9\u65f6\uff0csoftmax \u51fd\u6570\u7684\u68af\u5ea6\u5c31\u4e0d\u5bb9\u6613\u8d8b\u8fd1\u4e8e\u96f6\u4e86\uff0c\u56e0\u6b64\u4f7f\u7528\u7f29\u653e\u70b9\u79ef(scaled dot product)\u53ef\u4ee5\u907f\u514d\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898\u3002</p> <p></p> <p></p> <p>\u5b9e\u9645\u4e0a\uff0cDecoder\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u90fd\u5bf9\u5e94\u4e00\u4e2aquery vectort\uff08\u6ce8\u610f\u529b\u4e0d\u540c\uff09\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u5c06\u62d3\u5c55\u4e3a\\(\\mathbf{q}\\)\uff08shape\uff1aM x D\uff09</p> <p>\\(\\mathbf{e} = \\mathbf{q} \\mathbf{Z^T}\\)\uff08shape\uff1aM x N\uff09</p> <p>\u5bf9\u5e94\u7684\uff0c\\(\\mathbf{a} = Softmax(\\mathbf{e},dim=1)\\) \uff08shape\uff1aM x N\uff09</p> <p>shape\uff1aM x N\uff0c\u5373\u4e00\u5171M\u4e2aquery vector\u4ea7\u751f\u7684\u6743\u91cd\u5411\u91cf\\(\\mathbf{a_j}\uff0cj=1,2,..,M\\)\uff0c\u6bcf\u4e2a\u6743\u91cd\u5411\u91cf\u4e2d\u6709N\u4e2a\u6743\u91cd\uff08\u5bf9\u8f93\u5165\u7684N\u4e2a\u4fe1\u606f\u7684\u4e0d\u540c\u6ce8\u610f\u529b\uff09\\(a_{i,j}\uff0ci = 1,2,...,N\\)</p> <p>Output context vectors \uff1a\\(Y = \\mathbf{a} \\mathbf{X}\\) \uff08shape\uff1aM x D\uff09\uff0c\\(y_i = \\sum_j a_{i,j} x_j\\)\uff08\u8f93\u5165\u5411\u91cf\u7684\u52a0\u6743\u7ec4\u5408\uff09</p> <p>\u56de\u987e\u4e0a\u8ff0\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528query vector\u4e0einput vector\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u7136\u540e\u4f7f\u7528\u6ce8\u610f\u529b\u6743\u91cd\u5bf9input vector\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u5f97\u5230Output context vectors\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5728\u4e24\u4e2a\u4e0d\u540c\u529f\u80fd\u4e0a\u4f7f\u7528\u4e86input vector\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0\u4e0d\u540c\u7684FC\u5c42\u6765\u4eceinput vector\u4e2d\u5f97\u5230key vector\u4e0evalue vector\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u590d\u6742\uff08add more expressivity\uff09\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002</p> <ul> <li> <p>key vector: \\(k = xW_k\\)\uff0c\uff08shape of \\(W_k\uff1aD \\times D_k\\)\uff09\uff08shape of \\(k\uff1aN \\times D_k\\)\uff09</p> </li> <li> <p>value vector: \\(v = xW_v\\)\uff0c\uff08shape of \\(W_v\uff1aD \\times D_v\\)\uff09\uff08shape of \\(v\uff1aN \\times D_v\\)\uff09</p> </li> </ul> <p>\u76f8\u5e94\u7684\uff0cquery vectors\uff1a\\(\\mathbf{q}\\)\u7684shape\u4e3a\uff1a\\(M \\times D_k\\)</p> <p>\\(\\mathbf{e} = \\mathbf{q} k^T\\)\uff0c\uff08shape of \\(e\uff1aM \\times N\\)\uff09, </p> <p>\\(e_{i,j} = \\mathbf{q_i} k_j^T / \\sqrt{D_k}\\) (k\u7684\u7279\u5f81\u6570\u4e3a\\(D_k\\))</p> <p>\\(Y = \\mathbf{a} v\\)\uff0c\uff08shape of \\(e\uff1aM \\times D_v\\)\uff09</p> <p>\\(y_j = \\sum_i a_{i,j} v_i\uff0c j=1,2,...,M\\)</p> <p>\u5f15\u5165\u4e86key vector\u4e0evalue vector\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u6539\u53d8\u8f93\u51fa\u7684\u7ef4\u5ea6\u4e86\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u66f4\u52a0\u7075\u6d3b\u3002</p> <p></p>"},{"location":"python/cs231n-notebook/chapter7/#self-attention-layer","title":"Self-attention Layer","text":"<p>\u4e8b\u5b9e\u4e0a\uff0c\u6211\u4eec\u53ef\u4ee5\u4eceinput vectors\u8ba1\u7b97\u51faquery vectors\uff0c\u4ece\u800c\u5b9a\u4e49\u4e00\u4e2a\u201c\u81ea\u6ce8\u610f\u529b\u201d\u5c42\u3002(Self-attention)</p> <p>\u901a\u8fc7FC\u5c42\uff0c\u6211\u4eec\u4eceinput vectors\u8ba1\u7b97\u51faquery vectors:</p> <ul> <li>Query vectors: \\(\\mathbf{q} = xW_q\\)\uff0c\uff08shape of \\(W_q\uff1aD \\times D_k\\)\uff09\uff08shape of \\(q\uff1aN \\times D_k\\)\uff09</li> </ul> <p></p> <p>\u7531\u4e8eattention\u673a\u5236\u5e76\u4e0d\u5173\u5fc3\u8f93\u5165\u7684\u987a\u5e8f\uff0c\u5373\u62e5\u6709\u201c\u7f6e\u6362\u7b49\u53d8\u201d\uff08 Permutation equivariant\uff09\u7684\u7279\u6027\uff0c\u5018\u82e5\u66f4\u6362\u8f93\u5165\u5411\u91cf\u7684\u6b21\u5e8f\uff0c\u53ea\u662f\u4f1a\u6539\u53d8\u8f93\u51fa\u7684\u987a\u5e8f\uff0c\u800c\u4e0d\u4f1a\u6539\u53d8\u8f93\u51fa\u7684\u5185\u5bb9\u3002\u4f46\u663e\u7136\uff0c\u8f93\u5165\u4fe1\u606f\u7684\u524d\u540e\u987a\u5e8f\u5bf9\u8bed\u4e49\u5f71\u54cd\u6781\u5927\u3002</p> <p></p> <p></p>"},{"location":"python/cs231n-notebook/chapter7/#positional-encoding","title":"Positional encoding","text":"<p>\u4e3a\u4e86\u5177\u6709\u4f4d\u7f6e\u611f\u77e5\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u8f93\u5165\u4e0e\u4f4d\u7f6e\u7f16\u7801\u8fde\u63a5\u8d77\u6765</p> <p>\u800cPositional Encoding\uff08\u4f4d\u7f6e\u7f16\u7801\uff09\u6280\u672f\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u5355\u8bcd\u6dfb\u52a0\u4e00\u4e2a\u989d\u5916\u7684\u7f16\u7801\u6765\u8868\u793a\u5b83\u5728\u5e8f\u5217\u4e2d\u7684\u4f4d\u7f6e\uff0c\u8fd9\u6837\u6a21\u578b\u5c31\u80fd\u591f\u7406\u89e3\u5355\u8bcd\u5728\u5e8f\u5217\u4e2d\u7684\u76f8\u5bf9\u4f4d\u7f6e\u3002</p>"},{"location":"python/cs231n-notebook/chapter7/#masked-self-attention-layer\u63a9\u7801\u81ea\u6ce8\u610f\u529b","title":"Masked self-attention layer(\u63a9\u7801\u81ea\u6ce8\u610f\u529b)","text":""},{"location":"python/cs231n-notebook/chapter7/#multi-head-self-attention-layer","title":"Multi-head self-attention layer","text":""},{"location":"python/cs231n-notebook/chapter7/#image-captioning-using-transformers","title":"Image Captioning using Transformers","text":""},{"location":"python/cs231n-notebook/chapter7/#the-transformer-encoder-block","title":"The Transformer encoder block","text":""},{"location":"python/cs231n-notebook/chapter7/#the-transformer-decoder-block","title":"The Transformer decoder block","text":""},{"location":"python/cs231n-notebook/chapter8/","title":"\ud83d\udee3Stanford CS231n:Deep Learning for Computer Vision","text":"<p>\u60f3\u8bf4\u7684\u8bdd\ud83c\udf87</p> <p> <p>\ud83d\udd1d\u8bfe\u7a0b\u7f51\u7ad9\uff1ahttps://cs231n.stanford.edu/</p> <p>2024\u7248PPT: https://cs231n.stanford.edu/slides/2024/</p> <p></p>"},{"location":"python/cs231n-notebook/chapter8/#object-detection","title":"Object Detection","text":"<p>\u76ee\u6807\u68c0\u6d4b\u7684\u57fa\u672c\u601d\u8def\u662f\uff1a\u89e3\u51b3\u5b9a\u4f4d\uff08localization\uff09 + \u8bc6\u522b\uff08Recognition\uff09 </p> <p></p> <p></p>"},{"location":"python/cs231n-notebook/chapter8/#r-cnnregion-based-convolutional-neural-networks","title":"R-CNN\uff1aRegion-based Convolutional Neural Networks","text":"<p>R-CNN\u6838\u5fc3\u601d\u60f3\uff1a \u5bf9\u6bcf\u5f20\u56fe\u7247\u9009\u53d6\u591a\u4e2a\u533a\u57df(Region Proposal)\uff0c\u7136\u540e\u6bcf\u4e2a\u533a\u57df\u4f5c\u4e3a\u4e00\u4e2a\u6837\u672c\u8fdb\u5165\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u62bd\u53d6\u7279\u5f81\u3002</p> <ul> <li> <p>\u5229\u7528\u9009\u62e9\u6027\u641c\u7d22 Selective Search \u7b97\u6cd5\u5728\u56fe\u50cf\u4e2d\u4ece\u4e0b\u5230\u4e0a\u63d0\u53d62000\u4e2a\u5de6\u53f3\u7684\u53ef\u80fd\u5305\u542b\u7269\u4f53\u7684\u5019\u9009\u533a\u57df (Region Proposal)</p> </li> <li> <p>\u56e0\u4e3a\u83b7\u53d6\u5230\u7684\u5019\u9009\u533a\u57df\u5927\u5c0f\u5404\u4e0d\u76f8\u540c\uff0c\u6240\u4ee5\u9700\u8981\u5c06\u6bcf\u4e2a Region Proposal \u7f29\u653e(warp)\u6210\u7edf\u4e00\u7684227x227\u7684\u5927\u5c0f\u5e76\u8f93\u5165\u5230CNN\uff0c\u5c06CNN\u7684 fc7 \u5c42\u7684\u8f93\u51fa\u4f5c\u4e3a\u7279\u5f81</p> </li> <li> <p>\u5c06\u6bcf\u4e2a Region Proposal \u63d0\u53d6\u5230\u7684CNN\u7279\u5f81\u8f93\u5165\u5230SVM\u8fdb\u884c\u5206\u7c7b</p> </li> <li> <p>\u4f7f\u7528\u8fd9\u4e9b\u533a\u57df\u7279\u5f81\u6765\u8bad\u7ec3\u7ebf\u6027\u56de\u5f52\u5668\u5bf9\u533a\u57df\u4f4d\u7f6e\u8fdb\u884c\u8c03\u6574</p> </li> </ul> <p></p> <p>\u663e\u7136\uff0cRCNN\u8bad\u7ec3\u8ba1\u7b97\u91cf\u5f88\u5927\uff0c\u5e76\u4e14Selective search\u63d0\u53d6\u7684\u533a\u57df\u8d28\u91cf\u4e0d\u591f\u597d\uff0c\u7279\u5f81\u63d0\u53d6\u4e0e\u540e\u7eedSVM\u5206\u7c7b\u5668\u662f\u72ec\u7acb\u8bad\u7ec3\u7684\uff0c\u6ca1\u6709\u8054\u5408\u4f18\u5316\uff0c\u4e14\u8bad\u7ec3\u8017\u65f6\u957f\u3002</p>"},{"location":"python/d2l-chapter/Pytorch_backward/","title":"\ud83d\udd17Chapter 0\uff1aPyTorch\u53cd\u5411\u4f20\u64ad\u7279\u6027","text":"<p>Info</p> <p> pytorch\u4e2d\u7684\u4e00\u4e9b\u91cd\u8981\u673a\u5236\u7684\u5b66\u4e60 <p>\u4e2d\u6587\u6587\u6863:https://pytorch-cn.readthedocs.io/zh/latest/</p> <p></p>"},{"location":"python/d2l-chapter/chapter1/","title":"\ud83d\udd17Chapter 1\uff1alinear regression and multilayer perceptron","text":""},{"location":"python/d2l-chapter/chapter1/#\u7ebf\u6027\u56de\u5f52","title":"\u7ebf\u6027\u56de\u5f52","text":""},{"location":"python/d2l-chapter/chapter1/#\u7ebf\u6027\u56de\u5f52\u7684\u8868\u793a","title":"\u7ebf\u6027\u56de\u5f52\u7684\u8868\u793a","text":"<p>\u7ebf\u6027\u56de\u5f52(linear regression)\u57fa\u4e8e\u51e0\u4e2a\u7b80\u5355\u5047\u8bbe\uff1a</p> <ul> <li> <p>\u81ea\u53d8\u91cf\\(\\mathbf{x}\\)\u4e0e\u56e0\u53d8\u91cf\\(y\\)\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\u7ebf\u6027\u7684\uff0c\u5373\\(y\\)\u53ef\u4ee5\u8868\u793a\u4e3a\\(\\mathbf{x}\\)\u4e2d\u5143\u7d20\u7684\u52a0\u6743\u548c:</p> \\[     \\hat{y} = \\mathbf{w}^\\intercal \\mathbf{x} + b          \\] <p>\u5bf9\u4e8e\u591a\u7ef4\u6570\u636e\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\)\uff0c\u5373n\u4e2a\u6837\u672c\uff0cd\u79cd\u7279\u5f81\uff1a</p> \\[     {\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b \\] </li> <li> <p>\u5141\u8bb8\u89c2\u6d4b\u503c\u4e2d\u5b58\u5728\u566a\u58f0\uff0c\u5047\u8bbe\u4efb\u4f55\u566a\u58f0\u90fd\u6bd4\u8f83\u6b63\u5e38\uff0c\u5982\u566a\u58f0\u9075\u5faa\u6b63\u6001\u5206\u5e03\u3002</p> </li> </ul>"},{"location":"python/d2l-chapter/chapter1/#\u635f\u5931\u51fd\u6570","title":"\u635f\u5931\u51fd\u6570","text":"<p>\u5bf9\u4e8e\u771f\u5b9e\u6837\u672c\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u7684\\(\\mathbf{x}\\)\u4e0e\\(y\\)\u4e4b\u95f4\u4e0d\u53ef\u80fd\u5177\u6709\u6807\u51c6\u7684\u7ebf\u6027\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u786e\u5b9a\u4e00\u4e2a\u62df\u5408\u7a0b\u5ea6\u7684\u5ea6\u91cf,\u5373\u635f\u5931\u51fd\u6570\uff08loss function\uff09\uff0c\u6765\u91cf\u5316\u76ee\u6807\u7684\u5b9e\u9645\u503c\u4e0e\u9884\u6d4b\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u5bf9\u4e8e\u56de\u5f52\u95ee\u9898\u901a\u5e38\u4f7f\u7528\u5e73\u65b9\u8bef\u5dee\u51fd\u6570\uff08MSE\uff09\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff1a</p> \\[     L(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 =\\frac{1}{n} \\sum_{i=1}^n \\left(\\mathbf{w}^\\intercal \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2 \\] <p>\u5373\u6211\u4eec\u5e0c\u671b\u5bfb\u627e\u5230\u6700\u4f18\u7684\u6743\u91cd\u4e0e\u504f\u7f6e\\((\\mathbf{w}^*, b^*) = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b)\\)\uff0c\u6765\u4f7f\u5f97\u603b\u635f\u5931\u6700\u5c0f(\u4e0e\u771f\u5b9e\u503c\u7684\u5dee\u8ddd\u5c0f)\u3002</p>"},{"location":"python/d2l-chapter/chapter1/#\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6cd5","title":"\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6cd5","text":"<p>\u68af\u5ea6\u4e0b\u964d\u6cd5\uff08gradient descent\uff09\u5728\u635f\u5931\u51fd\u6570\u51cf\u5c0f\u7684\u65b9\u5411\u4e0a\u66f4\u65b0\u53c2\u6570\u6765\u964d\u4f4e\u8bef\u5dee\uff1a</p> <p></p> \\[     (\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\eta \\sum_{i =1}^n \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).    \\] <p>\u56e0\u4e3a\u4f20\u7edf\u7684\u68af\u5ea6\u4e0b\u964d\u6cd5\u9700\u8981\u904d\u5386\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u5728\u5b9e\u9645\u7684\u6267\u884c\u4e2d\u53ef\u80fd\u4f1a\u8f83\u6162\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728\u6bcf\u4e00\u6b21\u66f4\u65b0\u6743\u91cd\u65f6\u968f\u673a\u62bd\u53d6\u4e00\u5c0f\u6279\u6837\u672c\u6765\u8ba1\u7b97\u66f4\u65b0\uff0c\u8fd9\u79cd\u53d8\u4f53\u4e3a\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6cd5(minibatch stochastic gradient descent),\u5047\u8bbe\u4ece\u6570\u636e\u96c6\u4e2d\u968f\u673a\u62bd\u53d6\u4e00\u4e2a\u5c0f\u6279\u91cf\\(\\mathcal{B}\\)\uff1a</p> \\[ \\begin{split} \\begin{aligned}  \\mathbf{w} &amp;\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),    \\\\  b &amp;\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right).  \\\\ \\end{aligned} \\end{split} \\] <p>\u5176\u4e2d\\(|\\mathcal{B}|\\)\u4e3a\u8868\u793a\u6bcf\u4e2a\u5c0f\u6279\u91cf\u4e2d\u7684\u6837\u672c\u6570\uff0c\u5373\u6279\u91cf\u5927\u5c0f\uff08batch size\uff09</p> <p>\u5373\u4f7f\u6570\u636e\u96c6\u662f\u5b8c\u7f8e\u7b26\u5408\u7ebf\u6027\u4e14\u65e0\u566a\u58f0\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u6cd5\u5f97\u5230\u7684\u4f30\u8ba1\u503c\u4e5f\u4e0d\u4f1a\u4f7f\u635f\u5931\u51fd\u6570\u771f\u6b63\u5730\u8fbe\u5230\u6700\u5c0f\u503c\uff1a\u56e0\u4e3a\u7b97\u6cd5\u4f1a\u4f7f\u5f97\u635f\u5931\u5411\u6700\u5c0f\u503c\u7f13\u6162\u6536\u655b\uff0c\u4f46\u5374\u4e0d\u80fd\u5728\u6709\u9650\u7684\u6b65\u6570\u5185\u975e\u5e38\u7cbe\u786e\u5730\u8fbe\u5230\u6700\u5c0f\u503c\u3002</p>"},{"location":"python/d2l-chapter/chapter1/#\u7ebf\u6027\u56de\u5f52\u7684\u89e3\u6790\u89e3","title":"\u7ebf\u6027\u56de\u5f52\u7684\u89e3\u6790\u89e3","text":"<p>\u5bf9\u4e8e\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u6700\u5c0f\u5316\\(\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2\\)\uff0c\u5bf9\u4e8e\\({\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b\\)\uff1a</p> \\[     \\begin{aligned}     \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 =&amp; (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\intercal (\\mathbf{y} - \\mathbf{X}\\mathbf{w}) =  (\\mathbf{y}^\\intercal - \\mathbf{w}^\\intercal \\mathbf{X}^\\intercal) (\\mathbf{y} - \\mathbf{X}\\mathbf{w}) \\\\     =&amp; \\mathbf{y}^\\intercal \\mathbf{y} + \\mathbf{w}^\\intercal \\mathbf{X}^\\intercal \\mathbf{X}\\mathbf{w} - \\mathbf{y}^\\intercal \\mathbf{X}\\mathbf{w} - \\mathbf{w}^\\intercal \\mathbf{X}^\\intercal \\mathbf{y} \\\\     =&amp; \\mathbf{y}^\\intercal \\mathbf{y} + \\mathbf{w}^\\intercal \\mathbf{X}^\\intercal \\mathbf{X}\\mathbf{w} - 2 \\mathbf{y}^\\intercal \\mathbf{X}\\mathbf{w} \\\\     \\end{aligned}       \\] <p>\u4e3a\u6c42\u6781\u503c\uff0c\u4f7f\\(\\nabla_{\\mathbf{w}} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 = 0\\),\u6709\uff1a</p> \\[     \\nabla_{\\mathbf{w}} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 = 2 \\mathbf{X}^\\intercal \\mathbf{X}\\mathbf{w} - 2 \\mathbf{X}^\\intercal \\mathbf{y}     \\] <p>\u53ef\u5f97\uff1a</p> \\[     \\mathbf{w}^* = (\\mathbf X^\\intercal \\mathbf X)^{-1}\\mathbf X^\\intercal \\mathbf{y}     \\]"},{"location":"python/d2l-chapter/chapter1/#\u7ebf\u6027\u56de\u5f52\u7684\u57fa\u7840\u5b9e\u73b0","title":"\u7ebf\u6027\u56de\u5f52\u7684\u57fa\u7840\u5b9e\u73b0","text":"<p>\u4e3a\u4e86\u5b9e\u73b0\u4e00\u4e2a\u5b8c\u6574\u7684\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u6211\u4eec\u9700\u8981\u751f\u6210\u6570\u636e\u3001\u6784\u5efa\u6a21\u578b\u3001\u635f\u5931\u51fd\u6570\u548c\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\uff0c\u672c\u8282\u6765\u5b9e\u73b0\u4e00\u4e2a\u6700\u57fa\u7840\u7684\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff1a</p> <ul> <li>\u751f\u6210\u4e00\u4e2a\u5e26\u566a\u58f0\u7684\u6570\u636e\u96c6</li> </ul> <pre><code>def CreatData(features, num_examples, w, b):\n    X = torch.normal(0, 1, size = (num_examples, features))\n    y = torch.matmul(w, X.T) + b + torch.normal(0, 0.1, size=(1, num_examples))\n    return X, y.reshape((-1,1))\n</code></pre> <ul> <li>\u6784\u5efa\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u4ee5\u53ca\u635f\u5931\u51fd\u6570</li> </ul> <pre><code>def LinearRegression(X, w, b):\n    y_hat = torch.matmul(w, X.T) + b \n    return y_hat.reshape(-1, 1) \n\ndef MSELossfunction(y_hat, y):\n    return (y - y_hat) ** 2\n</code></pre> <ul> <li>\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d</li> </ul> <pre><code>def Dataiter_RandomBatch(batch_size, features, labels):   # \u8fed\u4ee3\u5668\n    num = len(features)\n    numlist = [i for i in range(0, num)]\n    random.shuffle(numlist)\n    for k in range(0, num, batch_size):\n        RandomBatch = numlist[k:min(k + batch_size, num)]\n        yield features[RandomBatch], labels[RandomBatch]\n\ndef SGD(params, alpha, batch_size):\n    with torch.no_grad():\n        for param in params:\n            param -= alpha * param.grad / batch_size\n            param.grad.zero_()\n</code></pre> <ul> <li>\u7a0b\u5e8f\u4e3b\u4f53</li> </ul> <pre><code>w_0 = torch.tensor([1, 3, 2, 4, 5, 6], dtype=torch.float)\nb_0 = 5.5\n\nnum_examples = 100 \nfeatures = len(w_0)\n\nX,y = CreatData(features, num_examples, w_0, b_0)\n\nw = torch.normal(mean = 0, std = 1, size = w_0.shape, requires_grad=True)\nb = torch.zeros(1, requires_grad=True)\n\nnum_epochs = 50\nalpha = 0.01\nbatch_size = 10\n\nfor epoch in range(num_epochs):    \n    for X_batch, y_batch in Dataiter_RandomBatch(batch_size, X, y):\n        loss = MSELossfunction(y_batch, LinearRegression(X_batch, w, b))\n        loss.sum().backward()\n        SGD([w, b], alpha, batch_size)  \n    with torch.no_grad():\n        train_loss = MSELossfunction(LinearRegression(X, w, b), y)\n        print(f'epoch {epoch + 1}, loss {float(train_loss.mean()):f}')\n\n\nprint(f'w\u7684\u4f30\u8ba1\u8bef\u5dee: {w_0 - w}')\nprint(f'b\u7684\u4f30\u8ba1\u8bef\u5dee: {b_0 - b}')\n</code></pre> <p>\u5bf9\u4e8eloss.sum().backward()\u7684\u7406\u89e3</p> <p> \u51fa\u5904\uff1a https://zhuanlan.zhihu.com/p/427853673 <p><pre><code>for epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y)  # `X`\u548c`y`\u7684\u5c0f\u6279\u91cf\u635f\u5931\n        # \u56e0\u4e3a`l`\u5f62\u72b6\u662f(`batch_size`, 1)\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u6807\u91cf\u3002`l`\u4e2d\u7684\u6240\u6709\u5143\u7d20\u88ab\u52a0\u5230\u4e00\u8d77\uff0c\n        # \u5e76\u4ee5\u6b64\u8ba1\u7b97\u5173\u4e8e[`w`, `b`]\u7684\u68af\u5ea6\n--------------------------------------------------------------\n        l.sum().backward()\n--------------------------------------------------------------\n        sgd([w, b], lr, batch_size)  # \u4f7f\u7528\u53c2\u6570\u7684\u68af\u5ea6\u66f4\u65b0\u53c2\u6570\n    with torch.no_grad():\n        train_l = loss(net(features, w, b), labels)\n        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n</code></pre> \u5982\u679cTensor \u662f\u4e00\u4e2a\u6807\u91cf(\u5373\u5b83\u5305\u542b\u4e00\u4e2a\u5143\u7d20\u7684\u6570\u636e)\uff0c\u5219\u4e0d\u9700\u8981\u4e3a backward() \u6307\u5b9a\u4efb\u4f55\u53c2\u6570\uff0c\u4f46\u662f\u5982\u679c\u5b83\u6709\u66f4\u591a\u7684\u5143\u7d20\uff0c\u5219\u9700\u8981\u6307\u5b9a\u4e00\u4e2a gradient \u53c2\u6570\uff0c\u8be5\u53c2\u6570\u662f\u5f62\u72b6\u5339\u914d\u7684\u5f20\u91cf\u3002\u672c\u4ee3\u7801\u4e2dl\u4e3a\u77e9\u9635\uff0c\u9700\u8981l.sum()\u8f6c\u5316\u4e3a\u6807\u91cf\u540e\u518d.backward()\u3002</p> <p></p> <p>pytorch\u4e2d\u81ea\u52a0\uff08+=\uff09\u4e0e\u666e\u901a\u52a0\u7684\u533a\u522b</p> <p> \u51fa\u5904\uff1a https://blog.csdn.net/senbinyu/article/details/102634505 <p>\u8ba8\u8bbapytorch\u4e2dx= x + a \u4e0e x += a\uff08\u81ea\u52a0\uff09\u7684\u533a\u522b\uff0c\u5728\u4e8e\u5185\u5b58\u5730\u5740</p> <pre><code>a = torch.tensor([1.0])\nid_a = id(a)\na += 1\n\nid(a) == id_a  #---&gt; True\n\na = a + 1\n\nid(a) == id_a  #---&gt; False\n</code></pre> <p>\u4e3a\u4e86\u65b9\u4fbf\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\uff0cPytorch\u4e2d\u7684\u51fd\u6570\u53ef\u4ee5\u5728\u8c03\u7528\u4e4b\u540e\u52a0\u4e0b\u5212\u7ebf \uff0c\u5f3a\u8c03\u8fd9\u662f\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c </p>"},{"location":"python/d2l-chapter/chapter1/#\u7ebf\u6027\u56de\u5f52\u7684\u7b80\u6d01\u5b9e\u73b0","title":"\u7ebf\u6027\u56de\u5f52\u7684\u7b80\u6d01\u5b9e\u73b0","text":"<ul> <li>\u5b9a\u4e49\u6a21\u578b\u4ee5\u53ca\u6570\u636e\u96c6\u8fed\u4ee3\u5668\uff0c\u521d\u59cb\u5316\u6a21\u578b\u53c2\u6570</li> </ul> <pre><code>dataset = data.TensorDataset(features, labels)\ndataLoader = data.DataLoader(dataset, batch_size, shuffle=True)\n\nnet = nn.Sequential(nn.Linear(2, 1))\nnet[0].weight.data.normal_(0, 0.01)\nnet[0].bias.data.fill_(0)\n\nloss = nn.MSELoss()\ntrainer = torch.optim.SGD(net.parameters(), lr=0.03)\n</code></pre> <ul> <li>\u8bad\u7ec3\u8fc7\u7a0b</li> </ul> <pre><code>num_epochs = 3\nfor epoch in range(num_epochs):\n    for X, y in dataLoader:\n        l = loss(net(X) ,y)\n        trainer.zero_grad() #\u6e05\u9664\u4e0a\u4e00\u6b21\u7684\u68af\u5ea6\u503c \n        l.backward() #\u635f\u5931\u51fd\u6570\u8fdb\u884c\u53cd\u5411\u4f20\u64ad \u6c42\u53c2\u6570\u7684\u68af\u5ea6\n        trainer.step() #\u6b65\u8fdb \u6839\u636e\u6307\u5b9a\u7684\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\n    l = loss(net(features), labels)\n    print(f'epoch {epoch + 1}, loss {l:f}')\n\nw = net[0].weight.data\nprint('w\u7684\u4f30\u8ba1\u8bef\u5dee\uff1a', true_w - w.reshape(true_w.shape))\nb = net[0].bias.data\nprint('b\u7684\u4f30\u8ba1\u8bef\u5dee\uff1a', true_b - b)\n</code></pre> <p>\u5982\u679c\u6211\u4eec\u7528nn.MSELoss(reduction=\u2018sum\u2019)\u66ff\u6362nn.MSELoss()\u4ee3\u7801\u7684\u884c\u4e3a\u76f8\u540c\uff0c\u9700\u8981\u600e\u4e48\u66f4\u6539\u5b66\u4e60\u901f\u7387\uff1f</p> <p> \u9700\u8981\u5c06\u5b66\u4e60\u7387lr\u9664\u4ee5batch_size(\u9ed8\u8ba4\u53c2\u6570\u662f'mean') <p>\u539f\u56e0\uff1a\u82e5\u635f\u5931\u51fd\u6570\u91c7\u7528'sum' \uff0c\u7b49\u6548\u4e8e\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8e'mean'\u653e\u5927\uff0c\u5c06\u4f1a\u4f7f\u5f97\u8ba1\u7b97\u6240\u5f97\u7684\u68af\u5ea6\u589e\u5927,\u8fd9\u65f6\u5019\u539f\u6709\u7684\u5b66\u4e60\u7387\u663e\u5f97\u8fc7\u5927\uff0c\u65e0\u6cd5\u9ad8\u6548\u903c\u8fd1\u6700\u4f18\u70b9\u3002</p> <p></p> <p>\u5982\u679c\u6211\u4eec\u5c06\u6743\u91cd\u521d\u59cb\u5316\u4e3a\u96f6\uff0c\u4f1a\u53d1\u751f\u4ec0\u4e48\u3002\u7b97\u6cd5\u4ecd\u7136\u6709\u6548\u5417\uff1f</p> <p> \u53c2\u8003\u6587\u7ae0\uff1a\u8c08\u8c08\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u4e3a\u4ec0\u4e48\u4e0d\u80fd\u521d\u59cb\u5316\u4e3a0 <p>\u5728\u5355\u5c42\u7f51\u7edc\u4e2d(\u4e00\u5c42\u7ebf\u6027\u56de\u5f52\u5c42)\uff0c\u53ef\u4ee5\u628a\u6743\u91cd\u521d\u59cb\u5316\u4e3a0\uff0c\u4f46\u662f\u5f53\u7f51\u7edc\u52a0\u6df1\u540e\uff0c\u5728\u5168\u8fde\u63a5\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u6743\u91cd\u7684\u5bf9\u79f0\u6027\u4f1a\u5bfc\u81f4\u51fa\u73b0\u9690\u85cf\u795e\u7ecf\u5143\u7684\u5bf9\u79f0\u6027\uff0c\u4f7f\u5f97\u591a\u4e2a\u9690\u85cf\u795e\u7ecf\u5143\u7684\u4f5c\u7528\u5c31\u5982\u540c1\u4e2a\u795e\u7ecf\u5143\uff0c\u7b97\u6cd5\u8fd8\u662f\u6709\u6548\u7684\uff0c\u4f46\u662f\u6548\u679c\u4e0d\u5927\u597d\u3002</p> <p></p>"},{"location":"python/d2l-chapter/chapter1/#softmax\u56de\u5f52","title":"Softmax\u56de\u5f52","text":"<p>\u5728\u5206\u7c7b\u95ee\u9898\u5f53\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u5f97\u5230\u6bcf\u4e2a\u7c7b\u522b\u7684\u6982\u7387\uff08\u56e0\u6b64\u6709\u591a\u4e2a\u8f93\u51fa\uff09,\u6211\u4eec\u5e0c\u671b\u6a21\u578b\u7684\u8f93\u51fa\\(\\hat{y}_j\\)\u53ef\u4ee5\u89c6\u4e3a\u5c5e\u4e8e\u7c7b\\(j\\)\u7684\u6982\u7387\uff0c\u7136\u540e\u9009\u62e9\u5177\u6709\u6700\u5927\u8f93\u51fa\u503c\u7684\u7c7b\u522b\\(\\operatorname*{argmax} y_j\\)\u4f5c\u4e3a\u6211\u4eec\u7684\u9884\u6d4b\u3002 </p> <p>\u4f8b\u5982\uff0c\u5982\u679c\\(\\hat{y}_1\\)\u3001\\(\\hat{y}_2\\)\u548c\\(\\hat{y}_3\\)\u5206\u522b\u4e3a0.1\u30010.8\u548c0.1\uff0c \u90a3\u4e48\u6211\u4eec\u9884\u6d4b\u7684\u7c7b\u522b\u5c31\u662f\u7b2c2\u7c7b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8981\u5c06\u591a\u4e2a\u8f93\u51fa\u6570\u5b57\u7684\u603b\u548c\u9650\u5236\u4e3a1\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8f93\u51fa\u6c38\u8fdc\u5927\u4e8e0\uff0c</p> <p>softmax\u51fd\u6570\u80fd\u591f\u5c06\u672a\u89c4\u8303\u5316\u7684\u9884\u6d4b\u53d8\u6362\u4e3a\u975e\u8d1f\u6570\u5e76\u4e14\u603b\u548c\u4e3a1\uff0c\u540c\u65f6\u8ba9\u6a21\u578b\u4fdd\u6301\u53ef\u5bfc\u7684\u6027\u8d28(\u89c4\u8303\u5316)\u3002</p> \\[     \\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{\u5176\u4e2d} \\quad 0 \\leq \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}  \\leq 1   \\] <p>\u5728\u5c0f\u6279\u91cf\u6837\u672c\u5904\u7406\u4e2d\uff0c\u5176\u4e2d\u7279\u5f81\u7ef4\u5ea6\uff08\u8f93\u5165\u6570\u91cf\uff09\u4e3ad\uff0c\u6279\u91cf\u5927\u5c0f\u4e3an\uff0c\u8f93\u51fa\u4e3aq\uff0c\u5047\u8bbe\u5c0f\u6279\u91cf\u6837\u672c\u7684\u7279\u5f81\u4e3a\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\)\uff0c\u6743\u91cd\u77e9\u9635\u4e3a\\(\\mathbf{W} \\in \\mathbb{R}^{d \\times q}\\)\uff0c\u504f\u7f6e\u4e3a\\(\\mathbf{b} \\in \\mathbb{R}^{1\\times q}\\)\uff0c\u5219softmax\u56de\u5f52\u53ef\u8868\u793a\u4e3a\uff1a</p> \\[    \\begin{split}    \\begin{aligned}     \\mathbf{O} &amp;= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\     \\hat{\\mathbf{Y}} &amp; = \\mathrm{softmax}(\\mathbf{O}).     \\end{aligned}    \\end{split}  \\] <p>\u7531\u4e0a\u8ff0\u6613\u77e5\uff0c\\(\\hat{\\mathbf{y}}\\)\u53ef\u7406\u89e3\u4e3a\u5bf9\u7ed9\u5b9a\u4efb\u610f\u8f93\u5165 \\(\\mathbf{x}\\)\u7684\u5c5e\u4e8e\u6bcf\u4e2a\u7c7b\u7684\u6761\u4ef6\u6982\u7387\u3002\u5bf9\u4e8esoftmax\u56de\u5f52\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\\(\\|\\boldsymbol{\\hat y}^{(i)}-\\boldsymbol{y}^{(i)}\\|^2/2\\)\uff0c\u4f46\u5b9e\u9645\u4e0a\u5e76\u4e0d\u9700\u8981\u9884\u6d4b\u6982\u7387\u4e0e\u6807\u7b7e\u6982\u7387\u5b8c\u5168\u76f8\u540c,\u53ea\u9700\u8981\u5bf9\u5e94\u7684\\(\\hat{\\mathbf{y}}^{(i)}\\)\u6bd4\u5176\u4ed6\u7c7b\u522b\u5927\u5373\u53ef\u3002</p> <p>\u5bf9\u4e8e\u6570\u636e\u96c6\\(\\{\\mathbf{X}, \\mathbf{Y}\\}\\)\u5177\u6709n\u4e2a\u6837\u672c\uff0c\u6709\uff1a</p> \\[    P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).  \\] <p>\u8981\u6700\u5927\u5316\\(P(\\mathbf{Y} \\mid \\mathbf{X})\\),\u5411\u4e0a\u5f0f\u505a\u8d1f\u5bf9\u6570\u5316:</p> \\[     -\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}) = \\sum_{i=1}^n H (\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)})     \\] <p>\u5176\u4e2d\uff1a</p> \\[     H (\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)} \\] <p>\u4e0a\u5f0f\u8fd9\u4e2a\u635f\u5931\u51fd\u6570\u88ab\u79f0\u4e3a\u4ea4\u53c9\u71b5\u635f\u5931\uff08cross-entropy loss\uff09,\u56e0\u4e3a\u5411\u91cf\\(\\boldsymbol y^{(i)}\\)\u53ea\u6709\u5176\u4e2d\u7684\u67d0\u4e00\u4e2a\u5143\u7d20\u4e3a1\uff0c\u5176\u4f59\u5747\u4e3a0,\u5373\u4ea4\u53c9\u71b5\u53ea\u5173\u5fc3\u5bf9\u6b63\u786e\u7c7b\u522b\u7684\u9884\u6d4b\u6982\u7387\uff0c\u56e0\u4e3a\u53ea\u8981\u5176\u503c\u8db3\u591f\u5927\uff0c\u5c31\u53ef\u4ee5\u786e\u4fdd\u5206\u7c7b\u7ed3\u679c\u6b63\u786e</p> <p>\u5047\u8bbe\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u4e3a n\uff0c\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u5b9a\u4e49\u4e3a:</p> \\[     \\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H (\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)})         \\] <p>\u6839\u636esoftmax\u7684\u5b9a\u4e49\uff0c\u53ef\u77e5\u5176\u5bfc\u6570\uff1a</p> \\[ \\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.  \\] <p>\u8be5\u5bfc\u6570\u5373\u662fsoftmax\u6a21\u578b\u5206\u914d\u7684\u6982\u7387\u4e0e\u5b9e\u9645\u53d1\u751f\u7684\u60c5\u51b5\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u8fd9\u4f7f\u68af\u5ea6\u8ba1\u7b97\u5728\u5b9e\u8df5\u4e2d\u53d8\u5f97\u5bb9\u6613\u5f88\u591a\u3002</p> <p>Advice</p> <p> \u5bf9\u4e0a\u5f0f\\(H (\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)}\\)\u7684\u89e3\u7b54\uff1a  <p></p> <p>\u57fa\u4e8e\u6570\u5b66\u5b9a\u4e49\u7684softmax\u51fd\u6570\u53ef\u80fd\u5bfc\u81f4\u4ec0\u4e48\u95ee\u9898\uff1f\u63d0\u793a\uff1a\u5c1d\u8bd5\u8ba1\u7b97exp(50)\u7684\u5927\u5c0f\u3002</p> <p> <p>\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6570\u636e\u6ea2\u51fa\u7684\u60c5\u51b5\u3002\u53ef\u4ee5\u4f7f\u7528LogSoftmax\u4ee3\u66ff\uff0c\u5373\u5728Softmax\u7684\u57fa\u7840\u4e0a\u518d\u505a\u4e00\u6b21log\u3002(nn.LogSoftmax())</p> \\[     LogSoftmax(x_i) = log(\\frac{exp(x_i)}{\\sum_j exp(x_j)})     \\] <p>LogSoftmax\u76f8\u5bf9\u4e8eSoftmax\u7684\u4f18\u52bf</p> <ul> <li> <p>\u5bf9\u6570\u8fd0\u7b97\u65f6\u6c42\u5bfc\u66f4\u5bb9\u6613\uff0c\u52a0\u5feb\u4e86\u53cd\u5411\u4f20\u64ad\u7684\u901f\u5ea6\u3002</p> </li> <li> <p>\u89e3\u51b3Softmax\u53ef\u80fd\u5b58\u5728\u7684\u4e0a\u6ea2\u548c\u4e0b\u6ea2\u7684\u95ee\u9898\u3002</p> </li> </ul> <p></p> <p>pytorch\u4e2d\u7684nn.CrossEntropyLoss()</p> <p> <p>\u5148\u8bf4\u8d1f\u5bf9\u6570\u4f3c\u7136\u51fd\u6570NLLLoss()\uff0c\u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u79cd\u4ea4\u53c9\u71b5\u51fd\u6570\uff1a</p> <p><pre><code>predict = torch.Tensor([[4, 5, 1],\n                    [5, 5, 9]])\nlabel = torch.tensor([1, 2])\nnn.nllloss(predict, label, reduction='mean')\n# output: tensor(-7)\n</code></pre> \u5373\u5728\u4f7f\u7528NLLLoss\u524d\u8981\u5148\u4f7f\u7528LogSoftmax</p> <pre><code>predict = torch.Tensor([[2, 3, 1],\n                    [3, 7, 9]])\npredict = torch.log(torch.softmax(predict, dim=-1))\n# predict: tensor([[-1.4076, -0.4076, -2.4076],\n                   [-6.1291, -2.1291, -0.1291]])\n\nlabel = torch.tensor([1, 2])\nnllloss(predict, label)\n# output: tensor(0.2684)\n</code></pre> <p>nn.CrossEntropyLoss()\u635f\u5931\u51fd\u6570</p> \\[     CrossEntropyLoss() = NLLLoss(LogSoftmax())     \\] <pre><code>cross_loss = nn.CrossEntropyLoss()\npredict = torch.Tensor([[2, 3, 1],\n                        [3, 7, 9]])\nlabel = torch.tensor([1, 2])\ncross_loss(predict, label)\n# output: tensor(0.2684)\n</code></pre> <p></p>"},{"location":"python/d2l-chapter/chapter1/#softmax\u7684\u7b80\u6d01\u5b9e\u73b0","title":"Softmax\u7684\u7b80\u6d01\u5b9e\u73b0","text":"<ul> <li>\u4f7f\u7528pytorch\u5185\u7f6e\u7684Fashion-MNIST\u5206\u7c7b\u6570\u636e\u96c6</li> </ul> <pre><code>mnist_train = torchvision.datasets.FashionMNIST(\n    root=\"../data\", train=True, transform=transforms.ToTensor(), download=True)\nmnist_test = torchvision.datasets.FashionMNIST(\n    root=\"../data\", train=False, transform=transforms.ToTensor(), download=True)\n\n# Fashion-MNIST\u753110\u4e2a\u7c7b\u522b\u7684\u56fe\u50cf\u7ec4\u6210,\u6bcf\u4e2a\u7c7b\u522b\u7531\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u76846000\u5f20\u56fe\u50cf\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u4e2d\u76841000\u5f20\u56fe\u50cf\u7ec4\u6210.\n\nbatch_size = 256\n\ntrain_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=0)\ntest_iter = data.DataLoader(mnist_test, batch_size, shuffle=True, num_workers=0)\n</code></pre> <ul> <li>\u8bbe\u7f6e\u7f51\u7edc\uff0c\u5e76\u4e14\u4f7f\u7528.apply()\u6280\u5de7\u5bf9\u591a\u5c42\u7f51\u7edc\u8fdb\u884c\u53c2\u6570\u521d\u59cb\u5316</li> </ul> <pre><code>def init_weights(module):   #\u6743\u91cd\u521d\u59cb\u5316\u3002apply()\n    if isinstance(module,nn.Linear):\n        nn.init.normal_(module.weight, std=0.01, mean=0)\n        nn.init.constant_(module.bias, 0)\n\nnet = nn.Sequential(nn.Flatten(), \n                    nn.Linear(784, 10))\nnet.apply(init_weights)\n</code></pre> <ul> <li>\u8bbe\u7f6e\u4f18\u5316\u5668\u4e0e\u635f\u5931\u51fd\u6570</li> </ul> <pre><code>loss = nn.CrossEntropyLoss(reduction='mean')\ntrainer = torch.optim.SGD(net.parameters(), lr=0.1)\n</code></pre> <ul> <li>\u8fdb\u884c\u53cd\u5411\u4f20\u64ad,\u8ba1\u7b97\u5206\u7c7b\u51c6\u786e\u5ea6</li> </ul> <pre><code>def evaluate_accuracy(net, iter):\n    cmp = 0\n    tot = 0\n    net.eval() # \u5c06\u6a21\u578b\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n    with torch.no_grad():\n        for X, y in iter:\n            y_hat = net(X)\n            y_hat = y_hat.argmax(axis=1)  # \u5728\u884c\u4e2d\u6bd4\u8f83\uff0c\u9009\u51fa\u6700\u5927\u7684\u5217\u7d22\u5f15\n            CountMatrix = y_hat == y\n            cmp += sum(CountMatrix)\n            tot += len(CountMatrix)\n    return cmp / tot\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    for X_train , y_train in train_iter:\n        l =loss(net(X_train), y_train)\n        trainer.zero_grad()\n        l.backward()\n        trainer.step()\n    print(f'epoch{epoch}-accuracy:{evaluate_accuracy(net, train_iter) * 100:.3f}%')\n</code></pre>"},{"location":"python/d2l-chapter/chapter1/#\u591a\u5c42\u611f\u77e5\u673a","title":"\u591a\u5c42\u611f\u77e5\u673a","text":"<p>\u591a\u5c42\u611f\u77e5\u673a\uff08multilayer perceptron\uff09\u901a\u8fc7\u5728\u7f51\u7edc\u4e2d\u52a0\u5165\u4e00\u4e2a\u6216\u591a\u4e2a\u9690\u85cf\u5c42\u6765\u514b\u670d\u7ebf\u6027\u6a21\u578b\u7684\u9650\u5236\uff0c\u4f7f\u5176\u80fd\u5904\u7406\u66f4\u666e\u904d\u7684\u51fd\u6570\u5173\u7cfb\u7c7b\u578b</p>"},{"location":"python/d2l-chapter/chapter1/#\u6743\u91cd\u8870\u51cf","title":"\u6743\u91cd\u8870\u51cf","text":"<p>\u5728\u8bad\u7ec3\u53c2\u6570\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\uff0c \u6743\u91cd\u8870\u51cf\uff08weight decay\uff09\u662f\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u6b63\u5219\u5316\u7684\u6280\u672f\u4e4b\u4e00\uff0c \u5b83\u901a\u5e38\u4e5f\u88ab\u79f0\u4e3a\\(L_2\\)\u6b63\u5219\u5316\u3002\u5c06\\(L_2\\)\u8303\u6570\u4f5c\u4e3a\u60e9\u7f5a\u9879\u52a0\u5230\u6700\u5c0f\u5316\u635f\u5931\u7684\u95ee\u9898\u4e2d,\u539f\u6765\u7684\u8bad\u7ec3\u76ee\u6807\u6700\u5c0f\u5316\u8bad\u7ec3\u6807\u7b7e\u4e0a\u7684\u9884\u6d4b\u635f\u5931\u4e3a\u6700\u5c0f\u5316\u9884\u6d4b\u635f\u5931\u548c\u60e9\u7f5a\u9879\u4e4b\u548c\u3002\u5982\u679c\u6743\u91cd\u5411\u91cf\u589e\u957f\u8fc7\u5927\uff0c\u7b97\u6cd5\u53ef\u80fd\u4f1a\u66f4\u96c6\u4e2d\u4e8e\u6700\u5c0f\u5316\u6743\u91cd\u8303\u6570\\(\\| \\mathbf{w} \\|^2\\) \u3002</p> \\[     L(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2,     \\] <ul> <li>\u5b9e\u73b0\uff1a</li> </ul> <pre><code>trainer = torch.optim.SGD([\n        {\"params\":net[0].weight,'weight_decay': Lambda},  # \u901a\u8fc7'weight_decay'\u8bbe\u7f6e\u6743\u91cd\u7684L2\u8303\u6570\u4f5c\u4e3a\u60e9\u7f5a\u9879\n        {\"params\":net[0].bias}], lr=0.01)\n</code></pre>"},{"location":"python/d2l-chapter/chapter1/#\u6682\u9000\u6cd5dropout","title":"\u6682\u9000\u6cd5dropout","text":"<p>\u5728\u6807\u51c6\u6682\u9000\u6cd5\u6b63\u5219\u5316\u4e2d\uff0c\u901a\u8fc7\u6309\u4fdd\u7559\uff08\u672a\u4e22\u5f03\uff09\u7684\u8282\u70b9\u7684\u5206\u6570\u8fdb\u884c\u89c4\u8303\u5316\u6765\u6d88\u9664\u6bcf\u4e00\u5c42\u7684\u504f\u5dee\u3002 \u6362\u8a00\u4e4b\uff0c\u6bcf\u4e2a\u4e2d\u95f4\u6d3b\u6027\u503ch\u4ee5\u6682\u9000\u6982\u7387p\u7531\u968f\u673a\u53d8\u91cf\u66ff\u6362h'\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> \\[     \\begin{split}     \\begin{aligned}     h' =     \\begin{cases}     0 &amp; \\text{ \u6982\u7387\u4e3a } p \\\\     \\frac{h}{1-p} &amp; \\text{ \u5176\u4ed6\u60c5\u51b5}     \\end{cases}     \\end{aligned}     \\end{split}     \\] <ul> <li>dropout\u57fa\u7840\u5b9e\u73b0</li> </ul> <pre><code>def dropout_layer(X, dropout):\n    assert 0 &lt;= dropout &lt;= 1\n\n    if dropout == 1:# \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4e22\u5f03\n        return torch.zeros_like(X)\n\n    if dropout == 0:# \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4fdd\u7559\n        return X\n\n    mask = (torch.rand(X.shape) &gt; dropout).float()\n    return mask * X / (1.0 - dropout)\n</code></pre> <ul> <li>\u7b80\u6d01\u5b9e\u73b0</li> </ul> <pre><code>net = nn.Sequential(nn.Flatten(),\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        # \u5728\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n        nn.Dropout(dropout1),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        # \u5728\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n        nn.Dropout(dropout2),\n        nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\n</code></pre> <p>\u603b\u7ed3\uff1a</p> <ul> <li> <p>\u6682\u9000\u6cd5\u5728\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\uff0c\u8ba1\u7b97\u6bcf\u4e00\u5185\u90e8\u5c42\u7684\u540c\u65f6\u4e22\u5f03\u4e00\u4e9b\u795e\u7ecf\u5143\u3002</p> </li> <li> <p>\u6682\u9000\u6cd5\u53ef\u4ee5\u907f\u514d\u8fc7\u62df\u5408\uff0c\u5b83\u901a\u5e38\u4e0e\u63a7\u5236\u6743\u91cd\u5411\u91cf\u7684\u7ef4\u6570\u548c\u5927\u5c0f\u7ed3\u5408\u4f7f\u7528\u7684\u3002</p> </li> </ul> <p>\u5982\u679c\u66f4\u6539\u7b2c\u4e00\u5c42\u548c\u7b2c\u4e8c\u5c42\u7684\u6682\u9000\u6cd5\u6982\u7387,\u4f1a\u53d1\u751f\u4ec0\u4e48\u60c5\u51b5?</p> <p> \u7b2c\u4e00\u5c42\u6682\u9000\u6cd5\u6982\u7387\u5927,\u7b2c\u4e8c\u5c42\u6682\u9000\u6cd5\u6982\u7387\u5c0f,\u5219\u6548\u679c\u4f1a\u8f83\u597d\u3002 <p>\u53ef\u80fd\u539f\u56e0:</p> <ul> <li> <p>\u524d\u9762\u5c42\u62bd\u53d6\u7684\u662f\u6bd4\u8f83\u5e95\u5c42\u7684\u4fe1\u606f,\u6709\u8f83\u591a\u7684\u65e0\u7528\u4fe1\u606f\u5197\u4f59\u901a\u8fc7\u5f3a\u795e\u7ecf\u5143,\u4ece\u800c\u4f7f\u5f97\u7f51\u7edc\u8bb0\u4f4f\u8fd9\u4e9b\u5197\u4f59\u4fe1\u606f\u800c\u5b66\u4e0d\u5230\u5173\u952e\u4fe1\u606f(\u5bfc\u81f4\u8fc7\u62df\u5408),\u7528\u8f83\u5927Dropout\u8f83\u597d,\u540e\u9762\u5c42\u4e3b\u7ba1\u9ad8\u5c42\u62bd\u8c61\u8bed\u4e49\u4fe1\u606f,\u8f83\u4e3a\u5173\u952e,\u662f\u628a\u63e1\u8bc6\u522b\u6574\u4f53\u7684\u5173\u952e\u90e8\u5206,\u7528\u8f83\u5c0fDropout\u8f83\u597d;</p> </li> <li> <p>\u4e00\u822c\u524d\u9762\u5c42\u5168\u8fde\u63a5\u6570\u76ee\u6bd4\u8f83\u5927,\u62bd\u53d6\u4fe1\u606f\u91cf\u6bd4\u8f83\u591a,\u81ea\u7136\u5e26\u6765\u5197\u4f59\u4fe1\u606f\u8f83\u591a,\u90a3\u4e48\u591a\u7684\u6570\u76ee\u8fde\u63a5,\u53ef\u4ee5\u901a\u8fc7\u8f83\u5927Dropout\u4e22\u5f03\u6389\u5927\u90e8\u5206\u7684\u5168\u8fde\u63a5</p> </li> </ul> <p>\u56e0\u6b64dropout\u80fd\u591f\u51cf\u5c0f\u7f51\u7edc\u5bf9\u67d0\u4e9b\u5f3a\u795e\u7ecf\u5143\u7684\u4f9d\u8d56\u6027\uff0c\u4f7f\u5f97\u5f3a\u5f31\u795e\u7ecf\u5143\u65b9\u5dee\u51cf\u5c0f</p> <p></p>"},{"location":"python/d2l-chapter/chapter1/#xavier\u521d\u59cb\u5316","title":"Xavier\u521d\u59cb\u5316","text":"<p>Xavier\u521d\u59cb\u5316\u662f\u4e00\u79cd\u5728\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65f6\u5e38\u7528\u7684\u6743\u91cd\u521d\u59cb\u5316\u65b9\u6cd5\u3002\u5b83\u662f Xavier Glorot \u548c Yoshua Bengio \u5728 2010 \u5e74\u63d0\u51fa\u7684\uff0c\u539f\u6587\u4e3a Understanding the difficulty of training deep feedforward neural networks\u3002\u8be5\u521d\u59cb\u5316\u65b9\u6cd5\u65e8\u5728\u4fdd\u6301\u6fc0\u6d3b\u51fd\u6570\u7684\u65b9\u5dee\u5728\u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u5927\u81f4\u76f8\u540c\uff0c\u4ece\u800c\u907f\u514d\u68af\u5ea6\u6d88\u5931\u6216\u68af\u5ea6\u7206\u70b8\u7684\u95ee\u9898\u3002\u5982\u679c\u65b9\u5dee\u8fc7\u5927\uff0c\u90a3\u4e48\u7f51\u7edc\u7684\u5c42\u5c06\u4f1a\u66f4\u96be\u4ee5\u5b66\u4e60\uff1b\u5982\u679c\u65b9\u5dee\u8fc7\u5c0f\uff0c\u90a3\u4e48\u8be5\u5c42\u7684\u6743\u91cd\u5c06\u4f1a\u96be\u4ee5\u66f4\u65b0\u3002</p> <p>(xavier\u521d\u59cb\u5316\u53ea\u9002\u7528\u4e8e\u5173\u4e8e0\u5bf9\u79f0\u3001\u5448\u7ebf\u6027\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u6bd4\u5982 sigmoid\u3001tanh)</p> <p>\u68af\u5ea6\u6d88\u5931\u548c\u7206\u70b8</p> <p> \u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d\uff0c\u68af\u5ea6\u6d88\u5931\u548c\u68af\u5ea6\u7206\u70b8\u662f\u4e00\u4e2a\u5e38\u89c1\u7684\u95ee\u9898\u3002\u5982\u679c\u6bcf\u4e00\u5c42\u90fd\u5c06\u65b9\u5dee\u653e\u5927\uff0c\u90a3\u4e48\u5728\u591a\u5c42\u7f51\u7edc\u4e2d\uff0c\u68af\u5ea6\u53ef\u80fd\u4f1a\u5f88\u5feb\u589e\u957f\u81f3\u975e\u5e38\u5927\u7684\u503c\uff08\u7206\u70b8\uff09\uff0c\u6216\u8005\u51cf\u5c0f\u81f3\u63a5\u8fd1\u96f6\uff08\u6d88\u5931\uff09\u3002 <p>Xavier \u521d\u59cb\u5316\u8bd5\u56fe\u4f7f\u5f97\u6bcf\u4e00\u5c42\u7684\u8f93\u51fa\u7684\u65b9\u5dee\u63a5\u8fd1\u4e8e\u5176\u8f93\u5165\u7684\u65b9\u5dee\uff0c\u4ece\u800c\u907f\u514d\u68af\u5ea6\u6d88\u5931\u6216\u68af\u5ea6\u7206\u70b8\u7684\u95ee\u9898\uff0c\u6bcf\u4e00\u5c42\u7684\u53c2\u6570\u66f4\u65b0\u7684\u5e45\u5ea6\u5c31\u4e0d\u4f1a\u76f8\u5dee\u592a\u5927\uff0c\u4ece\u800c\u52a0\u901f\u6536\u655b\u3002</p> <p></p> <ul> <li><code>torch.nn.init.xavier_uniform_</code> \u51fd\u6570\u4ece\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u53d6\u6743\u91cd\uff0c\u5176\u4e2d:</li> </ul> \\[     U\\left(-\\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}, \\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}\\right) \\] <ul> <li><code>torch.nn.init.xavier_normal_</code> \u51fd\u6570\u4ece\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u6743\u91cd\uff0c\u5176\u4e2d:</li> </ul> \\[     \\sigma^2 = \\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}     \\]"},{"location":"python/d2l-chapter/chapter2/","title":"\ud83d\udd17Chapter 2\uff1aconvolutional neural network","text":""},{"location":"python/d2l-chapter/chapter2/#\u81ea\u5b9a\u4e49\u5757","title":"\u81ea\u5b9a\u4e49\u5757","text":""},{"location":"python/d2l-chapter/chapter2/#\u5757\u7684\u57fa\u672c\u529f\u80fd","title":"\u5757\u7684\u57fa\u672c\u529f\u80fd","text":"<ul> <li> <p>\u5c06\u8f93\u5165\u6570\u636e\u4f5c\u4e3a\u5176\u524d\u5411\u4f20\u64ad\u51fd\u6570\u7684\u53c2\u6570;</p> </li> <li> <p>\u901a\u8fc7\u524d\u5411\u4f20\u64ad\u51fd\u6570\u6765\u751f\u6210\u8f93\u51fa;</p> </li> <li> <p>\u8ba1\u7b97\u5176\u8f93\u51fa\u5173\u4e8e\u8f93\u5165\u7684\u68af\u5ea6\uff0c\u53ef\u901a\u8fc7\u5176\u53cd\u5411\u4f20\u64ad\u51fd\u6570\u8fdb\u884c\u8bbf\u95ee(\u81ea\u52a8\u53d1\u751f)\u3002</p> </li> <li> <p>\u5b58\u50a8\u548c\u8bbf\u95ee\u524d\u5411\u4f20\u64ad\u8ba1\u7b97\u6240\u9700\u7684\u53c2\u6570</p> </li> </ul> <pre><code>import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden = nn.Linear(20,256)\n        self.out = nn.Linear(256, 10)\n    def forward(self, X_input):\n        return self.out(F.relu(self.hidden(X_input)))\n\nif __name__ == '__main__':\n    X = torch.randn(20)\n    net = MLP()\n    y = net(X)\n</code></pre> <p>\u5b9e\u73b0\u4e00\u4e2a\u5757\uff0c\u5b83\u4ee5\u4e24\u4e2a\u5757\u4e3a\u53c2\u6570\uff0c\u5e76\u8fd4\u56de\u524d\u5411\u4f20\u64ad\u4e2d\u4e24\u4e2a\u7f51\u7edc\u7684\u4e32\u8054\u8f93\u51fa\u3002\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u5e73\u884c\u5757</p> <p> <pre><code>class Net(nn.Module):\n    def __init__(self, net1, net2):\n        super().__init__()\n        self.Net_1 = net1\n        self.Net_2 = net2\n    def forward(self, X):\n        X = torch.cat((self.Net_1(X), self.Net_2(X)), 1)\n        return X\n\nnet0 = Net(nn.Linear(20,3), nn.Linear(20,6))\n</code></pre> </p>"},{"location":"python/d2l-chapter/chapter2/#\u53c2\u6570\u7ba1\u7406","title":"\u53c2\u6570\u7ba1\u7406","text":"<pre><code>import torch\nfrom torch import nn\n\nnet = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\nX = torch.rand(size=(2, 4))\nnet(X)\n\nprint(net[2].state_dict()) \n# .state_dict()\u83b7\u53d6\u6a21\u578b\u53c2\u6570\u72b6\u6001\u7684\u65b9\u6cd5\u3002\u200c\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u5bf9\u8c61\uff0c\u200c\u5176\u4e2d\u5305\u542b\u4e86\u6a21\u578b\u4e2d\u6240\u6709\u7684\u53c2\u6570\u548c\u5bf9\u5e94\u7684\u503c\n&gt;&gt;&gt; OrderedDict([('weight', tensor([[-0.0427, -0.2939, -0.1894,  0.0220, -0.1709, -0.1522, -0.0334, -0.2263]])), ('bias', tensor([0.0887]))])\n\nprint(type(net[2].bias))  # &lt;class 'torch.nn.parameter.Parameter'&gt;\nprint(net[2].bias)\n# Parameter containing:\n# tensor([0.0345], requires_grad=True)\nprint(net[2].bias.data)\n# tensor([0.0345])\nprint(net.state_dict()['2.bias'].data)\n# tensor([0.0345])\n</code></pre>"},{"location":"python/d2l-chapter/chapter2/#\u4e00\u6b21\u6027\u8bbf\u95ee\u6240\u6709\u53c2\u6570","title":"\u4e00\u6b21\u6027\u8bbf\u95ee\u6240\u6709\u53c2\u6570","text":"<pre><code>print(*[(name, param.shape) for name, param in net[0].named_parameters()])\nprint(*[(name, param.shape) for name, param in net.named_parameters()])\n\n&gt;&gt;&gt; ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n&gt;&gt;&gt; ('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n</code></pre>"},{"location":"python/d2l-chapter/chapter2/#\u4ece\u5d4c\u5957\u5757\u6536\u96c6\u53c2\u6570","title":"\u4ece\u5d4c\u5957\u5757\u6536\u96c6\u53c2\u6570","text":"<pre><code>def block1():\n    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n                         nn.Linear(8, 4), nn.ReLU())\ndef block2():\n    net = nn.Sequential()\n    for i in range(4):\n        net.add_module(f'block {i}', block1())\n    return net\n\nrgnet = nn.Sequential(block2(), nn.Linear(4, 1))\nrgnet(X)\n\nprint(rgnet)\n\n&gt;&gt;&gt; Output:\nSequential(\n  (0): Sequential(\n    (block 0): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n    (block 1): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n    (block 2): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n    (block 3): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n  )\n  (1): Linear(in_features=4, out_features=1, bias=True)\n)\n\nrgnet[0][1][0].bias.data # \u7b2c\u4e00\u4e2a\u4e3b\u8981\u7684\u5757\u4e2d\u3001\u7b2c\u4e8c\u4e2a\u5b50\u5757\u7684\u7b2c\u4e00\u5c42\u7684\u504f\u7f6e\u9879\n&gt;&gt;&gt; tensor([ 0.1999, -0.4073, -0.1200, -0.2033, -0.1573,  0.3546, -0.2141, -0.2483])\n</code></pre> <p>\u53ef\u4ee5\u50cf\u901a\u8fc7\u5d4c\u5957\u5217\u8868\u7d22\u5f15\u4e00\u6837\u8bbf\u95ee\u5b83\u4eec</p>"},{"location":"python/d2l-chapter/chapter2/#\u81ea\u5b9a\u4e49\u521d\u59cb\u5316","title":"\u81ea\u5b9a\u4e49\u521d\u59cb\u5316","text":"<p>\u5982\u6211\u4eec\u53ef\u4ee5\u81ea\u5b9a\u4e49\u8bbe\u7f6e\u6743\u91cd\u521d\u59cb\u5316\u53c2\u6570\u7684\u5206\u5e03\u4e3a\uff1a</p> \\[     \\begin{split}     \\begin{aligned}     w \\sim \\begin{cases}         U(5, 10) &amp; \\text{ \u53ef\u80fd\u6027 } \\frac{1}{4} \\\\             0    &amp; \\text{ \u53ef\u80fd\u6027 } \\frac{1}{2} \\\\         U(-10, -5) &amp; \\text{ \u53ef\u80fd\u6027 } \\frac{1}{4}     \\end{cases}     \\end{aligned}     \\end{split}     \\] <pre><code>def my_init(m):\n    if type(m) == nn.Linear:\n        print(\"Init\", *[(name, param.shape)\n                        for name, param in m.named_parameters()][0])\n        nn.init.uniform_(m.weight, -10, 10)\n        m.weight.data *= m.weight.data.abs() &gt;= 5\n        # \u5c06\u6743\u91cd\u6570\u636e\u4e2d\u7edd\u5bf9\u503c\u5927\u4e8e\u7b49\u4e8e5\u7684\u90e8\u5206\u4fdd\u7559\u4e0b\u6765\uff0c\u800c\u5c0f\u4e8e5\u7684\u90e8\u5206\u7f6e\u4e3a0\n\nnet.apply(my_init)\n</code></pre>"},{"location":"python/d2l-chapter/chapter2/#\u53c2\u6570\u7ed1\u5b9a\u5171\u4eab\u53c2\u6570","title":"\u53c2\u6570\u7ed1\u5b9a/\u5171\u4eab\u53c2\u6570","text":"<pre><code># \u7ed9\u5171\u4eab\u5c42\u4e00\u4e2a\u540d\u79f0shared\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u5f15\u7528\u5b83\u7684\u53c2\u6570\nsharedNet = nn.Linear(8, 8)\nnet = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n                    sharedNet, nn.ReLU(),\n                    sharedNet, nn.ReLU(),\n                    nn.Linear(8, 1))\n</code></pre> <p>\u5f53\u53c2\u6570\u7ed1\u5b9a\u65f6\uff0c\u68af\u5ea6\u4f1a\u53d1\u751f\u4ec0\u4e48\u60c5\u51b5\uff1f</p> <p>\u7531\u4e8e\u6a21\u578b\u53c2\u6570\u5305\u542b\u68af\u5ea6\uff0c\u56e0\u6b64\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u5171\u4eab\u7684\u53c2\u6570\u68af\u5ea6\u4f1a\u53e0\u52a0</p>"},{"location":"python/d2l-chapter/chapter2/#\u81ea\u5b9a\u4e49\u5c42","title":"\u81ea\u5b9a\u4e49\u5c42","text":"<pre><code>class MyLinear(nn.Module):  # \u81ea\u5b9a\u4e49\u5168\u8fde\u63a5\u5c42\n    def __init__(self, in_units, units): # \u8f93\u5165\u53c2\u6570\uff1ain_units\u548cunits\uff08\u8f93\u5165\u6570\u548c\u8f93\u51fa\u6570\uff09\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(in_units, units))\n        self.bias = nn.Parameter(torch.randn(units,))\n        # \u9700\u8981\u4e24\u4e2a\u53c2\u6570\uff0c\u4e00\u4e2a\u7528\u4e8e\u8868\u793a\u6743\u91cd\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u8868\u793a\u504f\u7f6e\u9879\n\n    def forward(self, X):\n        linear = torch.matmul(X, self.weight.data) + self.bias.data\n        return F.relu(linear)\n</code></pre>"},{"location":"python/d2l-chapter/chapter2/#\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","title":"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","text":"<p><code>nn.Conv2d(in_channels=, out_channels=, kernel_size=, stride=, padding=)</code></p> <ul> <li> <p>in_channels, out_channels\u6307\u5b9a\u8f93\u5165\u8f93\u51fa\u901a\u9053\u6570</p> </li> <li> <p>kernel_size \u5377\u79ef\u6838\u5f62\u72b6</p> </li> <li> <p>stride \u6b65\u5e45\u8bbe\u7f6e\u4e3a2</p> </li> <li> <p>padding \u586b\u5145\u64cd\u4f5c</p> </li> </ul>"},{"location":"python/d2l-chapter/chapter2/#\u591a\u8f93\u5165\u901a\u9053","title":"\u591a\u8f93\u5165\u901a\u9053","text":"<p>\u5f53\u8f93\u5165\u5305\u542b\u591a\u4e2a\u901a\u9053\u65f6(\u8f93\u51fa\u901a\u9053\u4e3a1)\uff0c\u9700\u8981\u6784\u9020\u4e00\u4e2a\u4e0e\u8f93\u5165\u6570\u636e\u5177\u6709\u76f8\u540c\u8f93\u5165\u901a\u9053\u6570\u7684\u5377\u79ef\u6838\uff0c\u4ee5\u4fbf\u4e0e\u8f93\u5165\u6570\u636e\u8fdb\u884c\u4e92\u76f8\u5173\u8fd0\u7b97\u3002\u5047\u8bbe\u8f93\u5165\u7684\u901a\u9053\u6570\u4e3a\\(c_i\\)\uff0c\u90a3\u4e48\u5377\u79ef\u6838\u7684\u8f93\u5165\u901a\u9053\u6570\u4e5f\u9700\u8981\u4e3a\\(c_i\\)\u3002\u5982\u679c\u5377\u79ef\u6838\u7684\u7a97\u53e3\u5f62\u72b6\u662f\\(k_h\\times k_w\\)\uff0c\u5219\u8be5\u5377\u79ef\u6838\u7684\u603b\u4f53\u5f62\u72b6\u4e3a\\(c_i\\times k_h\\times k_w\\)\u3002</p> <p></p>"},{"location":"python/d2l-chapter/chapter2/#\u591a\u8f93\u51fa\u901a\u9053","title":"\u591a\u8f93\u51fa\u901a\u9053","text":"<p>\u5f53\u8f93\u51fa\u901a\u9053\u7684\u6570\u76ee\u4e3a\\(c_0\\)\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u4e3a\u6bcf\u4e2a\u8f93\u51fa\u901a\u9053\u521b\u5efa\u4e00\u4e2a\u5f62\u72b6\u4e3a\\(c_i\\times k_h\\times k_w\\)\u7684\u5377\u79ef\u6838\u5f20\u91cf,\u5219\u5377\u79ef\u6838\u7684\u5f62\u72b6\u4e3a\\(c_o\\times c_i\\times k_h\\times k_w\\)\u3002</p> <p></p>"},{"location":"python/d2l-chapter/chapter2/#\u6c47\u805a\u5c42","title":"\u6c47\u805a\u5c42","text":"<pre><code>X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4)) # \u56db\u4e2a\u7ef4\u5ea6: [batch_size, channel, height, width]\n&gt;&gt;&gt;tensor([[[[ 0.,  1.,  2.,  3.],\n              [ 4.,  5.,  6.,  7.],\n              [ 8.,  9., 10., 11.],\n              [12., 13., 14., 15.]]]])\n\npool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n# \u8bbe\u5b9a\u4e00\u4e2a\u4efb\u610f\u5927\u5c0f\u7684\u77e9\u5f62\u6c47\u805a\u7a97\u53e3\uff0c\u5e76\u5206\u522b\u8bbe\u5b9a\u586b\u5145\u548c\u6b65\u5e45\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\n</code></pre>"},{"location":"python/d2l-chapter/chapter2/#\u7ecf\u5178\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","title":"\u7ecf\u5178\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","text":""},{"location":"python/d2l-chapter/chapter2/#alexnet","title":"AlexNet","text":"<p>AlexNet</p> <p> \u76f8\u5173\u8bba\u6587\uff1aImageNet Classification with Deep Convolutional Neural Networks <p>AlexNet\u662f\u6df1\u5ea6\u5b66\u4e60\u6d6a\u6f6e\u7684\u5960\u57fa\u4e4b\u4f5c\uff0c\u53d1\u5e03\u4e8e2012\u5e74\uff0c\u4ed6\u9996\u6b21\u628a\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u5206\u7c7b\u4e0a\uff0c\u53ef\u4ee5\u8bf4\u4e3a\u540e\u7eed\u6574\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002</p> <p>\u53c2\u8003\u5b66\u4e60\uff1ahttps://zhuanlan.zhihu.com/p/683888262</p> <p></p> <p></p> <p>AlexNet\u7f51\u7edc\u7279\u70b9</p> <ul> <li> <p>\u63d0\u51fa\u4e86\u975e\u9971\u548c\u795e\u7ecf\u5143 ReLU \u51cf\u5c0f \u68af\u5ea6\u4e0b\u964d \u7684\u8bad\u7ec3\u65f6\u95f4</p> <p>\u9971\u548c\u5373\u5f88\u5927\u6216\u5f88\u5c0f\u7684x\u7ecf\u8fc7\u6fc0\u6d3b\u51fd\u6570\u4e4b\u540e\u5dee\u522b\u4f1a\u53d8\u5c0f\u3002\u9971\u548c\u7684\u6fc0\u6d3b\u51fd\u6570\u4f1a\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6536\u655b\u65f6\u95f4\u53d8\u957f\u3002</p> <p>\u76f8\u6bd4\u4f7f\u7528tanh(x)\u548csigmoid(x)\u8fd9\u79cd\u9971\u548c\u6fc0\u6d3b\u51fd\u6570\uff0c\u4f7f\u7528\u975e\u9971\u548c\u7684\u6fc0\u6d3b\u51fd\u6570\u53ef\u4ee5\u4f7f\u8bad\u7ec3\u901f\u5ea6\u5feb\u51e0\u500d\uff0c\u6bd4\u5982\u4f7f\u7528ReLU\uff08Rectified Linear Unit\uff0c\u4fee\u6b63\u7ebf\u6027\u5355\u5143\uff09</p> </li> <li> <p>\u63d0\u51fa\u4e86\u91cd\u53e0\u6c60\u5316\uff08Overlapping Pooling\uff09 \u4f7f\u7528 stride=2\uff0ckernal_size=3 </p> <p>\u540e\u7eed\u5176\u4ed6\u8bba\u6587\u7684\u5de5\u4f5c\u8ba4\u4e3a\u8fd9\u4e00\u6b65\u5bf9\u9632\u6b62\u8fc7\u62df\u5408\u610f\u4e49\u4e0d\u5927:\u73b0\u5728\u6709\u7684\u7f51\u7edc\u4e2d\u4ecd\u91c7\u7528\u4e86\u6c60\u5316\u5c42\uff0c\u4f46\u4e00\u822c\u7a97\u53e3\u90fd\u662f\u4e0d\u91cd\u53e0\u7684\uff0c\u4fdd\u7559\u4e3b\u8981\u7279\u5f81\u7684\u540c\u65f6\u964d\u4f4e\u6570\u636e\u7684\u7ef4\u5ea6\u5e76\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u6709\u52a9\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b</p> </li> <li> <p>\u5f15\u5165\u4e86 dropout \u6b63\u5219\u5316\u65b9\u6cd5\u51cf\u5c11 \u5168\u8fde\u63a5\u5c42\u4e2d\u7684 \u8fc7\u62df\u5408 </p> </li> <li> <p>\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u2018\u6570\u636e\u589e\u5f3a\u2019\u7684\u65b9\u6cd5\u6269\u5145\u6570\u636e\u96c6\u7528\u4ee5\u51cf\u5c0f\u8fc7\u62df\u5408\u7ebf\u6027</p> </li> <li> <p>\u6743\u91cd\u5728\u8fdb\u884c\u521d\u59cb\u5316\u65f6\uff0c\u4f7f\u7528\u5747\u503c\u4e3a0\u3001 \u65b9\u5dee\u4e3a0.01\u7684\u9ad8\u65af\u968f\u673a\u53d8\u91cf\u8fdb\u884c\u521d\u59cb\u5316</p> </li> <li> <p>\u5728\u539f\u6765\u7684\u591a\u5206\u7c7b\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u4e86L2\u6b63\u5219\u5316\uff08\u6743\u91cd\u8870\u51cfweight decay\uff09</p> </li> </ul> <pre><code>import torchvision\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = torchvision.models.alexnet(pretrained=False)\nmodel.to(device)\nfrom torchinfo import summary\nsummary(model, [1,3,227,227])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAlexNet                                  --                        --\n\u251c\u2500Sequential: 1-1                        [1, 128, 6, 6]            --\n\u2502    \u2514\u2500Conv2d: 2-1                       [1, 48, 55, 55]           17,472\n\u2502    \u2514\u2500ReLU: 2-2                         [1, 48, 55, 55]           --\n\u2502    \u2514\u2500MaxPool2d: 2-3                    [1, 48, 27, 27]           --\n\u2502    \u2514\u2500Conv2d: 2-4                       [1, 128, 27, 27]          153,728\n\u2502    \u2514\u2500ReLU: 2-5                         [1, 128, 27, 27]          --\n\u2502    \u2514\u2500MaxPool2d: 2-6                    [1, 128, 13, 13]          --\n\u2502    \u2514\u2500Conv2d: 2-7                       [1, 192, 13, 13]          221,376\n\u2502    \u2514\u2500ReLU: 2-8                         [1, 192, 13, 13]          --\n\u2502    \u2514\u2500Conv2d: 2-9                       [1, 192, 13, 13]          331,968\n\u2502    \u2514\u2500ReLU: 2-10                        [1, 192, 13, 13]          --\n\u2502    \u2514\u2500Conv2d: 2-11                      [1, 128, 13, 13]          221,312\n\u2502    \u2514\u2500ReLU: 2-12                        [1, 128, 13, 13]          --\n\u2502    \u2514\u2500MaxPool2d: 2-13                   [1, 128, 6, 6]            --\n\u251c\u2500Sequential: 1-2                        [1, 1000]                 --\n\u2502    \u2514\u2500Linear: 2-14                      [1, 2048]                 9,439,232\n\u2502    \u2514\u2500ReLU: 2-15                        [1, 2048]                 --\n\u2502    \u2514\u2500Dropout: 2-16                     [1, 2048]                 --\n\u2502    \u2514\u2500Linear: 2-17                      [1, 2048]                 4,196,352\n\u2502    \u2514\u2500ReLU: 2-18                        [1, 2048]                 --\n\u2502    \u2514\u2500Linear: 2-19                      [1, 1000]                 2,049,000\n==========================================================================================\nTotal params: 16,630,440\nTrainable params: 16,630,440\nNon-trainable params: 0\nTotal mult-adds (M): 311.52\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 2.64\nParams size (MB): 66.52\nEstimated Total Size (MB): 69.77\n==========================================================================================\n</code></pre> <ul> <li>AlexNet\u5177\u4f53\u5b9e\u73b0 </li> </ul> <pre><code>class AlexNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2), # input[3.227.227] output[48,55,55]\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),          # output[48, 27, 27]\n            nn.Conv2d(48, 128, kernel_size=5, padding=2),   # output[128, 27,27]\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),          # output[128, 13, 13]\u3001\n            nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 13, 13]\n            nn.ReLU(inplace=True),\n            nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 13, 13]\n            nn.ReLU(inplace=True),\n            nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 13, 13]\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2)           # output[128, 6, 6]\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Linear(128*6*6, 2048),\n            nn.ReLU(inplace=True),\n# \u5f53`inplace=True`\u65f6\uff0cReLU\u51fd\u6570\u4f1a\u5728\u539f\u4f4d\uff08in-place\uff09\u4fee\u6539\u539f\u59cb\u5f20\u91cf\uff0c\u5373\u76f4\u63a5\u5728\u539f\u59cb\u5f20\u91cf\u4e0a\u6267\u884c\u8ba1\u7b97\n            nn.Dropout(p=0.5),\n            nn.Linear(2048, 2048),\n            nn.ReLU(inplace=True),\n            nn.Linear(2048, num_classes),\n        )\n        if init_weights:\n            self._initialize_weights()\n\n        def forward(self, x):\n            x = self.features(x)\n            x = torch.flatten(x, start_dim=1)\n            x = self.classifier(x)\n            return x\n\n        def _initialize_weights(self):  # \u6743\u91cd\u521d\u59cb\u5316\u5316\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') \n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.Linear):\n                    nn.init.normal_(m.weight, 0, 0.01)  #\u6b63\u6001\u5206\u5e03\u8d4b\u503c\n                    nn.init.constant_(m.bias, 0)\n</code></pre> <p>AlexNet\u5b8c\u6574\u9879\u76ee\u5199\u4f5c\u6a21\u578b\u8be6\u89e3</p>"},{"location":"python/d2l-chapter/chapter2/#vggnet","title":"VGGnet","text":"<p>VGG</p> <p> \u76f8\u5173\u8bba\u6587\uff1aVery Deep Convolutional Networks for Large-Scale Image Recognition <p>VGGNet\u7531\u725b\u6d25\u5927\u5b66\u8ba1\u7b97\u673a\u89c6\u89c9\u7ec4\u5408\u548cGoogle DeepMind\u516c\u53f8\u7814\u7a76\u5458\u4e00\u8d77\u7814\u53d1\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002\u5b83\u63a2\u7d22\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6df1\u5ea6\u548c\u5176\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u53cd\u590d\u7684\u5806\u53e033\u7684\u5c0f\u578b\u5377\u79ef\u6838\u548c22\u7684\u6700\u5927\u6c60\u5316\u5c42\uff0c\u6210\u529f\u7684\u6784\u5efa\u4e8616~19\u5c42\u6df1\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002VGGNet\u83b7\u5f97\u4e86ILSVRC 2014\u5e74\u6bd4\u8d5b\u7684\u4e9a\u519b\u548c\u5b9a\u4f4d\u9879\u76ee\u7684\u51a0\u519b\uff0c\u5728top5\u4e0a\u7684\u9519\u8bef\u7387\u4e3a7.5%\u3002\u76ee\u524d\u4e3a\u6b62\uff0cVGGNet\u4f9d\u7136\u88ab\u7528\u6765\u63d0\u53d6\u56fe\u50cf\u7684\u7279\u5f81\u3002</p> <p></p> <p></p> <p>VGG\u7f51\u7edc\u7279\u70b9\uff1a</p> <ul> <li> <p>\u9a8c\u8bc1\u4e86\u901a\u8fc7\u4e0d\u65ad\u52a0\u6df1\u7f51\u7edc\u7ed3\u6784\u53ef\u4ee5\u63d0\u5347\u6027\u80fd</p> </li> <li> <p>\u51e0\u4e2a\u5c0f\u6ee4\u6ce2\u5668\uff083x3\uff09\u5377\u79ef\u5c42\u7684\u7ec4\u5408\u6bd4\u4e00\u4e2a\u5927\u6ee4\u6ce2\u5668\uff085x5\u62167x7\uff09\u5377\u79ef\u5c42\u597d</p> <p>\u591a\u5c42\u5c0f\u7684\u5377\u79ef\u6838\u83b7\u5f97\u7684\u611f\u53d7\u91ce\u4e0e\u5355\u5c42\u8f83\u5927\u7684\u5377\u79ef\u6838\u4e00\u81f4    </p> <p></p> <p>\u5e76\u4e14\uff0c\u76f8\u8f83\u4e8e\u5927\u5377\u79ef\u6838\uff0c\u5c0f\u5377\u79ef\u6838\u7684\u4f7f\u7528\u5728\u589e\u52a0\u7f51\u7edc\u5c42\u6570\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u591a\u5c42\u53e0\u52a0\u4e5f\u589e\u5f3a\u4e86\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b</p> </li> <li> <p>VGGNet\u7684\u7ed3\u6784\u975e\u5e38\u7b80\u6d01(\u5377\u79ef\u5c42+ReLU\u3001\u6700\u5927\u6c60\u5316\u5c42\u3001\u5168\u8fde\u63a5\u5c42\u3001Softmax\u8f93\u51fa\u5c42),\u4ee3\u7801\u5b9e\u73b0\u7cbe\u5de7\uff0c\u6574\u4e2a\u7f51\u7edc\u90fd\u4f7f\u7528\u4e86\u540c\u6837\u5927\u5c0f\u7684\u5377\u79ef\u6838\u5c3a\u5bf8\uff083x3\uff09\u548c\u6700\u5927\u6c60\u5316\u5c3a\u5bf8\uff082x2\uff09</p> </li> <li> <p>\u7ed3\u8bba\u63d0\u51faalexnet\u4e2d\u7684LRN\u5c42\u6ca1\u6709\u6027\u80fd\u589e\u76ca</p> </li> </ul>"},{"location":"python/d2l-chapter/chapter2/#nin-network-in-network","title":"NiN (Network In NetWork)","text":"<p>NiN</p> <p> \u76f8\u5173\u8bba\u6587\uff1aNetwork In Network <p>Network In NetWork(NIN) \u662f\u7531\u65b0\u52a0\u5761\u56fd\u7acb\u5927\u5b66\u7684MinLin\u7b49\u4eba\u63d0\u51fa\u7684\uff0c\u5728CIFAR-10\u548cCIFAR-100\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86SOTA\u7ed3\u679c\u3002\u63d0\u51famlpconv\uff0c\u5f15\u5165\u4e861x1\u5377\u79ef\u548c<code>global average pooling</code>\uff0c\u63d0\u51faNetwork In Network(NIN)\uff0c</p> <p>LeNet\u3001AlexNet\u548cVGG\u90fd\u6709\u4e00\u4e2a\u5171\u540c\u7684\u8bbe\u8ba1\u6a21\u5f0f\uff1a\u901a\u8fc7\u4e00\u7cfb\u5217\u7684\u5377\u79ef\u5c42\u4e0e\u6c47\u805a\u5c42\u6765\u63d0\u53d6\u7a7a\u95f4\u7ed3\u6784\u7279\u5f81\uff1b\u7136\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u5bf9\u7279\u5f81\u7684\u8868\u5f81\u8fdb\u884c\u5904\u7406\u3002AlexNet\u548cVGG\u5bf9LeNet\u7684\u6539\u8fdb\u4e3b\u8981\u5728\u4e8e\u5982\u4f55\u6269\u5927\u548c\u52a0\u6df1\u8fd9\u4e24\u4e2a\u6a21\u5757\u3002</p> <p>AlexNet\u548cVGG\u90fd\u662f\u5148\u7531\u5377\u79ef\u5c42\u6784\u6210\u7684\u6a21\u5757\u5145\u5206\u62bd\u53d6\u7a7a\u95f4\u7279\u5f81\uff0c\u518d\u7531\u5168\u8fde\u63a5\u5c42\u6784\u6210\u7684\u6a21\u5757\u6765\u8f93\u51fa\u5206\u7c7b\u7ed3\u679c\u3002\u7136\u800c\uff0c\u5982\u679c\u4f7f\u7528\u4e86\u5168\u8fde\u63a5\u5c42\uff0c\u53ef\u80fd\u4f1a\u5b8c\u5168\u653e\u5f03\u8868\u5f81\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u4e14\u53c2\u6570\u91cf\u5de8\u5927\u3002 \u56e0\u6b64NiN\u63d0\u51fa\u75281*1\u5377\u79ef\u4ee3\u66ff\u5168\u8fde\u63a5\u5c42\uff0c\u4e32\u8054\u591a\u4e2a\u7531\u5377\u79ef\u5c42\u548c\u201c\u5168\u8fde\u63a5\u201d\u5c42\u6784\u6210\u7684\u5c0f\u7f51\u7edc\u6765\u6784\u5efa\u2f00\u4e2a\u6df1\u5c42\u7f51\u7edc\u3002</p> <p></p> <p>MLPConv Layer\u7531\u82e5\u5e72\u4e2a\u5c40\u90e8\u7684\u5168\u8fde\u63a5\u5c42\u548c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u7ec4\u6210\uff0c\u4ee3\u66ff\u4e86\u4f20\u7edf\u5377\u79ef\u5c42\u4e2d\u91c7\u7528\u7684\u7ebf\u6027\u5377\u79ef\u6838\uff08\u5b9e\u9645\u4e0a\u662f\u4f7f\u7528\u4e861x1\u5377\u79ef\uff09</p> <p></p> <ul> <li> <p>\u5404\u901a\u9053\u7279\u5f81\u878d\u5408\uff1a</p> <p>1\u00d71\u5377\u79ef\u6838\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u9645\u4e0a\u5bf9\u4e0d\u540c\u901a\u9053\u4e0a\u540c\u4e00\u4f4d\u7f6e\u5904\u7684\u7279\u5f81\u503c\u8fdb\u884c\u4e86\u4e00\u6b21\u7ebf\u6027\u7ec4\u5408\u3002\u56e0\u6b64\uff0c\u8fd9\u5c31\u53ef\u4ee5\u6839\u636e\u6700\u7ec8\u8bad\u7ec3\u5f97\u5230\u7684\u8fd9\u4e2a[1\u00d71]\u7684\u5377\u79ef\u6838\u6743\u91cd\u53c2\u6570\u6765\u786e\u5b9a\u6bcf\u4e2a\u7279\u5f81\u901a\u9053\u7684\u91cd\u8981\u6027\u5360\u6bd4\uff08\u6709\u70b9\u6ce8\u610f\u529b\u673a\u5236\u7684\u5473\u9053\uff09\uff0c\u5e76\u8fdb\u884c\u878d\u5408\u5f62\u6210\u4e00\u4e2a\u901a\u9053</p> </li> </ul> <p>\u5728NIN\u4e2d\uff0c\u5377\u79ef\u5c42\u540e\u4e0d\u63a5\u5168\u8fde\u63a5\u5c42(FC)\uff0c\u800c\u662f\u5c06\u6700\u540e\u4e00\u4e2a\u7684mlpconv\u7684\u8f93\u51fa\u6bcf\u4e2a\u7279\u5f81\u56fe\u5168\u5c40\u5e73\u5747\u6c60\u5316(global average pooling\uff0cGAP) \uff0c\u800c\u540esoftmax\u3002</p> <p></p> <p>1\u00d71\u5377\u79ef\u6838</p> <p> \u53c2\u8003\u4e00\u6587\u8bfb\u61c2\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u76841x1\u5377\u79ef\u6838_ <ul> <li> <p>\u589e\u52a0\u975e\u7ebf\u6027</p> <p>\u6bcf\u4f7f\u7528 1 * 1\u7684\u5377\u79ef\u6838\uff0c\u53ca\u589e\u52a0\u4e00\u5c42\u5377\u79ef\u5c42\uff0c\u6240\u4ee5\u7f51\u7edc\u6df1\u5ea6\u5f97\u4ee5\u589e\u52a0\u3002 \u800c\u4f7f\u7528 1 * 1\u7684\u5377\u79ef\u6838\u540e\uff0c\u53ef\u4ee5\u4fdd\u6301\u7279\u5f81\u56fe\u5927\u5c0f\u4e0e\u8f93\u5165\u5c3a\u5bf8\u76f8\u540c\uff0c\u5377\u79ef\u5c42\u5377\u79ef\u8fc7\u7a0b\u4f1a\u5305\u542b\u4e00\u4e2a\u6fc0\u6d3b\u51fd\u6570\uff0c\u4ece\u800c\u589e\u52a0\u4e86\u975e\u7ebf\u6027\u3002\u5728\u8f93\u5165\u5c3a\u5bf8\u4e0d\u53d1\u751f\u6539\u53d8\u7684\u60c5\u51b5\u4e0b\u800c\u589e\u52a0\u4e86\u975e\u7ebf\u6027\uff0c\u6240\u4ee5\u4f1a\u589e\u52a0\u6574\u4e2a\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u3002</p> </li> <li> <p>\u53c2\u6570\u91cf\u51cf\u5c11\uff0c\u964d\u4f4e\u8ba1\u7b97\u91cf</p> </li> <li> <p>\u8de8\u901a\u9053\u4fe1\u606f\u4ea4\u4e92\uff08\u901a\u9053\u7684\u53d8\u6362\uff09</p> <p>1 * 1\u7684\u5377\u79ef\u6838\u4e00\u822c\u53ea\u6539\u53d8\u8f93\u51fa\u901a\u9053\u6570\uff08C\uff09\uff0c\u800c\u4e0d\u6539\u53d8\u8f93\u51fa\u7684\u5bbd\u5ea6\uff08W\uff09\u548c\u9ad8\u5ea6\uff08H\uff09\u3002\u5b9e\u73b0\u964d\u7ef4\u548c\u5347\u7ef4\u7684\u64cd\u4f5c\u5176\u5b9e\u5c31\u662f Channel \u95f4\u4fe1\u606f\u7684\u7ebf\u6027\u7ec4\u5408\u53d8\u5316\u3002\u6bd4\u5982\uff1a\u5728\u5c3a\u5bf8 3 * 3\uff0c64\u901a\u9053\u4e2a\u6570\u7684\u5377\u79ef\u6838\u540e\u9762\u6dfb\u52a0\u4e00\u4e2a\u5c3a\u5bf81 * 1\uff0c28\u901a\u9053\u4e2a\u6570\u7684\u5377\u79ef\u6838\uff0c\u5c31\u53d8\u6210\u4e86\u5c3a\u5bf83 * 3\uff0c28\u5c3a\u5bf8\u7684\u5377\u79ef\u6838\u3002 </p> <p>\u539f\u6765\u768464\u4e2a\u901a\u9053\u5c31\u53ef\u4ee5\u7406\u89e3\u4e3a\u8de8\u901a\u9053\u7ebf\u6027\u7ec4\u5408\u53d8\u6210\u4e8628\u901a\u9053\uff0c\u8fd9\u5c31\u662f\u901a\u9053\u95f4\u7684\u4fe1\u606f\u4ea4\u4e92\u3002</p> </li> </ul> <p></p> <p>Global Average Pooling(GAP)</p> <p>\u5728\u6700\u540e\u4e00\u4e2amlpconv\u5c42\u4e2d\u4e3a\u5206\u7c7b\u4efb\u52a1\u7684\u6bcf\u4e2a\u5bf9\u5e94\u7c7b\u522b\u751f\u6210\u4e00\u4e2a\u7279\u5f81\u56fe\uff0c\u901a\u8fc7GAP\u5c42\u53d6\u6bcf\u4e2a\u7279\u5f81\u56fe\u7684\u7a7a\u95f4\u5e73\u5747\u503c\uff0c\u5c06\u7ed3\u679c\u5411\u91cf\u76f4\u63a5\u4f20\u5165\u5230softmax\u5c42\u3002\uff08\u4f46\u662f\u4f20\u7edf\u7684\u505a\u6cd5\u90fd\u662f\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u5f97\u5230\u7684\u7279\u5f81\u56fe\u540e\u9762\u8ddf\u4e0aFC\u5c42\uff09</p> <p>GAP\u8f83FC\u7684\u4f18\u70b9\uff1a - \u5f3a\u5236\u7279\u5f81\u56fe\u548c\u7c7b\u522b\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u66f4\u9002\u5408\u5377\u79ef\u7ed3\u6784\u3002</p> <ul> <li> <p>GAP\u4e2d\u6ca1\u6709\u8981\u4f18\u5316\u7684\u53c2\u6570\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\uff0c\u907f\u514d\u4e86\u8fc7\u62df\u5408</p> </li> <li> <p>GAP\u6c47\u603b\u4e86\u7a7a\u95f4\u4fe1\u606f\uff0c\u5bf9\u8f93\u5165\u7684\u7a7a\u95f4\u8f6c\u5316\u66f4\u52a0\u7a33\u5b9a\u3002</p> </li> <li> <p>FC\u8f93\u5165\u7684\u5927\u5c0f\u5fc5\u987b\u56fa\u5b9a\uff0c\u800cGAP\u5bf9\u7f51\u7edc\u8f93\u5165\u7684\u56fe\u50cf\u5c3a\u5bf8\u6ca1\u6709\u56fa\u5b9a\u8981\u6c42\u3002</p> </li> </ul> <p>\u5728\u4f7f\u7528 GAP \u65f6 feature map \u6570\u8981\u7b49\u4e8e\u8981\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u7136\u540e\u518d\u5bf9\u6bcf\u4e00\u4e2a feature map \u6c42\u5747\u503c\uff0c\u9001\u5230softmax\u4e2d\u3002</p> <p></p>"},{"location":"python/d2l-chapter/chapter2/#googlenet","title":"GoogLeNet","text":"<p>GoogLeNet</p> <p> \u76f8\u5173\u8bba\u6587\uff1aGoing Deeper with Convolutions <p>GoogLeNet\u5438\u6536\u4e86NiN\u4e2d\u4e32\u8054\u7f51\u7edc\u7684\u601d\u60f3\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u505a\u4e86\u6539\u8fdb\u3002 \u8fd9\u7bc7\u8bba\u6587\u7684\u4e00\u4e2a\u91cd\u70b9\u662f\u89e3\u51b3\u4e86\u4ec0\u4e48\u6837\u5927\u5c0f\u7684\u5377\u79ef\u6838\u6700\u5408\u9002\u7684\u95ee\u9898\u3002 \u6bd5\u7adf\uff0c\u4ee5\u524d\u6d41\u884c\u7684\u7f51\u7edc\u4f7f\u7528\u5c0f\u52301\u00d71\uff0c\u5927\u523011\u00d711\u7684\u5377\u79ef\u6838\u3002 </p> <p>\u672c\u6587\u7684\u4e00\u4e2a\u89c2\u70b9\u662f\uff0c\u6709\u65f6\u4f7f\u7528\u4e0d\u540c\u5927\u5c0f\u7684\u5377\u79ef\u6838\u7ec4\u5408\u662f\u6709\u5229\u7684\u3002</p> <p>\u5927\u91cf\u7684\u6587\u732e\u8868\u660e\u53ef\u4ee5\u5c06\u7a00\u758f\u77e9\u9635\u805a\u7c7b\u4e3a\u8f83\u4e3a\u5bc6\u96c6\u7684\u5b50\u77e9\u9635\u6765\u63d0\u9ad8\u8ba1\u7b97\u6027\u80fd\uff0c\u636e\u6b64\u8bba\u6587\u63d0\u51fa\u4e86\u540d\u4e3aInception\u7684\u7ed3\u6784\u6765\u5b9e\u73b0\u6b64\u76ee\u7684\uff0c\u65e2\u80fd\u4fdd\u6301\u7f51\u7edc\u7ed3\u6784\u7684\u7a00\u758f\u6027\uff0c\u53c8\u80fd\u5229\u7528\u5bc6\u96c6\u77e9\u9635\u7684\u9ad8\u8ba1\u7b97\u6027\u80fd\u3002</p> <p></p> <p>Inception\u7ed3\u6784</p> <p></p> <p>Inception\u5757\u7531\u56db\u6761\u5e76\u884c\u8def\u5f84\u7ec4\u6210:</p> <ul> <li> <p>\u524d\u4e09\u6761\u8def\u5f84\u4f7f\u7528\u7a97\u53e3\u5927\u5c0f\u4e3a1\u00d71\u30013\u00d73\u548c5\u00d75\u7684\u5377\u79ef\u5c42\uff0c\u4ece\u4e0d\u540c\u7a7a\u95f4\u5927\u5c0f\u4e2d\u63d0\u53d6\u4fe1\u606f\u3002</p> </li> <li> <p>\u4e2d\u95f4\u7684\u4e24\u6761\u8def\u5f84\u5728\u8f93\u5165\u4e0a\u6267\u884c1\u00d71\u5377\u79ef\uff0c\u4ee5\u51cf\u5c11\u901a\u9053\u6570\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u7684\u590d\u6742\u6027\u3002 </p> </li> <li> <p>\u7b2c\u56db\u6761\u8def\u5f84\u4f7f\u75283\u00d73\u6700\u5927\u6c47\u805a\u5c42\uff0c\u7136\u540e\u4f7f\u75281\u00d71\u5377\u79ef\u5c42\u6765\u6539\u53d8\u901a\u9053\u6570\u3002 </p> </li> </ul> <p>\u8fd9\u56db\u6761\u8def\u5f84\u90fd\u4f7f\u7528\u5408\u9002\u7684\u586b\u5145\u6765\u4f7f\u8f93\u5165\u4e0e\u8f93\u51fa\u7684\u9ad8\u548c\u5bbd\u4e00\u81f4\uff0c\u6700\u540e\u6211\u4eec\u5c06\u6bcf\u6761\u7ebf\u8def\u7684\u8f93\u51fa\u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u8fde\u7ed3\uff0c\u5e76\u6784\u6210<code>Inception</code>\u5757\u7684\u8f93\u51fa\u3002\u5728<code>Inception</code>\u5757\u4e2d\uff0c\u901a\u5e38\u8c03\u6574\u7684\u8d85\u53c2\u6570\u662f\u6bcf\u5c42\u8f93\u51fa\u901a\u9053\u6570\u3002</p> <pre><code>class Inception(nn.Module):\n    # c1--c4\u662f\u6bcf\u6761\u8def\u5f84\u7684\u8f93\u51fa\u901a\u9053\u6570\n    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n        super(Inception, self).__init__(**kwargs)\n        # \u7ebf\u8def1\uff0c\u53551x1\u5377\u79ef\u5c42\n        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n        # \u7ebf\u8def2\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a53x3\u5377\u79ef\u5c42\n        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # \u7ebf\u8def3\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a55x5\u5377\u79ef\u5c42\n        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # \u7ebf\u8def4\uff0c3x3\u6700\u5927\u6c47\u805a\u5c42\u540e\u63a51x1\u5377\u79ef\u5c42\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n\n    def forward(self, x):\n        p1 = F.relu(self.p1_1(x))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n        p4 = F.relu(self.p4_2(self.p4_1(x)))\n        # \u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u8fde\u7ed3\u8f93\u51fa\n        return torch.cat((p1, p2, p3, p4), dim=1)\n</code></pre> <p>GoogLeNet\u4e00\u5171\u4f7f\u75289\u4e2a<code>Inception</code>\u5757\u548c\u5168\u5c40\u5e73\u5747\u6c47\u805a\u5c42\u7684\u5806\u53e0\u6765\u751f\u6210\u5176\u4f30\u8ba1\u503c\u3002<code>Inception</code>\u5757\u4e4b\u95f4\u7684\u6700\u5927\u6c47\u805a\u5c42\u53ef\u964d\u4f4e\u7ef4\u5ea6\u3002 \u7b2c\u4e00\u4e2a\u6a21\u5757\u7c7b\u4f3c\u4e8eAlexNet\u548cLeNet\uff0c<code>Inception</code>\u5757\u7684\u7ec4\u5408\u4eceVGG\u7ee7\u627f\uff0c\u5168\u5c40\u5e73\u5747\u6c47\u805a\u5c42\u907f\u514d\u4e86\u5728\u6700\u540e\u4f7f\u7528\u5168\u8fde\u63a5\u5c42</p> <p></p>"},{"location":"python/d2l-chapter/chapter2/#\u6279\u91cf\u89c4\u8303\u5316batch-normalization","title":"\u6279\u91cf\u89c4\u8303\u5316\uff08Batch Normalization\uff09","text":"<p>\u7528\\(\\mathbf{x} \\in \\mathcal{B}\\)\u8868\u793a\u4e00\u4e2a\u6765\u81ea\u5c0f\u6279\u91cf\\(\\mathcal{B}\\)\u7684\u8f93\u5165\uff0c\u6279\u91cf\u89c4\u8303\u5316\u6839\u636e\u4ee5\u4e0b\u8868\u8fbe\u5f0f\u8f6c\u6362</p> \\[     \\mathrm{BatchNorm}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}     \\] <p>\\(\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}\\)\u662f\u5c0f\u6279\u91cf\u7684\u6837\u672c\u5747\u503c\uff0c\\(\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}\\)\u662f\u5c0f\u6279\u91cf\u7684\u6837\u672c\u6807\u51c6\u5dee,\u901a\u5e38\u8fd8\u5305\u542b\u62c9\u4f38\u53c2\u6570\uff08scale\uff09\\(\\boldsymbol{\\gamma}\\)\u548c\u504f\u79fb\u53c2\u6570\uff08shift\uff09\\(\\boldsymbol{\\beta},\u8fd9\u4e24\u4e2a\u53c2\u6570\u5c06\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u66f4\u65b0\\)\u3002</p> \\[     \\begin{split}     \\begin{aligned}      \\hat{\\boldsymbol{\\mu}}_\\mathcal{B} &amp;= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x},\\\\     \\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}^2 &amp;= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}})^2 + \\epsilon.     \\end{aligned}     \\end{split}     \\] <p>\u7531\u4e8e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e2d\u95f4\u5c42\u7684\u53d8\u5316\u5e45\u5ea6\u4e0d\u80fd\u8fc7\u4e8e\u5267\u70c8\uff0c\u800c\u6279\u91cf\u89c4\u8303\u5316\u5c06\u6bcf\u4e00\u5c42\u4e3b\u52a8\u5c45\u4e2d\uff0c\u5e76\u5c06\u5b83\u4eec\u91cd\u65b0\u8c03\u6574\u4e3a\u7ed9\u5b9a\u7684\u5e73\u5747\u503c\u548c\u5927\u5c0f\u3002\u5728\u65b9\u5dee\u4f30\u8ba1\u503c\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5c0f\u7684\u5e38\u91cf\\(\\epsilon &gt; 0\\),\u4ee5\u786e\u4fdd\u6211\u4eec\u6c38\u8fdc\u4e0d\u4f1a\u5c1d\u8bd5\u9664\u4ee5\u96f6.</p> <p>\u53ea\u6709\u4f7f\u7528\u8db3\u591f\u5927\u7684\u5c0f\u6279\u91cf\uff0c\u6279\u91cf\u89c4\u8303\u5316\u8fd9\u79cd\u65b9\u6cd5\u624d\u662f\u6709\u6548\u4e14\u7a33\u5b9a\u7684\uff0c\u5728\u5e94\u7528\u6279\u91cf\u89c4\u8303\u5316\u65f6\uff0c\u6279\u91cf\u5927\u5c0f\u7684\u9009\u62e9\u53ef\u80fd\u6bd4\u6ca1\u6709\u6279\u91cf\u89c4\u8303\u5316\u65f6\u66f4\u91cd\u8981</p> <p>nn.BatchNorm2d()</p> <p> <code>nn.BatchNorm2d(num_features= , num_dims= ,eps=0.00001 ,momentum=0.1)</code> <ul> <li> <p>num_features\uff1a\u5b8c\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u6570\u91cf\u6216\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u3002</p> </li> <li> <p>num_dims\uff1a2\u8868\u793a\u5b8c\u5168\u8fde\u63a5\u5c42\uff0c4\u8868\u793a\u5377\u79ef\u5c42 </p> </li> </ul> <p>\u603b\u7ed3\uff1a</p> <ul> <li> <p>\u5728\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6279\u91cf\u89c4\u8303\u5316\u5229\u7528\u5c0f\u6279\u91cf\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\uff0c\u4e0d\u65ad\u8c03\u6574\u795e\u7ecf\u7f51\u7edc\u7684\u4e2d\u95f4\u8f93\u51fa\uff0c\u4f7f\u6574\u4e2a\u795e\u7ecf\u7f51\u7edc\u5404\u5c42\u7684\u4e2d\u95f4\u8f93\u51fa\u503c\u66f4\u52a0\u7a33\u5b9a\u3002</p> </li> <li> <p>\u6279\u91cf\u89c4\u8303\u5316\u5728\u5168\u8fde\u63a5\u5c42\u548c\u5377\u79ef\u5c42\u7684\u4f7f\u7528\u7565\u6709\u4e0d\u540c\u3002</p> </li> <li> <p>\u6279\u91cf\u89c4\u8303\u5316\u5c42\u548c\u6682\u9000\u5c42\u4e00\u6837\uff0c\u5728\u8bad\u7ec3\u6a21\u5f0f\u548c\u9884\u6d4b\u6a21\u5f0f\u4e0b\u8ba1\u7b97\u4e0d\u540c\u3002</p> </li> <li> <p>\u6279\u91cf\u89c4\u8303\u5316\u6709\u8bb8\u591a\u6709\u76ca\u7684\u526f\u4f5c\u7528\uff0c\u4e3b\u8981\u662f\u6b63\u5219\u5316\u3002</p> </li> </ul>"},{"location":"python/d2l-chapter/chapter2/#resnet-\u6b8b\u5dee\u7f51\u7edc","title":"ResNet \u6b8b\u5dee\u7f51\u7edc","text":"<p>ResNet</p> <p> \u76f8\u5173\u8bba\u6587\uff1aDeep Residual Learning for Image Recognition <p>\u4f55\u607a\u660e\u7b49\u4eba\u63d0\u51fa\u4e86\u6b8b\u5dee\u7f51\u7edc\uff08ResNet\uff09\u3002\u6b8b\u5dee\u7f51\u7edc\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u6bcf\u4e2a\u9644\u52a0\u5c42\u90fd\u5e94\u8be5\u66f4\u5bb9\u6613\u5730\u5305\u542b\u539f\u59cb\u51fd\u6570\u4f5c\u4e3a\u5176\u5143\u7d20\u4e4b\u4e00\u3002 \u4e8e\u662f\uff0c\u6b8b\u5dee\u5757\uff08residual blocks\uff09\u4fbf\u8bde\u751f\u4e86\uff0c\u8fd9\u4e2a\u8bbe\u8ba1\u5bf9\u5982\u4f55\u5efa\u7acb\u6df1\u5c42\u795e\u7ecf\u7f51\u7edc\u4ea7\u751f\u4e86\u6df1\u8fdc\u7684\u5f71\u54cd\u3002 </p> <p></p> <p></p>"},{"location":"python/d2l-chapter/chapter2/#\u6b8b\u5dee\u5757-residual-block","title":"\u6b8b\u5dee\u5757 residual block","text":"<p>residual\u7ed3\u6784\u4f7f\u7528\u4e86\u4e00\u79cdshortcut\u7684\u8fde\u63a5\u65b9\u5f0f\uff0c\u4e5f\u53ef\u7406\u89e3\u4e3a\u6377\u5f84\u3002\u8ba9\u7279\u5f81\u77e9\u9635\u9694\u5c42\u76f8\u52a0\uff0c\u6ce8\u610fF(X)\u548cX\u5f62\u72b6\u8981\u76f8\u540c\uff0c\u6240\u8c13\u76f8\u52a0\u662f\u7279\u5f81\u77e9\u9635\u76f8\u540c\u4f4d\u7f6e\u4e0a\u7684\u6570\u5b57\u8fdb\u884c\u76f8\u52a0\u3002</p> <p></p> <pre><code>class Residual(nn.Module):  \n    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, num_channels,\n                               kernel_size=3, padding=1, stride=strides)\n        self.conv2 = nn.Conv2d(num_channels, num_channels,\n                               kernel_size=3, padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(input_channels, num_channels,\n                                   kernel_size=1, stride=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        Y += X\n        return F.relu(Y)\n</code></pre> <p>\u4e24\u4e2aResidual\u7ed3\u6784\uff1a</p> <p></p> <p>\u5de6\u4fa7\u6b8b\u5dee\u7ed3\u6784\u79f0\u4e3a BasicBlock,\u53f3\u4fa7\u6b8b\u5dee\u7ed3\u6784\u4e3aBottleneck</p> <p>\u5bf9\u4e8e\u66f4\u6df1\u5c42\u6b21\u7684\u7f51\u7edc\uff0cResNet\u5f15\u5165\u4e86\u201cBottleNeck\u201d\u67b6\u6784\u6765\u964d\u4f4e\u6a21\u578b\u590d\u6742\u6027\uff0c\u5176\u4e2d\u7b2c\u4e00\u5c42\u76841\u00d71\u7684\u5377\u79ef\u6838\u7684\u4f5c\u7528\u662f\u5bf9\u7279\u5f81\u77e9\u9635\u8fdb\u884c\u964d\u7ef4\u64cd\u4f5c\uff0c\u5c06\u7279\u5f81\u77e9\u9635\u7684\u6df1\u5ea6\u7531256\u964d\u4e3a64;\u7b2c\u4e09\u5c42\u76841\u00d71\u7684\u5377\u79ef\u6838\u662f\u5bf9\u7279\u5f81\u77e9\u9635\u8fdb\u884c\u5347\u7ef4\u64cd\u4f5c\uff0c\u5c06\u7279\u5f81\u77e9\u9635\u7684\u6df1\u5ea6\u753164\u5347\u6210256\u3002\u964d\u4f4e\u7279\u5f81\u77e9\u9635\u7684\u6df1\u5ea6\u4e3b\u8981\u662f\u4e3a\u4e86\u51cf\u5c11\u53c2\u6570\u7684\u4e2a\u6570\u3002</p>"},{"location":"python/d2l-chapter/chapter2/#densenet-\u7a20\u5bc6\u8fde\u63a5\u7f51\u7edc","title":"DenseNet \u7a20\u5bc6\u8fde\u63a5\u7f51\u7edc","text":"<p>ResNet\u63d0\u51fa\u4e86\u6052\u7b49\u6620\u5c04\uff08 identity mapping\uff09,\u4f7f\u7528\u4e86\u5143\u7d20\u52a0\u6cd5(Element-wise addition)\u3002\u6709\u52a9\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u4ece\u800c\u80fd\u591f\u8bad\u7ec3\u51fa\u66f4\u6df1\u7684\u7f51\u7edc\u3002</p> <p>\u800c\u5728DenseNet\u4e2d\uff1a \u91c7\u7528\u4e86\u5bc6\u96c6\u8fde\u63a5\u7684\u673a\u5236\uff0c\u5373\u76f8\u4e92\u8fde\u63a5\u6240\u6709\u7684\u5c42\uff0c\u6bcf\u4e2a\u5c42\u90fd\u4f1a\u4e0e\u524d\u9762\u6240\u6709\u7684\u5c42\u5728\u901a\u9053\u7684\u7ef4\u5ea6\u4e0a\u8fde\u63a5\u5728\u4e00\u8d77\uff08Channel-wise concatenation\uff09, \u5b9e\u73b0\u7279\u5f81\u91cd\u7528\uff0c\u4f5c\u4e3a\u4e0b\u4e00\u5c42\u7684\u8f93\u5165\u3002\u8fd9\u6837\u4e0d\u4f46\u51cf\u7f13\u4e86\u68af\u5ea6\u6d88\u5931\u7684\u73b0\u8c61\uff0c\u4e5f\u4f7f\u5176\u53ef\u4ee5\u5728\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6bd4ResNet\u66f4\u4f18\u7684\u6027\u80fd\u3002</p> <p>\u5982ResNet\u5c06\u51fd\u6570\u5c55\u5f00\u4e3a\u4e00\u4e2a\u7b80\u5355\u7684\u7ebf\u6027\u9879\u548c\u4e00\u4e2a\u590d\u6742\u7684\u975e\u7ebf\u6027\u9879\uff1a</p> \\[     f(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x})    \\] <p>DenseNet\u7684\u8fde\u63a5\u4e0d\u662fResnet\u7684\u7b80\u5355\u76f8\u52a0\uff0c\u800c\u662f\u4e00\u79cd\u7a20\u5bc6\u7684\u8fde\u63a5\uff1a</p> <p></p> <p></p> <p>DenseNet\u4f7f\u7528\u4e86ResNet\u6539\u826f\u7248\u7684\u201c\u6279\u91cf\u89c4\u8303\u5316\u3001\u6fc0\u6d3b\u548c\u5377\u79ef\u201d\u67b6\u6784\uff08BN-ReLU-Conv\uff09\uff0c\u8fd9\u79cd\u65b9\u5f0f\u4e5f\u88ab\u79f0\u4e3apre-activation\u3002\u7f51\u7edc\u4e3b\u8981\u75312\u90e8\u5206\u6784\u6210\uff1a\u7a20\u5bc6\u5757\uff08dense block\uff09\u548c\u8fc7\u6e21\u5c42\uff08transition layer\uff09\u3002 \u524d\u8005\u5b9a\u4e49\u5982\u4f55\u8fde\u63a5\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u800c\u540e\u8005\u5219\u63a7\u5236\u901a\u9053\u6570\u91cf</p> <pre><code>class DenseBlock(nn.Module):\n    def __init__(self, num_convs, input_channels, num_channels):\n        super(DenseBlock, self).__init__()\n        layer = []\n        for i in range(num_convs):\n            layer.append(conv_block(num_channels * i + input_channels, num_channels))\n        self.net = nn.Sequential(*layer)\n\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            # \u8fde\u63a5\u901a\u9053\u7ef4\u5ea6\u4e0a\u6bcf\u4e2a\u5757\u7684\u8f93\u5165\u548c\u8f93\u51fa\n            X = torch.cat((X, Y), dim=1)\n        return X \n\n    def conv_block(input_channels, num_channels):\n        return nn.Sequential(\n            nn.BatchNorm2d(input_channels), \n            nn.ReLU(),\n            nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1)\n            )\n</code></pre> <p>\u5728\u6bcf\u4e2adense block\u4e2d\uff0c\u7279\u5f81\u6620\u5c04\u7684\u5927\u5c0f\u662f\u76f8\u540c\u7684\uff0c\u56e0\u6b64\u5b83\u4eec\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730concat\u5728\u4e00\u8d77\u3002\u5c42\uff08dense layer\uff09\u4e0e\u5c42\u4e4b\u95f4\u91c7\u7528\u7684\u662f\u5bc6\u96c6\u8fde\u63a5\u7684\u65b9\u5f0f\u3002</p> <p>\u5047\u5b9a\u8f93\u5165\u5c42\u7684\u901a\u9053\u6570\u662f\\(k_0\\)\uff0c<code>DenseBlock</code>\u4e2d\u5404\u4e2a\u5c42\u5377\u79ef\u4e4b\u540e\u5747\u8f93\u51fa\\(k\\)\u4e2a\u7279\u5f81\u56fe\uff0c\u5373\u5f97\u5230\u7684\u7279\u5f81\u56fe\u7684\u901a\u9053\u6570\u4e3a\\(k\\)\uff0c\u90a3\u4e48\\(l\\)\u5c42\u8f93\u5165\u7684\u901a\u9053\u6570\u4e3a\\(k_0\u200b+(l\u22121)k\\) ,\u6211\u4eec\u5c06k\u79f0\u4e3a\u7f51\u7edc\u7684\u589e\u957f\u7387\uff08growth rate\uff09\u3002\u63d0\u53d6\u51fa\u7684k\u4e2a\u901a\u9053\u5373\u4e3a\u4ece\u65b0\u5c42\u4e2d\u63d0\u53d6\u51fa\u7684\u7279\u5f81\u3002</p> <p>Transition Layer\u662f\u4e24\u4e2a\u76f8\u90bbdense block\u4e4b\u95f4\u7684\u8fc7\u6e21\u5c42\uff08BN+ReLU+1x1Conv+2x2AvgPooling\uff09\uff0c\u5e76\u4e14\u964d\u4f4e\u7279\u5f81\u56fe\u7684\u5927\u5c0f\uff0c\u5728\u6700\u540e\u4e00\u4e2adense block\u7684\u672b\u5c3e\uff0c\u6267\u884c\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff0c\u7136\u540e\u9644\u52a0\u4e00\u4e2asoftmax\u5206\u7c7b\u5668\u3002</p> <pre><code>def transition_block(input_channels, num_channels):\n    return nn.Sequential(\n        nn.BatchNorm2d(input_channels), \n        nn.ReLU(),\n        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n        nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n</code></pre> <p>Denseblock \u4f18\u70b9\uff1a</p> <ul> <li> <p>\u66f4\u5f3a\u7684\u68af\u5ea6\u6d41\u52a8</p> <p>\u2460\u7531\u4e8e\u5bc6\u96c6\u8fde\u63a5\u65b9\u5f0f\uff0cDenseNet\u4fc3\u8fdb\u4e86\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u4f7f\u5f97\u7f51\u7edc\u66f4\u5bb9\u6613\u8bad\u7ec3\u3002\u7531\u4e8e\u6bcf\u5c42\u53ef\u4ee5\u76f4\u8fbe\u6700\u540e\u7684\u8bef\u5dee\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u9690\u5f0f\u7684\u201cdeep supervision\u201d\u3002\u8bef\u5dee\u4fe1\u53f7\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u4f20\u64ad\u5230\u8f83\u65e9\u7684\u5c42\uff0c\u6240\u4ee5\u8f83\u65e9\u7684\u5c42\u53ef\u4ee5\u4ece\u6700\u7ec8\u5206\u7c7b\u5c42\u83b7\u5f97\u76f4\u63a5\u76d1\u7763\u3002</p> <p>\u2461\u51cf\u8f7b\u4e86vanishing-gradient\uff08\u68af\u5ea6\u6d88\u5931\uff09 \u8fc7\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u5728\u7f51\u7edc\u6df1\u5ea6\u8d8a\u6df1\u7684\u65f6\u5019\u8d8a\u5bb9\u6613\u51fa\u73b0\uff0c\u539f\u56e0\u5c31\u662f\u8f93\u5165\u4fe1\u606f\u548c\u68af\u5ea6\u4fe1\u606f\u5728\u5f88\u591a\u5c42\u4e4b\u95f4\u4f20\u9012\u5bfc\u81f4\u7684\uff0c\u800c\u73b0\u5728\u8fd9\u79cddense connection\u76f8\u5f53\u4e8e\u6bcf\u4e00\u5c42\u90fd\u76f4\u63a5\u8fde\u63a5input\u548closs\uff0c\u56e0\u6b64\u5c31\u53ef\u4ee5\u51cf\u8f7b\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\uff0c\u8fd9\u6837\u66f4\u6df1\u7f51\u7edc\u4e0d\u662f\u95ee\u9898\u3002</p> </li> <li> <p>\u51cf\u5c11\u53c2\u6570\u4e0e\u63d0\u5347\u8ba1\u7b97\u6548\u7387</p> </li> <li> <p>\u66f4\u591a\u6837\u5316\u7684\u7279\u5f81\uff08\u4fdd\u5b58\u4e86\u4f4e\u7eac\u5ea6\u7684\u7279\u5f81\uff09  </p> <p>DenseNet\u4e2d\u7684\u6bcf\u4e00\u5c42\u90fd\u63a5\u6536\u524d\u9762\u6240\u6709\u7684\u5c42\u4f5c\u4e3a\u8f93\u5165\uff0c\u56e0\u6b64\u7279\u5f81\u66f4\u52a0\u591a\u6837\u5316\uff0c\u503e\u5411\u4e8e\u7ed9\u51fa\u66f4\u5e73\u6ed1\u7684\u51b3\u7b56\u8fb9\u754c\u3002</p> </li> </ul>"},{"location":"python/d2l-chapter/chapter3/","title":"\ud83d\udd17Chapter 3\uff1aRecurrent neural network","text":""},{"location":"python/d2l-chapter/chapter3/#\u81ea\u56de\u5f52\u6a21\u578b-autoregressive-models","title":"\u81ea\u56de\u5f52\u6a21\u578b autoregressive models","text":"<p>\u81ea\u56de\u5f52\u6a21\u578b\u662f\u7edf\u8ba1\u4e0a\u4e00\u79cd\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u7684\u65b9\u6cd5\uff0c\u7528\u540c\u4e00\u53d8\u6570\u4f8b\u5982\\(x\\)\u7684\u4e4b\u524d\u5404\u671f\uff0c\u4ea6\u5373\\(x_1\\)\u81f3\\(x_{t-1}\\)\u6765\u9884\u6d4b\u672c\u671f\\(x_t\\)\u7684\u8868\u73b0\uff0c\u5e76\u5047\u8bbe\u5b83\u4eec\u4e3a\u7ebf\u6027\u5173\u7cfb\u3002\u56e0\u4e3a\u8fd9\u662f\u4ece\u56de\u5f52\u5206\u6790\u4e2d\u7684\u7ebf\u6027\u56de\u5f52\u53d1\u5c55\u800c\u6765\uff0c\u53ea\u662f\u4e0d\u7528\\(x\\)\u9884\u6d4b\\(y\\)\uff0c\u800c\u662f\u7528\\(x\\)\u9884\u6d4b\\(x\\)\uff08\u81ea\u5df1\uff09\uff1b\u6240\u4ee5\u53eb\u505a\u81ea\u56de\u5f52\u3002</p> \\[     X_t = \\sum_{i=1}^p \\phi_i X_{t-i} + c + \\epsilon_t         \\] <p>\u7b2c\u4e8c\u79cd\u5982\u4e0b\u56fe\uff0c\u662f\u4fdd\u7559\u4e00\u4e9b\u5bf9\u8fc7\u53bb\u89c2\u6d4b\u7684\u603b\u7ed3\\(h_t\\), \u5e76\u4e14\u540c\u65f6\u66f4\u65b0\u9884\u6d4b\\(\\hat{x}_t\\)\u548c\u603b\u7ed3\\(h_t\\)\u3002\u5373\u4e3a\u57fa\u4e8e\\hat{x}t = P(x_t \\mid h)\\(\u6765\u4f30\u8ba1\\)x_t\\(\uff0c\u4ee5\u53ca\u901a\u8fc7\\)h_t = g(h_{t-1}, x_{t-1})\\(\u66f4\u65b0\u603b\u7ed3\\)h_t$\u7684\u6a21\u578b\uff0c\u79f0\u4e3a\u9690\u53d8\u91cf\u81ea\u56de\u5f52\u6a21\u578b\uff08latent autoregressive models\uff09\u3002</p> <p></p>"},{"location":"python/d2l-chapter/chapter3/#\u8bcd\u5143\u5316","title":"\u8bcd\u5143\u5316","text":"<p>\u8bcd\u5143\uff08token\uff09\u662f\u6587\u672c\u7684\u57fa\u672c\u5355\u4f4d\uff0c</p> <pre><code>    def tokenize(text, token='word'):\n        if token == 'word':\n            return text.split()\n        elif token == 'char':\n            return list(text)\n        else:\n            raise ValueError('Unknown token type: ' + token)\n</code></pre>"},{"location":"python/d2l-chapter/chapter3/#\u8bcd\u8868vocabulary","title":"\u8bcd\u8868(vocabulary)","text":"<p>\u8bcd\u8868\u662f\u8bcd\u5143\u7684\u96c6\u5408\uff0c\u8bcd\u8868\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u5143\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684\u7d22\u5f15\u3002</p> <p>\u6211\u4eec\u5148\u5c06\u8bad\u7ec3\u96c6\u4e2d\u7684\u6240\u6709\u6587\u6863\u5408\u5e76\u5728\u4e00\u8d77\uff0c\u5bf9\u5b83\u4eec\u7684\u552f\u4e00\u8bcd\u5143\u8fdb\u884c\u7edf\u8ba1\uff0c\u5f97\u5230\u7684\u7edf\u8ba1\u7ed3\u679c\u79f0\u4e4b\u4e3a\u8bed\u6599\uff08corpus\uff09\u3002 \u7136\u540e\u6839\u636e\u6bcf\u4e2a\u552f\u4e00\u8bcd\u5143\u7684\u51fa\u73b0\u9891\u7387\uff0c\u4e3a\u5176\u5206\u914d\u4e00\u4e2a\u6570\u5b57\u7d22\u5f15\u3002\u53e6\u5916\uff0c\u8bed\u6599\u5e93\u4e2d\u4e0d\u5b58\u5728\u6216\u5df2\u5220\u9664\u7684\u4efb\u4f55\u8bcd\u5143\u90fd\u5c06\u6620\u5c04\u5230\u4e00\u4e2a\u7279\u5b9a\u7684\u672a\u77e5\u8bcd\u5143\u201c\u201d\u3002 \u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u589e\u52a0\u4e00\u4e2a\u5217\u8868\uff0c\u7528\u4e8e\u4fdd\u5b58\u90a3\u4e9b\u88ab\u4fdd\u7559\u7684\u8bcd\u5143\uff0c \u4f8b\u5982\uff1a\u586b\u5145\u8bcd\u5143\uff08\u201c\u201d\uff09\uff1b \u5e8f\u5217\u5f00\u59cb\u8bcd\u5143\uff08\u201c\u201d\uff09\uff1b \u5e8f\u5217\u7ed3\u675f\u8bcd\u5143\uff08\u201c\u201d\uff09\u3002 <pre><code>class Vocab(object):\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        # \u6309\u51fa\u73b0\u9891\u7387\u6392\u5e8f\n        counter = count_corpus(tokens)\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],reverse=True)\n        # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0\n        self.idx_to_token = ['&lt;unk&gt;'] + reserved_tokens\n        self.token_to_idx = {token: idx\n                             for idx, token in enumerate(self.idx_to_token)}\n        for token, freq in self._token_freqs:\n            if freq &lt; min_freq:\n                break\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\n    @property\n    def unk(self):  # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0\n        return 0\n\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n\ndef count_corpus(tokens):  #@save\n    \"\"\"\u7edf\u8ba1\u8bcd\u5143\u7684\u9891\u7387\"\"\"\n    # \u8fd9\u91cc\u7684tokens\u662f1D\u5217\u8868\u62162D\u5217\u8868\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        # \u5c06\u8bcd\u5143\u5217\u8868\u5c55\u5e73\u6210\u4e00\u4e2a\u5217\u8868\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)\n</code></pre>"},{"location":"python/d2l-chapter/chapter3/#\u5faa\u73af\u795e\u7ecf\u7f51\u7edc","title":"\u5faa\u73af\u795e\u7ecf\u7f51\u7edc","text":"<p>\u5047\u8bbe\u5728\u65f6\u95f4\u6b65\\(t\\)\u6709\u5c0f\u6279\u91cf\u8f93\u5165\\(\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}\\)\uff0c\u7528\\(\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}\\)\u8868\u793a\u65f6\u95f4\u6b65\\(t\\)\u7684\u9690\u85cf\u53d8\u91cf\u3002\u5bf9\u4e8eRNN\u7f51\u7edc\uff0c\u6211\u4eec\u4fdd\u5b58\u4e0a\u4e00\u6b65\u7684\u9690\u85cf\u53d8\u91cf\\(\\mathbf{H}_{t-1}\\)\uff0c\u5f15\u5165\u4e00\u4e2a\u65b0\u7684\u6743\u91cd\\(\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}\\)\uff0c\u5219\u6709\uff1a</p> \\[     \\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)     \\] <p>\u5728RNN\u4e4b\u524d\u7684\u7f51\u7edc\u6709\u4e00\u4e2a\u4e3b\u8981\u7279\u5f81\uff1a\u6ca1\u6709\u8bb0\u5fc6\u80fd\u529b\uff0c\u5373\u65e0\u6cd5\u4fdd\u5b58\u4e4b\u524d\u65f6\u95f4\u6b65\u7684\u4fe1\u606f\u3002\u90fd\u662f\u5bf9\u6bcf\u4e2a\u8f93\u5165\u5355\u72ec\u5904\u7406\uff0c\u5728\u8f93\u5165\u4e4b\u524d\u6ca1\u6709\u4fdd\u5b58\u4efb\u4f55\u72b6\u6001\u3002</p> <p><code>nn.RNN()</code></p> <p> <code>nn.RNN(input_size=, hidden_size=, num_layers=, )</code> <ul> <li> <p><code>input_size (int)</code>\uff1a\u8f93\u5165\u6570\u636e\u7684\u7279\u5f81\u5927\u5c0f(\u7279\u5f81\u7ef4\u5ea6)\u3002\u5373\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u5411\u91cf\\(x_t\\)\u7684\u7ef4\u5ea6\u3002</p> </li> <li> <p><code>hidden_size (int)</code>\uff1a\u9690\u85cf\u5c42\u7684\u7279\u5f81\u5927\u5c0f\uff0c\u5373\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u85cf\u72b6\u6001\u5411\u91cfht\u7684\u7ef4\u5ea6\u3002\u5b83\u51b3\u5b9a\u4e86\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u548c\u8bb0\u5fc6\u80fd\u529b\u3002\u8f83\u5927\u7684<code>hidden_size</code>\u901a\u5e38\u5141\u8bb8\u6a21\u578b\u5b66\u4e60\u66f4\u590d\u6742\u7684\u6a21\u5f0f\uff0c\u4f46\u4e5f\u9700\u8981\u66f4\u591a\u7684\u8ba1\u7b97\u8d44\u6e90\u3002</p> </li> <li> <p><code>num_layers (int)</code>:RNN\u7684\u5c42\u6570\uff0c\u7528\u4e8e\u5806\u53e0\u591a\u4e2aRNN\u5c42\uff0c\u9ed8\u8ba4\u503c\u4e3a1\u3002\u5f53\u5c42\u6570\u5927\u4e8e1\u65f6\uff0cRNN\u4f1a\u53d8\u4e3a\u591a\u5c42RNN\u3002\u591a\u5c42RNN\u53ef\u4ee5\u6355\u6349\u66f4\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u4e5f\u4f1a\u589e\u52a0\u6a21\u578b\u7684\u590d\u6742\u6027\u3002</p> </li> <li> <p><code>nonlinearity (str)</code>:\u6307\u5b9a\u6fc0\u6d3b\u51fd\u6570\uff0c\u9ed8\u8ba4\u503c\u4e3a'tanh'\u3002\u53ef\u9009\u503c\u6709'tanh'\u548c'relu'\u3002</p> </li> <li> <p><code>bias (bool)</code>:\u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u5219\u5728RNN\u4e2d\u6dfb\u52a0\u504f\u7f6e\u9879\u3002\u9ed8\u8ba4\u503c\u4e3aTrue\u3002\u504f\u5dee\u9879\u901a\u5e38\u6709\u52a9\u4e8e\u6a21\u578b\u66f4\u597d\u5730\u62df\u5408\u6570\u636e\u3002</p> </li> <li> <p><code>dropout (float)</code>:\u5982\u679c\u975e\u96f6\uff0c\u5219\u5728\u9664\u6700\u540e\u4e00\u5c42\u4e4b\u5916\u7684\u6bcf\u4e2aRNN\u5c42\u4e4b\u95f4\u6dfb\u52a0dropout\u5c42\uff0c\u5176\u4e22\u5f03\u6982\u7387\u4e3adropout\u3002\u9ed8\u8ba4\u503c\u4e3a0\u3002\u8fd9\u6709\u52a9\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\u3002</p> </li> <li> <p><code>bidirectional (bool)</code>:\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u786e\u5b9a\u662f\u5426\u4f7f\u7528\u53cc\u5411RNN\u3002\u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0cRNN\u5c06\u540c\u65f6\u5728\u65f6\u95f4\u6b65\u7684\u6b63\u5411\u548c\u53cd\u5411\u65b9\u5411\u4e0a\u8fd0\u884c\uff08\u5219\u4f7f\u7528\u53cc\u5411RNN\uff09\uff0c\u4ee5\u6355\u6349\u524d\u540e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u9ed8\u8ba4\u503c\u4e3aFalse\u3002</p> </li> </ul> <p></p>"},{"location":"python/python-something/fluent-python/","title":"\ud83d\udee3 \u300a\u6d41\u7545\u7684Python\u300b","text":""},{"location":"python/python-something/fluent-python/#__getitem__-\u4e0e-__len__","title":"<code>__getitem__</code> \u4e0e <code>__len__</code>","text":"<p>\u9b54\u6cd5\u65b9\u6cd5</p> <p> \u9b54\u6cd5\u65b9\u6cd5(magic method)\u662fPython\u4e2d\u5177\u6709\u53cc\u4e0b\u5212\u7ebf\u5f00\u5934\u548c\u7ed3\u5c3e\u7684\u7279\u6b8a\u65b9\u6cd5\uff0c\u56e0\u6b64\u88ab\u79f0\u4e3a\u201c\u53cc\u4e0b\u5212\u7ebf\u65b9\u6cd5\u201d\u3002 </p> <ul> <li> <p><code>__getitem__</code>\u65b9\u6cd5\uff1a\u5141\u8bb8\u6211\u4eec\u4f7f\u7528\u65b9\u62ec\u53f7[]\u8868\u793a\u6cd5\u6765\u5b9a\u4e49\u8bbf\u95ee\u81ea\u5b9a\u4e49\u7c7b\u7684\u5143\u7d20\u7684\u65b9\u6cd5\uff0c\u7c7b\u4f3c\u4e8e\u6211\u4eec\u5728\u5217\u8868\u3001\u5b57\u5178\u6240\u505a\u7684\u64cd\u4f5c:</p> <p><code>element = MyInstance[index]</code></p> </li> <li> <p><code>__len__</code>\u65b9\u6cd5\uff1a\u7528\u4e8e\u8fd4\u56de\u5bf9\u8c61\u7684\u957f\u5ea6\uff0c\u5f53\u6211\u4eec\u4f7f\u7528\u5185\u7f6e\u7684len()\u51fd\u6570\u5bf9\u4e00\u4e2a\u5bf9\u8c61\u8fdb\u884c\u957f\u5ea6\u5224\u65ad\u65f6\uff0c\u5b9e\u9645\u4e0a\u662f\u8c03\u7528\u4e86\u8be5\u5bf9\u8c61\u7684<code>__len__()</code>\u65b9\u6cd5</p> </li> </ul> <p>\u4f7f\u7528\u4e0a\u8ff0\u65b9\u6cd5\u5b9e\u73b0\u4e00\u7ec4\u7eb8\u724c\u7c7b\uff08Card\uff09</p> <pre><code>import collections \n\nCard = collections.namedtuple('Card', ['rank', 'suit'])\n# \u521b\u5efa\u5177\u6709\u547d\u540d\u5b57\u6bb5\u7684 tuple \u5b50\u7c7b\u7684 factory \u51fd\u6570 (\u5177\u540d\u5143\u7ec4)\n\nclass FrenchDeck:\n    ranks = [str(n) for n in range(2, 11)] + list('JQKA')\n    suits = 'spades diamonds clubs hearts'.split()\n    def __init__(self):\n        self._cards = [Card(rank, suit) for suit in self.suits\n                                        for rank in self.ranks]\n\n    def __len__(self):\n        return len(self._cards)\n\n    def __getitem__(self, position):\n        return self._cards[position]\n\ndeck = FrenchDeck() #\u5b9e\u4f8b\u5316\n</code></pre> <p><code>__getitem__</code> \u4e0e <code>__len__</code>\u5b9e\u73b0\u4e86<code>FrenchDeck</code>\u7c7b\u7684\u8fed\u4ee3\u3001\u5207\u7247\u3001len()\u7b49\u64cd\u4f5c\u3002\u5b9e\u73b0 <code>__len__</code> \u548c <code>__getitem__</code> \u4e24\u4e2a\u7279\u6b8a\u65b9\u6cd5\u540e\uff0cFrenchDeck \u7684\u884c\u4e3a\u5c31\u50cf\u6807\u51c6\u7684 Python \u5e8f\u5217\u4e00\u6837</p> <pre><code>&gt;&gt;&gt; deck[:3]\n&gt;&gt;&gt; [Card(rank='2', suit='spades'), Card(rank='3', suit='spades'),\nCard(rank='4', suit='spades')]\n\n&gt;&gt;&gt; random.choice(deck)\n&gt;&gt;&gt; Card(rank='K', suit='spades')\n\n&gt;&gt;&gt; len(deck)\n&gt;&gt;&gt; 52\n\nfor card in deck:\n    print(card)\n&gt;&gt;&gt;Card(rank='2', suit='spades')\nCard(rank='3', suit='spades')\nCard(rank='4', suit='spades')\n...\n\nCard('7', 'beasts') in deck\n&gt;&gt;&gt;False\n</code></pre>"},{"location":"python/python-something/fluent-python/#__repr____abs____add__-\u548c-__mul__","title":"<code>__repr__</code>\u3001<code>__abs__</code>\u3001<code>__add__</code> \u548c <code>__mul__</code>","text":"<p>\u5b9e\u73b0\u4e00\u4e2a\u4e8c\u7ef4\u5411\u91cf\u7c7b\uff0c\u5373\uff1a <pre><code>&gt;&gt;&gt; v1 = Vector(2, 4)\n&gt;&gt;&gt; v2 = Vector(2, 1)\n&gt;&gt;&gt; v1 + v2\nVector(4, 5)\n</code></pre> \u5e76\u4e14\u8003\u8651\u5411\u91cf\u7684\u52a0\u6cd5\u3001\u4e58\u6cd5\u3001\u6807\u91cf\u79ef\u3001\u7edd\u5bf9\u503c\u7279\u6027</p> <pre><code>class Vector:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __repr__(self):\n        return f'Vector({self.x!r}, {self.y!r})'\n        # \u4f7f\u7528 !r \u4ee5\u6807\u51c6\u7684\u8868\u793a\u5f62\u5f0f\u663e\u793a\u5c5e\u6027\n\n    def __abs__(self):\n        return math.hypot(self.x, self.y)\n\n    def __bool__(self):\n        return bool(abs(self))\n\n    def __add__(self, other):\n        x = self.x + other.x\n        y = self.y + other.y\n        return Vector(x, y)\n\n    def __mul__(self, scalar):\n        return Vector(self.x * scalar, self.y * scalar)\n</code></pre> <p>\u7279\u6b8a\u65b9\u6cd5 <code>__repr__</code>\u4f9b\u5185\u7f6e\u51fd\u6570repr\u8c03\u7528\uff0c\u83b7\u53d6\u5bf9\u8c61\u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\u3002\u5982\u672a\u5b9a\u4e49 <code>__repr__</code> \u65b9\u6cd5\uff0c<code>Vector</code> \u5b9e\u4f8b\u5728 Python \u63a7\u5236\u53f0\u4e2d\u663e\u793a\u4e3a <code>&lt;Vector object at 0x10e100070&gt;</code> \u5f62\u5f0f\u3002</p>"},{"location":"python/python-something/fluent-python/#__bool__","title":"<code>__bool__</code>","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u7528\u6237\u5b9a\u4e49\u7c7b\u7684\u5b9e\u4f8b\u90fd\u662f\u771f\u503c\uff0c\u9664\u975e\u5b9e\u73b0\u4e86 <code>__bool__</code> \u6216 <code>__len__</code> \u65b9\u6cd5\u3002\u7b80\u5355\u6765\u8bf4\uff0c<code>bool(x)</code> \u8c03\u7528 <code>x.__bool__()</code>\uff0c\u4ee5\u540e\u8005\u8fd4\u56de\u7684\u7ed3\u679c\u4e3a\u51c6\u3002\u5982\u679c\u6ca1\u6709\u5b9e\u73b0 <code>__bool__</code> \u65b9\u6cd5\uff0c\u5219 Python \u5c1d\u8bd5\u8c03\u7528 <code>x.__len__()</code>\uff1b\u5982\u679c\u8be5\u65b9\u6cd5\u8fd4\u56de\u96f6\u503c\uff0c\u5219 <code>bool</code> \u51fd\u6570\u8fd4\u56de <code>False</code>\uff0c\u5426\u5219\u8fd4\u56de <code>True</code>\u3002</p> <pre><code># Vector.__bool__\u65b9\u6cd5\n\ndef __bool__(self):\n    return bool(self. or self.y)\n</code></pre>"},{"location":"python/python-something/fluent-python/#\u5e8f\u5217\u7c7b\u578b","title":"\u5e8f\u5217\u7c7b\u578b","text":"<ul> <li> <p>\u5bb9\u5668\u5e8f\u5217\uff1a\u5b58\u653e\u6240\u5305\u542b\u5bf9\u8c61\u7684\u5f15\u7528\uff0c\u5bf9\u8c61\u53ef\u4ee5\u662f\u4efb\u4f55\u7c7b\u578b\uff1b</p> </li> <li> <p>\u6241\u5e73\u5e8f\u5217\uff1a\u5728\u81ea\u5df1\u7684\u5185\u5b58\u7a7a\u95f4\u4e2d\u5b58\u50a8\u6240\u542b\u5185\u5bb9\u7684\u503c</p> </li> </ul>"},{"location":"read/chapter1/","title":"\u5bf9\u7f51\u7edc\u5154\u5b50\u6d1e\u7684\u4e2a\u4eba\u63a2\u7d22\u30101\u3011","text":""},{"location":"read/chapter1/#\u524d\u8a00","title":"\u524d\u8a00","text":"<p>\u4f60\u662f\u5426\u4e5f\u7ecf\u5386\u8fc7\u8fd9\u6837\u7684\u6df1\u591c\uff1f\u660e\u660e\u53ea\u662f\u60f3\u4e0a\u767e\u5ea6\u67e5\u4e2a\u5b66\u4e60\u8d44\u6599\uff0c\u5374\u83ab\u540d\u5176\u5999\u5730\u5728\u5c42\u5c42\u94fe\u63a5\u4e2d\u5f00\u59cb\u7814\u7a76\u8d77\u4e86\u67d0\u4e2a\u90fd\u5e02\u4f20\u8bf4\u6216\u8005\u6545\u4e8b\uff0c\u4e0d\u80fd\u81ea\u62d4\u3002\u672c\u5e16\u662f\u672c\u4eba\u5bf9\u4e92\u8054\u7f51\u4e0a\u4e00\u4e9b\u89c9\u5f97\u6709\u610f\u601d\u7684\u5947\u95fb\u5f02\u4e8b\u7684\u4e00\u4e9b\u63a2\u7d22\u3002</p> <p>\u5154\u5b50\u6d1e\uff1a\u51fa\u81ea\u7ae5\u8bdd\u6545\u4e8b\u300a\u7231\u4e3d\u4e1d\u68a6\u6e38\u4ed9\u5883\u300b\u4e2d\u7231\u4e3d\u4e1d\u5728\u8ffd\u9010\u5154\u5b50\u7684\u9014\u4e2d\u901a\u8fc7\u5154\u5b50\u6d1e\u8fdb\u5165\u5947\u5e7b\u4e16\u754c\u7684\u7ecf\u5386\uff0c\u5bd3\u610f\u4e3a\u201c\u672a\u77e5\u3001\u4e0d\u786e\u5b9a\u7684\u4e16\u754c\u201d\u3002\u6211\u5728\u7f51\u4e0a\u67e5\u8fd9\u4e2a\u8bcd\u7684\u65f6\u5019\u53d1\u73b0\u201c\u5154\u5b50\u6d1e\u201d\u7ecf\u5e38\u7528\u6765\u5f62\u5bb9\u7528\u6237\u88ab\u7f51\u7edc\u5e73\u53f0\u5185\u5bb9\u548c\u7b97\u6cd5\u5438\u5f15,\u65e0\u6cd5\u8f7b\u6613\u79bb\u5f00,\u9010\u6e10\u8131\u79bb\u73b0\u5b9e\u751f\u6d3b\uff0c\u662f\u5bf9\u7f51\u7edc\u6c89\u8ff7\u884c\u4e3a\u7684\u6279\u5224\u3002\u4f46\u5bf9\u6211\u6765\u8bf4\uff0c\u7f51\u7edc\u4e0a\u786e\u5b9e\u6709\u503c\u5f97\u4e00\u63a2\u7a76\u7adf\u7684\u6709\u8da3\u4e8b\u7269\uff0c\u4e8e\u662f\u4fbf\u6709\u4e86\u8fd9\u7bc7\u5e16\u5b50\uff0c\u6765\u8bb0\u5f55\u6211\u5bf9\u4e00\u4e9b\u5154\u5b50\u6d1e\u7684\u63a2\u7d22\u3002</p> <p>\u5e16\u5b50\u91cc\u5199\u7684\u4e8b\u7269\u6709\u5f88\u5927\u90e8\u5206\u6765\u81ea\u4e8e\u5154\u5b50\u6d1e\u51b0\u5c71\u56fe\u4ee5\u53ca\u81ea\u5df1\u63a2\u7d22\u5230\u7684\u4e00\u4e9b\u5185\u5bb9\uff1a\u5173\u4e8e\u5154\u5b50\u6d1e\u51b0\u5c71\u56fe\u7684\u76f8\u5173\u4ecb\u7ecd\u6587\u7ae0</p>"},{"location":"read/chapter1/#\u7231\u4e3d\u4e1d\u6f2b\u6e38\u4ed9\u5883\u7efc\u5408\u5f81alice-in-wonderland-syndromeaiws","title":"\u7231\u4e3d\u4e1d\u6f2b\u6e38\u4ed9\u5883\u7efc\u5408\u5f81(Alice in wonderland syndrome\uff0cAIWS)","text":""},{"location":"read/chapter1/#_1","title":"\u5bf9\u7f51\u7edc\u5154\u5b50\u6d1e\u7684\u4e2a\u4eba\u63a2\u7d22\u30101\u3011","text":""},{"location":"read/chapter1/#\u540e\u4e16\u7684\u541b\u5b50\u4eec\u4f60\u4eec\u597d\u554a","title":"\u201c\u540e\u4e16\u7684\u541b\u5b50\u4eec\uff0c\u4f60\u4eec\u597d\u554a\u201d","text":""},{"location":"read/chapter1/#\u6ce2\u6ce2\u6512","title":"\u6ce2\u6ce2\u6512","text":""},{"location":"read/chapter1/#_2","title":"\u5bf9\u7f51\u7edc\u5154\u5b50\u6d1e\u7684\u4e2a\u4eba\u63a2\u7d22\u30101\u3011","text":""},{"location":"sci-paper/","title":"\ud83c\udf65 \u8bba\u6587\u9605\u8bfb\u4e3b\u9875","text":"<p>\ud83d\udd2d\u4e3b\u8981\u9605\u8bfb\u65b9\u5411</p> <p> <ul> <li> <p>\u80fd\u6e90\u4f18\u5316,\u80fd\u6e90\u8d1f\u8377\u9884\u6d4b\u7b49\u4e0e\u6211\u672c\u4e13\u4e1a\u65b9\u5411,\u4fa7\u91cdMulti-energy Load Forecasting</p> </li> <li> <p>Deep Learing,Time series(\u65f6\u95f4\u5e8f\u5217\u5206\u6790,\u9884\u6d4b)\u65b9\u5411\u8bba\u6587</p> </li> </ul> <p></p> <p>\u56e0\u4e3a\u592a\u61d2,\u8fd8\u6ca1\u5f00\u59cb/(\u3112o\u3112)/~~</p>"},{"location":"sci-paper/ComputationalPhysics/DeepONet/","title":"DeepONet: Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aLearning nonlinear operators via DeepONet based on the universal approximation theorem of operators</p> <p>\u4ee3\u7801:DeepXDE</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/DeepONet/#abstract","title":"Abstract","text":"<p>a neural network with a single hidden layer can approximate accurately any nonlinear continuousfunctional (a mapping from a space of functions into the real numbers) or (nonlinear) operator (a mapping from a space of functions into another space of functions).</p> <p>But the theorem only guarantees a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors.</p> <p>DeepONet consists of two sub-networks:branch net(encoding the input func) and trunk net(encoding the location s for output func).</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/DeepONet/#methodology","title":"Methodology","text":"<p>Let \\({G}\\) be an operator taking input function \\(u\\), and then \\(G(u)\\) is the output function.Thus the output \\(G(u)(y)\\) is real number. To represent the input function discretely, we sample enough the function value at locations \\({x_1,x_2,...,x_m}\\)(we call them as 'sensors'(\u4f20\u611f\u5668)).The only condition required is that the sensor loactions \\({x_1,x_2,...,x_m}\\) are the same, while we don't enforce any limitation on the output locations \\(y\\).</p> <p>In the general setting, the network inputs consist of two separate components:\\([u(x_1), u(x_2),..., u(x_m)]\\) and \\(y\\).The trunk network takes \\(y\\) as the input and the outputs are \\([t_1,t_2,...,t_p]\\), and each of \\(p\\) branch networks takes \\([u(x_1), u(x_2),..., u(x_m)]^{T}\\) as input and the outputs a scalar \\(b_k \\in \\mathcal{R}\\) for \\(k=1,2,...,p\\).</p> \\[ G(u)(y) \u2248 \\sum_{k=1}^p b_k t_k(y) + b_0 \\] <p>trunk network applies activation func in the last layer(\\(t_k = \\sigma (\u00b7)\\)), and thus this trunk-branch network can also be seen as a trunk network with each weight in the last layer parameterized by another branch network instead of the classical single variable.</p> <p>In practice, \\(p\\) is at least of the order of 10, and using many branch networks is computationally and memory expensive. Hence, we merge all the branch networks(\"stacked DeepONet\") into one single branch network(\"unstacked DeepONet\").</p> <p>\\(G(u)(y)\\) can be viewed as a function of yconditioning on \\(u\\).</p>"},{"location":"sci-paper/ComputationalPhysics/DeepONet/#data-generation","title":"Data generation","text":"<p>In this study, we mainly consider two function spaces: Gaussian random field (GRF,\u9ad8\u65af\u968f\u673a\u573a) and orthogonal (Chebyshev) polynomials.</p> \\[ u \\sim \\mathcal{G}(0,k_l(x_1,x_2)) \\] \\[ k_l(x_1,x_2) = exp(-|| x_1 - x_2 ||^2 / 2l^2) \\] <p>where the covariance function \\(k_l(x_1,x_2)\\) is the radial-bias function(RBF,\u5f84\u5411\u57fa\u51fd\u6570) kernel with the length scale, which determines the smoothness of the sampled function \\(l\\) and larger \\(l\\) leads to smoother \\(u\\).</p> <p>Let \\(M&gt;0\\) and \\(T_i\\) are Chebyshev polynomials of the first kind(\u7b2c\u4e00\u7c7b\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f).We define the orthogonal polynomials of degree \\(N\\) (N\u9636\u7684\u6b63\u4ea4\u591a\u9879\u5f0f)as:</p> \\[ V_{poly} = \\Big \\{ \\sum_{i=0}^{N-1} a_i T_i(x): |a_i| \u2264 M \\Big \\} \\] <p>We generate the dataset from \\(V_{poly}\\) by randomly sampling \\(a_i\\) from \\([\u2212M, M]\\) to get a sample of \\(u\\).</p> <p>Accroding to reference solution, one data point is a triple \\((u,y,G(u)(y))\\), and thus one specific input \\(u\\) may appear in multiple data points with different values of \\(y\\). For example, a dataset of size 10000 may only be generated from 100 \\(u\\) trajectories, and each evaluates \\(G(u)(y)\\) for 100 \\(y\\) locations.</p> <p>\u6570\u636e\u96c6\u7684\u89c4\u6a21\u5c31\u662f\u53d6\u6837\\(u\\)\u7684\u4e2a\u6570\u548c\u53d6\u70b9\\(y\\)\u7684\u4e2a\u6570\u7684\u4e58\u79ef</p>"},{"location":"sci-paper/ComputationalPhysics/DeepONet/#exmaple","title":"Exmaple","text":"<p>A 1D dynamic system is described by</p> \\[ \\frac{ds(x)}{dx} = g(s(x),u(x),x), x \\in [0,1] \\] \\[ s(0) = 0 \\] <p>The goal is to predict \\(s(x)\\) over the whole domain \\([0,1]\\) for any \\(u(x)\\).</p> <p></p> <p>(1)Linear case: \\(g(s(x),u(x),x) = u(x)\\)</p> \\[ G : u(x) \u2192 s(x) = \\int_0^x u(\\tau) d \\tau \\] <p>This task is equivalent to learning the antiderivative operator(\u4e0d\u5b9a\u79ef\u5206\u7b97\u5b50)</p> <p>Compared to FNNs, DeepONets have much smaller generalization error and thus smaller test error.Compared to stacked DeepONets, although unstacked DeepONets have larger training error, the test error is smaller, due to the smaller generalization error. Therefore, unstacked DeepONets with bias achieve the best performance. In addition, unstacked DeepONets have fewer number of parameters than stacked DeepONets, and thus can be trained faster using much less memory.</p> <p></p> <p>(2)Nonlinear case: \\(g(s(x),u(x),x) = -s^2(x)+ u(x)\\)</p> <p></p> <p>During the network training, the training MSE and test MSE of both stacked and unstacked DeepONets decrease, but the correlation between training MSE and test MSE of unstacked DeepONets is tighter(smaller generalization error).</p> <p>DeepONets work even for out-of-distribution(OOD) predictions</p> <p></p> <p>(3)Gravity pendulum with an external force(\u91cd\u529b\u6446\u65b9\u7a0b): </p> \\[ \\frac{ds_1}{dt} = s_2 \\] \\[ \\frac{ds_2}{dt} = -ksin s_1 + u(t) \\] \\[ s(0) = 0 \\] <p>\\(k = \\frac{g}{l}\\) is determined by the acceleration due to gravity and the length of the pendulum.</p> <p>This problem is characterized by three factors:</p> <p>(1) \\(k\\)</p> <p>(2) maximum prediction time \\(T\\), </p> <p>(3) input function space(corresponding to \\(l\\))</p> <p></p> <p></p> <p>we investigate the error tendency under different conditions, including prediction time, network size, and training dataset size.We first observe that both the training and test errors grow exponentially(\u4ee5\u6307\u6570\u65b9\u5f0f) with the maximum prediction time \\(T\\):</p> \\[ MSE \u221d 8^T \\] <p>Next we investigate the effect of different function spaces, including GRF with different length scale \\(l\\) and the space of Chebyshev polynomials with different number of \\(Bases\\). </p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/DeepONet/#diffusion-reaction-system-with-a-source-term\u5177\u6709\u6e90\u9879\u7684\u6269\u6563\u53cd\u5e94\u7cfb\u7edf","title":"Diffusion-reaction system with a source term(\u5177\u6709\u6e90\u9879\u7684\u6269\u6563\u53cd\u5e94\u7cfb\u7edf)","text":"\\[  \\frac{\\partial s}{\\partial t} = D \\frac{\\partial^2 s}{\\partial x^2} + ks^2 + u(x),\\space x \\in (0, 1),  t \\in (0, 1]  \\] <p>with zero initial/boundary conditions, where \\(D = 0.01\\) is the diffusion coefficient, and \\(k = 0.01\\) is the reaction rate.We use DeepONets to learn the operator mapping from \\(u(x)\\) to the PDE solution \\(s(x, t)\\).</p> <p>To generate the training dataset, we solve the diffusion-reaction system using a second-order implicit finite difference method on a 100 by 100 grid, and then for each \\(s\\) we randomly select \\(P\\) points out of these \\(10000 = 100 \u00d7 100\\) grid points. Hence, the dataset size is equal to the product of \\(P\\) by the number of \\(u\\) samples. We confirm that the training and test datasets do not include the data from the same \\(s\\).</p> <p></p> <p>When we use 100 random \\(u\\) samples, the test error decreases first as \\(P\\) increases, and then saturates due to other factors, such as the finite number of \\(u\\) samples and fixed neural network size. We observe a similar error tendency but with less saturation as the number of \\(u\\) samples increases with \\(P\\) fixed.</p> <p>In addition, in this PDE problem the DeepONet is able to learn from a small dataset.DeepONet can reach the test error of \u223c \\(10^{\u22125}\\) when it is only trained with 100 \\(u\\) samples (\\(P = 1000\\)).</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/FNO/","title":"FNO: Fourier Neural Operator for Parametric Partial Differential Equations","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aFourier Neural Operator for Parametric Partial Differential Equations</p> <p>\u4ee3\u7801:https://github.com/li-Pingan/fourier-neural-operator</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/FNO/#\u4ecb\u7ecd","title":"\u4ecb\u7ecd","text":"<p>\u795e\u7ecf\u7f51\u7edc(neural networks)\u7684\u7ecf\u5178\u53d1\u5c55\u4e3b\u8981\u96c6\u4e2d\u5728\u5b66\u4e60\u6709\u9650\u7ef4\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e4b\u95f4\u7684\u6620\u5c04\u3002\u6700\u8fd1\uff0c\u8fd9\u5df2\u7ecf\u63a8\u5e7f\u5230\u795e\u7ecf\u7b97\u5b50(neural operators)\uff0c\u5b83\u4eec\u5b66\u4e60\u51fd\u6570\u7a7a\u95f4\u4e4b\u95f4\u7684\u6620\u5c04\u3002\u5bf9\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\uff0c\u795e\u7ecf\u7b97\u5b50\u76f4\u63a5\u5b66\u4e60\u4ece\u4efb\u4f55\u51fd\u6570\u53c2\u6570\u4f9d\u8d56\u6027\u5230\u89e3\u7684\u6620\u5c04\u3002\u56e0\u6b64\uff0c\u5b83\u4eec\u5b66\u4e60\u4e86\u4e00\u6574\u4e2aPDEs\u65cf\uff0c\u4e0e\u7ecf\u5178\u65b9\u6cd5\u89e3\u51b3\u65b9\u7a0b\u7684\u4e00\u4e2a\u5b9e\u4f8b\u5f62\u6210\u5bf9\u6bd4\u3002</p> <p>\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u76f4\u63a5\u5728\u5085\u91cc\u53f6\u7a7a\u95f4(Fourier space)\u4e2d\u53c2\u6570\u5316(parameterize)\u79ef\u5206\u6838(integral kernel)\u6765\u5236\u5b9a\u4e00\u4e2a\u65b0\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5bcc\u6709\u8868\u73b0\u529b\u548c\u9ad8\u6548\u7684\u67b6\u6784\u3002\u6211\u4eec\u5bf9Burgers\u65b9\u7a0b\u3001Darcy\u6d41\u548cNavier-Stokes\u65b9\u7a0b\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/FNO/#\u65b9\u6cd5","title":"\u65b9\u6cd5","text":"<p>\u9996\u5148\u6982\u8ff0\u4e24\u79cd\u4e3b\u6d41\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684PDE\u65b9\u6cd5\uff1a\u6709\u9650\u7ef4\u7b97\u5b50(finite-dimensional operators)\u548c\u795e\u7ecf\u6709\u9650\u5143\u65b9\u6cd5(Neural-FEM)\u3002</p> <p>\u6709\u9650\u7ef4\u7b97\u5b50(finite-dimensional operators)</p> <p>\u5c06\u89e3\u7b97\u5b50\u53c2\u6570\u5316\u4e3a\u6709\u9650\u7ef4\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e4b\u95f4\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002\u672c\u8d28\u4e0a\u662f\u4e0e\u7f51\u683c\u76f8\u5173\u7684\uff0c\u9700\u8981\u5bf9\u4e0d\u540c\u7684\u5206\u8fa8\u7387\u548c\u79bb\u6563\u5316\u8fdb\u884c\u4fee\u6539\u548c\u8c03\u6574\u3002\uff08\u6362\u4e00\u79cd\u5206\u8fa8\u7387\u6216\u8005\u6362\u4e00\u4e2a\u521d\u8fb9\u754c\u6761\u4ef6\uff0c\u5f80\u5f80\u8bef\u5dee\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u64c5\u957f\u7279\u5b9a\u573a\u666f\u7684\u7279\u5b9a\u4f18\u5316\uff09</p> <p>\u795e\u7ecf\u6709\u9650\u5143\u65b9\u6cd5(Neural-FEM)</p> <p>\u76f4\u63a5\u5c06\u89e3\u51fd\u6570\u53c2\u6570\u5316\u4e3a\u795e\u7ecf\u7f51\u7edc\uff08PINNs\u7c7b\u6a21\u578b\uff09\uff0c\u4e0e\u7f51\u683c\u65e0\u5173\u4e14\u7cbe\u786e\uff0c\u4f46\u5bf9\u4e8e\u4efb\u4f55\u7ed9\u5b9a\u7684\u65b0\u7684\u51fd\u6570\u53c2\u6570/\u7cfb\u6570\u5b9e\u4f8b\uff0c\u90fd\u9700\u8981\u8bad\u7ec3\u4e00\u4e2a\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\uff08\u5bf9\u4e8ePINNs\u6765\u8bf4\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u5c31\u662f\u6c42\u89e3\u8fc7\u7a0b\uff09\u3002</p> <p>\u672c\u6587\u7684\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u6709\u9650\u7684\u89c2\u6d4b\u6570\u636e\u5bf9\u5b66\u4e60\u4e00\u4e2a\u6620\u5c04\u5173\u7cfb\uff0c\u8fd9\u4e2a\u6620\u5c04\u662f\u5b9a\u4e49\u5728\u65e0\u9650\u7ef4\u7a7a\u95f4\u4e4b\u95f4\u7684\u3002\u5728\u5f88\u591a\u5e94\u7528\u4e2d\uff0c\u5c24\u5176\u662f\u4e0e\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u76f8\u5173\u7684\u95ee\u9898\uff0c\u8f93\u5165\u548c\u8f93\u51fa\u901a\u5e38\u662f\u65e0\u9650\u7ef4\u7684\u51fd\u6570\u7a7a\u95f4\u800c\u4e0d\u662f\u6709\u9650\u7ef4\u7684\u5411\u91cf\u7a7a\u95f4\u3002FNO\u7684\u76ee\u6807\u662f\u4ece\u8fd9\u4e9b\u8f93\u5165\u8f93\u51fa\u6570\u636e\u5bf9\u4e2d\u5b66\u4e60\u5230\u8fd9\u79cd\u6620\u5c04\u3002</p> <p>\u8bbe\\(D \u2282 \\mathbb{R}^d\\) \u4e3a\u4e00\u4e2a\u6709\u754c\u5f00\u96c6\uff0c\\(\\mathcal{A} = \\mathcal{A}(D; \\mathbb{R}^{d_a})\\) \u548c \\(\\mathcal{U} = \\mathcal{U}(D; \\mathbb{R}^{d_u})\\) \u5206\u522b\u662f\u4ece\\(D\\)\u5230\\(\\mathbb{R}^{d_a}\\)\u3001\u4ece\\(D\\)\u5230\\(\\mathbb{R}^{d_u}\\)\u7684\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u3002</p> <p>\u53e6\u5916\uff0c\u8bbe\\(G^\u2020 : \\mathcal{A} \u2192 \\mathcal{U}\\)\u662f\u4e00\u4e2a\u975e\u7ebf\u6027\u7684\u6620\u5c04\u3002\u901a\u5e38\u5728\u5b9e\u9645\u95ee\u9898\u4e2d\uff0c\\(G^\u2020\\)\u4ee3\u8868\u7684\u662f\u67d0\u79cd\u7269\u7406\u6216\u6570\u5b66\u8fc7\u7a0b\uff0c\u6bd4\u5982\u672c\u6587\u4e2d\\(G^\u2020\\)\u4e3a\u5e26\u53c2\u6570\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u89e3\u7b97\u5b50\uff08the solution operators of parametric PDEs\uff09\u3002</p> <p>\u5047\u8bbe\u6211\u4eec\u6709\u89c2\u5bdf\\(\\{ a_j , u_j \\}^N_{j=1}\\)\uff0c\u5176\u4e2d\\(a_j \u2208 \\mathcal{A}\\)\u4ece\\(\\mathcal{A}\\)\u4e2d\u6839\u636e\u67d0\u4e2a\u6982\u7387\u5206\u5e03\\(\\mu\\)\u62bd\u53d6\uff0c\u800c\\(u_j = G^\u2020(a_j)\\)\u53ef\u80fd\u88ab\u566a\u58f0\u635f\u574f\u3002\u6211\u4eec\u901a\u8fc7\u6784\u5efa\u53c2\u6570\u6620\u5c04:</p> \\[  G : \\mathcal{A} \\times \\Theta \\rightarrow \\mathcal{U}  \\] <p>\u6765\u6784\u5efa\\(G^\u2020\\)\u7684\u8fd1\u4f3c\uff0c\u5176\u4e2d\\(\\Theta\\)\u662f\u6709\u9650\u7ef4\u7684\u53c2\u6570\u7a7a\u95f4</p> <p>Notation:</p> <ul> <li> <p>\\(x \\in D \u2282 \\mathbb{R}^d\\): \u504f\u5fae\u5206\u65b9\u7a0b\u7684\u7a7a\u95f4\u57df\uff0c\u65b9\u7a0b\u6240\u5b9a\u4e49\u7684\u7a7a\u95f4\u533a\u57df\uff0c\u5373\u81ea\u53d8\u91cf\uff08\u901a\u5e38\u662f\u7a7a\u95f4\u5750\u6807\uff09\u53d6\u503c\u7684\u8303\u56f4\u3002</p> </li> <li> <p>\\(a \\in \\mathcal{A} = (D; \\mathbb{R}^{d_a})\uff0cu \\in \\mathcal{U} = (D; \\mathbb{R}^{d_u})\\)\uff0c\u8f93\u5165\u51fd\u6570\u4e0e\u8f93\u51fa\u89e3\u51fd\u6570\u3002</p> </li> <li> <p>\\(v(x) \\in \\mathbb{R}^{d_v}\\)\uff1a\u795e\u7ecf\u7f51\u7edc\u4e2d\u95f4\u5c42\u7684\u8f93\u51fa\u3002</p> </li> <li> <p>\\(\\mathcal{k}:\\mathbb{R}^{2(d+1)} \\rightarrow \\mathbb{R}^{d_v \\times d_v}\\)\uff0c\u7b97\u5b50\u6838\u5c06\\((x, y, a(x)\uff0c a(y))\\)\u6620\u5c04\u5230\\(dv \\times dv\\)\u77e9\u9635</p> </li> <li> <p>\\(\\phi\\):\u7b97\u5b50\u6838\u7f51\u7edc\\(\\mathcal{k}\\)\u7684\u53c2\u6570</p> </li> <li> <p>\\(k\\):\u5085\u91cc\u53f6\u6a21\u5f0f/\u6ce2\u6570(Fourier modes / wave numbers)\u3002\\(k_{max}\\):\u5085\u7acb\u53f6\u5c42\u4e2d\u4f7f\u7528\u7684\u6700\u5927\u5085\u7acb\u53f6\u6a21\u5f0f\u3002</p> </li> </ul> <p>\u79bb\u6563\u5316(Discretization)</p> <p>\u7531\u4e8e\\(a_j\\)\u4e0e\\(u_j\\)\u901a\u5e38\u662f\u51fd\u6570\uff0c\u4e3a\u4e86\u7528\u6570\u503c\u65b9\u6cd5\u5904\u7406\u5b83\u4eec\uff0c\u6211\u4eec\u4f7f\u7528\u9010\u70b9\u8bc4\u4f30\uff08point-wise evaluations\uff09\u7684\u65b9\u6cd5\uff0c\u8bbe\\(D_j=\\{x_1,...,x_n\\} \\subset D\\)\u4e3a\\(D\\)\u7684n\u70b9\u79bb\u6563\u5316\uff0c\u5e76\u4e14\u6211\u4eec\u89c2\u5bdf\u5230\\(a_j|_{D_j} \\in \\mathbb{R}^{n\\times d_a}\\)\u548c\\(u_j|_{D_j} \\in \\mathbb{R}^{n\\times d_u}\\)\uff0c\u7ec4\u6210\u4e86\u7531j\u7d22\u5f15\u7684\u6709\u9650\u4e2ainput-output\u6570\u636e\u5bf9\u3002</p> <p></p> <p>\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u7684\u5b8c\u6574\u67b6\u6784\uff1a</p> <p>(1)\u4ece\u8f93\u5165a\u5f00\u59cb; </p> <p>(2)\u901a\u8fc7\u795e\u7ecf\u7f51\u7edcP\u63d0\u5347\u5230\u66f4\u9ad8\u7ef4\u5ea6\u7684\u901a\u9053\u7a7a\u95f4;</p> <p>(3)\u5e94\u7528\u56db\u5c42\u79ef\u5206\u7b97\u5b50\u548c\u6fc0\u6d3b\u51fd\u6570;</p> <p>(4)\u901a\u8fc7\u795e\u7ecf\u7f51\u7edcQ\u4f20\u56de\u76ee\u6807\u7ef4\u5ea6;</p> <p>(5)\u8f93\u51fau\u3002</p> <p>\u5bf9\u4e8e\u5085\u91cc\u53f6\u5c42\uff08Fourier layer\uff09\uff1a\u4ece\u8f93\u5165\\(v\\)\u5f00\u59cb\u3002</p> <p>\u5728\u9876\u90e8\uff1a\u5e94\u7528\u5085\u91cc\u53f6\u53d8\u6362\\(\\mathcal{F}\\)\uff0c\u518d\u5bf9\u4f4e\u9891\u7387\u7684\u4fe1\u53f7\u8fdb\u884c\u7ebf\u6027\u53d8\u6362   \uff0c\u6ee4\u9664\u8f83\u9ad8\u9891\u7387\u7684\u4fe1\u53f7\uff1b\u7136\u540e\u5e94\u7528\u5085\u91cc\u53f6\u9006\u53d8\u6362\\(\\mathcal{F}^{-1}\\)\u3002</p> <p>\u5728\u5e95\u90e8\uff1a\u5e94\u7528\u4e00\u4e2a\u5c40\u90e8\u7ebf\u6027\u53d8\u6362\\(W\\)\u3002</p> <p>\u8fed\u4ee3\u66f4\u65b0(Iterative updates)</p> <p>\u6211\u4eec\u5c06\u901a\u8fc7\u4e00\u4e2aFourier layer\u7684\u8fc7\u7a0b\u8868\u793a\u4e3a\\(v_t \u2192 v_{t+1}\\):</p> \\[ v_{t+1}(x) := \\sigma \\Big( W v_t(x) + ( \\mathcal{K}(a;\\phi) v_t(x))    \\Big) \\] <p>\u5176\u4e2d\\(\\mathcal{K} : \\mathcal{A} \\times \\Theta_{\\mathcal{K}} \\rightarrow \\mathcal{L} (\\mathcal{U}(D;\\mathbb{R}^{d_v}),\\mathcal{U}(D;\\mathbb{R}^{d_v}))\\)\u662f\u6620\u5c04\u5230\\(\\mathcal{U}(D;\\mathbb{R}^{d_v})\\)\u4e0a\u7684\u6709\u754c\u7ebf\u6027\u7b97\u5b50\uff0c\u7531\\(\\phi \\in \\Theta_K\\)\u53c2\u6570\u5316\uff0c\\(W : \\mathbb{R}^{d_v} \\rightarrow \\mathbb{R}^{d_v}\\)\u662f\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\u3002</p> <p>\u6211\u4eec\u9009\u7528\\(\\mathcal{K}(a;\\phi)\\)\u4f5c\u4e3a\u79ef\u5206\u7b97\u5b50\u6838\u53d8\u6362\uff08kernel integral transformation\uff09\uff0c\u5e76\u7528\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6765\u53c2\u6570\u5316\u3002</p> <p>\u79ef\u5206\u7b97\u5b50\u6838\\(\\mathcal{K}\\)(Kernel integral operator)</p> \\[ (\\mathcal{K}(v,a) v_t)(x) := \\int_D \u03ba_\\phi \\Bigl(x,y,a(x),a(y)\\Bigr)v_t(y)dy\uff0c\\forall x \\in D \\] <p>\u5176\u4e2d\\(\\kappa_\\phi : \\mathbb{R}^{2(d+d_a)} \\rightarrow \\mathbb{R}^{d_v \\times d_v}\\)\u662f\u7531\\(\\phi \\in \\Theta_K\\)\u53c2\u6570\u5316\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u626e\u6f14\u4e86\u6838\u51fd\u6570\u7684\u89d2\u8272\uff0c\u6211\u4eec\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5b83\u3002\u6ce8\u610f\uff0c\u5373\u4f7f\u79ef\u5206\u7b97\u5b50\u662f\u7ebf\u6027\u7684\uff0c\u795e\u7ecf\u7b97\u5b50\u4e5f\u53ef\u4ee5\u901a\u8fc7\u7ec4\u5408\u7ebf\u6027\u79ef\u5206\u7b97\u5b50\u548c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u6765\u5b66\u4e60\u9ad8\u5ea6\u975e\u7ebf\u6027\u7b97\u5b50\uff0c\u7c7b\u4f3c\u4e8e\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u3002</p> <p>\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50(Fourier Neural Operator)</p> <p>\u6211\u4eec\u63d0\u51fa\u7528\u4e00\u4e2a\u5b9a\u4e49\u5728\u5085\u91cc\u53f6\u7a7a\u95f4\u4e0a\u7684\u5377\u79ef\u7b97\u5b50\uff0c\u66ff\u4ee3\u4e0a\u8ff0\u7684\u79ef\u5206\u7b97\u5b50\u6838\u3002\u8bbe\\(\\mathcal{F}\\)\u4e3a\\(f:D\\rightarrow \\mathbb{R}^{d_v}\\)\u7684\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u5219\uff1a</p> \\[  (\\mathcal{F} f)_j(k)=\\int_D f_j(x) e^{-2 i \\pi\\langle x, k\\rangle} \\mathrm{d} x, \\quad\\left(\\mathcal{F}^{-1} f\\right)_j(x)=\\int_D f_j(k) e^{2 i \\pi\\langle x, k\\rangle} \\mathrm{d} k  \\] <p>\u5bf9\\(j=1,...,d_v\\)\u6210\u7acb</p> <p>\u4ee4\\(\\kappa_\\phi(x, y, a(x), a(y))=\\kappa_\\phi(x-y)\\)\uff0c\u5e94\u7528\u5377\u79ef\u5b9a\u7406\u5f97\u5230\uff1a\\(\\left(\\mathcal{K}(a ; \\phi) v_t\\right)(x)=\\mathcal{F}^{-1}\\left(\\mathcal{F}\\left(\\kappa_\\phi\\right) \\cdot \\mathcal{F}\\left(v_t\\right)\\right)(x), \\quad \\forall x \\in D\\)\uff0c\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u76f4\u63a5\u5728\u5085\u91cc\u53f6\u7a7a\u95f4\u4e2d\u53c2\u6570\u5316\\(\\kappa_\\phi\\)\u3002</p> <p>\u5085\u91cc\u53f6\u79ef\u5206\u7b97\u5b50</p> \\[ \\left(\\mathcal{K}(\\phi) v_t\\right)(x)=\\mathcal{F}^{-1}\\left(R_\\phi \\cdot\\left(\\mathcal{F} v_t\\right)\\right)(x) \\quad \\forall x \\in D \\tag{4}  \\] <p>\u5176\u4e2d\\(R_\\phi\\)\u662f\u5468\u671f\u51fd\u6570\\(\\kappa: \\bar{D} \\rightarrow \\mathbb{R}^{d_v \\times d_v}\\)\u7684\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u7531\\(\\phi \\in \\Theta_{\\mathcal{K}}\\)\u53c2\u6570\u5316\uff0c\u5373\u4e3a\u4e0a\u56fe\u4e2d\u7684\\(R\\)</p> <p>\u8bbe\u9891\u7387\u6a21\u5f0f\\(k \\in D\\)\uff0c\u6709\\((\\mathcal{F} v_t)(k) \\in \\mathbb{C}^{d_v}\\)\u548c\\(R_\\phi(k) \\in \\mathbb{C}^{d_v \\times d_v}\\)\u3002\u6211\u4eec\u5047\u8bbe\\(\\mathcal{k}\\)\u4e3a\u5468\u671f\u6027\u7684\uff0c\u5141\u8bb8\u5085\u91cc\u53f6\u7ea7\u6570\u5c55\u5f00\uff0c\u6240\u4ee5\u6211\u4eec\u4f7f\u7528\u79bb\u6563\u5f62\u5f0f\u7684\\(k \\in \\mathbb{Z}^d\\)\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u6700\u5927\u6a21\u6570\u5904\u622a\u65ad\u5085\u91cc\u53f6\u7ea7\u6570\\(k_{\\max }=\\left|Z_{k_{\\max }}\\right|= \\{ k \\in \\mathbb{Z}^d: | k_j| \\leq  k_{\\max , j} ,j=1,...,d \\}\\)\u6765\u9009\u62e9\u6709\u9650\u7ef4\u53c2\u6570\u5316\u3002\u56e0\u6b64\\(R_\\phi\\)\u53ef\u4ee5\u53c2\u6570\u5316\u4e3a\\(\\mathbb{C}^{k_{max} \\times d_v \\times d_v}\\)\u7684\u5f20\u91cf\u3002</p> <p>\u79bb\u6563\u60c5\u51b5\u548c\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362(FFT)</p> <p>\u8bbe\u57dfD\\(\u79bb\u6563\u5316\u4e3a\\)n \\in \\mathcal{N}\\(\u4e2a\u79bb\u6563\u70b9\uff0c\u5219\u6709\\)v_t \\in \\mathbb{R}^{n \\times d_v}\\(\u548c\\)\\mathcal{F} (v_t) \\in \\mathbb{C}^{n \\times d_v}\\(\u3002\u7531\u4e8e\u91c7\u7528\u4e86\u622a\u65ad\uff0c\u6240\u4ee5\u5b9e\u9645\u4e0a\\)\\mathcal{F} (v_t) \\in \\mathbb{C}^{k_{max} \\times d_v}\\(\uff0c\u6743\u91cd\u5f20\u91cf\\)R_\\phi \\in \\mathbb{C}^{k_{max} \\times d_v \\times d_v}$\u3002</p> \\[ (R \\cdot (\\mathcal{F} v_t))_{k,l} = \\sum_{j=1}^{d_v} R_{k,l,j}(\\mathcal{F} v_t)_{k,j}\uff0c\\quad k =1,...,k_{max}, j = 1,...,d_v  \\] <p>\u5f53\u79bb\u6563\u5316\u662f\u5747\u5300\u7684\uff0c\u5206\u8fa8\u7387\u4e3a\\(s_1 \\times s_2 \\times...\\times s_d = n\\)\uff0c\\(\\mathcal{F}\\)\u53ef\u4ee5\u66ff\u6362\u4e3a\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u3002\u5f53\\(f \\in \\mathbb{R}^{n\\times d_v}, k=\\left(k_1, \\ldots, k_d\\right) \\in \\mathbb{Z}_{s_1} \\times \\cdots \\times \\mathbb{Z}_{s_d}\\)\uff0c\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u5b9a\u4e49\u4e3a\uff1a</p> \\[  \\begin{aligned} &amp; (\\hat{\\mathcal{F}} f)_l(k)=\\sum_{x_1=0}^{s_1-1} \\cdots \\sum_{x_d=0}^{s_d-1} f_l\\left(x_1, \\ldots, x_d\\right) e^{-2 i \\pi \\sum_{j=1}^d \\frac{x_j k_j}{s_j}} \\\\ &amp; \\left(\\hat{\\mathcal{F}}^{-1} f\\right)_l(x)=\\sum_{k_1=0}^{s_1-1} \\cdots \\sum_{k_d=0}^{s_d-1} f_l\\left(k_1, \\ldots, k_d\\right) e^{2 i \\pi \\sum_{j=1}^d \\frac{x_j k_j}{s_j}} \\end{aligned}  \\] <p>\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u622a\u65ad\u6a21\u5f0f\u7684\u96c6\u5408\u53d8\u4e3a\uff1a</p> \\[ Z_{k_{max}} = \\{(k_1, \\cdots, k_d) \\in \\mathbb{Z}_{s_1} \\times \\cdots \\times \\mathbb{Z}_{s_d} | k_j \\leq k_{max,j} \\space or \\space s_j - k_j \\leq k_{max,j} , \u5bf9\u4e8e j = 1, \\cdots , d\\}  \\] <p>\u5373\\(Z_{k_{max}}\\)\u662f\u622a\u65ad\u540e\u7684\u5085\u91cc\u53f6\u6a21\u5f0f\u96c6\u5408\uff0c\u5728\u79bb\u6563\u5316\u7684\u591a\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u4e0a\u53ea\u4fdd\u7559\u524d\\(k_{max}\\)\u4e2a\u4f4e\u9891\u4ee5\u53ca\u5bf9\u5e94\u7684\u9ad8\u9891(\u7531\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5468\u671f\u6027)</p> <p>\\(R\\)\u6743\u91cd\u5f20\u91cf\u4e00\u4e2a\\(s_1 \\times  \u00b7 \u00b7 \u00b7 \\times s_d \\times d_v \\times d_v\\)\u7684\u5f20\u91cf\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u9009\u62e9\u6bcf\u4e2a\u901a\u9053\u4ea7\u751f\\(k_{max,j} = 12\\)\u8db3\u4ee5\u6ee1\u8db3\u6211\u4eec\u8003\u8651\u7684\u6240\u6709\u4efb\u52a1\u3002</p> <p>\u5bf9\u79bb\u6563\u5316\u7684\u4e0d\u53d8\u6027:</p> <p>\u5085\u91cc\u53f6\u5c42\u662f\u79bb\u6563\u5316\u4e0d\u53d8\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u4ee5\u5b66\u4e60\u548c\u8bc4\u4f30\u4ee5\u4efb\u610f\u65b9\u5f0f\u79bb\u6563\u5316\u7684\u51fd\u6570\u3002\u8be5\u67b6\u6784\u5728\u8f93\u5165\u548c\u8f93\u51fa\u7684\u4efb\u4f55\u5206\u8fa8\u7387\u4e0b\u90fd\u5177\u6709\u4e00\u81f4\u7684\u8bef\u5dee\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/GINO/","title":"GINO: Geometry-Informed Neural Operator for Large-Scale 3D PDEs","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u51e0\u4f55\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50\uff0c\u8bba\u6587\u5730\u5740\uff1aGeometry-Informed Neural Operator for Large-Scale 3D PDEs</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/GINO/#\u51e0\u4f55\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50\u4ecb\u7ecd","title":"\u51e0\u4f55\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50\u4ecb\u7ecd","text":"<ul> <li>\u795e\u7ecf\u7b97\u5b50\u7684\u8f93\u5165\u51fd\u6570\u53ef\u4ee5\u4ee5\u4efb\u610f\u7684\u79bb\u6563\u5316\u3001\u7f51\u683c\u3001\u5206\u8fa8\u7387\u5448\u73b0\uff0c\u8f93\u51fa\u51fd\u6570\u53ef\u4ee5\u5728\u4efb\u4f55\u4efb\u610f\u70b9\u8fdb\u884c\u8bc4\u4f30\u3002</li> </ul>"},{"location":"sci-paper/ComputationalPhysics/GNO/","title":"GNO: Neural Operator: Graph Kernel Network for Partial Differential Equations","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u56fe\u795e\u7ecf\u6838\u7b97\u5b50\uff0c\u8bba\u6587\u5730\u5740\uff1aNeural Operator: Graph Kernel Network for Partial Differential Equations</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/GNO/#\u4ecb\u7ecd","title":"\u4ecb\u7ecd","text":"<p>\u76ee\u6807\u662f\u901a\u8fc7\u5b66\u4e60input-output\u5bf9\u7684\u6709\u9650\u96c6\u5408\u6765\u5b66\u4e60\u4e24\u4e2a\u65e0\u9650\u7ef4\u7a7a\u95f4\u4e4b\u95f4\u7684\u6620\u5c04\uff08supervised learning\uff09\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/HelmFluid/","title":"HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599:HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction</p> <p>Code</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/HelmFluid/#abstract","title":"Abstract","text":"<p>\u795e\u7ecf\u7b97\u5b50\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u5229\u7528\u6df1\u5ea6\u6a21\u578b\u7684\u975e\u7ebf\u6027\u5efa\u6a21\u80fd\u529b\u6765\u8fd1\u4f3c\u8fc7\u53bb\u548c\u672a\u6765\u6d41\u4f53\u4e4b\u95f4\u7684\u590d\u6742\u6620\u5c04\uff0c\u53ef\u80fd\u65e0\u6cd5\u4e3a\u9884\u6d4b\u7ed3\u679c\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u7684\u4f9d\u636e\u3002\u672c\u6587\u901a\u8fc7physical insights\uff0c\u6765\u6355\u83b7\u590d\u6742\u7684\u52a8\u529b\u5b66\uff0c\u4ee5\u5b9e\u73b0\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684\u6d41\u4f53\u9884\u6d4b\uff0c\u63d0\u51fa\u4e86\u4ea5\u59c6\u970d\u5179\u52a8\u529b\u5b66\uff08Helmholtz dynamics\uff09\u4f5c\u4e3a\u8868\u793a\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u65b0\u8303\u5f0f\u3002</p> <p>\u53d7\u5230Helmholtz theorem\u7684\u542f\u53d1\uff0c\u5e76\u5c06\u590d\u6742\u7684\u52a8\u529b\u5b66\u5f52\u56e0\u4e8e\u6d41\u4f53\u7684\u52bf\u51fd\u6570(potential function)\u548c\u6d41\u51fd\u6570(stream function)\uff0c\u5b83\u4eec\u662f\u6d41\u4f53\u7684\u5185\u5728\u7269\u7406\u91cf\uff0c\u53ef\u4ee5\u76f4\u63a5\u63a8\u5bfc\u51fa\u6d41\u4f53\u7684\u65e0\u65cb\u548c\u65e0\u6563\u5ea6\u90e8\u5206(derive the curl-free and divergence-free parts of fluid respectively)\u3002\u4e0e\u8868\u9762\u901f\u5ea6\u573a\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u4ea5\u59c6\u970d\u5179\u52a8\u529b\u5b66\u5c06\u590d\u6742\u7684\u52a8\u529b\u5b66\u5206\u89e3\u4e3a\u66f4\u53ef\u89e3\u7684\u5206\u91cf\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u6df1\u5ea6\u6a21\u578b\u7684\u52a8\u529b\u5b66\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u4e14\u4f7f\u9884\u6d4b\u5177\u6709\u5185\u5728\u7684\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002</p> <p><code>HelmDynamics</code>\u4f5c\u4e3a\u4e00\u79cd\u7075\u6d3b\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u65b9\u4fbf\u5730\u5c06\u8fb9\u754c\u6761\u4ef6\u7f16\u7801\u5230\u76f8\u5173\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u5728\u591a\u79cd\u73b0\u5b9e\u5e94\u7528\u4e2d\u9002\u5e94\u590d\u6742\u7684\u8fb9\u754c\u8bbe\u7f6e\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/HelmFluid/#helmholtz-theorem","title":"Helmholtz Theorem","text":"<p>\u4ea5\u59c6\u970d\u5179\u5b9a\u7406\uff1a\u5bf9\u4e8e\u4e00\u4e2a\u5728\u6709\u754c\u57df\\(V\\)\u4e0a\u7684\u77e2\u91cf\u573a\\(\\mathbf{F(r)}\\)\uff0c\u5176\u5728\u6ee1\u8db3\u4e8c\u9636\u8fde\u7eed\u53ef\u5fae\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u63cf\u8ff0\u4e3a\u4e00\u4e2a\u65e0\u65cb\u573a\u548c\u4e00\u4e2a\u65e0\u6563\u573a\u7684\u53e0\u52a0\uff0c\u516c\u5f0f\u8868\u8ff0\u4e3a\uff1a</p> \\[ \\mathbf{F} = \\nabla \\Phi + \\nabla \\times \\mathbf{A}, \\ \\mathbf{r} \\in \\mathbb{V}. \\] <p>Given a 3D dynamic field \\(\\mathbf{F}: \\mathbb{V} \\rightarrow \\mathbb{R}^3\\) with a bounded domain \\(\\mathbb{V} \\subseteq \\mathbb{R}^3\\), we can obtain the decomposition based on the Helmholtz Theorem:</p> \\[ \\mathbf{F} (\\mathbf{r}) = \\nabla \\Phi(\\mathbf{r}) + \\nabla \\times \\mathbf{A}(\\mathbf{r}), \\ \\mathbf{r} \\in \\mathbb{V}. \\] <p>\u5176\u4e2d\\(\\Phi: \\mathbb{V} \\rightarrow \\mathbb{R}\\)\u4ee3\u8868\u52bf\u51fd\u6570\uff0c\u662f\u4e00\u4e2a\u6807\u91cf\u573a\uff0c\u5176\u68af\u5ea6\u573a\\(\\nabla \\Phi\\)\u4ee3\u8868\\(\\mathbf{F}\\)\u7684\u65e0\u65cb\u90e8\u5206\uff08\\(\\nabla \\times (\\nabla \\Phi)=\\mathbf{0}\\)\uff09\uff1b\\(\\mathbf{A}: \\mathbb{V} \\rightarrow \\mathbb{R}^3\\)\u662f\u6d41\u51fd\u6570\uff0c\u5176\u5411\u91cf\u573a\\(\\nabla \\times \\mathbf{A}\\)\u4ee3\u8868\\(\\mathbf{F}\\)\u7684\u65e0\u6563\u90e8\u5206\uff08\\(\\nabla  (\\nabla \\times \\Phi)=\\mathbf{0}\\)\uff09\uff0c\u4ece\u800c\u4e5f\u8868\u660e\u6d41\u573a\u7684\u4e0d\u53ef\u538b\u7f29\u6027\u3002\u3002</p> <p>Helmholtz dynamics for 2D fluid</p> <p>\u9650\u5236\\(\\mathbf{F}\\)\u7684z\u8f74\u5206\u91cf\u4e3a0\uff0c\u8bb2Helmholtz Theorem\u5e94\u7528\u52302D\u5e73\u9762\uff0c\\(\\mathbf{F}(\\mathbf{r}) = (\\mathbf{F}_x(\\mathbf{r}), \\mathbf{F}_y(\\mathbf{r}),0)^T\\)\u3002\u8fd9\u4e5f\u4f7f\u5f97\u6d41\u51fd\u6570\\(\\mathbf{A}(\\mathbf{r}) = (0,0,\\mathbf{A}_z(\\mathbf{r}))^T\\)\uff0c\u8868\u660e\u6d41\u51fd\u6570\u9000\u5316\u4e3a\u6807\u91cf\u573a\u3002\u5bf9\u4e8e\u5728\\(\\mathbb{V} \\subseteq \\mathbb{R}^2\\)\u76842D\u6d41\u4f53\u573a\uff0c\u6709</p> <p>\\(\\nabla \\times \\mathbf{A(r)} = (\\frac{\\partial A}{\\partial y}-\\frac{\\partial A}{\\partial z}, \\frac{\\partial A}{\\partial z}-\\frac{\\partial A}{\\partial x}, \\frac{\\partial A}{\\partial x}-\\frac{\\partial A}{\\partial y})\\)\uff0c\u56e0\u4e3a\u5047\u8bbe\u7269\u7406\u573a\u4e3a\u4e8c\u7ef4\u573a\uff0c\u6240\u4ee5\u8bbe\\(\\mathbf{A} = (0, 0, A_z)\\)</p> \\[ \\mathbf{F}_{Helm} (\\Phi, \\mathbf{A}) = \\nabla \\Phi + \\nabla \\times \\mathbf{A} = \\underbrace{(\\frac{\\partial \\Phi}{\\partial x}, \\frac{\\partial \\Phi}{\\partial y})}_{\\text{Curl-free Velocity}} + \\underbrace{(\\frac{\\partial A_z}{\\partial y}, -\\frac{\\partial A_z}{\\partial x})}_{\\text{Divergence-free Velocity}}. \\] <p>\u901a\u8fc7\\(\\Phi\\)\u548c\\(\\mathbf{A}\\)\uff0cHelmholtz dynamics\u81ea\u7136\u5730\u5c06\u590d\u6742\u7684\u6d41\u4f53\u5206\u89e3\u4e3a\u66f4\u53ef\u89e3\u7684\u7ec4\u4ef6\uff0c\u5e76\u5c06\u590d\u6742\u7684\u52a8\u529b\u5b66\u62c6\u89e3\u4e3a\u5185\u5728\u7269\u7406\u91cf\u4e2d\uff0c\u4ece\u800c\u6709\u5229\u4e8e\u66f4\u53ef\u89e3\u91ca\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/HelmFluid/#helmdynamics-block","title":"HelmDynamics Block","text":"<p>\u672c\u6587\u63d0\u51fa\u4e86<code>HelmDynamics Block</code>\u6765\u4ece\u8fc7\u53bb\u89c2\u6d4b\u4e2d\u4f30\u8ba1\u52bf\u51fd\u6570\u548c\u6d41\u51fd\u6570\u3002\u9996\u5148\u5c06\u5c06\u8f93\u5165\u89c2\u6d4b\u5d4c\u5165\u5230\u4e24\u4e2a\u8fde\u7eed\u7684\u6df1\u5ea6\u8868\u793a\u4e2d\uff0c\u4ee5\u663e\u5f0f\u5730\u4fdd\u6301\u65f6\u95f4\u52a8\u6001\u4fe1\u606f\u3002\u7ed9\u5b9a\u4e00\u4e2a\u8fde\u7eed\u89c2\u5bdf\u5230\u7684\\(T\\)\u5e27\u5e8f\u5217\\(x=[x_1,...,x_T], \\ x_i \\in \\mathbb{R}^{H \\times W}\\)\uff08\\(x_i\\)\u8868\u793a\u4e00\u5f20\u7269\u7406\u573a\u56fe\uff09:</p> \\[ \\begin{aligned} \\hat{x}_{T-1} = Embed(x_{T-\\tau:T-1}) \\\\ \\hat{x}_T = Embed(x_{T-\\tau+1:T}) \\\\ \\end{aligned} \\] <p>\u5176\u4e2d\\(\\hat{x}_{T-1}, \\ \\hat{x}_T \\in \\mathbb{R}^{d_{model} \\times H \\times W}\\)\u662f\\(T-1\\)\u548c\\(T\\)\u65f6\u523b\u7684\u7279\u5f81\u5f20\u91cf\uff08feature tensors\uff09\uff0c\u6211\u4eec\u4ece\\(\\tau\\)\u5f00\u59cb\u56de\u6eaf\u7a97\u53e3\u5d4c\u5165\u4ee5\u6355\u83b7\u65f6\u7a7a\u4fe1\u606f\uff08\u901a\u8fc7\u4e24\u4e2a\u6fc0\u6d3b\u51fd\u6570\u4e2d\u95f4\u7684\u5377\u79ef\u5c42\uff09\uff0c\u6295\u5f71\u5230\u901a\u9053\u7ef4\u5ea6\\(d_{model}\\)\u4e2d\u3002</p> <p>\u800c\u540e\u91c7\u7528\u524d\u4e00\u4e2a\u65f6\u523b\u4e0e\u5f53\u524d\u65f6\u523b\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u6765\u8868\u793a\u52a8\u6001\u4fe1\u606f\uff0c\u5e76\u4e14\u5c06\u8fb9\u754c\u6761\u4ef6\\(S\\)\u5305\u542b\u8fdb\u53bb\uff1a</p> \\[ \\mathbf{c} (\\mathbf{r}) = \\text{Concat} \\Big( [\\hat{x}_T (\\mathbf{r}) \\cdot \\hat{x}_{T-1} (\\mathbf{r'})]_{\\mathbf{r'} \\in \\mathbf{N_r}} \\ , [\\mathbb{1}_S (\\mathbf{r'}) (\\hat{x}_T (\\mathbf{r}) \\cdot \\hat{x}_{T-1}(\\mathbf{r'})) ]_{\\mathbf{r'} \\in \\mathbf{N_r}} \\Big), \\ \\mathbf{r} \\in \\mathbb{V}. \\] <p>\u5176\u4e2d\\(N_r\\)\u8868\u793a\u4f4d\u7f6e\\(r\\)\u5468\u56f4\u7684\u90bb\u5c45\u8282\u70b9\uff0c\\(\\mathbb{1}_S\\)\u8868\u793a\u8fb9\u754c\u6761\u4ef6\u3002\\(c(r) \\in \\mathbb{R}^{2|N_r|}\\)\u8868\u793a\u5f53\u524d\u6d41\u4f53\u5728\\(r\\)\u4f4d\u7f6e\u4e0e\\(|Nr|\\)\u90bb\u5c45\u7684\u76f8\u5173\u56fe\uff0c\u5e76\u989d\u5916\u8003\u8651\u8fb9\u754c\u6761\u4ef6\\(S\\)\u3002\u56e0\u6b64\\(\\mathbf{c} \\in \\mathbb{R}^{2|N_r| \\times H \\times W}\\)\u3002</p> \\[ \\begin{aligned} \\hat{\\Phi} = Decoder_{\\Phi} (\\mathbf{c})&amp;, \\ \\hat{\\mathbf{A}} = Decoder_{\\mathbf{A}} (\\mathbf{c}) \\\\ \\hat{F}_{Helm} =&amp; \\nabla \\hat{\\Phi} + \\nabla \\times \\hat{\\mathbf{A}} \\end{aligned} \\] <p>\u5176\u4e2d\\(\\hat{\\Phi}, \\ \\hat{\\mathbf{A}} \\in \\mathbb{R}^{H \\times W}\\)\u548c\\(\\hat{F}_{Helm} \\in \\mathbb{R}^{2 \\times H \\times W}\\)\u8868\u793a\u5b66\u4e60\u5230\u7684\u4e8c\u7ef4\u573a\u3002\u672c\u6587\u4e2d\\(Decoder_{\\Phi}, \\ Decoder_{\\mathbf{A}}\\)\u662f\u4e24\u4e2a\u5377\u79ef\u5c42\u3002\u603b\u7ed3\uff1a</p> \\[ \\hat{F}_{Helm} = \\text{HelmDynamics} (\\hat{x}_{(T-1)}, \\hat{x}_T) \\]"},{"location":"sci-paper/ComputationalPhysics/HelmFluid/#multi-scale-multi-head-integral-architecture\u591a\u5c3a\u5ea6\u591a\u5934\u79ef\u5206\u67b6\u6784","title":"Multi-scale Multi-head Integral Architecture(\u591a\u5c3a\u5ea6\u591a\u5934\u79ef\u5206\u67b6\u6784)","text":"<p>\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u6d41\u4f53\u52a8\u529b\u5b66\u4ee5\u53ca\u975e\u7ebf\u6027\u6355\u6349\u7684\u80fd\u529b\uff0c\u672c\u6587\u91c7\u7528\u4e86\u5e38\u89c1\u7684\u591a\u5934\u8bbe\u8ba1\u3002\u7ed9\u5b9a\u4e0a\u8ff0\u7684\\(\\hat{x}_{T-1}, \\ \\hat{x}_T \\in \\mathbb{d_{model} \\times H \\times W}\\)\uff0c\u6211\u4eec\u5c06\u5b83\u4eec\u6cbf\u7740\u901a\u9053\u7ef4\u5ea6\u5206\u4e3a\u591a\u4e2aHeads\uff0c\u5373\\(\\hat{x}_{(T-1),i}, \\ \\hat{x}_{T,i} \\in \\mathbb{R}^{d_{model} \\times H \\times W}, \\ i \\in \\{ 1,...,M \\}\\)\uff08\\(M\\)\u4e3a\u5934\u6570\uff09\uff1a</p> \\[ \\hat{\\mathbf{F}}_{Helm,i} = \\text{HelmDynamics} (\\hat{x}_{{T-1},i}, \\hat{x}_{T,i}) \\] <p>where \\(\\hat{\\mathbf{F}}_{Helm,i} \\in \\mathbb{R}^{2 \\times H \\times W}\\)\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/HelmFluid/#multi-scale-modeling","title":"Multi-scale modeling","text":"<p>\u5728\u7269\u7406\u5b66\u4e2d\uff0c\u6d41\u4f53\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6027\u8d28\u3002\u8fd9\u4e9b\u591a\u5c3a\u5ea6\u52a8\u529b\u5b66\u76f8\u4e92\u7ea0\u7f20\uff0c\u4f7f\u5f97\u6d41\u4f53\u6781\u5176\u68d8\u624b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u91c7\u7528\u591a\u5c3a\u5ea6\u5efa\u6a21\u6846\u67b6\u6765\u589e\u5f3a\u52a8\u6001\u5efa\u6a21\u3002</p> <p>\u7ed9\u5b9a\u4e0a\u8ff0\u7684\\(\\hat{x}_{T-1}, \\ \\hat{x}_T \\in \\mathbb{d_{model} \\times H \\times W}\\)\uff0c\u6211\u4eec\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff08multi-scale encoder\uff09\uff0c\u5728\\(L scales\\)\u4e0a\u83b7\u5f97\u6df1\u5ea6\u8868\u793a\uff1a\\(\\hat{x}_{T-1}^l, \\ \\hat{x}_T^l \\in \\mathbb{d_{model}^l \\times [\\frac{H}{2^{(l-1)}}] \\times [\\frac{W}{2^{(l-1)}}]}, \\ l \\in \\{1,...,L \\}\\)\u3002\u663e\u7136\u8f83\u5927\u5c3a\u5ea6\u7684\u7269\u7406\u573a\u53d7\u566a\u58f0\u5f71\u54cd\u8f83\u5c0f\uff0c\u5e76\u4e14\u80fd\u591f\u4e3a\u8f83\u5c0f\u7684\u5c3a\u5ea6\u63d0\u4f9b\u53ef\u9760\u7684\u80cc\u666f\u901f\u5ea6\u573a\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5230\u7684\u52a8\u529b\u5b66\u4ece\u7c97\u5230\u7ec6\u96c6\u6210\uff0c\u4ee5\u7b80\u5316\u591a\u5c3a\u5ea6\u52a8\u529b\u5b66\u5efa\u6a21\u8fc7\u7a0b\uff1a</p> \\[ \\hat{v}_i^l = \\begin{cases} \\hat{F}^l_{Helm,i}, \\ l=L \\\\ \\hat{F}^l_{Helm,i} + \\text{Upsample} (\\hat{v}_{i}^{l+1}) \\, \\ 1 \\leq l&lt;L \\\\ \\end{cases}  \\] <p>where \\(\\hat{v}_i^l \\in \\mathbb{R}^{2 \\times H \\times W}\\)\u3002\\(\\text{Upsample}(\\cdot)\\)\u662f\u4fdd\u6301\u5206\u8fa8\u7387\u517c\u5bb9\u7684\u53cc\u7ebf\u6027\u63d2\u503c(bilinear interpolation)\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/HelmFluid/#tempointegral-block","title":"TempoIntegral block","text":"<p>\u4e3a\u4e86\u9884\u6d4b\u672a\u6765\u7684\u6d41\u4f53\u573a\uff0c\u6211\u4eec\u6cbf\u65f6\u95f4\u7ef4\u5ea6\u6574\u5408\u7279\u5f81\u7a7a\u95f4\u3002\u5bf9\u4e8e\u5c3a\u5ea6\\(l \\in 1,2,...,L\\)\uff0chead \\(i \\in 1,2,...,M\\)\u3002\u6211\u4eec\u901a\u8fc7\u5176\u5bf9\u5e94\u7684\u901f\u5ea6\u573a\\(\\hat{v}_i^l\\)\u6765\u4e0e\u6df1\u5ea6\u8868\u793a\\(\\hat{x}^l_{T,i}\\)\u76f8\u6574\u5408\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/NSFnets/","title":"NSFnets: Physics-informed neural networks for the incompressible Navier-Stokes equations","text":""},{"location":"sci-paper/ComputationalPhysics/Nerualoperator/","title":"\u603b\u7ed3\u9875\uff1a\u795e\u7ecf\u7b97\u5b50Nerual Operator","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u76f8\u5173\u9605\u8bfb\u8d44\u6599:\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u7684\u6570\u5b66\u6307\u5357</p> <p></p> <p>\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u65e8\u5728\u4ece\u6570\u636e\u4e2d\u63a2\u7d22\u5e95\u5c42\u52a8\u6001\u7cfb\u7edf\u6216\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u7279\u6027\u3002\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u7684\u9010\u6b65\u6307\u5bfc\u624b\u518c\u3002\u6211\u4eec\u9610\u660e\u4e86\u54ea\u4e9b\u7c7b\u578b\u7684\u95ee\u9898\u548cPDE\u9002\u5408\u4e8e\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\uff0c\u8ba8\u8bba\u4e86\u4e0d\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u8bf4\u660e\u4e86\u5982\u4f55\u6709\u6548\u5730\u5e94\u7528\u6570\u503cPDE\u6c42\u89e3\u5668\u3002\u6211\u4eec\u8fd8\u7ed9\u51fa\u4e86\u5173\u4e8e\u5982\u4f55\u521b\u5efa\u548c\u7ba1\u7406\u8bad\u7ec3\u6570\u636e\u53ca\u5176\u4f18\u5316\u7684\u5efa\u8bae\u3002\u901a\u8fc7\u4ece\u6570\u503c\u7ebf\u6027\u4ee3\u6570\u7684\u89c6\u89d2\u51fa\u53d1\uff0c\u6211\u4eec\u4e3a\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u7b97\u5b50\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u89e3\u91ca\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/PoF/","title":"PI-DeepONet: A fast general thermal simulation model based on Multi-Branch Physics-Informed deep operator neural network","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aA fast general thermal simulation model based on Multi-Branch Physics-Informed deep operator neural network</p> <p>\u4ee3\u7801:DeepXDE</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/Transolver%2B%2B/","title":"Transolver++: An Accurate Neural Solver for PDEs on Million-Scale Geometries","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1a</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/Transolver%2B%2B/#abstract","title":"Abstract","text":"<p>\u5bf9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6c42\u89e3PDEs\u65b9\u9762\u7684\u5e94\u7528\uff0c\u8fc7\u53bb\u7684\u5de5\u4f5c\u4e3b\u8981\u5c40\u9650\u4e8e\u4ec5\u6709\u6570\u4e07\u4e2amesh points\u7684\u6570\u636e\uff0c\u800c\u5de5\u4e1a\u6a21\u62df\u4e0a\u5bf9\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4e00\u822c\u9700\u8981\u767e\u4e07\u7ea7\u7684\u7f51\u683c\u5c3a\u5ea6\u3002</p> <p>\u57fa\u4e8e\u4e4b\u524d\u901a\u8fc7<code>Transolver</code>\u5b66\u4e60\u7269\u7406\u72b6\u6001\u6765\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u8fdb\u5c55\uff0c<code>Transolver++</code>\u8fdb\u4e00\u6b65\u914d\u5907\u4e86\u4e00\u4e2a\u5e76\u884c\u6846\u67b6(parallelism framework)\u548c\u4e00\u4e2a\u5c40\u90e8\u81ea\u9002\u5e94\u673a\u5236(local adaptive mechanism)\uff0c\u4ee5\u6709\u6548\u5730\u4ece\u6d77\u91cf\u7f51\u683c\u70b9\u4e2d\u6355\u83b7\u7269\u7406\u72b6\u6001\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5728\u6269\u5927\u8f93\u5165\u7f51\u683c\u5927\u5c0f\u65f6\u8ba1\u7b97\u548c\u7269\u7406\u5b66\u4e60\u4e2d\u7684\u68d8\u624b\u6311\u6218\u3002</p> <p>\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u6269\u5c55\u5230\u8d85\u8fc7400k\u4e2a\u70b9\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u901a\u5e38\u6d89\u53ca\u767e\u4e07\u5c3a\u5ea6\u7684\u7f51\u683c\u70b9\u751a\u81f3\u66f4\u591a\u3002\u800c\u6709\u9650\u7684\u7f51\u683c\u5c3a\u5bf8\u4f1a\u4e25\u91cd\u727a\u7272\u51e0\u4f55\u7684\u7cbe\u5ea6\uff0c\u4f7f\u5f97\u7f51\u683c\u8868\u9762\u53d8\u5f97\u7c97\u7cd9\u548c\u4e0d\u5747\u5300\uff0c\u8fd9\u5c06\u4e25\u91cd\u9650\u5236\u6a21\u578b\u7684\u6a21\u62df\u7cbe\u5ea6\u3002<code>Transolver</code>\u7684\u6700\u5927\u8f93\u5165\u5c40\u9650\u4e8e700k\u4e2a\u70b9\uff0c\u4e14\u5b9e\u9a8c\u6570\u636e\u6bd4\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7b80\u5355\u8bb8\u591a\u3002\u800c\u5f3a\u884c\u5c06\u6a21\u578b\u62d3\u5c55\u81f3\u767e\u4e07\u89c4\u6a21\u7684\u9ad8\u4fdd\u771fPDE\u89e3\u51b3\u4efb\u52a1\u65f6\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5b83\u5728\u7269\u7406\u5b66\u4e60\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u74f6\u9888\uff0c\u5927\u91cf\u7684\u7f51\u683c\u70b9\u53ef\u80fd\u4f1a\u538b\u5012(overwhelm)\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u7269\u7406\u72b6\u6001\u548c\u6a21\u578b\u9000\u5316\u3002</p> <p>\u56e0\u6b64\uff0c<code>Transolver++</code>\u4f7f\u7528\u9ad8\u5ea6\u4f18\u5316\u7684\u5e76\u884c\u6846\u67b6\u548c\u5c40\u90e8\u81ea\u9002\u5e94\u673a\u5236\u5bf9<code>Transolver</code>\u8fdb\u884c\u5347\u7ea7\uff0c\u89e3\u9501\u591aGPU\u5e76\u884c\u6027\u7684\u529b\u91cf\uff0c\u4ee5\u6709\u6548\u5730\u4ece\u5927\u91cf\u7f51\u683c\u70b9\u4e2d\u6355\u83b7\u7269\u7406\u72b6\u6001\u3002\u57fa\u4e8e\u6a21\u578b\u67b6\u6784\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u6211\u4eec\u7684\u5e76\u884c\u6846\u67b6\u663e\u7740\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u8d44\u6e90\u7684\u7ebf\u6027\u53ef\u6269\u5c55\u6027\u3002</p> <p>Highlight</p> <ul> <li> <p><code>Transolver++</code>\u5c55\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u5ea6\u5e76\u884c\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728GPU\u4e4b\u95f4\u5177\u6709\u7ebf\u6027\u53ef\u6269\u5c55\u6027\u548c\u6700\u5c0f\u7684\u5f00\u9500</p> </li> <li> <p>\u4ece\u516d\u4e2a\u6807\u51c6\u57fa\u51c6\u5e73\u5747\u5b9e\u73b0\u4e8613%\u7684\u76f8\u5bf9\u589e\u76ca\uff0c\u5728\u9ad8\u4fdd\u771f\u767e\u4e07\u89c4\u6a21\u7684\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e8620%\u4ee5\u4e0a</p> </li> </ul>"},{"location":"sci-paper/ComputationalPhysics/Transolver%2B%2B/#method","title":"Method","text":"<p>Physics-Attention with Eidetic States</p> <p>\u5728\u5927\u5c3a\u5ea6\u7684\u7f51\u683c\u4e2d\uff0c\u539f\u672c\u7684<code>Physics-Attention</code>\u4e2d\u7684\u5207\u7247\u6743\u91cdw\u5bb9\u6613\u8d8b\u4e8e\u5747\u5300\uff0c\u9000\u5316\u4e3a\u5e73\u5747\u6c60\u5316\uff0c\u5bfc\u81f4\u540c\u8d28\u5316\u7684\u7269\u7406\u72b6\u6001\uff08homogeneous physical states\uff09\uff0c\u56e0\u6b64\u5931\u53bb\u7269\u7406\u5efa\u6a21\u80fd\u529b\u3002<code>Transolver</code>\u4f7f\u7528softmax\u51fd\u6570\u8ba1\u7b97\u5207\u7247\u6743\u91cd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u65e0\u6cd5\u533a\u5206\u7684\u72b6\u6001\uff0c\u4f46\u4ecd\u65e0\u6cd5\u907f\u514d\u5728\u6a21\u578b\u6df1\u5ea6\u589e\u52a0\u65f6\u7684\u9000\u5316\u73b0\u8c61\u3002</p> <p>\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u5b66\u4e60\u7ec6\u5fae\u7269\u7406\u4fe1\u606f\u7684\u80fd\u529b\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684</p> <p>\u5728<code>Transolver++</code>\u4e2d\uff0c\u901a\u8fc7\u4f7f\u7528\u5c40\u90e8\u81ea\u9002\u5e94\u673a\u5236\uff08Local adaptive mechanism\uff09\u548c\u5207\u7247\u91cd\u65b0\u53c2\u6570\u5316\uff08Slice reparameterization\uff09\u6765\u589e\u5f3a<code>Transolver</code>\u7684\u72b6\u6001\u5b66\u4e60\u6765\u6355\u6349\u7ec6\u81f4\u7684\u7269\u7406\u72b6\u6001\uff0c\u4ee5\u4ed4\u7ec6\u63a7\u5236\u5b66\u4e60\u5230\u7684\u5207\u7247\u6743\u91cd\u5206\u5e03\u3002</p> <p>\u5c40\u90e8\u81ea\u9002\u5e94\u673a\u5236\uff08Local adaptive mechanism\uff09\u5c06\u5c40\u90e8\u4fe1\u606f\u4f5c\u4e3a\u5bf9\u7269\u7406\u5b66\u4e60\u8fc7\u7a0b\u7684\u9010\u70b9\u8c03\u6574\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u4e00\u4e2aSoftmax\u51fd\u6570\u4e2d\u53ef\u5b66\u4e60\u7684\u6e29\u5ea6\u7cfb\u6570\\(\u03c4_0\\)\u6765\u6539\u53d8\u72b6\u6001\u5206\u5e03\u7684\u9510\u5ea6\uff1a</p> \\[ \\text{Ada-Temp:} \\ \\tau_0 = { \\{ \\tau_i \\} }_{i=1}^N = { \\{ \\tau_0 + Linear(\\mathbf{x_i}) \\} }_{i=1}^N \\] <p>\u5176\u4e2d\\(\\tau \\in \\mathbb{R}^{N \\times 1}\\)\uff0c\u57fa\u4e8e\u6bcf\u4e2a\u70b9\u7684\u5c40\u90e8\u5c5e\u6027\u6211\u4eec\u901a\u8fc7\u7ebf\u6027\u5c42\u6765\u8c03\u8282\u72b6\u6001\u5206\u5e03\u3002</p> <p>\u8f83\u9ad8\u7684\u6e29\u5ea6\u7cfb\u6570\u80fd\u5f62\u6210\u66f4\u5747\u5300\u7684\u5206\u5e03\uff0c\u800c\u8f83\u4f4e\u7684\u6e29\u5ea6\u4f7f\u5206\u5e03\u66f4\u96c6\u4e2d\u5728\u5173\u952e\u72b6\u6001\u4e0a\u3002</p> <p>\u4f46\u662f\u4ec5\u4ec5\u5b66\u4e60\u6e29\u5ea6\u7cfb\u6570\u5e76\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u95ee\u9898\u3002\u6211\u4eec\u91c7\u7528 <code>Gumbel-Softmax</code>:</p> \\[ \\text{Rep-Slice}( \\mathbf{x}, \\tau ) =  \\text{Softmax} \\Big( \\frac{\\text{Linear}(\\mathbf{x}) - \\log{(-\\log{\\epsilon})} }{\\tau} \\Big) \\] <p>\u5176\u4e2d\\(\\tau \\in \\mathbb{R}^{N \\times 1}\\)\u662f\u5c40\u90e8\u81ea\u9002\u5e94\u6e29\u5ea6\uff0c\u4e14\\(epsilon=\\{ {\\epsilon_i}^N_{i=1} \\}, \\ \\epsilon_i \\sim \\mathcal{U}(0,1)\\)\u3002\u6240\u8c13\\(\\log{(-\\log{\\epsilon})} \\sim \\text{Gumbel}(0,1)\\)\u3002\u5982\u4e0a\u56fe\u00a9\uff0c<code>Transolver++</code>\u4e2d\u7684\u5207\u7247\u6743\u91cd\u53ef\u4ee5\u5f88\u597d\u5730\u9002\u5e94\u590d\u6742\u51e0\u4f55\u56fe\u5f62\u4e0a\u590d\u6742\u7684\u7269\u7406\u573a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5177\u6709\u7f13\u6162\u53d8\u5316\u7269\u7406\u91cf\u7684\u533a\u57df\u88ab\u5206\u914d\u5230\u4e00\u4e2a\u7279\u5b9a\u7684\u72b6\u6001\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u533a\u57df\u7531\u4e00\u4e2a\u7eaf\u7269\u7406\u72b6\u6001\u63a7\u5236\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5177\u6709\u5feb\u901f\u53d8\u5316\u7684\u7269\u7406\u91cf\u7684\u533a\u57df\u5728\u7269\u7406\u72b6\u6001\u4e0b\u8868\u73b0\u51fa\u591a\u6a21\u5206\u5e03\uff0c\u8fd9\u53cd\u6620\u4e86\u8fd9\u4e9b\u533a\u57df\u53d7\u5230\u591a\u79cd\u72b6\u6001\u7684\u6df7\u5408\u7684\u5f71\u54cd\u3002</p> <p>Parallel Transolver++</p> <p>\u4e0d\u592a\u4e86\u89e3\u5e76\u884c\u8ba1\u7b97\uff0c\u5148\u8df3\u8fc7</p>"},{"location":"sci-paper/ComputationalPhysics/Transovler/","title":"Transolver: A Fast Transformer Solver for PDEs on General Geometries","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aTransolver: A Fast Transformer Solver for PDEs on General Geometries</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/thuml/transolver</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/Transovler/#abstract","title":"Abstract","text":"<p>Since PDEs are typically discretized into large-scale meshes with complex geometries, it's hard for traditional Transformers to capture intricate physical corrlations directly form massive individual points.</p> <p>So the team present <code>Transolver(Physics Attention)</code> learning intrinsic physical states hidden behind discretized geometries.<code>Physics Attention</code> adaptively split the discretized domain into a set of slides with flexible shapes.by calculatiing attention to physics-aware tokens encoded from slices, <code>Transolver</code> can effectivly capture the physical correlations under complex geometries and compute in linear complexity.</p>"},{"location":"sci-paper/ComputationalPhysics/Transovler/#method","title":"Method","text":"<p>Problem Defination:</p> <p>Input domain \\(\\Omega \\subset \\mathbb{R}^{C_g}\\), where \\(C_g\\) denotes the dim of input space.\\(\\Omega\\) is discretized into a set of meshes points \\(\\mathbb{g} \\subset \\mathbb{R}^{C_g \\times N}\\).The task is to estimate target physical quantities based on input geometrics \\(\\mathbb{g}\\) and quantities \\(\\mathbb{u} \\in \\mathbb{R}^{N \\times C_u}\\).</p> <p>Here \\(\\mathbb{u}\\) is optional in some PDE-governed problems.</p> <p>The key to solving PDEs is to capture intricate physical correlations.But it's hard for <code>Attention</code> to learn from such massive discretized mesh points.</p> <p>These mesh points are a finite discrete sampling(\u6709\u9650\u79bb\u6563\u91c7\u6837) of the underlying continuous physics space,which inspires us to capture the intrinsic physical states hidden behind discretized geometries.</p> <p>We find that the surface mesh set can be ascribed to several physically internal-consistent subsets(\u7269\u7406\u4e0a\u5185\u90e8\u4e00\u81f4\u7684\u5b50\u96c6),such as front, bevel, back areas. </p> <p>Given a mesh set \\(\\mathbb{g}=\\{\\mathbb{g}_i\\}^N_{i=1}\\) with the coordinates information(\u5750\u6807\u4fe1\u606f) of \\(N\\) mesh points and observed quantities \\(\\mathbb{u}\\), we embed them into deep features \\(\\mathbb{x}=\\{\\mathbb{x}_i \\in \\mathbb{R}^{1 \\times C} \\}^N_{i=1}\\) by a linear layer, where \\(x_i\\) involves both geometry and physics information.</p> <p>We ascribes each mesh point \\(g_i\\) to \\(M\\) potential slices based on its learned feature \\(X_i\\):</p> \\[ \\{w_i \\in \\mathbb{R}^{1\\times M}\\}_{i=1}^N = \\{Softmax(Projection(x_i))\\}^N_{i=1} \\] \\[ s_j \\in \\mathbb{R}^{N \\times C} = \\{ w_{i,j} x_i \\}^N_{i=1} \\] <p></p> <p>\\(\\sum^M_{j=1} w_{i,j}=1\\).Specifically, \\(w_{i,j}\\) represents the degree that the i-th point belongs to the j-th slices.</p> <p>\\(s_j\\) represents the j-th slice feature, which is a weighted sum of all mesh points' features \\(x\\).</p> <p>mesh points with close features will derive similar slice weights, which means they are more likely to be assigned to the same slice.</p> \\[ z_j \\in \\mathbb{R}^{1 \\times C} = \\frac{\\sum_{i=1}^N s_{j,i}}{\\sum_{i=1}^N w_{i,j}} = \\frac{\\sum_{i=1}^N w_{i,j} x_i}{\\sum_{i=1}^N w_{i,j}} \\] <p>Since each slice contains mesh points with similar geometry and physics features, we encode them into physical-aware tokens \\(z_j\\).</p> <p>Why does this silce method work? :</p> <p>In the method, slice weights are projected from mesh features, which means that mesh points with similar features will get similar slice weights(will be assigned to the same slice). it is useful to capture points under similar physical states but spatially distant.</p> <p></p> <p>Physics-Attention</p> <p>We employ attention mechanism to capture intricate correlations among diffenert physical states:</p> \\[ \\mathbf{q,k,v} \\in \\mathbb{R}^{M \\times C} = Linear(\\mathbf{z}), \\mathbf{z}' = Softmax(\\frac{\\mathbf{q} \\mathbf{k}^T}{\\sqrt{C}}) \\mathbf{v} \\] <p>Then \\(z'={z'_j}^M_{j=1}\\) are transformed back to mesh points:</p> \\[ \\mathbf{x}'_i = \\sum_{j=1}^M \\mathbf{w}_{i,j} \\mathbf{z}'_{j} \\] <p>where \\(1 \u2264 i \u2264 N\\) and each token is broadcasted to all mesh points.</p> <p>The whole process can be summarized as:</p> \\[ \\mathbf{\\hat{x}}^l = Physics-Atten(LayerNorm(\\mathbb{\\mathbf{x}^{l-1}})) + \\mathbf{x}^{l-1} \\] \\[ \\mathbf{x}_l = FeedForward(\\mathbf{\\hat{x}}^l) + \\mathbf{\\hat{x}}^l \\] <p>where \\(l \\in \\{ 1,...,L \\}\\), \\(\\mathbf{x}^l \\in \\mathbb{R}^{N \\times C}\\) is the l-th layer output.\\(\\mathbf{x}^0 \\in \\mathbb{R}^{N \\times C}\\) represents the input deep feature, which is embedde from input geometries \\(g \\in \\mathbb{g}^{N \\times C_g}\\) and observed quantities \\(\\mathbb{u} \\in \\mathbb{R}^{N \\times C_u}\\). by linear layer.</p> <p></p> <p></p> <p>Toward an intuitive understanding of Transolver, we visualize Physics-Attention.In the inner-stress estimation task of Elasticity,we can observe that slices learn to cover diverse subareas that are under similar extrusion degress, such as the left and right of the hollow area, corners of the material.</p> <p></p> <p>Which suprising us is that for 50% randomly sampled input mesh of Elasticity, Transolver can still capture physical states precisely even for broken meshes.</p> <p></p> <p>Out-of-distribution(OOD) generalization:</p> <p>In the previous research, neural solvers are mainly trained and tested with samples under the same or in-distribution PDE coefficients and varied initial or boundary conditions. </p> <p>For example, in the car design task, different samples of diverse shapes are all generated under the headwind(\u9006\u98ce) with the same speed. As for the airfoil design, although training samples contain various Reynolds and angles of attacks, the test set is still under the same range of Reynolds and angles as the training set. </p> <p>To further examine the generalizability(\u6cdb\u5316\u6027) of Transolver in real-world applications, we experiment with OOD airfoil design tasks, where the test set has completely different Reynolds and angles of attacks.</p>"},{"location":"sci-paper/ComputationalPhysics/Transovler/#code","title":"Code","text":"models <pre><code>import torch\nimport numpy as np\nimport torch.nn as nn\nfrom timm.models.layers import trunc_normal_\nfrom einops import rearrange, repeat\n\nACTIVATION = {'gelu': nn.GELU, 'tanh': nn.Tanh, 'sigmoid': nn.Sigmoid, 'relu': nn.ReLU, 'leaky_relu': nn.LeakyReLU(0.1),\n              'softplus': nn.Softplus, 'ELU': nn.ELU, 'silu': nn.SiLU}\n\n\nclass Physics_Attention_Irregular_Mesh(nn.Module):\n    def __init__(self, dim, heads=8, dim_head=64, dropout=0., slice_num=64): \n        super().__init__()\n        inner_dim = dim_head * heads\n        self.dim_head = dim_head\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.softmax = nn.Softmax(dim=-1)\n        self.dropout = nn.Dropout(dropout)\n        self.temperature = nn.Parameter(torch.ones([1, heads, 1, 1]) * 0.5)\n\n        self.in_project_x = nn.Linear(dim, inner_dim)\n        self.in_project_fx = nn.Linear(dim, inner_dim)\n        self.in_project_slice = nn.Linear(dim_head, slice_num)\n        for l in [self.in_project_slice]:\n            torch.nn.init.orthogonal_(l.weight)  # use a principled initialization\n            # \u6b63\u4ea4\u521d\u59cb\u5316\n        self.to_q = nn.Linear(dim_head, dim_head, bias=False)\n        self.to_k = nn.Linear(dim_head, dim_head, bias=False)\n        self.to_v = nn.Linear(dim_head, dim_head, bias=False)\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        # Batch Num Channel\n        B, N, C = x.shape\n\n        ### (1) Slice\n        fx_mid = self.in_project_fx(x).reshape(B, N, self.heads, self.dim_head) \\\n            .permute(0, 2, 1, 3).contiguous()  # B H N C\n        x_mid = self.in_project_x(x).reshape(B, N, self.heads, self.dim_head) \\\n            .permute(0, 2, 1, 3).contiguous()  # B H N C\n        slice_weights = self.softmax(self.in_project_slice(x_mid) / self.temperature)  # B H N G(slices)\n        # Temperature Parameter \u8c03\u8282\u5206\u5e03\u7684\u5e73\u6ed1\u5ea6\n        slice_norm = slice_weights.sum(2)  # B H G\n        slice_token = torch.einsum(\"bhnc,bhng-&gt;bhgc\", fx_mid, slice_weights) # B H G C\n        # \u7231\u56e0\u65af\u5766\u6c42\u548c\u7ea6\u5b9a \n        slice_token = slice_token / ((slice_norm + 1e-5)[:, :, :, None].repeat(1, 1, 1, self.dim_head))\n                                    # (B H G 1) --&gt; (B H G C)\n        ### (2) Attention among slice tokens\n        q_slice_token = self.to_q(slice_token) # B H G C\n        k_slice_token = self.to_k(slice_token)\n        v_slice_token = self.to_v(slice_token)\n        dots = torch.matmul(q_slice_token, k_slice_token.transpose(-1, -2)) * self.scale # B H G G\n        attn = self.softmax(dots) # B H G G\n        attn = self.dropout(attn) \n        out_slice_token = torch.matmul(attn, v_slice_token)  # B H G G x B H G C \u2014\u2014&gt; B H G C \u6ce8\u610f\u529b\u5206\u914d\u540e\u7684\u5207\u7247\n\n        ### (3) Deslice\n        out_x = torch.einsum(\"bhgc,bhng-&gt;bhnc\", out_slice_token, slice_weights) # \u53cd\u5207\u7247 B H G C x B H N G \u2014\u2014&gt; B H N C\n        out_x = rearrange(out_x, 'b h n d -&gt; b n (h d)')\n        return self.to_out(out_x) # B N dim\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_input, n_hidden, n_output, n_layers=1, act='gelu', res=True):\n        super(MLP, self).__init__()\n\n        if act in ACTIVATION.keys():\n            act = ACTIVATION[act]\n        else:\n            raise NotImplementedError\n        self.n_input = n_input\n        self.n_hidden = n_hidden\n        self.n_output = n_output\n        self.n_layers = n_layers\n        self.res = res\n        self.linear_pre = nn.Sequential(nn.Linear(n_input, n_hidden), act())\n        self.linear_post = nn.Linear(n_hidden, n_output)\n        self.linears = nn.ModuleList([nn.Sequential(nn.Linear(n_hidden, n_hidden), act()) for _ in range(n_layers)])\n\n    def forward(self, x):\n        x = self.linear_pre(x)\n        for i in range(self.n_layers):\n            if self.res:\n                x = self.linears[i](x) + x\n            else:\n                x = self.linears[i](x)\n        x = self.linear_post(x)\n        return x\n\n\nclass Transolver_block(nn.Module):\n    \"\"\"Transformer encoder block.\"\"\"\n\n    def __init__(\n            self,\n            num_heads: int,\n            hidden_dim: int,\n            dropout: float,\n            act='gelu',\n            mlp_ratio=4,\n            last_layer=False,\n            out_dim=1,\n            slice_num=32,\n    ):\n        super().__init__()\n        self.last_layer = last_layer\n        self.ln_1 = nn.LayerNorm(hidden_dim)\n        self.Attn = Physics_Attention_Irregular_Mesh(hidden_dim, heads=num_heads, dim_head=hidden_dim // num_heads,\n                                                     dropout=dropout, slice_num=slice_num)\n        self.ln_2 = nn.LayerNorm(hidden_dim)\n        self.mlp = MLP(hidden_dim, hidden_dim * mlp_ratio, hidden_dim, n_layers=0, res=False, act=act)\n        if self.last_layer:\n            self.ln_3 = nn.LayerNorm(hidden_dim)\n            self.mlp2 = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, fx):\n        fx = self.Attn(self.ln_1(fx)) + fx\n        fx = self.mlp(self.ln_2(fx)) + fx\n        if self.last_layer:\n            return self.mlp2(self.ln_3(fx))\n        else:\n            return fx # B N dim\n\n\nclass Model(nn.Module):\n    def __init__(self,\n                 space_dim=1,\n                 n_layers=5,\n                 n_hidden=256,\n                 dropout=0,\n                 n_head=8,\n                 act='gelu',\n                 mlp_ratio=1,\n                 fun_dim=1,\n                 out_dim=1,\n                 slice_num=32,\n                 ref=8, \n                 unified_pos=False # \u5728\u8f93\u5165\u7279\u5f81\u4e2d\u6dfb\u52a0\u7edf\u4e00\u7684\u7a7a\u95f4\u4f4d\u7f6e\u4fe1\u606f\n                 ):\n        super(Model, self). __init__()\n        self.__name__ = 'UniPDE_3D'\n        self.ref = ref\n        self.unified_pos = unified_pos\n        if self.unified_pos:\n            self.preprocess = MLP(fun_dim + self.ref * self.ref * self.ref, n_hidden * 2, n_hidden, n_layers=0,\n                                  res=False, act=act)\n            # MLP(self, n_input, n_hidden, n_output,)\n        else:\n            self.preprocess = MLP(fun_dim + space_dim, n_hidden * 2, n_hidden, n_layers=0, res=False, act=act)\n\n        self.n_hidden = n_hidden\n        self.space_dim = space_dim\n\n        self.blocks = nn.ModuleList([Transolver_block(num_heads=n_head, hidden_dim=n_hidden,\n                                                      dropout=dropout,\n                                                      act=act,\n                                                      mlp_ratio=mlp_ratio,\n                                                      out_dim=out_dim,\n                                                      slice_num=slice_num,\n                                                      last_layer=(_ == n_layers - 1))\n                                     for _ in range(n_layers)])\n        self.initialize_weights()\n        self.placeholder = nn.Parameter((1 / (n_hidden)) * torch.rand(n_hidden, dtype=torch.float))\n        # \u5360\u4f4d\u7b26\n    def initialize_weights(self):\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_grid(self, my_pos):\n        # my_pos 1 N 3\n        batchsize = my_pos.shape[0]\n\n        gridx = torch.tensor(np.linspace(-1.5, 1.5, self.ref), dtype=torch.float)\n        gridx = gridx.reshape(1, self.ref, 1, 1, 1).repeat([batchsize, 1, self.ref, self.ref, 1])\n        gridy = torch.tensor(np.linspace(0, 2, self.ref), dtype=torch.float)\n        gridy = gridy.reshape(1, 1, self.ref, 1, 1).repeat([batchsize, self.ref, 1, self.ref, 1])\n        gridz = torch.tensor(np.linspace(-4, 4, self.ref), dtype=torch.float)\n        gridz = gridz.reshape(1, 1, 1, self.ref, 1).repeat([batchsize, self.ref, self.ref, 1, 1])\n        grid_ref = torch.cat((gridx, gridy, gridz), dim=-1).cuda().reshape(batchsize, self.ref ** 3, 3)  # B 4 4 4 3\n\n        pos = torch.sqrt(\n            torch.sum((my_pos[:, :, None, :] - grid_ref[:, None, :, :]) ** 2,\n                      dim=-1)). \\\n            reshape(batchsize, my_pos.shape[1], self.ref * self.ref * self.ref).contiguous()\n        return pos\n\n    def forward(self, data):\n        cfd_data, geom_data = data\n        x, fx, T = cfd_data.x, None, None\n        x = x[None, :, :]\n        if self.unified_pos:\n            new_pos = self.get_grid(cfd_data.pos[None, :, :])\n            x = torch.cat((x, new_pos), dim=-1)\n\n        if fx is not None:\n            fx = torch.cat((x, fx), -1)\n            fx = self.preprocess(fx)\n        else:\n            fx = self.preprocess(x)\n            fx = fx + self.placeholder[None, None, :]\n\n        for block in self.blocks:\n            fx = block(fx)\n\n        return fx[0]\n</code></pre>"},{"location":"sci-paper/ComputationalPhysics/Transovler/#\u5b9e\u9a8c\u7ec6\u8282","title":"\u5b9e\u9a8c\u7ec6\u8282","text":""},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/","title":"AeroGTO:An Efficient Graph-Transformer Operator for Learning Large-Scale Aerodynamics of 3D Vehicle Geometries","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u4ee3\u7801:AeroGTO</p> <p></p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#abstract","title":"Abstract","text":"<p>\u4ece\u5e7f\u6cdb\u548c\u4e0d\u540c\u7684\u51e0\u4f55\u5f62\u72b6\u4e2d\u6355\u6349\u590d\u6742\u7684\u7269\u7406\u76f8\u5173\u6027\uff0c\u540c\u65f6\u5e73\u8861\u5927\u89c4\u6a21\u79bb\u6563\u5316\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002AeroGTO\u7ed3\u5408\u4e86\u901a\u8fc7\u4fe1\u606f\u4f20\u9012\u8fdb\u884c\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u548c\u901a\u8fc7<code>projection-inspired attention</code>\u8fdb\u884c\u5168\u5c40\u76f8\u5173\u6027\u6355\u83b7\uff0c\u91c7\u7528\u9891\u7387\u589e\u5f3a\u7684\u56fe\u795e\u7ecf\u7f51\u7edc(<code>frequency-enhanced graph neural network</code>)\uff0c\u5e76\u8f85\u4ee5k\u8fd1\u90bb(<code>k-nearest neighbors</code>)\u6765\u5904\u7406\u4e09\u7ef4\uff083D\uff09\u4e0d\u89c4\u5219\u51e0\u4f55\u4f53\u3002</p> <p>\u4e0e\u4e94\u79cd\u5148\u8fdb\u578b\u53f7\u76f8\u6bd4\uff0cAeroGTO\u5728\u4e24\u4e2a\u6807\u51c6\u57fa\u51c6<code>Ahmed Body</code>\u548c<code>DrivAerNet</code>\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u5728\u8868\u9762\u538b\u529b\u9884\u6d4b\u65b9\u9762(surface pressure prediction)\u63d0\u9ad8\u4e867.36%\uff0c\u963b\u529b\u7cfb\u6570\u4f30\u8ba1(drag coefficient estimation)\u63d0\u9ad8\u4e8610.71%\uff0cFLOPs\u66f4\u5c11\uff0c\u4ec5\u4f7f\u7528\u4e86\u5148\u524d\u9886\u5148\u65b9\u6cd5\u4f7f\u7528\u7684\u53c2\u6570\u76841%\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#introduction","title":"Introduction","text":"<p>\u5bf9\u4e8e\u4f20\u7edf\u7684GNNs\uff0c\u589e\u52a0\u56fe\u5927\u5c0f\u4f1a\u5f15\u5165\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a</p> <p>\uff081\uff09Complexity\uff1a\u968f\u7740nodes\u548c\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u90fd\u7ebf\u6027\u589e\u52a0\uff0c\u8ba1\u7b97\u56fe\u7684\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\u4e0d\u53ef\u907f\u514d\u5730\u53d8\u4e3a\u4e8c\u6b21\u65b9;</p> <p>\uff082\uff09Oversmoothing:\u56fe\u5377\u79ef\u5145\u5f53\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u6291\u5236\u9ad8\u9891\u4fe1\u53f7\u3002\u56e0\u6b64\uff0c\u5806\u53e0\u7684 MPs \u8fed\u4ee3\u5730\u5c06\u4fe1\u606f\u6295\u5f71\u5230\u56fe\u7684\u7279\u5f81\u7a7a\u95f4\u4e0a\uff0c\u5e73\u6ed1\u9ad8\u9891\u4fe1\u53f7\uff0c\u8fd9\u4f7f\u8bad\u7ec3\u8fc7\u7a0b\u590d\u6742\u5316\u3002</p> <p>\u5bf9\u4e8e\u4f20\u7edf\u7684<code>Transformer</code>\u6a21\u578b\uff0c\u6570\u636e\u70b9\u88ab\u6295\u5f71\u5230\u6f5c\u7a7a\u95f4\u4e2d\uff0c\u7136\u540e\u7531\u6ce8\u610f\u5757\u5904\u7406\u3002\u7136\u800c\uff0c\u4ec5\u4f7f\u7528<code>MLP</code>\u6765\u5b66\u4e60\u6c7d\u8f66\u51e0\u4f55\u5f62\u72b6\u4e2d\u5927\u89c4\u6a21\u70b9\u4e91\uff08point clouds\uff09\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u53ef\u80fd\u4f1a\u5bfc\u81f4\u91cd\u8981\u62d3\u6251\u4fe1\u606f\u7684\u635f\u5931\uff08losses of important topological information\uff09\uff0c\u8fd9\u964d\u4f4e\u4e86\u5b83\u6355\u83b7\u590d\u6742\u7269\u7406\u76f8\u5173\u6027\u7684\u80fd\u529b\u3002</p> <p>AeroGTO\u901a\u8fc7\u4f7f\u7528kNN\u6765\u589e\u5f3a<code>frequency-enhanced GNN</code>\u8fdb\u884c\u7cbe\u786e\u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff0c\u6709\u6548\u5730\u9694\u79bb\u548c\u6355\u83b7\u70b9\u548c\u8fb9\u7684\u7269\u7406\u4fe1\u606f\uff0c\u5e76\u5c06\u8be5\u7a7a\u95f4\u6570\u636e\u6295\u5f71\u5230\u62d3\u6251\u7ed3\u6784\u5316\u7684\u9690\u85cf\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002</p> <p>\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u5177\u6709\u5168\u5c40\u7ebf\u6027\u590d\u6742\u5ea6\u6ce8\u610f\u529b\u7684Transformer\uff0c\u5b83\u6355\u83b7\u7f51\u683c\u70b9\u4e4b\u95f4\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4fc3\u8fdb\u5c40\u90e8\u548c\u5168\u5c40\u590d\u6742\u7269\u7406\u76f8\u5173\u6027\u4e4b\u95f4\u7684\u591a\u7ea7\u4ea4\u4e92\u3002\u8fd9\u79cd\u8bbe\u8ba1\u6709\u6548\u5730\u964d\u4f4e\u4e86\u590d\u6742\u6570\u636e\u96c6\u7684\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u51c6\u786e\u7684\u63a8\u7406\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#methodology","title":"Methodology","text":""},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#problem-setting-and-notations","title":"Problem Setting and Notations","text":"<p>Neural operators of PDEs \u662f\u4ece\u8f93\u5165\u51fd\u6570(\u5982\u521d\u59cb/\u8fb9\u754c\u6761\u4ef6\u3001\u51e0\u4f55\u3001\u7cfb\u6570\u548c\u6e90\u573a)\u5230\u89e3\u7684\u6620\u5c04</p> <p>\u8bbe\\(\\mathcal{A}\\)\u8868\u793a\u8f93\u5165\u51fd\u6570\u7684\u7a7a\u95f4\uff0c\u89e3\u7a7a\u95f4\u4e3a\\(\\mathcal{S}\\)\uff0c\u800c\u795e\u7ecf\u7b97\u5b50\u662f\u5b66\u4e60\u4e00\u4e2a\u7b97\u5b50\\(\\mathcal{G}:\\mathcal{A} \\rightarrow \\mathcal{S}\\)\u3002\u5728\u8be5\u6a21\u578b\u4efb\u52a1\u4e0b\uff0c\\(\\mathcal{G}\\)\u662f\u5c06\u6c7d\u8f66\u7684\u4efb\u610f\u5f62\u72b6\u548c\u7ed9\u5b9a\u6761\u4ef6\u6620\u5c04\u5230\u5176\u8868\u9762\u538b\u529b\u3002</p> <p>\u5bf9\u7ed9\u5b9a\u7684k\u4e2a\u6c7d\u8f66\u5f62\u72b6\uff0c\u8f93\u5165\\(\\mathcal{a}^k\\)\u7531\u6c7d\u8f66\u7684\u5f62\u72b6\u7ec4\u6210\uff0c\u5e76\u7528\u4e00\u7ec4\u7f51\u683c\\(\\mathbf{M}^k = (\\mathbf{X}^k, \\mathbf{C}^k)\\)\u79bb\u6563\u5316\uff0c\\(\\mathbf{X}^k= \\{ x_i^k, y_i^k, z_i^k, \\}^{N'}_{i=1}\\)\u662f\u5355\u4e2a\u5f62\u72b6\u7684\u8282\u70b9\u5750\u6807\u96c6\uff0c\\(\\mathbf{C}^k= \\{ i_l^k,..., j_l^k \\}^{E'}_{i=1}\\)\u8868\u793a\u5355\u4e2a\u5f62\u72b6\u5185\u7684\u7f51\u683c\uff0c\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u4e2d\u8282\u70b9\u7684\u8fde\u63a5\uff08\\(1 \u2264 i_l^k,..., j_l^k \u2264 N'\\)\u8868\u793a\u8282\u70b9\u7684index\uff0c\\(N'\\)\u662f\u8282\u70b9\u6570\uff0c\\(E'\\)\u662f\u7f51\u683c\u7684\u6570\u91cf\uff09\u3002\\(\\mathbf{A}^k \\in \\mathbb{R}^m\\)\u5305\u542b\u4e00\u4e9b\u5168\u5c40\u8bbe\u8ba1\u53c2\u6570\uff0c\u5982\u6c7d\u8f66\u7684\u957f\u5ea6\u3001\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u3001\u96f7\u8bfa\u6570\u3001\u5165\u53e3\u901f\u5ea6\u7b49\u3002\u671f\u671b\u7684\u8f93\u51fa\u662f\u79bb\u6563\u7684\u538b\u529b\u5206\u5e03\\(\\mathbf{s}^k=\\{ \\mathbf{p}^k_i \\}^{N'}_{i=1}\\)\u3002</p> <p>\u5bf9\u53c2\u6570\u5316\u7684\u7b97\u5b50\\(\\hat{\\mathcal{G}}_{\\theta}\\)\uff0c\u6709\\(\\hat{\\mathcal{G}}_{\\theta}(\\mathcal{a}^k)=\\hat{\\mathbf{s}}^k\\)\uff0c\u5176\u4e2d\\(\\theta\\)\u662f\u6a21\u578b\u53c2\u6570\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u6700\u5c0f\u5316\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u9884\u6d4b \\(\\hat{s}^k\\) \u548c\u771f\u5b9e\u6570\u636e\\(s^k\\)\u4e4b\u95f4\u7684L2\u76f8\u5bf9\u8bef\u5dee\u635f\u5931:</p> \\[ \\min_{\\theta \\in \\Theta} \\frac{1}{D} \\sum_{k=1}^{D} \\mathcal{L}_k (\\theta) = \\min_{\\theta \\in \\Theta} \\frac{1}{D} \\sum_{k=1}^{D} \\frac{\\|\\hat{\\mathbf{s}}^k-\\mathbf{s}^k\\|_{2}}{\\|\\mathbf{s}^k\\|_{2}} \\] <p>\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5927\u5c0f\uff0c\\(\\theta\\) \u662f\u7f51\u7edc\u53c2\u6570\uff0c\\(\\Theta\\)\u662f\u53c2\u6570\u7a7a\u95f4\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#model-architecture","title":"Model Architecture","text":""},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#encoder","title":"Encoder","text":"<p>Encoder\u5206\u4e3a\u8282\u70b9\u7f16\u7801\u4e0e\u8fb9\u7f16\u7801\u4e24\u90e8\u5206\uff0c\u8f93\u5165\u5305\u542b\u79bb\u6563\u5316\u7f51\u683c\\(\\mathbf{M}^k = (\\mathbf{X}^k, \\mathbf{C}^k)\\)\u548c\u6761\u4ef6\\(\\mathbf{A}^k \\in \\mathbb{R}^m\\)\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#edge-focused-sampling","title":"Edge-Focused Sampling","text":"<p>\u8003\u8651\u7531\u65e0\u65b9\u5411\u7684\u8fb9\\(E^M\\)\u7ec4\u6210\u7684\u65e0\u5411\u7f51\u683c\\(C\\)\uff0c\u4e3a\u4e86\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u8fd1\u4f3c\u53cc\u5411\u8fb9\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u4ea4\u6362\u8fb9\u7684\u7d22\u5f15i\u548cj\u6765\u968f\u673a\u6253\u4e71\u8fde\u63a5\u65b9\u5411\u3002\u7136\u540e\uff0c\u6211\u4eec\u4ee5\\(a_E \\in (0,1]\\)\u7684\u6bd4\u4f8b\u7edf\u4e00\u91c7\u6837\u8fd9\u4e9b\u8fb9\u3002\u5728\u91c7\u6837\u540e\uff0c\u5e94\u7528kNN\u589e\u5f3a\u6765\u6355\u83b7\u539f\u59cb\u7f51\u683c\u7ed3\u6784\u53ef\u80fd\u4e0d\u8868\u793a\u7684\u9644\u52a0\u8fde\u63a5\u3002</p> <p>\u8be5\u65b9\u6cd5\u7279\u522b\u5408\u9002\u5f53\u6211\u4eec\u5df2\u77e5\u539f\u59cb\u7f51\u683c\u4e2d\u7684\u5355\u5143\u4fe1\u606f\uff0c\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u63d0\u53d6\u8fb9\u7f18\u5173\u7cfb\u7684\u60c5\u666f\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#node-focused-sampling","title":"Node-Focused Sampling","text":"<p>\u968f\u673a\u91c7\u6837\u8282\u70b9\uff0c\u9009\u62e9\u603b\u8282\u70b9\u7684\u6bd4\u4f8b\\(a_E \\in (0,1]\\)\u3002\u91c7\u6837\u540e\uff0c\u4ec5\u4f7f\u7528 kNN\u65b9\u6cd5\u6784\u5efa\u8fb9\u7f18\uff0c\u91cd\u70b9\u5173\u6ce8\u6240\u9009\u8282\u70b9\u4e4b\u95f4\u7684\u5173\u7cfb\u3002</p> <p>\u8fd9\u79cd\u65b9\u6cd5\u66f4\u9002\u5408\u539f\u59cb\u7f51\u683c\u975e\u5e38\u590d\u6742\uff0c\u4f7f\u5f97\u4ece\u5355\u5143\u683c\u4fe1\u606f\u4e2d\u63d0\u53d6\u8fb9\u7f18\u5173\u7cfb\u53d8\u5f97\u68d8\u624b\u7684\u60c5\u51b5\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#nodes","title":"Nodes","text":"<p>\u8282\u70b9\u4fe1\u606f\u5305\u542b\u5750\u6807\\(X_i=(x_i,y_i,z_i)\\)\u548c\u6c7d\u8f66\u4fe1\u606f\\(A\\)\uff0c\u6211\u4eec\u4f7f\u7528MLP\u5c06\u8282\u70b9\u4fe1\u606f\u4ece\u7269\u7406\u7a7a\u95f4\u7f16\u7801\u5230\u6f5c\u7a7a\u95f4\u4e2d\uff0c\\(v_i = \\Phi_1^V(X_i, A)\\)\u3002</p> <p>\u4e3a\u4e86\u6709\u6548\u5730\u6355\u6349\u5750\u6807\u7684\u7a7a\u95f4\u548c\u9891\u7387\u76f8\u5173\u7279\u5f81\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801(SPE)\uff1a</p> \\[ F(X)=[ cos(2^i \\pi X), sin(2^i \\pi X) ]_{i=-\\delta,...,\\delta}\uff0c\\delta \\in \\mathbb{Z}^+ \\] <p>\u6700\u540e\u518d\u4f7f\u7528MLP \\(v_i = \\Phi^V_2(v_i, F(X_i))\\)</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#edges","title":"Edges","text":"<p>\u4e3a\u4e86\u66f4\u597d\u5730\u8868\u8fbe\u51e0\u4f55\u62d3\u6251\u7ed3\u6784\uff0c\u6211\u4eec\u540c\u6837\u8981\u5bf9\u8fb9\u8fdb\u884c\u7f16\u7801\u3002\u6211\u4eec\u5f97\u5230\u7f51\u683c\u8fb9\\(E^M\\)\u4ee5\u53ca\u901a\u8fc7kNN\u83b7\u53d6\u7684\u8fb9\\(E^k\\)\uff0c\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u76f8\u5bf9\u4f4d\u79fb\u5411\u91cf\\(x_{ij}=x_i-x_j\\)\u53ca\u5176\u8303\u6570\\(|x_{ij}|\\)\u7f16\u7801\u5230\u8fb9\u7684\u7ec4\u5408\u96c6\\(e_{ij} \\in E^M \u222a E_k\\)\u3002</p> <p>\u7136\u540e\u4f7f\u7528MLP\\(\\Phi^E\\)\u5c06\u6765\u81ea\u7f51\u683c\u8fb9\u548ckNN\u8fb9\u7684\u8fde\u63a5\u7279\u5f81\u7f16\u7801\u4e3a\u6bcf\u6761\u8fb9\u7684\u5927\u5c0f\u4e3aC\u7684\u6f5c\u5728\u5411\u91cf\uff0c\\(e_{ij}= \\Phi^E(e_{ij})\\)\u3002</p> <p>\u6700\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u6c7d\u8f66\u51e0\u4f55\u7684\u6f5c\u5728\u8868\u793a\\(V=\\{v_i \\} \\in \\mathbb{R}^{[N,C]} , E = \\{ e_{ij} \\} \\in \\mathbb{R}^{[E,C]}\\)\uff0c\u5176\u4e2d\\(N\\)\uff0c\\(E\\)\u5206\u522b\u662f\u91c7\u6837\u540e\u8282\u70b9\u548c\u8fb9\u7684\u6570\u91cf\u3002</p> <p>\\(\u03a6^V_1\\)\u3001\\(\u03a6^V_2\\) \u548c \\(\u03a6^E\\)\u4f7f\u7528\u4e24\u4e2a\u7ebf\u6027\u5c42\u5b9e\u73b0\uff0c\u5e76\u4e14\u5177\u6709\u76f8\u540c\u7684\u5bbd\u5ea6\\(C\\)\u548cSiLU\u6fc0\u6d3b\u51fd\u6570\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#processor","title":"Processor","text":"<p>AeroGTO\u57fa\u4e8eprojection-inspired attention\uff0c\u901a\u8fc7\u4fe1\u606f\u4f20\u9012\u548c\u5168\u5c40\u76f8\u5173\u6027\u6355\u83b7\u6765\u7ec4\u5408\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u3002</p> <p>\u6bcf\u4e2a\u7f51\u683c\u8fb9\\(e_{ij}\\)\u548c\u8282\u70b9\\(v_i\\)\u901a\u8fc7\u9891\u7387\u589e\u5f3a\u8fdb\u884c\u66f4\u65b0:</p> \\[ \\begin{cases}   \\mathbf{e}_{ij} = \\mathbf{e}_{ij} + \\Phi_{P}^{E}(\\mathbf{e}_{ij}, \\mathbf{v}_{i}, \\mathbf{v}_{j}) \\\\   \\mathbf{v}_{i} = \\mathbf{v}_{i} + \\Phi_{P}^{V}\\left(\\mathbf{v}_{i}, \\sum_{j} \\mathbf{e}_{ij}, F(\\boldsymbol{X}_{i})\\right) \\end{cases} \\] <p>\u5176\u4e2d\u7684MLP\u662f\u4f7f\u7528GELU\u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u6b8b\u5dee\u8fde\u63a5\u7ebf\u6027\u5c42</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#global-attention-via-projection-inspired-attention","title":"Global Attention via Projection-Inspired Attention","text":"\\[ \\begin{cases}   W_1 = softmax(\\frac{Q_{ls} W_0^T}{\\sqrt{C}}) W_0 \\\\   W_2 = softmax(\\frac{W_1 W_1^T}{\\sqrt{C}}) W_1      \\\\   W_3 = softmax(\\frac{W_0 W_2^T}{\\sqrt{C}}) W_2 \\end{cases} \\] <p>\u5176\u4e2d\uff0c\\(W_0:=\\{ v_i \\} \\in \\mathbb{R}^{[N,C]}\\)\u4eceMP\u5757\u83b7\u5f97\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u5b50\u7a7a\u95f4\u67e5\u8be2\u5411\u91cf\\(Q_{ls}\\)\u223c\\(\\mathcal{N}(0,1) \\in \\mathbb{R}^{[M,C]}\uff0cM \u226a N\\)\uff0c\\(M\\)\u4ee3\u8868\u6bcf\u4e2atoken\u7684\u6570\u91cf\u3002</p> <p>\\(Q_{ls}\\)\u4e2d\u7684\u6bcf\u4e00\u4e2atoken\u53ef\u4ee5\u89e3\u91ca\u4e3a\u5b50\u7a7a\u95f4\u7684\u57fa\u5411\u91cf\uff08basis vector of the learned subspace\uff09\uff0c\u8868\u793a\u6570\u636e\u4e2d\u7684\u7279\u5b9a\u6a21\u5f0f\u6216\u7279\u5f81\u3002\u968f\u540e\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9etoken\u786e\u5b9e\u5f62\u6210\u4e86\u4e00\u4e2abasis\uff0cM\u4e2a\u5411\u91cf\u662f\u7ebf\u6027\u72ec\u7acb\u7684\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#pre-norm-structure","title":"Pre-Norm Structure","text":"<p>\u6211\u4eec\u91c7\u7528pre-norm\u7ed3\u6784\u6765\u4fc3\u8fdb\u6574\u4e2a\u5904\u7406\u5668\u7684\u66f4\u6709\u6548\u7684\u8ba1\u7b97:</p> \\[ \\begin{cases}   V_G^1 = W_0 + W_3 \\\\   V_G^2 = V_G^1 + FFN(LN(V_G^1))  \\end{cases} \\] <p>\u5728\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u591a\u5934\u6ce8\u610f\u529b\u6765\u589e\u5f3a\u6a21\u578b\u6355\u83b7\u4e0d\u540c\u5934\u90e8\u7684\u5404\u79cd\u4ea4\u4e92\u548c\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u8868\u793a\u6570\u636e\u4e2d\u590d\u6742\u5173\u7cfb\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5806\u53e0\u591a\u4e2a\u8fc7\u7a0b\u5757\uff0c\u8be5\u6a21\u578b\u5229\u7528\u4e86\u5c42\u7684\u96c6\u4f53\u6548\u5e94\uff0c\u4ece\u800c\u5728\u590d\u6742\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7269\u7406\u76f8\u5173\u6027\u4e4b\u95f4\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u591a\u7ea7\u4ea4\u4e92\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#decoder","title":"Decoder","text":"<p>Decoder\u5c06\u5c40\u90e8\u7279\u5f81\u548c\u5168\u5c40\u7279\u5f81\u8868\u793a\u6295\u5f71\u56de\u7269\u7406\u7a7a\u95f4\uff0c\\(p_i=\\Phi_D^V (v_i, F(X_i)\\)\u3002\u5176\u4e2d\uff0c\\(F(X_i)\\)\u662f\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801SPE\uff0c\\(v_i \\in \\mathbb{R}^{[C]}\\)\u662f\u5904\u7406\u5668\u7684\u8f93\u51fa\u3002\\(p_i \\in \\mathbb{R}^{[1]}\\)\u662f\u91c7\u6837\u8282\u70b9i\u5904\u7684\u8f93\u51fa\u538b\u529b\u3002</p>"},{"location":"sci-paper/ComputationalPhysics/AeroGTO/aerogto/#inference","title":"Inference","text":"<p>\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u8003\u8651\u4e0d\u540c\u7684\u91c7\u6837\u65b9\u6cd5\u3002\u5bf9\u4e8e<code>Edge-Focused</code>\u91c7\u6837\u65b9\u6cd5\uff0c\u8282\u70b9\u7684\u6570\u91cf\u5728pipeline\u4e2d\u4fdd\u6301\u4e0d\u53d8\uff08\u5373 \\(N = N\u2032\\)\uff09\uff0c\u800c\u8fb9\u6839\u636e\u8bbe\u7f6e\u7b56\u7565\u8fdb\u884c\u91c7\u6837\u3002\u5bf9\u4e8e <code>Node-Focused</code>\uff0c\u57fa\u4e8e\u91c7\u6837\u7387\\(a_N \\in (0, 1]\\)\u5c06\u5168\u5c3a\u5bf8\u7f51\u683c\u968f\u673a\u5212\u5206\u4e3a\\([\\frac{1}{a_N}]\\) batches\u3002 \u3001 AeroGTO\u5229\u7528GPU\u7684\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\u5c06\u8fd9\u4e9b\u6279\u6b21\u4e00\u8d77\u5904\u7406\u3002\u63a8\u65ad\u4e0e\u91c7\u6837\u51e0\u4f55\u5bf9\u5e94\u7684\u7269\u7406\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u805a\u5408\u4ee5\u4ea7\u751f\u5168\u7f51\u683c\u8f93\u51fa\u3002</p>"},{"location":"sci-paper/Load-Forecasting/%5BApplied%20Energy%5DA%20novel%20short-term%20multi-energy%20load%20forecasting%20method%20for%20integrated%20energy%20system%20based%20on%20feature%20separation-fusion%20technology%20and%20improved%20CNN/","title":"[Applied Energy]A novel short-term multi-energy load forecasting method for integrated energy system based on feature separation-fusion technology and improved CNN","text":"<p>\ud83d\udcdcInformation</p> <ul> <li>\u6587\u7ae0\u9898\u76ee\uff1a\u57fa\u4e8e\u7279\u5f81\u5206\u79bb\u878d\u5408\u6280\u672f\u4e0e\u6539\u8fdbCNN\u7684\u7efc\u5408\u80fd\u6e90\u7cfb\u7edf\u591a\u80fd\u8d1f\u8377\u9884\u6d4b </li> <li>Key word: <code>Integrated energy system</code> <code>Deep learning</code> <code>Multi-energy load forecasting</code> <code>Multi-task learning</code> <code>Convolutional neural network</code></li> <li>\u4f5c\u8005\uff1aKe Li, Yuchen Mu, Fan Yang, Haiyang Wang, Yi Yan, Chenghui Zhang</li> </ul>"},{"location":"sci-paper/Load-Forecasting/%5BApplied%20Energy%5DA%20novel%20short-term%20multi-energy%20load%20forecasting%20method%20for%20integrated%20energy%20system%20based%20on%20feature%20separation-fusion%20technology%20and%20improved%20CNN/#\u7b80\u8ff0","title":"\ud83d\udce2\u7b80\u8ff0","text":"<p>\u9488\u5bf9IES\u6570\u636e\u91cf\u5927\u3001\u968f\u673a\u6027\u5f3a\u548c\u591a\u80fd\u8026\u5408\u7684\u7279\u70b9\uff0c\u6587\u7ae0\u4e2d\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5206\u79bb-\u878d\u5408\u6280\u672f\u4e0e\u6539\u8fdbCNN\u7684\u591a\u80fd\u8d1f\u8377\u77ed\u671f\u9884\u6d4b\u65b9\u6cd5\u3002</p> <ul> <li> <p>\u9996\u5148\uff0c\u57fa\u4e8e\u9759\u6001\u56fe\u7247\u50cf\u7d20\u70b9\u7684\u5206\u5e03\u89c4\u5f8b\u5c06\u65e0\u660e\u663e\u89c4\u5f8b\u7684\u591a\u80fd\u8d1f\u8377\u6570\u636e\u70b9\u8fdb\u884c\u50cf\u7d20\u91cd\u6784\uff0c\u4f7f\u4e4b\u5728\u6c34\u5e73\u548c\u7ad6\u76f4\u4e24\u4e2a\u65b9\u5411\u5206\u522b\u5177\u6709\u4e00\u5b9a\u7684\u5173\u8054\u7279\u5f81\u3002</p> </li> <li> <p>\u5176\u6b21\uff0c\u91c7\u7528\u7279\u5f81\u5206\u79bb\u2014\u878d\u5408\u6280\u672f\uff0c\u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c\u5dee\u5f02\uff0c\u5c06\u8f93\u5165\u7279\u5f81\u5206\u4e3a\u4e24\u7c7b\u8fdb\u884c\u5dee\u5f02\u5316\u5904\u7406\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u591a\u5c3a\u5ea6\u878d\u5408\u7684\u6539\u8fdbCNN\u5bf9\u91cd\u6784\u540e\u7684\u4e09\u7ef4\u8d1f\u8377\u50cf\u7d20\u5728\u9ad8\u7ef4\u7a7a\u95f4\u5185\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u3002</p> </li> <li> <p>\u6700\u540e\uff0c\u5c06\u4e24\u7c7b\u7279\u5f81\u5408\u5e76\u8f93\u5165\u5230\u4ee5BiLSTM\u4e3a\u5171\u4eab\u5c42\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u91c7\u7528\u786c\u53c2\u6570\u5171\u4eab\u673a\u5236\u5b66\u4e60IES\u591a\u80fd\u8026\u5408\u4fe1\u606f\u3002</p> </li> </ul> <p>\u6b64\u5916\uff0c\u4e3a\u517c\u987e\u5404\u79cd\u8d1f\u8377\u4e0d\u540c\u7684\u9884\u6d4b\u9700\u6c42\uff0c\u8bbe\u8ba1\u4e86\u4e09\u79cd\u4e0d\u540c\u7ed3\u6784\u7684FCN\u7f51\u7edc\u4f5c\u4e3a\u7279\u5f81\u89e3\u91ca\u6a21\u5757\u3002\u5b9e\u9645\u7b97\u4f8b\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u51ac\u5b63\u65e5\u52a0\u6743\u5e73\u5747\u7cbe\u5ea6\u8fbe98.01%\uff0c\u9884\u6d4b\u7ed3\u679c\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u6807\u51c6\u5dee\u4f4e\u81f30.0242\uff0c\u5728\u6240\u6709\u5bf9\u6bd4\u6a21\u578b\u4e2d\uff0c\u9884\u6d4b\u7cbe\u5ea6\u6700\u9ad8\uff0c\u8bef\u5dee\u5206\u5e03\u6700\u7a33\u5b9a\u3002</p>"},{"location":"sci-paper/Load-Forecasting/%5BApplied%20Energy%5DA%20novel%20short-term%20multi-energy%20load%20forecasting%20method%20for%20integrated%20energy%20system%20based%20on%20feature%20separation-fusion%20technology%20and%20improved%20CNN/#highlights","title":"\ud83c\udf87Highlights","text":"<p>\u2022 Propose a feature processing method based on information value differences.</p> <p>\u2022 Propose an improved CNN based on multi-time scale fusion and reconstruct the original load data into a three-dimensional pixel matrix.</p> <p>\u2022 Build a multi-task hard shared learning framework based on BiLSTM, innovatively adopting feature interpretation modules with different structures.</p> <p>\u2022 The simulation results show that the winter daily WMA of the proposed model can reach 98.01%, and the RESD is as low as 0.0242.</p>"},{"location":"sci-paper/Load-Forecasting/%5BApplied%20Energy%5DA%20novel%20short-term%20multi-energy%20load%20forecasting%20method%20for%20integrated%20energy%20system%20based%20on%20feature%20separation-fusion%20technology%20and%20improved%20CNN/#\u6587\u7ae0\u89c2\u70b9","title":"\ud83c\udf83\u6587\u7ae0\u89c2\u70b9","text":"<p>1.IES\u4e2d\u591a\u80fd\u91cf\u8d1f\u8377\u9884\u6d4b\u7684\u7814\u7a76\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\u3002\u591a\u80fd\u91cf\u8d1f\u8377\u9884\u6d4b\u7684\u5e94\u7528\u573a\u666f\u590d\u6742\u8bb8\u591a\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651IES\u7684\u5916\u90e8\u56e0\u7d20\u548c\u5185\u90e8\u591a\u80fd\u91cf\u6d41\u7684\u76f8\u4e92\u4f20\u9012\u548c\u4ea4\u53c9\u8026\u5408\u3002\u800c\u5355\u8d1f\u8377\u9884\u6d4b\u65b9\u6cd5\u7684\u6269\u5c55\u96be\u4ee5\u6709\u6548\u5730\u5b66\u4e60\u591a\u80fd\u91cf\u8026\u5408\u4fe1\u606f\u3002</p>"},{"location":"sci-paper/cs/Attention-is-all-you-need/","title":"Attention is all you need","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aAttention is all you need</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/master?tab=readme-ov-file</p> <p>\u8d44\u6e90\uff1a\u30103Blue1Brown\u3011Visualizing Attention, a Transformer's Heart</p> <p>\u672c\u9875\u5185\u5bb9\u662f\u5bf9<code>Transformers</code>\u7684\u6587\u7ae0\u603b\u7ed3/\u4ee3\u7801\u9605\u8bfb(\u4fa7\u91cd\u4ee3\u7801\u5b66\u4e60)</p> <p>\u5fc5\u8bfb\u8bba\u6587\uff0c\u61c2\u7684\u90fd\u61c2\uff0c\u4e0d\u61c2\u7684\u5feb\u53bb\u770b\u3002</p> <p></p> <p>\u6587\u7ae0\u6458\u8981</p> <p> <p>\u5728\u5e8f\u5217\u5efa\u6a21\u548c\u8f6c\u6362\u95ee\u9898\u4e2d\uff0c\u7531\u4e8eRNN\u3001LSTM\u548c\u95e8\u63a7\u5faa\u73af\u795e\u7ecf\u5b58\u5728\u5404\u79cd\u95ee\u9898\uff0c\u5982RNN\u96be\u4ee5\u5efa\u7acb\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0cLSTM\u65e0\u6cd5\u5e76\u884c\u5316\u5b66\u4e60\u7b49\uff0c\u6545\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eattention\u673a\u5236\u5e76\u5b8c\u5168\u907f\u514d\u5faa\u73af\u548c\u5377\u79ef\u7684\u7b80\u5355\u7684\u7f51\u7edc\u67b6\u6784Transformer\u3002</p> <p></p> <p></p>"},{"location":"sci-paper/cs/Attention-is-all-you-need/#modulepy","title":"<code>Module.py</code>","text":"<ul> <li> <p>\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b<code>ScaledDotProductAttention</code></p> \\[ Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V \\] <p></p> <p>\u5047\u8bbe\\(Q,K\\)\u7684\u7ef4\u5ea6\u4e3a\\((N,d_k)\\)\uff0c\\(V\\)\u7684\u7ef4\u5ea6\u4e3a\\((N,d_v)\\)\uff0c\u5176\u4e2d\\(Q,K,V\\)\u4ee3\u8868Query\uff0c Key\uff0c Value\uff0c \\(d_k\\)\u4ee3\u8868Key\u7684\u7ef4\u5ea6\uff0c\u9664\u4ee5\\(\\sqrt{d_k}\\)\u662f\u4e3a\u4e86\u9632\u6b62\u70b9\u79ef\u8fc7\u5927\uff0c\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\u3002\u5176\u4e2d\\(Softmax(QK^T)\\)\u5f97\u5230\u7684\u7ef4\u5ea6\u4e3a\\((N,N)\\)\u3002</p> <p>\\(Q\\)\u4ee3\u8868query\uff0c\u662f\u5f53\u524d\u8981\u5904\u7406\u7684\u8bcd\u5bf9\u5e94\u7684\u5411\u91cf\uff0c\\(K\\)\u4ee3\u8868key\uff0c\u901a\u8fc7\u8ba1\u7b97\\(Q\\)\u4e0e\\(K\\)\u7684\u5173\u7cfb\u53ef\u4ee5\u5f97\u5230\u5f53\u524d\u9700\u8981\u5bf9\u5176\u4ed6\u8bcd\u7684\u5173\u6ce8\u5ea6\u3002</p> <p></p> <p>\u70b9\u79ef\u6ce8\u610f\u529b\u5373\u662f\u901a\u8fc7\\(Q\\)\u4e0e\\(K\\)\u7684\u70b9\u79ef\u76f8\u4e58\u8ba1\u7b97\u4e86\u76f8\u4f3c\u5ea6\uff0c\u5176<code>Softmax</code>\u5206\u6570\u51b3\u5b9a\u4e86\u5728\u8be5\u4f4d\u7f6e\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u5373\u5bf9\u5176\u4ed6\u8bcd\u7684\u6ce8\u610f\u529b\u7a0b\u5ea6\uff0c\u540e\u4e0e\\(V\\)\u76f8\u4e58\u5f97\u5230\u7ed3\u679c\u3002</p> <p>\u5728\u666e\u901a\u7684<code>Attention</code>\u4e2d\uff0c\\(K,V\\)\u5bf9\u5e94\u7f16\u7801\u5668\u8f93\u51fa\uff0c\\(Q\\)\u5bf9\u5e94\u89e3\u7801\u5668\u5f53\u524d\u7684\u8f93\u5165\u3002<code>Self-Attention</code>\u4e2d\uff0c\\(Q,K,V\\)\u90fd\u5bf9\u5e94\u4e8e\u5f53\u524d\u7684\u8f93\u5165\\(X\\)\u3002</p> <p></p> <p> Code <pre><code>class ScaledDotProductAttention(nn.Module):\n''' Scaled Dot-Product Attention '''\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n\n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, -1e9)\n            '''\n            \u4f7f\u7528.masked_fill\u65b9\u6cd5\u662f\u5c06\u8fd9\u4e9b\u4f4d\u7f6e\u7684\u5206\u6570\u8bbe\u7f6e\u4e3a\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u8d1f\u6570\n            \u56e0\u4e3asoftmax\u7684\u5f88\u5927\u8d1f\u6570\u8f93\u5165\u4f1a\u8fd1\u4f3c\u4e8e0\uff0c\u4ece\u800c\u786e\u4fdd\u8fd9\u4e9b\u4f4d\u7f6e\u5bf9\u5e94\u7684\u6743\u91cd\u63a5\u8fd1\u4e8e0\u3002\n            \u5982\u679cmask\u4e2d\u7684\u67d0\u4e2a\u4f4d\u7f6e\u4e3a0\uff0c\u5219\u8be5\u4f4d\u7f6e\u7684score\u503c\u5c06\u88ab\u66ff\u6362\u4e3a-1e9\n            '''\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        # dim=-1 \u8868\u793a\u5e94\u7528Softmax\u5230\u8f93\u5165\u6570\u636e\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\n        output = torch.matmul(attn, v)\n\n        return output, attn\n</code></pre> <p> </p>"},{"location":"sci-paper/cs/Attention-is-all-you-need/#sublayerspy","title":"<code>SubLayers.py</code>","text":"<ul> <li> <p>\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236<code>MultiHeadAttention</code></p> <p><code>Multi-Head Attention</code>\u662f\u4e00\u79cd\u5c06<code>Scaled Dot-Product Attention</code>\u6269\u5c55\u5230\u591a\u5934\u7684\u65b9\u6cd5\uff0c\u5b83\u5c06Query, Key, Value \u5206\u522b\u7ecf\u8fc7\u591a\u4e2a\u7ebf\u6027\u53d8\u6362\uff08\u79f0\u4e3a\u201c\u5934\u201d\uff09\u540e\u518d\u8f93\u5165\u5230 <code>Scaled Dot-Product Attention</code> \u4e2d\u8ba1\u7b97\uff0c\u6700\u540e\u5c06\u591a\u4e2a<code>Attention</code>\u8f93\u51fa\u6309\u7167\u901a\u9053\u7ef4\u5ea6\u62fc\u63a5\u8d77\u6765\u3002</p> \\[ MultiHeadAttention(Q,K,V) = Concat(head_1, head_2,...,head_n)W^O \\] <p>\u5176\u4e2d\\(head_i\\)\u8868\u793a\u7b2c\\(i\\)\u4e2a<code>Attention</code>\u5934\uff0c\\(W^O\\)\u8868\u793a\u6700\u7ec8\u8f93\u51fa\u7684\u7ebf\u6027\u53d8\u6362\u77e9\u9635\uff0c\\(n\\)\u8868\u793a\u5934\u7684\u6570\u91cf\u3002</p> <p></p> <p><code>MultiHead</code>\u4e3a<code>Attention</code>\u5c42\u63d0\u4f9b\u4e86\u591a\u4e2a\u201c\u8868\u793a\u5b50\u7a7a\u95f4\u201d\uff0c\u5bf9\u4e8e<code>Transformer</code>\u4f7f\u75288\u5934\u3002\u8fd9\u4e9b\u96c6\u5408\u4e2d\u6bcf\u4e00\u4e2a\u90fd\u662f\u968f\u673a\u521d\u59cb\u5316\u7684\uff0c\u5728\u8bad\u7ec3\u4e4b\u540e\uff0c\u6bcf\u7ec4\u7528\u4e8e\u5c06\u8f93\u5165embedding\u6295\u5f71\u5230\u4e0d\u540c\u7684\u8868\u793a\u5b50\u7a7a\u95f4\u4e2d\u3002</p> <p> Code <pre><code>class MultiHeadAttention(nn.Module):\n''' Multi-Head Attention module '''\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n\n        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n\n    def forward(self, q, k, v, mask=None):\n        '''\n        d_k-&gt;int: key, query's dimension\n        d_v-&gt;int: value's dimension\n        n_head-&gt;int: number of atten head\n        sz_b-&gt;int: number of batch \n        len_q: length of query\n        len_k: length of key\n        len_v: length of value\n        '''\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n\n        residual = q\n\n        # Pass through the pre-attention projection: b x lq x (n*dv)\n        # Separate different heads: b x lq x n x dv\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        # Transpose for attention dot product: b x n x lq x dv\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n\n        if mask is not None:\n            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n        # \u5c06 `mask` \u7684\u5f62\u72b6\u4ece `(batch_size, seq_len)` \u6269\u5c55\u4e3a `(batch_size, 1, seq_len)`\u3002\u5bf9\u63a9\u7801\u8fdb\u884c\u5e7f\u64ad\u4ee5\u5339\u914d\u6ce8\u610f\u529b\u6743\u91cd\u5f20\u91cf\u7684\u7ef4\u5ea6\uff0c\u786e\u4fdd\u5728\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u6b63\u786e\u5730\u5e94\u7528\u63a9\u7801\n\n\n        q, attn = self.attention(q, k, v, mask=mask)\n\n        # Transpose to move the head dimension back: b x lq x n x dv\n        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n        # \u4f7f\u7528.transpose()\uff0c\u5c3d\u7ba1\u5f20\u91cf\u7684\u5f62\u72b6\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u4f46\u5176\u5185\u5b58\u5e03\u5c40\u5e76\u6ca1\u6709\u76f8\u5e94\u5730\u91cd\u65b0\u6392\u5217\uff0c\u5bfc\u81f4\u8fd9\u4e9b\u5f20\u91cf\u5728\u5185\u5b58\u4e2d\u662f\u975e\u8fde\u7eed\u7684\u3002.contiguous()\u786e\u4fdd\u5f20\u91cf\u5728\u5185\u5b58\u4e2d\u662f\u8fde\u7eed\u5b58\u50a8\u7684\n        q = self.dropout(self.fc(q))\n        q += residual\n\n        q = self.layer_norm(q)\n\n        return q, attn\n</code></pre> <p> </p> <li> <p><code>Position-wise</code>\u524d\u9988\u7f51\u7edc</p> \\[ FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 \\] <p>\u5373\u4f7f\u7528\u4e24\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c\u5e76\u5728\u5176\u4e2d\u63d2\u5165\u4e00\u6b21ReLU\u6fc0\u6d3b\u51fd\u6570\u4f5c\u4e3a\u524d\u9988\u7f51\u7edc</p> <p> Code <pre><code>class PositionwiseFeedForward(nn.Module):\n    ''' A two-feed-forward-layer module '''\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n\n        residual = x\n\n        x = self.w_2(F.relu(self.w_1(x)))\n        x = self.dropout(x)\n        x += residual\n\n        x = self.layer_norm(x)\n\n        return x\n</code></pre> <p> </p>"},{"location":"sci-paper/cs/Attention-is-all-you-need/#modelspy","title":"<code>Models.py</code>","text":"<ul> <li> <p>\u4f4d\u7f6e\u7f16\u7801<code>PositionalEncoding</code></p> <p>\u5bf9\u4e00\u79cd\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u7684\u8981\u6c42\uff1a</p> <ul> <li> <p>\u9700\u8981\u4f53\u73b0\u540c\u4e00\u5355\u8bcd\u5728\u4e0d\u540c\u4f4d\u7f6e\u7684\u533a\u522b\u3002</p> </li> <li> <p>\u9700\u8981\u4f53\u73b0\u4e00\u5b9a\u7684\u5148\u540e\u6b21\u5e8f\uff0c\u5e76\u4e14\u5728\u4e00\u5b9a\u8303\u56f4\u5185\u7684\u7f16\u7801\u5dee\u5f02\u4e0d\u5e94\u8be5\u4f9d\u8d56\u4e8e\u6587\u672c\u7684\u957f\u5ea6\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u4e0d\u53d8\u6027\u3002</p> </li> <li> <p>\u9700\u8981\u6709\u503c\u57df\u7684\u8303\u56f4\u9650\u5236\u3002</p> </li> </ul> \\[ PE_{(pos, 2i)} = sin(\\frac{pos} {10000^{2i/d_{\\text{model}}}}) \\] \\[ PE_{(pos, 2i+1)} = cos(\\frac{pos} {10000^{2i/d_{\\text{model}}}}) \\] <p></p> <p></p> <p></p> <p>\u5373\u5076\u6570\u4f4d\u7528\u6b63\u5f26\u51fd\u6570\uff0c\u5947\u6570\u4f4d\u7528\u4f59\u5f26\u51fd\u6570\u6765\u5904\u7406\u7f16\u7801\uff0c\u5176\u4e2d\\(pos\\)\u4ee3\u8868\u4e00\u53e5\u8bdd\u4e2dtoken\u7684\u4f4d\u7f6e\uff0c\u6bcf\u4e2atoken\u7684\u4f4d\u7f6e\u7f16\u7801\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\\(i\\)\u8868\u793a\u8fd9\u4e2a\u5411\u91cf\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684index\uff0c\\(d_{model}\\)\u4ee3\u8868\u4f4d\u7f6e\u7f16\u7801\u5411\u91cf\u7684\u7ef4\u5ea6\u3002</p> <p></p> <p></p> <p>\u67d0\u4e2apos\u4f4d\u7f6e\u7684\u4f4d\u7f6e\u7f16\u7801\u53ef\u8868\u793a\u4e3a\uff1a</p> \\[ PE_{pos}=\\begin{bmatrix}sin(\\omega_1\\cdot pos) \\\\ cos(\\omega_1\\cdot pos) \\\\ sin(\\omega_2\\cdot pos) \\\\ cos(\\omega_2\\cdot pos) \\\\ \\vdots \\\\ sin(\\omega_{d/2}\\cdot pos) \\\\ cos(\\omega_{d/2}\\cdot pos) \\end{bmatrix} \\] <p>\u5176\u4e2d\u7f16\u7801\u56e0\u5b50\\(\\omega_i = \\frac{1}{10000^{\\frac{2i}{d_{model}}}}\\)</p> <p>\u5047\u8bbe\u67d0\u4e00\u4e2a<code>token</code>\u7684\u4f4d\u7f6e\u4e3a<code>pos</code>\uff0c\u53e6\u4e00\u4e2a<code>token</code>\u8868\u793a\u4e3a<code>pos+k</code> \uff0c\u90a3\u5c31\u8868\u660e\u8fd9\u4e2a\u4f4d\u7f6e\u8ddd\u4e0a\u4e00\u4e2a<code>token</code>\u4e3a<code>k</code>\uff0c\u6839\u636e<code>Transformer</code>\u4e2d\u7ed9\u51fa\u7684\u4f4d\u7f6e\u7f16\u7801\u516c\u5f0f\uff0c\u5219:</p> \\[ \\begin{aligned} PE_{(pos+k,2i)}&amp;=sin(\\omega _i\\cdot (pos+k)) \\\\ &amp;=sin(\\omega _i\\cdot pos)cos(\\omega _i\\cdot k)+cos(\\omega _i\\cdot pos)sin(\\omega _i\\cdot k) \\end{aligned} \\] \\[ \\begin{aligned} PE_{(pos+k,2i+1)}&amp;=cos(\\omega _i\\cdot (pos+k)) \\\\ &amp;=cos(\\omega _i\\cdot pos)cos(\\omega _i\\cdot k)-sin(\\omega _i\\cdot pos)sin(\\omega _i\\cdot k) \\end{aligned} \\] <p>\u4f7f\u7528\\(w_i\\)\u4ee3\u66ff\\(\\frac{1}{10000^{\\frac{2i}{d_{model}}}}\\)\uff1b</p> \\[ PE_{(pos+k,2i)}=cos(\\omega _i\\cdot k)PE_{(pos,2i)}+sin(\\omega _i\\cdot k)PE_{(pos,2i+1)} \\] \\[ PE_{(pos+k,2i+1)}=cos(\\omega _i\\cdot k)PE_{(pos,2i+1)}-sin(\\omega _i\\cdot k)PE_{(pos,2i)} \\] <p>\u56e0\u4e3a\\(k\\)\u4e3a\u5e38\u6570\uff0c\u5219\u8bbe\\(u=cos(\\omega _i\\cdot k), v=sin(\\omega _i\\cdot k)\\)\uff1a</p> <p>$$ \\begin{bmatrix} PE_{(pos+k,2i)} \\ PE_{(pos+k,2i+1)})  \\end{bmatrix} </p> <p>= </p> \\[\\begin{bmatrix}  u &amp; v\\\\ -v &amp; u  \\end{bmatrix}\\] <p>\\times </p> \\[\\begin{bmatrix}  PE_{(pos,2i)}\\\\ PE_{(pos,2i+1)}  \\end{bmatrix}\\] <p>$$</p> <p>\u7531\u6b64\u53ef\u77e5\uff0c\u4f4d\u7f6e<code>pos</code>\u7684\u7f16\u7801\u4e0e\u4f4d\u7f6e<code>pos+k</code>\u7684\u4f4d\u7f6e\u7f16\u7801\u662f\u7ebf\u6027\u5173\u7cfb\u3002</p> \\[ \\begin{aligned} PE_{pos}\\cdot PE_{pos+k}&amp;=\\sum_{i=0}^{\\frac{d}{2}-1}sin(\\omega _i\\cdot pos)\\cdot sin(\\omega _i(pos+k)) \\\\ &amp;= \\sum_{i=0}^{\\frac{d}{2}-1}cos(\\omega _i(pos-(pos+k)))  \\\\ &amp;= \\sum_{i=0}^{\\frac{d}{2}-1}cos(\\omega _i\\cdot k) \\\\ \\end{aligned} \\] <p>\u5bf9\u4e8e\\(PE_{pos}\\)\u4e0e\\(PE_{pos+k}\\)\u7684\u70b9\u79ef\uff0c\u53ef\u5f97\u4e00\u4e2a\u4f59\u5f26\u7684\u548c\u503c\uff0c\u5e76\u4e14\u8fd9\u4e2a\u548c\u503c\u968f\u7740\\(k\\)\u7684\u589e\u5927\u800c\u51cf\u5c0f\uff0c\u5373\u4e24\u4e2a<code>token</code>\u7684\u8ddd\u79bb\u8d8a\u5927\uff0c\u4e5f\u5c31\u662f<code>K</code>\u8d8a\u5927\uff0c\u4e24\u4e2a\u4f4d\u7f6e\u7684<code>PE</code>\u76f8\u4e58\u7ed3\u679c\u8d8a\u5c0f\uff08\u4f4d\u7f6e\u7f16\u7801\u53ef\u4ee5\u8868\u793a\u76f8\u5bf9\u4f4d\u7f6e\u5173\u7cfb\uff09</p> <p>\u4f46\u663e\u7136\u8fd9\u6837\u7684\u4f4d\u7f6e\u5173\u7cfb\u5e76\u4e0d\u662f\u663e\u5f0f\u7684\uff0c\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u6765\u8ba9\u6a21\u578b\u5145\u5206\u5b66\u4e60\u5230\u4f4d\u7f6e\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u5e8f\u5217\u548c\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u65f6\u3002</p> <p>\u4e3a\u4ec0\u4e48\u4f4d\u7f6e\u7f16\u7801\u4e0d\u76f4\u63a5\u62fc\u63a5\u5230\u7f16\u7801\u77e9\u9635\u4e2d\u5462\uff1f</p> <p>\u76f4\u63a5\u62fc\u63a5\u4f1a\u6269\u5927\u53c2\u6570\u7a7a\u95f4\uff0c\u5360\u7528\u5185\u5b58\u589e\u52a0\uff0c\u800c\u4e14\u4e0d\u6613\u62df\u5408\uff0c\u800c\u4e14\u5176\u5b9e\u6ca1\u6709\u8bc1\u636e\u8868\u660e\u62fc\u63a5\u5c31\u6bd4\u52a0\u6765\u7684\u597d</p> <p> Code <pre><code>from transformer.Layers import EncoderLayer, DecoderLayer\n\ndef get_pad_mask(seq, pad_idx):  # \u63a9\u7801\u77e9\u9635\uff0c\u7528\u4e8e\u5c06\u586b\u5145\u4f4d\u7f6e\uff08\u5373pad_idx\uff09\u5728\u6ce8\u610f\u529b\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u7f6e\u4e3a0\u3002\nreturn (seq != pad_idx).unsqueeze(-2)\n\ndef get_subsequent_mask(seq):    # \u63a9\u7801\u77e9\u9635\uff0c\u7528\u4e8e\u5c06\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6ce8\u610f\u529b\u6743\u91cd\u9650\u5236\u5728\u5f53\u524d\u4f4d\u7f6e\u53ca\u5176\u4e4b\u524d\u7684\u6240\u6709\u4f4d\u7f6e\u3002\n    ''' For masking out the subsequent info. '''\n    sz_b, len_s = seq.size()\n    subsequent_mask = (1 - torch.triu(torch.ones((1, len_s, len_s), \n                                        device=seq.device), \n                                        diagonal=1)).bool()\n    return subsequent_mask\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_hid, n_position=200):\n        super(PositionalEncoding, self).__init__()\n\n        # Not a parameter\n        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n\n        '''self.register_buffer(\u2018name\u2019,Tensor)\u65b9\u6cd5\u7528\u4e8e\u5b9a\u4e49\u4e00\u7ec4\u53c2\u6570\n        \u8be5\u7ec4\u53c2\u6570\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\u4e0d\u4f1a\u66f4\u65b0\uff08\u5373optimizer.step()\u540e\u8be5\u7ec4\u53c2\u6570\u4e0d\u4f1a\u53d8\u5316\uff0c\u53ea\u53ef\u4eba\u4e3a\u5730\u6539\u53d8\u5b83\u4eec\u7684\u503c\uff09\n        '''\n\n    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n        ''' Sinusoid position encoding table '''\n        # TODO: make it with torch instead of numpy\n\n        def get_position_angle_vec(position):\n            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n\n        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n    def forward(self, x):\n        return x + self.pos_table[:, :x.size(1)].clone().detach() \n        # .detach() \u65b9\u6cd5\u5c06\u5176\u4ece\u8ba1\u7b97\u56fe\u4e2d\u5206\u79bb\uff0c\u786e\u4fdd\u5728\u540e\u7eed\u7684\u8ba1\u7b97\u4e2d\u4e0d\u4f1a\u5f71\u54cd\u68af\u5ea6\u4f20\u64ad\n</code></pre> <p> </p> <li> <p>\u7f16\u7801\u5668<code>Encoder</code></p> <p> Code <pre><code>class Encoder(nn.Module):\n''' A encoder model with self attention mechanism. '''\n    def __init__(\n            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n            d_model, d_inner, pad_idx, dropout=0.1, n_position=200, scale_emb=False):\n\n        super().__init__()\n        '''\n        n_src_vocab: \u8bcd\u6c47\u8868\u5927\u5c0f\n        d_word_vec\uff1a\u8bcd\u5411\u91cf\u7ef4\u5ea6\n        n_layers\uff1a\u7f16\u7801\u5668\u5c42\u6570\n        n_head\uff1a\u6ce8\u610f\u529b\u5934\u6570\n        d_k\uff1akey\u7ef4\u5ea6\n        d_v\uff1avalue\u7ef4\u5ea6\n        d_model\uff1a\u6a21\u578b\u7ef4\u5ea6\n        d_inner\uff1a\u5185\u90e8\u7ef4\u5ea6\n        pad_idx\uff1a\u586b\u5145\u7d22\u5f15('&lt;pad&gt;')\n        dropout\uff1adropout\u6982\u7387\n        n_position\uff1a\u4f4d\u7f6e\u7f16\u7801\u7684\u6700\u5927\u4f4d\u7f6e\n        scale_emb\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u8868\u793a\u662f\u5426\u5bf9\u8bcd\u5d4c\u5165\u8fdb\u884c\u7f29\u653e\n        '''\n        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx)                      \n        # \u8bcd\u5411\u91cf\u8f6c\u5316, padding_idx\u5c31\u662f\u628aEmbeddings\u8f6c\u5316\u77e9\u9635\u67d0\u4e00\u884c\u7f6e\u4e3a0\n        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_stack = nn.ModuleList([\n            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n        self.scale_emb = scale_emb\n        # \u7f29\u653e\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u4f7f\u5f97\u4f4d\u7f6e\u7f16\u7801\u7684\u6570\u503c\u5927\u5c0f\u4e0e\u8bcd\u5d4c\u5165\u7684\u6570\u503c\u5927\u5c0f\u5e94\u8be5\u5904\u4e8e\u540c\u4e00\u6570\u91cf\u7ea7\n        self.d_model = d_model\n\n    def forward(self, src_seq, src_mask, return_attns=False):\n\n        enc_slf_attn_list = []\n\n        # -- Forward\n        enc_output = self.src_word_emb(src_seq)\n        if self.scale_emb:\n            enc_output *= self.d_model ** 0.5\n            # \u7f29\u653e\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u786e\u4fdd\u8bcd\u5411\u91cf\u5177\u6709\u9002\u5f53\u7684\u5c3a\u5ea6\n        enc_output = self.dropout(self.position_enc(enc_output))\n        enc_output = self.layer_norm(enc_output)\n\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n\n        if return_attns:\n            return enc_output, enc_slf_attn_list\n        return enc_output,\n</code></pre> <p> </p> <li> <p>\u89e3\u7801\u5668<code>Decoder</code></p> <p> Code <pre><code>class Decoder(nn.Module):\n''' A decoder model with self attention mechanism. '''\n    def __init__(\n            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n            d_model, d_inner, pad_idx, n_position=200, dropout=0.1, scale_emb=False):\n\n        super().__init__()\n\n        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_stack = nn.ModuleList([\n            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n        self.scale_emb = scale_emb\n        self.d_model = d_model\n\n    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):\n\n        dec_slf_attn_list, dec_enc_attn_list = [], []\n\n        # -- Forward\n        dec_output = self.trg_word_emb(trg_seq)\n        if self.scale_emb:\n            dec_output *= self.d_model ** 0.5\n        dec_output = self.dropout(self.position_enc(dec_output))\n        dec_output = self.layer_norm(dec_output)\n\n        for dec_layer in self.layer_stack:\n            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n\n        if return_attns:\n            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n        return dec_output,\n</code></pre> <p> </p> <li> <p><code>Transformer</code></p> <p></p> <p></p> <p> Code <pre><code>class Transformer(nn.Module):\n''' A sequence to sequence model with attention mechanism. '''\n    def __init__(\n            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n            d_word_vec=512, d_model=512, d_inner=2048,\n            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,\n            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True,\n            scale_emb_or_prj='prj'):\n\n        super().__init__()\n\n        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n\n        # In section 3.4 of paper \"Attention Is All You Need\", there is such detail:\n        # \"In our model, we share the same weight matrix between the two\n        # embedding layers and the pre-softmax linear transformation...\n        # In the embedding layers, we multiply those weights by \\sqrt{d_model}\".\n        #\n        # Options here:\n        #   'emb': multiply \\sqrt{d_model} to embedding output\n        #   'prj': multiply (\\sqrt{d_model} ^ -1) to linear projection output\n        #   'none': no multiplication\n\n        assert scale_emb_or_prj in ['emb', 'prj', 'none']\n        scale_emb = (scale_emb_or_prj == 'emb') if trg_emb_prj_weight_sharing else False\n        self.scale_prj = (scale_emb_or_prj == 'prj') if trg_emb_prj_weight_sharing else False\n\n        '''\n        \u663e\u793a\u5224\u65ad\u662f\u5426\u7f29\u653e\u5d4c\u5165\u5c42\u3001\u6295\u5f71\u5c42\uff0c\u6216\u8005\u90fd\u4e0d\u7f29\u653e\u5982\u679c\u542f\u7528\u4e86\u76ee\u6807\u8bcd\u5d4c\u5165\u4e0e\u6295\u5f71\u5c42\u6743\u91cd\u7684\u5171\u4eab\uff0c\u5e76\u4e14 scale_emb_or_prj \u7684\u503c\u4e3a \u2018emb\u2019\uff0c\u5219 scale_emb \u88ab\u8bbe\u7f6e\u4e3a True\uff0c\u8868\u793a\u5728\u5d4c\u5165\u5c42\u8f93\u51fa\u540e\u8fdb\u884c\u6743\u91cd\u7f29\u653e\u3002\n        \u5426\u5219\uff0cscale_emb \u88ab\u8bbe\u7f6e\u4e3a False\uff0c\u5bf9\u4e8eprj\u540c\u7406\u3002\n        \u5728\u6a21\u578b\u4e2d\u540c\u65f6\u4f7f\u7528\u4e24\u79cd\u7f29\u653e\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6216\u6df7\u4e71\uff0c\u56e0\u6b64\u4e00\u822c\u6765\u8bf4\uff0c\u9009\u62e9\u5176\u4e2d\u4e00\u79cd\u7f29\u653e\u65b9\u5f0f\u66f4\u4e3a\u5e38\u89c1\u548c\u5408\u7406\u3002\n        '''\n        self.d_model = d_model\n\n        self.encoder = Encoder(\n            n_src_vocab=n_src_vocab, n_position=n_position,\n            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n            pad_idx=src_pad_idx, dropout=dropout, scale_emb=scale_emb)\n\n        self.decoder = Decoder(\n            n_trg_vocab=n_trg_vocab, n_position=n_position,\n            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n            pad_idx=trg_pad_idx, dropout=dropout, scale_emb=scale_emb)\n\n        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n        # \u5c06\u89e3\u7801\u5668\u6700\u540e\u4e00\u5c42\u7684\u8f93\u51fa\u6620\u5c04\u4e3a\u76ee\u6807\u8bed\u8a00\u8bcd\u6c47\u8868\u7684\u6982\u7387\u5206\u5e03\uff0c\u4ece\u800c\u7528\u4e8e\u751f\u6210\u76ee\u6807\u8bed\u8a00\u7684\u4e0b\u4e00\u4e2a\u8bcd\u3002\n\n        for p in self.parameters(): # \u5404\u5c42\u53c2\u6570\u7684\u8fed\u4ee3\n            if p.dim() &gt; 1:\n                nn.init.xavier_uniform_(p) # \u53c2\u6570\u521d\u59cb\u5316\n\n        assert d_model == d_word_vec, \\\n        'To facilitate the residual connections, \\\n        the dimensions of all module outputs shall be the same.'\n\n        if trg_emb_prj_weight_sharing:\n            # Share the weight between target word embedding &amp; last dense layer\n            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight\n        # \u5171\u4eab\u8fd9\u4e24\u4e2a\u5c42\u7684\u6743\u91cd\uff0c\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u4f1a\u5b66\u4e60\u4e00\u7ec4\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u540c\u65f6\u7528\u4e8e\u5d4c\u5165\u76ee\u6807\u8bcd\u548c\u5c06\u89e3\u7801\u5668\u7684\u8f93\u51fa\u6620\u5c04\u5230\u76ee\u6807\u8bcd\u6c47\u8868\u3002\n\n        if emb_src_trg_weight_sharing:\n            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight\n\n    def forward(self, src_seq, trg_seq):\n\n        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) &amp; get_subsequent_mask(trg_seq)\n\n        enc_output, *_ = self.encoder(src_seq, src_mask)\n        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n        seq_logit = self.trg_word_prj(dec_output)\n        if self.scale_prj:\n            seq_logit *= self.d_model ** -0.5\n\n        return seq_logit.view(-1, seq_logit.size(2))\n</code></pre> <p> </p>"},{"location":"sci-paper/cs/Attention-is-all-you-need/#layerspy","title":"<code>Layers.py</code>","text":"<ul> <li> <p><code>Encoder</code>\u4e0e<code>Decoder</code>\u7ec4\u6210\u90e8\u5206</p> <p></p> <p></p> <p> Code <pre><code>class EncoderLayer(nn.Module):\n    ''' Compose with two layers '''\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, enc_input, slf_attn_mask=None):\n        enc_output, enc_slf_attn = self.slf_attn(\n            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n        enc_output = self.pos_ffn(enc_output)\n        return enc_output, enc_slf_attn\n\nclass DecoderLayer(nn.Module):\n    ''' Compose with three layers '''\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(\n            self, dec_input, enc_output,\n            slf_attn_mask=None, dec_enc_attn_mask=None):\n        dec_output, dec_slf_attn = self.slf_attn(\n            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n        dec_output, dec_enc_attn = self.enc_attn(\n            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n        dec_output = self.pos_ffn(dec_output)\n        return dec_output, dec_slf_attn, dec_enc_attn\n</code></pre>"},{"location":"sci-paper/cs/Attention-is-all-you-need/#optimpy","title":"<code>Optim.py</code>","text":"<ul> <li> <p>\u4f18\u5316\u51fd\u6570</p> <p>\u6839\u636e\u4ee5\u4e0b\u516c\u5f0f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6539\u53d8\u5b66\u4e60\u7387\uff1a</p> \\[ lrate = d_{model}^{-0.5} \\cdot min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5}) \\] <p> Code <pre><code>class ScheduledOptim():\n    '''A simple wrapper class for learning rate scheduling'''\n\n    def __init__(self, optimizer, lr_mul, d_model, n_warmup_steps):\n        '''\n        optimizer\uff1a\u4f18\u5316\u5668\uff0c\u5982 Adam \u6216 SGD\u3002\n        lr_mul\uff1a\u8c03\u6574\u5b66\u4e60\u7387\u7684\u4e58\u6570\u56e0\u5b50\u3002\n        d_model: \u9690\u85cf\u5c42\u7684\u5927\u5c0f\u3002\n        n_warmup_steps\uff1a\u9884\u70ed\u9636\u6bb5\u7684\u6b65\u6570\u3002\u5728\u9884\u70ed\u9636\u6bb5\uff0c\u5b66\u4e60\u7387\u4f1a\u9010\u6e10\u589e\u52a0\u3002\n        n_steps\uff1a\u5f53\u524d\u8bad\u7ec3\u6b65\u6570\uff0c\u521d\u59cb\u5316\u4e3a0\u3002\n        '''\n        self._optimizer = optimizer\n        self.lr_mul = lr_mul\n        self.d_model = d_model\n        self.n_warmup_steps = n_warmup_steps\n        self.n_steps = 0\n\n\n    def step_and_update_lr(self):\n        \"Step with the inner optimizer\"\n        self._update_learning_rate()\n        self._optimizer.step()\n\n\n    def zero_grad(self):\n        \"Zero out the gradients with the inner optimizer\"\n        self._optimizer.zero_grad()\n\n\n    def _get_lr_scale(self):\n        d_model = self.d_model\n        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n\n\n    def _update_learning_rate(self):\n        ''' Learning rate scheduling per step '''\n\n        self.n_steps += 1\n        lr = self.lr_mul * self._get_lr_scale()\n\n        for param_group in self._optimizer.param_groups:\n            param_group['lr'] = lr #\u66f4\u65b0\u5b66\u4e60\u7387\n</code></pre>"},{"location":"sci-paper/cs/Bert/","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/master?tab=readme-ov-file</p> <p>\u8d44\u6e90\uff1a</p> <p></p>"},{"location":"sci-paper/cs/Bert/#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p>BERT\uff08Bidirectional Encoder Representations from Transformers\uff09\u662f\u4e00\u79cdTransformer\u7684\u53cc\u5411\u7f16\u7801\u5668\uff0c\u65e8\u5728\u901a\u8fc7\u5728\u5de6\u53f3\u4e0a\u4e0b\u6587\u4e2d\u5171\u6709\u7684\u6761\u4ef6\u8ba1\u7b97\u6765\u9884\u5148\u8bad\u7ec3\u6765\u81ea\u65e0\u6807\u53f7\u6587\u672c\u7684\u6df1\u5ea6\u53cc\u5411\u8868\u793a\u3002\u56e0\u6b64\uff0c\u7ecf\u8fc7\u9884\u5148\u8bad\u7ec3\u7684BERT\u6a21\u578b\u53ea\u9700\u4e00\u4e2a\u989d\u5916\u7684\u8f93\u51fa\u5c42\u5c31\u53ef\u4ee5\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u4e3a\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u751f\u6210\u6700\u65b0\u6a21\u578b\u3002</p> <p>\u4e5f\u662f\u6211\u4eec\u5e38\u8bf4\u7684 \u3010\u9884\u8bad\u7ec3\u3011+\u3010\u5fae\u8c03\u3011</p> <p></p> <p>BERT\u7684\u6838\u5fc3\u601d\u60f3\u5982\u4e0b\uff1a</p> <ul> <li> <p>\u968f\u673a\u906e\u6321\u53e5\u5b50\u4e2d\u4e00\u4e2a\u6216\u591a\u4e2a\u5355\u8bcd\uff0c\u8ba9Encoder\u6839\u636e\u53e5\u5b50\u4e0a\u4e0b\u6587\u9884\u6d4b\u88ab\u906e\u6321\u7684\u5355\u8bcd\uff08Predict Masked Word\uff09</p> </li> <li> <p>\u5c06\u4e24\u4e2a\u53e5\u5b50\u62fc\u63a5\uff0c\u8ba9Encoder\u5224\u65ad\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u4e3a\u539f\u6587\u4e2d\u76f8\u90bb\u7684\u53e5\u5b50(Predict Next Sentence)</p> </li> </ul> <p>BERT\u901a\u8fc7\u4e0a\u8ff0\u4e24\u4e2a\u4efb\u52a1\uff0c\u9884\u8bad\u7ec3Transformer\u6a21\u578bEncoder\u7f51\u7edc\u3002</p> <p>\u4f5c\u8005\u7ed9\u51fa\u4e86\u4e24\u79cd\u4e0d\u540c\u5927\u5c0f\u7684<code>bert</code>\u6a21\u578b\uff1a</p> <ul> <li> <p><code>BERT-Base</code>\uff1a12 Layers <code>Transformer Encoder</code>\uff0c768 Hidden(\u9690\u85cf\u5355\u5143)\uff0c12 Head\uff0c110M Parameters.</p> </li> <li> <p><code>BERT-Large</code>\uff1a24 Layers <code>Transformer Encoder</code>\uff0c1024Hidden(\u9690\u85cf\u5355\u5143)\uff0c16 Head\uff0c340M Parameters.</p> </li> </ul> <p>Bert\u76f4\u63a5\u5f15\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757\uff0c\u820d\u5f03\u4e86Decoder\u6a21\u5757, \u7531\u591a\u4e2aEncoder block\u6a21\u5757\u5806\u53e0\u800c\u6210\uff0c\u8fd9\u6837\u4fbf\u81ea\u52a8\u62e5\u6709\u4e86\u53cc\u5411\u7f16\u7801\u80fd\u529b\u548c\u5f3a\u5927\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002</p>"},{"location":"sci-paper/cs/Bert/#embedding","title":"<code>Embedding</code>","text":"<p><code>BERT</code>\u7684\u8f93\u5165<code>Embedding</code>\u6a21\u5757\u7531\u4e09\u90e8\u5206\u7ec4\u6210\uff1a</p> <ul> <li> <p><code>Token Embeddings</code>\uff1a\u8f93\u5165\u6587\u672c\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u6216\u5b57\u7b26\u8f6c\u6362\u4e3a\u4e00\u4e2a\u56fa\u5b9a\u7ef4\u5ea6\u7684\u5411\u91cf\u3002<code>Base</code>\u7248\u4e3a768\u7ef4\uff0c<code>Large</code>\u7248\u4e3a1024\u7ef4\u3002</p> </li> <li> <p><code>Position Embeddings</code>\uff1a\u5355\u8bcd\u6216\u5b57\u7b26\u5728\u53e5\u5b50\u4e2d\u7684\u4f4d\u7f6e\u4fe1\u606f\u3002<code>BERT</code>\u4e2d\u7684\u4f4d\u7f6e\u5d4c\u5165\u662f\u53ef\u5b66\u4e60\u7684\uff0c\u5b83\u4f1a\u968f\u7740\u6a21\u578b\u7684\u8bad\u7ec3\u800c\u66f4\u65b0\uff0c\u975e\u56fa\u5b9a\u7684\u4e09\u89d2\u51fd\u6570\u3002</p> </li> <li> <p><code>Segment Embeddings</code>\uff1a\u7528\u4e8e\u533a\u5206\u540c\u4e00\u8f93\u5165\u5e8f\u5217\u4e2d\u4e0d\u540c\u53e5\u5b50\u7684\u6765\u6e90\u3002\u5bf9\u4e8e\u591a\u53e5\u8f93\u5165\uff0c<code>BERT</code>\u4f1a\u4e3a\u6bcf\u4e2a\u53e5\u5b50\u5206\u914d\u4e00\u4e2a\u4e0d\u540c\u7684\u6bb5\u7f16\u53f7\uff0c\u6765\u533a\u5206\u5b83\u4eec\u3002<code>Segment Embeddings</code>\u7684\u53d6\u503c\u901a\u5e38\u662f0\u548c1\uff0c\u5982\u679c\u8f93\u5165\u5305\u542b\u4e24\u4e2a\u53e5\u5b50\uff0c\u901a\u5e38\u7b2c\u4e00\u4e2a\u53e5\u5b50\u7684<code>token</code>\u4f1a\u88ab\u8d4b\u4e88\u51680\u7684\u5411\u91cf\uff0c\u7b2c\u4e8c\u4e2a\u53e5\u5b50\u7684token\u4f1a\u88ab\u8d4b\u4e88\u51681\u7684\u5411\u91cf\u3002\u4e0b\u56fe\u662f\u4e00\u4e2a\u793a\u4f8b\u3002</p> </li> </ul> <p></p>"},{"location":"sci-paper/cs/Bert/#pre-training","title":"Pre-training","text":"<p><code>BERT</code>\u7684\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e3b\u8981\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a<code>Masked Language Model\uff08MLM\uff09</code>\u548c<code>Next Sentence Prediction\uff08NSP\uff09</code>\u3002</p> <ul> <li><code>Masked Language Model\uff08MLM\uff09</code>\u76ee\u6807\uff1a\u586b\u7a7a\uff0c\u6559\u5bfc<code>BERT</code>\u4e0a\u4e0b\u6587</li> </ul> <p>\u4e3a\u4e86\u8bad\u7ec3\u6df1\u5ea6\u53cc\u5411\u8868\u793a\uff08deep bidirectional representation\uff09\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u5bf9\u6bcf\u4e2a\u5e8f\u7c7b\u4e2d\u768415%<code>token</code>\u968f\u673a\u8fdb\u884c\u63a9\u7801\uff08\u66ff\u6362\u4e3a<code>[MASK]</code>\uff09\uff0c\u5e76\u4e14\u8ba9\u6a21\u578b\u53ea\u9884\u6d4b\u63a9\u7801\u8bcd\uff0c\u800cBERT\u5b66\u4f1a\u4ece\u4e0a\u4e0b\u6587\u4e2d\u9884\u6d4b\u8fd9\u4e9b\u5355\u8bcd\u3002\u8fd9\u6709\u52a9\u4e8eBERT\u7406\u89e3\u5355\u8bcd\u5f7c\u6b64\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u65e0\u8bba\u662f\u5728\u524d\u9762\u8fd8\u662f\u5728\u540e\u9762\u3002</p> <p>\u4e8b\u5b9e\u4e0a\uff0c<code>[MASK] token</code>\u5728fine-tuning\u671f\u95f4\u4e0d\u4f1a\u51fa\u73b0\uff0c\u56e0\u6b64\u5bf9\u4e8e\u6bcf\u4e2a\u88ab\u9009\u4e2d\u7684<code>token</code>\uff0c\u670980%\u7684\u6982\u7387\u76f4\u63a5\u7528<code>[MASK]</code>\u6807\u8bb0\u66ff\u6362\uff0c10%\u7684\u6982\u7387\u7528\u968f\u673a\u7684\u4e00\u4e2a\u5355\u8bcd\u66ff\u6362\uff08\u8fd9\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u7406\u89e3\u4e0a\u4e0b\u6587\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4f9d\u8d56\u4e8e<code>[MASK]</code>\u6807\u8bb0\uff09\uff0c\u5269\u4e0b\u768410%\u5219\u4fdd\u6301\u4e0d\u53d8\uff08\u8fd9\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u5fae\u8c03\u9636\u6bb5\u66f4\u597d\u5730\u5904\u7406\u672a\u906e\u853d\u7684\u5355\u8bcd\uff09\u3002</p> <p>\u5982\u56fe\uff0c\u5047\u8bbe\u8f93\u5165\u53e5\u5b50\u4e3a<code>[the cat sat on the mat]</code>\uff0c\u88ab\u968f\u673a\u906e\u6321\u7684\u5355\u8bcd\u4e3a<code>[cat]</code>\uff0c\u90a3\u4e48<code>BERT</code>\u7684\u8f93\u51fa\u4e3a:</p> <ul> <li> <p>80%\u7684\u65f6\u5019\u662f<code>[MASK]</code>\uff1a<code>[the [[MASK]] sat on the mat]</code></p> </li> <li> <p>10%\u7684\u65f6\u5019\u662f\u968f\u673a\u5355\u8bcd\uff1a<code>[the [bike] sat on the mat]</code></p> </li> <li> <p>10%\u7684\u65f6\u5019\u662f\u539f\u8bcd\uff1a<code>[the [cat] sat on the mat]</code></p> </li> </ul> <p></p> <p>\u8fd9\u6837\u505a\u4f7f\u5f97\u7f16\u7801\u5668\u4e0d\u77e5\u9053\u6b64\u6b21\u9700\u8981\u9884\u6d4b\u54ea\u4e9b<code>token</code>\u6216\u5df2\u88ab\u968f\u673a<code>token</code>\u66ff\u6362\uff0c\u56e0\u6b64\u6a21\u578b\u88ab\u8feb\u4fdd\u7559\u6bcf\u4e2a<code>token</code>\u7684\u5206\u5e03\u4e0a\u4e0b\u6587\u8868\u793a\u3002\uff08is forced to keep a distributional contextual representation of every input token\uff09</p> <ul> <li><code>Next Sentence Prediction\uff08NSP\uff09</code>\u76ee\u6807\uff1a\u6559\u5bfc<code>BERT</code>\u7406\u89e3\u53e5\u5b50\u4e4b\u95f4\u7684\u5173\u7cfb</li> </ul> <p>\u4ece\u6587\u672c\u6570\u636e\u4e2d\u751f\u6210\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u62bd\u53d6\u4e24\u4e2a\u53e5\u5b50\uff0c\u5e76\u5c06\u4e24\u4e2a\u53e5\u5b50\u62fc\u63a5\uff0c\u4e24\u4e2a\u53e5\u5b50\u4e2d\u95f4\u7528<code>[SEP]</code>\u7b26\u53f7\u5206\u9694\uff0c\u5728\u62fc\u63a5\u5f62\u6210\u7684\u53e5\u5b50\u524d\u6dfb\u52a0\u4e00\u4e2a<code>[CLS]</code>\u7b26\u53f7(<code>[CLS]</code>\u662f\u7528\u4e8e\u5206\u7c7b\u7684\u6807\u8bb0\uff1b<code>[SEP]</code>\u7528\u4e8e\u5206\u9694\u53e5\u5b50\u3002)\u3002\u5176\u4e2d\u8bad\u7ec3\u96c6\u768450%\u4e3a\u6587\u672c\u4e2d\u76f8\u90bb\u7684\u4e24\u4e2a\u53e5\u5b50\uff0c\u53e6\u591650%\u4e3a\u968f\u673a\u62bd\u53d6\u7684\u4e0d\u76f8\u90bb\u4e24\u4e2a\u53e5\u5b50\u3002\u8bad\u7ec3\u96c6\u4e2d\u76f8\u90bb\u53e5\u5b50\u7684\u6807\u7b7e\u8bbe\u7f6e\u4e3a1\uff0c\u4e0d\u76f8\u90bb\u53e5\u5b50\u7684\u6807\u7b7e\u8bbe\u7f6e\u4e3a0\u3002</p> <p>\u5728<code>BERT</code>\u7684\u540e\u7eed\u7248\u672c\u4e2d\uff0c<code>Next Sentence Prediction\uff08NSP\uff09</code>\u4efb\u52a1\u88ab\u5e9f\u5f03\u4e86\u3002\u56e0\u4e3a\u7814\u7a76\u4eba\u5458\u53d1\u73b0\u8fd9\u4e2a\u4efb\u52a1\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u56e0\u6b64\u5728<code>BERT</code>\u7684\u4e00\u4e9b\u540e\u7eed\u53d8\u4f53\u4e2d\u88ab\u5f03\u7528\u4e86\u3002</p> <p></p> <p><code>Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</code></p> <p><code>Label = IsNext</code></p> <p><code>Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</code></p> <p><code>Label = NotNext</code></p> <p></p> <p>\u5728\u8bad\u7ec3<code>BERT</code>\u65f6\uff0c\u4e3a\u4e86\u4e0d\u4f7f\u6a21\u578b\u8ba4\u4e3a<code>[MASK]</code>\u7b26\u53f7\u539f\u672c\u5c31\u5f52\u5c5e\u4e8e\u8bad\u7ec3\u53e5\u5b50\uff0c\u5728\u968f\u673a\u906e\u6321\u5355\u8bcd\u65f6\u91c7\u7528\u4e86\u5c06\u4e00\u5b9a\u6570\u91cf\u7684<code>[MASK]</code>\u66ff\u6362\u6210\u53e5\u5b50\u4e2d\u539f\u672c\u7684\u5355\u8bcd\uff0c\u5c06\u4e00\u5b9a\u6570\u91cf\u7684<code>[MASK]</code>\u7b26\u53f7\u66ff\u6362\u6210\u968f\u673a\u5355\u8bcd\u7b49\u7b49\u5c0f\u6280\u5de7\u3002</p>"},{"location":"sci-paper/cs/Bert/#fine-tuning","title":"Fine-tuning","text":"<p><code>self-attention</code> \u673a\u5236\u5141\u8bb8 <code>BERT</code> \u5bf9\u4efb\u4f55\u4e0b\u6e38\u4efb\u52a1\u5efa\u6a21 \u2014\u2014 \u65e0\u8bba\u662f single text \u8fd8\u662f text pairs \u2014\u2014 \u53ea\u9700\u8981\u9002\u5f53\u66ff\u6362\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u56e0\u6b64\u5bf9 <code>BERT</code> \u8fdb\u884c\u5fae\u8c03\u662f\u975e\u5e38\u65b9\u4fbf\u7684\u3002</p> <p>\u6839\u636e\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e0b\u6e38\u4efb\u52a1\u8f93\u5165\u548c\u8f93\u51fa\u5f62\u5f0f\u7684\u4e0d\u540c\uff0c\u5fae\u8c03\u4efb\u52a1\u53ef\u4ee5\u5206\u4e3a\u56db\u7c7b\uff0c\u5206\u522b\u662f\u53e5\u5bf9\u5206\u7c7b\u3001\u5355\u53e5\u5206\u7c7b\u3001\u6587\u672c\u95ee\u7b54\u548c\u5355\u53e5\u6807\u6ce8</p> <p></p> <ul> <li><code>The General Language Understanding Evaluation (GLUE) benchmark</code></li> </ul> <p>GLUE\u662f\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u7684\u96c6\u5408\uff0c\u4e3a\u4e86\u5bf9 GLUE \u8fdb\u884c\u5fae\u8c03\uff0c\u6211\u4eec\u4f7f\u7528\u5bf9\u5e94\u4e8e\u7b2c\u4e00\u4e2a\u8f93\u5165\u6807\u8bb0<code>[CLS]</code>\u7684\u8f93\u51fa\u5411\u91cf\\(C \\in \\mathbb{R}^H\\)\uff0c\u5f15\u5165\u4e00\u4e2a\u65b0\u7684\u5206\u7c7b\u5c42\u8fdb\u884c\u5206\u7c7b\u64cd\u4f5c\u3002</p> <p></p> <ul> <li><code>The Stanford Question Answering Dataset (SQuAD v1.1)</code></li> </ul> <p>SQuAD v1.1 \u662f100k\u4e2a\u516c\u5f00\u6765\u6e90\u7684question/answer\u5bf9\u7684\u96c6\u5408\uff0c\u901a\u5e38\u7ed9\u5b9a\u4e00\u4e2a\u95ee\u9898\u548c\u4e00\u4e2a\u7ef4\u57fa\u767e\u79d1\u6bb5\u843d\uff08\u5305\u542b\u7b54\u6848\uff09\uff0c\u4efb\u52a1\u662f\u9884\u6d4b\u6587\u7ae0\u4e2d\u7684\u7b54\u6848\u6587\u672c\u8de8\u5ea6\u3002</p> <p></p> <p>\u6211\u4eec\u5c06\u95ee\u9898\u548c\u6bb5\u843d\u8868\u793a\u4e3a\u5355\u4e2a\u6253\u5305\u5e8f\u5217\u8f93\u5165\uff08\u8f93\u5165\u683c\u5f0f\u4e3a<code>[CLS]+\u95ee\u9898+[SEP]+\u6bb5\u843d\u4fe1\u606f</code>\uff09\uff0c\u5728fine-tuning\uff0c\u6211\u4eec\u5f15\u5165\u4e00\u4e2a\u8d77\u59cb\u5411\u91cf\\(S \\in \\mathbb{R}^H\\)\u548c\u4e00\u4e2a\u7ed3\u675f\u5411\u91cf\\(E \\in \\mathbb{R}^H\\)\uff0c\u6765\u8ba1\u7b97\u7b54\u6848\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u3002</p> <p>\u5f00\u59cb\u4f4d\u7f6e\u7684\u8ba1\u7b97\u516c\u5f0f\uff1a</p> \\[ P_i = \\frac{e^{S \\cdot T_i}}{\\sum_{j=1}^{L} e^{S \\cdot T_j}} \\] <p>\u7ed3\u675f\u4f4d\u7f6e\u7684\u8ba1\u7b97\u516c\u5f0f\uff1a</p> \\[ P_i = \\frac{e^{E \\cdot T_i}}{\\sum_{j=1}^{L} e^{E \\cdot T_j}} \\] <p>\u4ece\u4f4d\u7f6e i \u5230\u4f4d\u7f6e j (i &lt;= j&gt;)\u7684\u5019\u9009\u8de8\u5ea6\u7684\u5206\u6570\u5b9a\u4e49\u4e3a:</p> \\[ Score = S \\cdot T_i + E \\cdot T_j \\] <p>\u6700\u5927\u5206\u503c\u7684\u8303\u56f4\u7528\u4f5c\u9884\u6d4b</p> <ul> <li><code>SQuAD 2.0 task</code></li> </ul> <p>SQuAD 2.0 \u4efb\u52a1\u901a\u8fc7\u5141\u8bb8\u63d0\u4f9b\u7684\u6bb5\u843d\u4e2d\u4e0d\u5b58\u5728\u7b54\u6848\u7684\u53ef\u80fd\u6027\u6765\u6269\u5c55 SQuAD 1.1 \u95ee\u9898\u5b9a\u4e49\uff0c\u4ece\u800c\u4f7f\u95ee\u9898\u66f4\u52a0\u73b0\u5b9e\u3002</p> <p>\u82e5\u4e0d\u5b58\u5728\u7b54\u6848\uff0c\u5219\u8ba1\u7b97\u4e00\u4e2a\u6ca1\u6709\u7b54\u6848\u7684\u5f97\u5206\uff1a</p> \\[ Score = S \\cdot C + E \\cdot C \\] <p>\u5176\u4e2d\\(C\\)\u5c31\u662f<code>[CLS]</code>\u7684\u5bf9\u5e94\u8f93\u51fa\uff0c\u6b64\u65f6\u5982\u679c\u6ca1\u6709\u7b54\u6848\u7684\u5206\u6570\u8981\u6bd4\u627e\u5230\u7684\u7b54\u6848\u7684\u5206\u6570\u8981\u597d\uff0c\u90a3\u4e48\u5c31\u9884\u6d4b\u4e3a\u6ca1\u6709\u7b54\u6848\u3002</p> <ul> <li><code>The Situations With Adversarial Generations (SWAG) dataset</code> </li> </ul> BERT\u7b80\u6d01\u5b9e\u73b0 <pre><code># sample IsNext and NotNext to be same in small batch size\ndef make_batch():\n    '''\n    word_dict\n    number_dict\n    positive = negative-&gt;int : \u6b63\u6837\u672c/\u8d1f\u6837\u672c\u6570\u91cf\n    input_ids-&gt;list: \u6a21\u578b\u8f93\u5165\u7684token id\n    segment_ids: \u533a\u5206\u53e5\u5b50\u7684segment id\n    n_pred: \u901a\u8fc7MASK\u540e\u6a21\u578b\u8981\u9884\u6d4b\u7684token\u6570\u91cf\n    n_pad: padding\u6570\u91cf\uff0c\u4f7fbatch\u4e2d\u7684\u53e5\u5b50\u957f\u5ea6\u4e00\u81f4\n    max_pred: \u9884\u6d4b\u7684\u6700\u5927\u63a9\u7801\u6570\u91cf\n    masked_tokens: MASK\u7684token\n    masked_pos: MASK\u7684token\u7684\u4f4d\u7f6e\n    '''\n    batch = []\n    positive = negative = 0 # \u6b63\u6837\u672cIsNext\u548c\u8d1f\u6837\u672cNotNext\u7684\u6570\u91cf\n    while positive != batch_size/2 or negative != batch_size/2:\n        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n\n        # MASK LM\n        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n        cand_maked_pos = [i for i, token in enumerate(input_ids)\n                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n        shuffle(cand_maked_pos)\n        masked_tokens, masked_pos = [], []\n        for pos in cand_maked_pos[:n_pred]:\n            masked_pos.append(pos)\n            masked_tokens.append(input_ids[pos])\n            if random() &lt; 0.8:  # 80%\n                input_ids[pos] = word_dict['[MASK]'] # make mask\n            elif random() &lt; 0.5:  # 10%\n                index = randint(0, vocab_size - 1) # random index in vocabulary\n                input_ids[pos] = word_dict[number_dict[index]] # replace\n\n        # Zero Paddings\n        n_pad = maxlen - len(input_ids)\n        input_ids.extend([0] * n_pad)\n        segment_ids.extend([0] * n_pad)\n\n        # Zero Padding (100% - 15%) tokens\n        if max_pred &gt; n_pred:\n            n_pad = max_pred - n_pred\n            masked_tokens.extend([0] * n_pad)\n            masked_pos.extend([0] * n_pad)\n\n        if tokens_a_index + 1 == tokens_b_index and positive &lt; batch_size/2:\n            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n            positive += 1\n        elif tokens_a_index + 1 != tokens_b_index and negative &lt; batch_size/2:\n            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n            negative += 1\n    return batch\n# Proprecessing Finished\n\ndef get_attn_pad_mask(seq_q, seq_k): # \u5bf9&lt;PAD&gt;\u7684\u63a9\u7801\n    batch_size, len_q = seq_q.size()\n    batch_size, len_k = seq_k.size()\n    # eq(zero) is PAD token\n    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n\ndef gelu(x):\n    \"Implementation of the gelu activation function by Hugging Face\"\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\nclass Embedding(nn.Module):\n    def __init__(self):\n        super(Embedding, self).__init__()\n        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n        self.norm = nn.LayerNorm(d_model)\n\n    def forward(self, x, seg):\n        seq_len = x.size(1)\n        pos = torch.arange(seq_len, dtype=torch.long)\n        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -&gt; (batch_size, seq_len)\n        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n        return self.norm(embedding)\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaledDotProductAttention, self).__init__()\n\n    def forward(self, Q, K, V, attn_mask):\n        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n        attn = nn.Softmax(dim=-1)(scores)\n        context = torch.matmul(attn, V)\n        return context, attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super(MultiHeadAttention, self).__init__()\n        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n        self.W_K = nn.Linear(d_model, d_k * n_heads)\n        self.W_V = nn.Linear(d_model, d_v * n_heads)\n    def forward(self, Q, K, V, attn_mask):\n        # q: [batch_size x len_q x d_model] \n        # k: [batch_size x len_k x d_model] \n        # v: [batch_size x len_k x d_model]\n        residual, batch_size = Q, Q.size(0)\n\n        # (B, S, D) -proj-&gt; (B, S, D) -split-&gt; (B, S, H, W) -trans-&gt; (B, H, S, W)\n        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n\n        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n\n        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n        output = nn.Linear(n_heads * d_v, d_model)(context)\n        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n\nclass PoswiseFeedForwardNet(nn.Module):\n    def __init__(self):\n        super(PoswiseFeedForwardNet, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        # (batch_size, len_seq, d_model) -&gt; (batch_size, len_seq, d_ff) -&gt; (batch_size, len_seq, d_model)\n        return self.fc2(gelu(self.fc1(x)))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self):\n        super(EncoderLayer, self).__init__()\n        self.enc_self_attn = MultiHeadAttention()\n        self.pos_ffn = PoswiseFeedForwardNet()\n\n    def forward(self, enc_inputs, enc_self_attn_mask):\n        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n        return enc_outputs, attn\n\nclass BERT(nn.Module):\n    def __init__(self):\n        super(BERT, self).__init__()\n        self.embedding = Embedding()\n        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n        self.fc = nn.Linear(d_model, d_model)\n        self.activ1 = nn.Tanh()\n        self.linear = nn.Linear(d_model, d_model)\n        self.activ2 = gelu\n        self.norm = nn.LayerNorm(d_model)\n        self.classifier = nn.Linear(d_model, 2)\n        # decoder is shared with embedding layer\n        embed_weight = self.embedding.tok_embed.weight\n        n_vocab, n_dim = embed_weight.size()\n        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n        self.decoder.weight = embed_weight\n        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n\n    def forward(self, input_ids, segment_ids, masked_pos):\n        output = self.embedding(input_ids, segment_ids)\n        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n        for layer in self.layers:\n            output, enc_self_attn = layer(output, enc_self_attn_mask)\n        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n        # it will be decided by first token(CLS)\n        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n\n        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n        # \u7528None\u6765\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u5c06 masked_pos \u4ece[batch_size, max_pred] \u2014&gt; [batch_size, max_pred, 1]\n        # get masked position from final output of transformer.\n        h_masked = torch.gather(output, 1, masked_pos) \n        # torch.gather(input, dim=0, index) \u53d6\u503c\n        # masking position [batch_size, max_pred, d_model]\n        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n\n        return logits_lm, logits_clsf\n\nif __name__ == '__main__':\n    # BERT Parameters\n    maxlen = 30 # maximum of length\n    batch_size = 6\n    max_pred = 5  # max tokens of prediction\n    n_layers = 6 # number of Encoder of Encoder Layer\n    n_heads = 12 # number of heads in Multi-Head Attention\n    d_model = 768 # Embedding Size\n    d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n    d_k = d_v = 64  # dimension of K(=Q), V\n    n_segments = 2\n\n    text = (\n        'Hello, how are you? I am Romeo.\\n'\n        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n        'Nice meet you too. How are you today?\\n'\n        'Great. My baseball team won the competition.\\n'\n        'Oh Congratulations, Juliet\\n'\n        'Thanks you Romeo'\n    )\n    sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n    word_list = list(set(\" \".join(sentences).split()))\n    word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n    for i, w in enumerate(word_list):\n        word_dict[w] = i + 4\n    number_dict = {i: w for i, w in enumerate(word_dict)}\n    vocab_size = len(word_dict)\n\n    token_list = list()\n    for sentence in sentences:\n        arr = [word_dict[s] for s in sentence.split()]\n        token_list.append(arr)\n\n    model = BERT()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    batch = make_batch()\n    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n\n    for epoch in range(100):\n        optimizer.zero_grad()\n        logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n        loss_lm = (loss_lm.float()).mean()\n        loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n        loss = loss_lm + loss_clsf\n        if (epoch + 1) % 10 == 0:\n            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n        loss.backward()\n        optimizer.step()\n\n    # Predict mask tokens ans isNext\n    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n    print(text)\n    print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n\n    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n    logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n    print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n    print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n\n    logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n    print('isNext : ', True if isNext else False)\n    print('predict isNext : ',True if logits_clsf else False)\n</code></pre>"},{"location":"sci-paper/cs/Bert/#\u603b\u7ed3","title":"\u603b\u7ed3","text":"<ul> <li> <p>\u6df1\u5ea6\u53cc\u5411\u9884\u8bad\u7ec3\u8868\u793a\uff1a</p> <p>\u4e0e\u4ee5\u5f80\u7684\u8bed\u8a00\u8868\u793a\u6a21\u578b\u4e0d\u540c\uff0cBERT\u65e8\u5728\u901a\u8fc7\u8054\u5408\u8003\u8651\u6240\u6709\u5c42\u4e2d\u7684\u5de6\u4fa7\u548c\u53f3\u4fa7\u4e0a\u4e0b\u6587\u6765\u9884\u8bad\u7ec3\u6df1\u5ea6\u53cc\u5411\u8868\u793a\u3002\u8fd9\u4f7f\u5f97BERT\u80fd\u591f\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u6355\u83b7\u66f4\u4e30\u5bcc\u7684\u8bed\u8a00\u7279\u5f81\u3002</p> </li> <li> <p>\u7b80\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u4fee\u6539\uff1a</p> <p>\u9884\u8bad\u7ec3\u7684BERT\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0\u5c11\u91cf\u989d\u5916\u7684\u8f93\u51fa\u5c42\u6765\u5fae\u8c03\uff08fine-tune\uff09\uff0c\u4ece\u800c\u9002\u5e94\u5e7f\u6cdb\u7684\u4efb\u52a1\uff0c\u5982\u95ee\u7b54\u548c\u8bed\u8a00\u63a8\u65ad\uff0c\u800c\u65e0\u9700\u5bf9\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u5927\u91cf\u7279\u5b9a\u4efb\u52a1\u7684\u4fee\u6539\u3002</p> </li> <li> <p>\u591a\u9879\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u65b0\u6700\u4f73\u7ed3\u679c\uff1a</p> <p>BERT\u5728\u5341\u4e00\u4e2a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5305\u62ec\u5c06GLUE\u57fa\u51c6\u7684\u5206\u6570\u63a8\u9ad8\u523080.5%\uff08\u7edd\u5bf9\u63d0\u9ad8\u4e867.7\u4e2a\u767e\u5206\u70b9\uff09\uff0cMultiNLI\u51c6\u786e\u7387\u63d0\u9ad8\u523086.7%\uff08\u63d0\u9ad8\u4e864.6\u4e2a\u767e\u5206\u70b9\uff09\uff0cSQuAD v1.1\u95ee\u7b54\u6d4b\u8bd5\u7684F1\u5206\u6570\u63d0\u9ad8\u523093.2\uff08\u63d0\u9ad8\u4e861.5\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u4ee5\u53caSQuAD v2.0\u6d4b\u8bd5\u7684F1\u5206\u6570\u63d0\u9ad8\u523083.1\uff08\u63d0\u9ad8\u4e865.1\u4e2a\u767e\u5206\u70b9\uff09\u3002</p> </li> <li> <p>\u9884\u8bad\u7ec3\u4efb\u52a1\u7684\u91cd\u8981\u6027(\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\uff0c <p>\u6027\u80fd\u4f18\u4e8e\u4ece\u5934\u5f00\u59cb\u5b66\u4e60\u7684\u5d4c\u5165)\uff1aBERT\u901a\u8fc7\u4f7f\u7528\u201c\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u201d\uff08Masked Language Model, MLM\uff09\u548c\u201c\u4e0b\u4e00\u53e5\u9884\u6d4b\u201d%\uff08Next Sentence Prediction, NSP\uff09\u4efb\u52a1\u6765\u5c55\u793a\u6df1\u5ea6\u53cc\u5411\u9884\u8bad\u7ec3\u7684\u91cd\u8981\u6027\u3002MLM\u4efb\u52a1\u901a\u8fc7\u968f\u673a\u63a9\u76d6\u8f93\u5165\u4e2d\u7684\u4e00\u4e9b\u6807\u8bb0\uff0c\u7136\u540e\u9884\u6d4b\u8fd9\u4e9b\u63a9\u76d6\u6807\u8bb0\u7684\u539f\u59cb\u8bcd\u6c47ID\uff0c\u4ece\u800c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u878d\u5408\u5de6\u53f3\u4e0a\u4e0b\u6587\u3002NSP\u4efb\u52a1\u5219\u901a\u8fc7\u9884\u6d4b\u4e24\u4e2a\u6587\u672c\u7247\u6bb5\u4e4b\u95f4\u7684\u5173\u7cfb\u6765\u8bad\u7ec3\u6a21\u578b\u7406\u89e3\u53e5\u5b50\u95f4\u7684\u5173\u7cfb\u3002</p>"},{"location":"sci-paper/cs/CLIP/","title":"CLIP\uff1aLearning Transferable Visual Models From Natural Language Supervision","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aLearning Transferable Visual Models From Natural Language Supervision</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/openai/CLIP</p> <p></p>"},{"location":"sci-paper/cs/CLIP/#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p><code>CLIP\uff08Contrastive Language-Image Pre-training\uff09</code>\u662f\u7531OpenAI\u5f00\u53d1\u7684\u4e00\u79cd\u591a\u6a21\u6001\uff08\u6587\u672c\u548c\u56fe\u50cf\uff09\u9884\u8bad\u7ec3\u6a21\u578b\u3002<code>CLIP</code>\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u5982\u4f55\u5bf9\u6587\u672c\u548c\u56fe\u50cf\u8fdb\u884c\u5bf9\u6bd4\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u7406\u89e3\u3002\u8fd9\u79cd\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\u4f7f\u5f97CLIP\u80fd\u591f\u5728\u6ca1\u6709\u4efb\u4f55\u76d1\u7763\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u5230\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u3002</p> <p><code>CLIP</code>\u6a21\u578b\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u6587\u672c\u548c\u56fe\u50cf\u5d4c\u5165\u5230\u4e00\u4e2a\u5171\u540c\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u4f7f\u5f97\u76f8\u5173\u7684\u6587\u672c\u63cf\u8ff0\u548c\u56fe\u50cf\u5185\u5bb9\u5728\u8fd9\u4e2a\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\u5f7c\u6b64\u9760\u8fd1\uff0c\u800c\u4e0d\u76f8\u5173\u7684\u5219\u8fdc\u79bb\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4f7f\u5f97CLIP\u6a21\u578b\u80fd\u591f\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5982\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u50cf\u68c0\u7d22\u3001\u6587\u672c\u5206\u7c7b\u7b49\u3002</p>"},{"location":"sci-paper/cs/CLIP/#\u65b9\u6cd5","title":"\u65b9\u6cd5","text":"<p>CLIP\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u4ece\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u4fe1\u53f7\u4e2d\u5b66\u4e60\u611f\u77e5\u7684\u60f3\u6cd5\uff08learning perception from supervision contained in natural language\uff09\uff0c\u5de5\u4f5c\u5728\u81ea\u7136\u8bed\u8a00\u4e0a\u7684\u65b9\u6cd5\u53ef\u4ee5\u88ab\u52a8\u5730\u4ece\u4e92\u8054\u7f51\u4e0a\u6d77\u91cf\u6587\u672c\u4e2d\u8574\u542b\u7684\u76d1\u7763\u4e2d\u5b66\u4e60\u3002\u4e0e\u5927\u591a\u6570\u65e0\u76d1\u7763\u6216\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u4e2d\u5b66\u4e60\u4e5f\u6709\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u4e0d\u4ec5\"\u53ea\u662f\"\u5b66\u4e60\u4e00\u79cd\u8868\u793a\uff0c\u800c\u4e14\u8fd8\u5c06\u8fd9\u79cd\u8868\u793a\u4e0e\u8bed\u8a00\u8054\u7cfb\u8d77\u6765\uff0c\u4ece\u800c\u5b9e\u73b0\u7075\u6d3b\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff08zero-shot transfer\uff09\u3002</p> <p>\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u7684\u4e00\u4e2a\u4e3b\u8981\u52a8\u673a\u662f\u4e92\u8054\u7f51\u4e0a\u516c\u5f00\u63d0\u4f9b\u7684\u5927\u91cf\u6570\u636e\u3002\u7531\u4e8e\u73b0\u6709\u7684\u6570\u636e\u96c6\u6ca1\u6709\u5145\u5206\u53cd\u6620\u8fd9\u79cd\u53ef\u80fd\u6027\uff0c\u53ea\u8003\u8651\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u7ed3\u679c\u4f1a\u4f4e\u4f30\u8fd9\u4e00\u7814\u7a76\u9886\u57df\u7684\u6f5c\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u4ece\u4e92\u8054\u7f51\u4e0a\u7684\u5404\u79cd\u516c\u5f00\u6765\u6e90\u6536\u96c6\u4e864\u4ebf\uff08\u56fe\u50cf, \u6587\u672c\uff09\u5bf9(WebImageText)\u3002\u4e3a\u4e86\u5c3d\u53ef\u80fd\u5e7f\u6cdb\u5730\u6db5\u76d6\u4e00\u7ec4\u89c6\u89c9\u6982\u5ff5\uff0c\u672c\u6587\u641c\u7d22\uff08\u56fe\u50cf, \u6587\u5b57\uff09\u5bf9\u4f5c\u4e3a\u6784\u9020\u8fc7\u7a0b\u7684\u4e00\u90e8\u5206\uff0c\u5176\u4e2d\u6587\u672c\u5305\u62ec\u4e00\u7ec450\u4e07\u4e2a\u67e5\u8be2\u7ed3\u679c\u4e2d\u7684\u4e00\u4e2a\u3002\u4e3a\u5927\u81f4\u4fdd\u6301\u7c7b\u522b\u5747\u8861\uff0c\u6bcf\u4e2a\u67e5\u8be2\u6700\u591a\u5305\u542b2\u4e07\u4e2a\uff08\u56fe\u50cf, \u6587\u672c\uff09\u5bf9\u3002</p> <p>\u7531\u4e8e\u8bad\u7ec3\u7684\u8ba1\u7b97\u91cf\u8fc7\u4e8e\u5e9e\u5927\uff0c\u4e14\u8bad\u7ec3\u6548\u7387\u662f\u6210\u529f\u6269\u5c55\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u7684\u5173\u952e\uff0c\u56e0\u6b64\u672c\u6587\u91c7\u7528\u4e86\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u63a2\u7d22\u8bad\u7ec3\u4e00\u4e2a\u7cfb\u7edf\u6765\u89e3\u51b3\u53ef\u80fd\u7684\u66f4\u5bb9\u6613\u7684\u4ee3\u7406\u4efb\u52a1\uff0c\u5373\u9884\u6d4b\u56fe\u50cf\u4e0e\u54ea\u6bb5\u6587\u672c\u6700\u5339\u914d\uff0c\u800c\u4e0d\u662f\u9884\u6d4b\u6587\u672c\u7684\u786e\u5207\u5355\u8bcd\u3002</p> <p></p>"},{"location":"sci-paper/cs/CLIP/#\u4f2a\u4ee3\u7801","title":"\u4f2a\u4ee3\u7801","text":"<p>\u7ed9\u5b9a\u4e00\u4e2a<code>batch</code>\u7684\\(N\\)\u4e2a\uff08\u56fe\u50cf, \u6587\u672c\uff09\u5bf9\uff0c<code>CLIP</code>\u88ab\u8bad\u7ec3\u7528\u6765\u9884\u6d4b\u5728\u4e00\u4e2a<code>batch</code>\u4e2d\u5b9e\u9645\u53d1\u751f\u7684\\(N\u00d7N\\)\u4e2a\u53ef\u80fd\u7684\uff08\u56fe\u50cf, \u6587\u672c\uff09\u5bf9\u7684\u76f8\u4f3c\u5ea6\u3002\u4e3a\u6b64\uff0c<code>CLIP</code>\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\u548c\u6587\u672c\u7f16\u7801\u5668\u6765\u5b66\u4e60\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\uff0c\u4ee5\u6700\u5927\u5316<code>batch</code>\u5185\\(N\\)\u4e2a\u771f\u5b9e\u5bf9\u7684\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u540c\u65f6\u6700\u5c0f\u5316\\(N^2\u2212N\\)\u4e2a\u9519\u8bef\u5bf9\u7684\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002\u5728\u8fd9\u4e9b\u76f8\u4f3c\u5ea6\u5f97\u5206\u4e0a\u4f18\u5316\u4e00\u4e2a\u5bf9\u79f0\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u3002</p> <p>\u672c\u6587\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3<code>CLIP</code>\uff0c\u4e0d\u9700\u8981\u521d\u59cb\u5316\u5e26\u6709ImageNet\u6743\u91cd\u7684\u56fe\u50cf\u7f16\u7801\u5668\u6216\u5e26\u6709\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u6587\u672c\u7f16\u7801\u5668\u3002\u5728\u56fe\u50cf\u548c\u6587\u672c\u7f16\u7801\u5668\u540e\u4f7f\u7528\u4e00\u4e2a\u7ebf\u6027\u6295\u5f71\uff0c\u5c06\u6bcf\u4e2a\u7f16\u7801\u5668\u7684\u8868\u793a\u6620\u5c04\u5230\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u3002</p> <p>\u7531\u4e8e\u6211\u4eec\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u89c4\u6a21\u5e9e\u5927\uff0c\u8fc7\u62df\u5408\u5e76\u4e0d\u662f\u4e3b\u8981\u7684\u62c5\u5fe7\u3002\u6211\u4eec\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3<code>CLIP</code>\uff0c\u65e2\u4e0d\u4f7f\u7528ImageNet\u6743\u91cd\u521d\u59cb\u5316\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u4e5f\u4e0d\u4f7f\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u521d\u59cb\u5316\u6587\u672c\u7f16\u7801\u5668\u3002\u6211\u4eec\u4ec5\u4f7f\u7528\u7ebf\u6027\u6295\u5f71\u5c06\u6bcf\u4e2a\u7f16\u7801\u5668\u7684\u8868\u793a\u6620\u5c04\u5230\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u3002</p>"},{"location":"sci-paper/cs/CLIP/#\u9009\u62e9\u548c\u6269\u5c55\u6a21\u578b","title":"\u9009\u62e9\u548c\u6269\u5c55\u6a21\u578b","text":"<p>\u67b6\u6784\u4e00\uff1a</p> <p>\u4f7f\u7528 <code>ResNet50</code> \u4f5c\u4e3a\u56fe\u50cf\u7f16\u7801\u5668\u7684\u57fa\u7840\u67b6\u6784\uff0c\u56e0\u4e3a\u5176\u5e7f\u6cdb\u4f7f\u7528\u548c\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6027\u80fd</p> <ul> <li> <p>\u6211\u4eec\u4f7f\u7528<code>ResNet</code>\u548c<code>blur pooling</code>\uff0c\u5bf9\u539f\u59cb\u7248\u672c\u8fdb\u884c\u4e86\u4e00\u4e9b\u4fee\u6539\u3002</p> </li> <li> <p>\u6211\u4eec\u8fd8\u7528\u6ce8\u610f\u529b\u6c60\u5316\uff08<code>attention pooling</code>\uff09\u673a\u5236\u66ff\u6362\u4e86\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff08<code>global average pooling layer</code>\uff09\u3002</p> </li> <li> <p><code>Attention pooling</code> \u88ab\u5b9e\u73b0\u4e3a\u5355\u5c42\u7684 \u201c<code>transformer</code> \u5f0f\u201d\u591a\u5934 <code>QKV attention</code>\uff0c\u5176\u4e2d <code>query</code> \u4ee5\u56fe\u50cf\u7684 <code>global average-pooled representation</code> \u4e3a\u6761\u4ef6\u3002</p> </li> </ul> <p>\u67b6\u6784\u4e8c\uff1a</p> <p>\u4f7f\u7528\u4e86 <code>Vision Transformer (ViT)</code>\uff0c\u53ea\u5bf9 <code>transformer</code> \u4e4b\u524d\u7684\u7ec4\u5408 <code>patch</code> \u548c <code>position embeddings</code> \u6dfb\u52a0\u4e86\u989d\u5916\u7684 <code>layer normalization</code>\uff0c\u5e76\u4f7f\u7528\u4e86\u7a0d\u5fae\u4e0d\u540c\u7684\u521d\u59cb\u5316\u65b9\u6848\u3002</p> <ul> <li> <p>\u4f5c\u4e3a\u57fa\u7840\uff0c\u6211\u4eec\u4f7f\u7528\u5177\u6709 8 \u4e2a\u6ce8\u610f\u529b\u5934\u7684 12 \u5c42 512 \u5bbd\u7684\u6a21\u578b\u3002<code>Transformer</code> \u5bf9\u6587\u672c\u5c0f\u5199\u7684\u5b57\u8282\u5bf9\u7f16\u7801 (<code>byte pair encoding\uff0cBPE</code>) <code>representation</code> \u8fdb\u884c\u64cd\u4f5c\u3002</p> </li> <li> <p>\u6587\u672c\u5e8f\u5217\u7528 <code>[SOS]</code> \u548c <code>[EOS]</code> \u6807\u8bb0\u62ec\u8d77\u6765\uff0c<code>transformer</code> \u6700\u9ad8\u5c42\u5728 <code>[EOS]</code> \u6807\u8bb0\u5904\u7684\u6fc0\u6d3b\u88ab\u7528\u4f5c\u6587\u672c\u7684 <code>feature representation</code>\uff0c\u8be5\u6587\u672c\u88ab\u5c42\u5f52\u4e00\u5316\uff0c\u7136\u540e\u7ebf\u6027\u6295\u5f71\u5230\u591a\u6a21\u6001 <code>embedding space</code>\u3002</p> </li> <li> <p><code>Masked self-attention</code> \u5728\u6587\u672c\u7f16\u7801\u5668\u4e2d\u4f7f\u7528\uff0c\u4ee5\u4fdd\u7559\u6dfb\u52a0\u8bed\u8a00\u5efa\u6a21\u4f5c\u4e3a\u8f85\u52a9\u76ee\u6807\u7684\u80fd\u529b\uff0c\u5c3d\u7ba1\u5bf9\u6b64\u7684\u63a2\u7d22\u7559\u7ed9\u672a\u6765\u7684\u5de5\u4f5c\u3002</p> </li> </ul>"},{"location":"sci-paper/cs/CLIP/#prompt-engineering-and-ensembling","title":"PROMPT ENGINEERING AND ENSEMBLING","text":"<p>\u7edd\u5927\u591a\u6570\u6570\u636e\u96c6\u4ec5\u4f7f\u7528\u6807\u7b7e\u7684\u6570\u5b57id\u5bf9\u56fe\u50cf\u8fdb\u884c\u6ce8\u91ca\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u5c06\u8fd9\u4e9b id \u6620\u5c04\u56de\u5176\u82f1\u6587\u540d\u79f0\u7684\u6587\u4ef6\u3002\u56e0\u6b64\u4f1a\u4ea7\u751f\u4e00\u4e2a\u5e38\u89c1\u7684\u95ee\u9898\uff1a\u8bcd\u8bed\u7684\u591a\u4e49\u6027\u3002\u5f53\u7c7b\u7684\u540d\u79f0\u662f\u63d0\u4f9b\u7ed9<code>CLIP</code>\u6587\u672c\u7f16\u7801\u5668\u7684\u552f\u4e00\u4fe1\u606f\u65f6\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u533a\u5206\u54ea\u4e2a\u8bcd\u4e49\u7684\u542b\u4e49\u3002\u8b6c\u5982construction cranes\u4e0ecranes\u3002</p> <p>\u53e6\u4e00\u4e2a\u95ee\u9898\u662f\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\uff0c\u90e8\u5206\u63d0\u4f9b\u7684\u4e0e\u56fe\u50cf\u914d\u5bf9\u7684\u6587\u672c\u5f88\u5c11\uff08\u4ec5\u4e00\u4e2a\u5355\u8bcd\uff09\uff0c\u800c\u901a\u5e38\u6587\u672c\u662f\u4ee5\u67d0\u79cd\u65b9\u5f0f\u63cf\u8ff0\u56fe\u50cf\u7684\u5b8c\u6574\u53e5\u5b50\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u5f25\u8865\u6587\u672c\u957f\u5ea6\u7684\u5dee\u8ddd\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u63d0\u793a\u6a21\u677f\uff08prompt template\uff09\uff1a <code>\"A photo of a {label}.\"</code>\uff0c\u6709\u52a9\u4e8e\u6307\u5b9a\u6587\u672c\u662f\u5173\u4e8e\u56fe\u50cf\u5185\u5bb9\u7684\u3002\u8fd9\u901a\u5e38\u4f1a\u6bd4\u4ec5\u4f7f\u7528\u6807\u7b7e\u6587\u672c\u7684\u57fa\u7ebf\u63d0\u9ad8\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\u9488\u5bf9\u4e0d\u540c\u7684\u4efb\u52a1\u5b9a\u5236\u63d0\u793a\u8bcd\u6a21\u677f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8zero-shot\u7684\u6027\u80fd\uff0c\u5982\u5bf9\u4e8e<code>Oxford-IIIT Pets</code>\u6570\u636e\u96c6\uff0c\u4f7f\u7528<code>\"A photo of a {label}, a type of pet.\"</code>\uff0c\u5bf9\u4e8esatellite image classification datasets\uff0c\u6211\u4eec\u91c7\u7528<code>\"a satellite photo of a {label}.\"</code>\u3002</p> <p></p> <p>\u540c\u6837\u7684\uff0c\u6211\u4eec\u521b\u9020\u4e8680\u4e2a\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u63d0\u793a\u6a21\u677f<code>\"A photo of a big {label}\"</code> <code>\"A photo of a small {label}\"</code> <code>\"A photo hard to see {label}\"</code>\u7b49\u7b49\u3002\u5728 ImageNet \u4e0a\uff0c\u8fd9\u6bd4\u4e0a\u9762\u8ba8\u8bba\u7684\u5355\u4e2a\u9ed8\u8ba4\u63d0\u793a\u63d0\u9ad8\u4e863.5%\u7684\u6027\u80fd\u3002</p> <p></p> <p>\u67e5\u770b<code>zero-shot CLIP</code>\u660e\u663e\u8868\u73b0\u4e0d\u4f73\u7684\u5730\u65b9\uff0c\u6211\u4eec\u53d1\u73b0 <code>zero-shot CLIP</code> \u5728\u536b\u661f\u56fe\u50cf\u5206\u7c7b\uff08<code>EuroSAT</code> \u548c <code>RESISC45</code>\uff09\u3001\u6dcb\u5df4\u7ed3\u80bf\u7624\u68c0\u6d4b\uff08<code>PatchCamelyon</code>\uff09\u3001\u5728\u5408\u6210\u573a\u666f\uff08<code>CLEVRCounts</code>\uff09\u4e2d\u8ba1\u6570\u3001\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\u4efb\u52a1 [\u5982\u5fb7\u56fd\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\uff08<code>GTSRB</code>\uff09\u3001\u8bc6\u522b\u5230\u6700\u8fd1\u6c7d\u8f66\u7684\u8ddd\u79bb\uff08<code>KITTI Distance</code>\uff09]\u7b49\u51e0\u4e2a\u4e13\u95e8\u7684\u3001\u590d\u6742\u7684\u6216\u62bd\u8c61\u7684\u4efb\u52a1\u4e0a\u76f8\u5f53\u8584\u5f31\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86 <code>zero-shot CLIP</code> \u5728\u66f4\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8f83\u5dee\u80fd\u529b\u3002</p> <p></p> <p>\u53cd\u76f4\u89c9\u7684\u662f\uff0c\u5f53<code>CLIP</code>\u4ece <code>zero-shot</code> \u8fc7\u6e21\u5230 <code>few-shot</code> \u65f6\uff0c\u53cd\u800c\u4f1a\u5bfc\u81f4\u6027\u80fd\u51fa\u73b0\u4e0b\u964d</p> <p></p> <p>\u4e0a\u56fe\u603b\u7ed3\u4e86\u6211\u4eec\u7684\u53d1\u73b0\u3002\u4f7f\u7528 CLIP \u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u4ee5\u5f88\u597d\u5730\u6269\u5c55\u8ba1\u7b97\uff0c\u6211\u4eec\u6700\u5927\u7684\u6a21\u578b\u5728\u603b\u4f53\u5f97\u5206\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7565\u4f18\u4e8e\u6700\u4f73\u6a21\u578b\uff08<code>Noisy Student EfficientNet-L2</code>\uff09\u3002 \u6211\u4eec\u8fd8\u53d1\u73b0 <code>CLIP-ViT</code> \u7684\u8ba1\u7b97\u6548\u7387\u6bd4 <code>CLIP- ResNets</code>\u9ad8\u51fa\u7ea6 3 \u500d\uff0c\u8fd9\u5728\u6211\u4eec\u7684\u8ba1\u7b97\u9884\u7b97\u5185\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6574\u4f53\u6027\u80fd\uff0c\u8fd9\u4e2a\u7ed3\u679c\u91cd\u590d\u4e86 Dosovitskiy \u7b49\u4eba\u7684\u53d1\u73b0\uff1a\u5728\u8db3\u591f\u5927\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\uff0c<code>ViT</code>\u6bd4<code>CNN</code>\u7684\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002</p> <p></p> <p><code>zero-shot CLIP</code> \u6a21\u578b\u90fd\u5927\u5927\u63d0\u9ad8\u4e86\u6709\u6548\u9c81\u68d2\u6027\uff0c\u5e76\u5c06 <code>ImageNet</code> \u7cbe\u5ea6\u4e0e\u5206\u5e03\u8fc1\u79fb\u4e0b\u7684\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\u964d\u4f4e\u4e86\u9ad8\u8fbe 75%\u3002\u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c<code>zero-shot</code>\u6a21\u578b\u4e0d\u80fd\u5229\u7528\u4ec5\u9002\u7528\u4e8e\u6d4b\u8bd5\u96c6\u7279\u5b9a\u5206\u5e03\u7684\u865a\u5047\u76f8\u5173\u6027\u6216\u6a21\u5f0f\uff0c\u56e0\u4e3a\u5b83\u6ca1\u6709\u9488\u5bf9\u8be5\u5206\u5e03\u8fdb\u884c\u8bad\u7ec3\uff0c\u56e0\u6b64<code>zero-shot</code>\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6709\u6548\u9c81\u68d2\u6027\u3002</p>"},{"location":"sci-paper/cs/CLIP/#\u9650\u5236","title":"\u9650\u5236","text":"<ul> <li> <p>\u5728\u6709\u8bad\u7ec3\u5206\u5272\u7684\u6570\u636e\u96c6\u4e0a\uff0c<code>zero-shot CLIP</code>\u7684\u8868\u73b0\u5e73\u5747\u800c\u8a00\u4e0e\u5728<code>ResNet-50</code>\u7279\u5f81\u4e0a\u7684\u7b80\u5355\u76d1\u7763\u57fa\u7ebf\uff08\u7ebf\u6027\u5206\u7c7b\u5668\uff09\u76f8\u5f53\u3002\u5728\u5927\u591a\u6570\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\uff0c\u8fd9\u4e00\u57fa\u7ebf\u7684\u8868\u73b0\u73b0\u5728\u5df2\u8fdc\u4f4e\u4e8e\u6574\u4f53\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u63d0\u9ad8<code>CLIP</code>\u5728\u8ba1\u7b97\u548c\u6570\u636e\u6548\u7387\u4e0a\u7684\u8868\u73b0\u3002</p> </li> <li> <p><code>CLIP</code>\u7684<code>zero-shot</code>\u8868\u73b0\u5728\u51e0\u79cd\u7279\u5b9a\u4efb\u52a1\u4e0a\u8fd8\u662f\u6bd4\u7279\u5b9a\u4efb\u52a1\u7684\u6a21\u578b\u7684\u6548\u679c\u5dee\uff0c\u4f8b\u5982\u533a\u5206\u6c7d\u8f66\u578b\u53f7\u3001\u82b1\u7684\u54c1\u79cd\u548c\u98de\u673a\u7684\u53d8\u578b\u3002<code>CLIP</code>\u8fd8\u5728\u66f4\u62bd\u8c61\u548c\u7cfb\u7edf\u6027\u7684\u4efb\u52a1\u4e0a\u9047\u5230\u56f0\u96be\uff0c\u5982\u8ba1\u7b97\u56fe\u50cf\u4e2d\u7269\u4f53\u7684\u6570\u91cf\u3002\u6700\u540e\uff0c\u5bf9\u4e8e\u90a3\u4e9b\u4e0d\u592a\u53ef\u80fd\u5305\u542b\u5728CLIP\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u65b0\u9896\u4efb\u52a1\uff0c\u4f8b\u5982\u5206\u7c7b\u7167\u7247\u4e2d\u6700\u8fd1\u7684\u6c7d\u8f66\u8ddd\u79bb\uff0c<code>CLIP</code>\u7684\u8868\u73b0\u53ef\u80fd\u63a5\u8fd1\u968f\u673a\u3002</p> </li> <li> <p><code>zero-shot CLIP</code>\u5bf9\u4e8e\u8d85\u51fa\u5206\u5e03\u8303\u56f4\u7684\u6570\u636e\u6cdb\u5316\u6027\u4ecd\u7136\u5f88\u5dee\uff1a<code>zero-shot CLIP</code>\u5728\u624b\u5199\u6570\u5b57<code>MNIST</code>\u6570\u636e\u96c6\u4e0a\u4ec5\u4ec5\u83b7\u5f9788%\u7684\u51c6\u786e\u7387\u3002\u5bf9\u539f\u59cb\u50cf\u7d20\u8fdb\u884cLogistic\u56de\u5f52\uff0c\u8868\u73b0\u6bd4<code>zero-shot CLIP</code>\u66f4\u597d\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u5e9e\u5927\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u51e0\u4e4e\u6ca1\u6709\u7c7b\u4f3c<code>MNIST</code>\u6570\u5b57\u7684\u56fe\u50cf\uff0c\u8fd9\u8868\u660e<code>CLIP</code>\u5bf9\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8106\u5f31\u6cdb\u5316\u95ee\u9898\u51e0\u4e4e\u6ca1\u6709\u5e2e\u52a9\u3002\u76f8\u53cd\uff0cCLIP\u8bd5\u56fe\u89c4\u907f\u95ee\u9898\uff0c\u5e76\u5e0c\u671b\u901a\u8fc7\u5728\u5982\u6b64\u5e9e\u5927\u548c\u591a\u6837\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6240\u6709\u6570\u636e\u5c06\u6709\u6548\u5730\u5904\u4e8e\u5206\u5e03\u5185\u3002\u6b63\u5982MNIST\u6240\u8bc1\u660e\u7684\uff0c\u8fd9\u662f\u4e00\u4e2a\u5929\u771f\u7684\u5047\u8bbe\uff0c\u5f88\u5bb9\u6613\u88ab\u8fdd\u53cd\u3002</p> </li> </ul>"},{"location":"sci-paper/cs/CLIP/#\u7ed3\u8bba","title":"\u7ed3\u8bba","text":"<p><code>CLIP</code>\u6a21\u578b\u5b66\u4e60\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u6267\u884c\u5404\u79cd\u5404\u6837\u7684\u4efb\u52a1\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u6765\u5229\u7528\u8fd9\u4e9b\u4efb\u52a1\u5b66\u4e60\uff0c\u4ece\u800c\u5b9e\u73b0\u5411\u8bb8\u591a\u73b0\u6709\u6570\u636e\u96c6\u7684 <code>zero-shot</code> \u8fc1\u79fb\u3002\u5728\u8db3\u591f\u5927\u7684\u89c4\u6a21\u4e0b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\u53ef\u4e0e\u76d1\u7763\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u4f46\u4ecd\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002</p> CLIP <pre><code>from collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride &gt; 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride &gt; 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride &gt; 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.relu1(self.bn1(self.conv1(x)))\n        out = self.relu2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -&gt; (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x[:1], key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n        return x.squeeze(0)\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride &gt; 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.input_resolution = input_resolution\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        def stem(x):\n            x = self.relu1(self.bn1(self.conv1(x)))\n            x = self.relu2(self.bn2(self.conv2(x)))\n            x = self.relu3(self.bn3(self.conv3(x)))\n            x = self.avgpool(x)\n            return x\n\n        x = x.type(self.conv1.weight.dtype)\n        x = stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width ** -0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n        self.ln_pre = LayerNorm(width)\n\n        self.transformer = Transformer(width, layers, heads)\n\n        self.ln_post = LayerNorm(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def forward(self, x: torch.Tensor):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -&gt; LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -&gt; NLD\n\n        x = self.ln_post(x[:, 0, :])\n\n        if self.proj is not None:\n            x = x @ self.proj\n\n        return x\n\n\nclass CLIP(nn.Module):\n    def __init__(self,\n                 embed_dim: int,\n                 # vision\n                 image_resolution: int,\n                 vision_layers: Union[Tuple[int, int, int, int], int],\n                 vision_width: int,\n                 vision_patch_size: int,\n                 # text\n                 context_length: int,\n                 vocab_size: int,\n                 transformer_width: int,\n                 transformer_heads: int,\n                 transformer_layers: int\n                 ):\n        super().__init__()\n\n        self.context_length = context_length\n\n        if isinstance(vision_layers, (tuple, list)):\n            vision_heads = vision_width * 32 // 64\n            self.visual = ModifiedResNet(\n                layers=vision_layers,\n                output_dim=embed_dim,\n                heads=vision_heads,\n                input_resolution=image_resolution,\n                width=vision_width\n            )\n        else:\n            vision_heads = vision_width // 64\n            self.visual = VisionTransformer(\n                input_resolution=image_resolution,\n                patch_size=vision_patch_size,\n                width=vision_width,\n                layers=vision_layers,\n                heads=vision_heads,\n                output_dim=embed_dim\n            )\n\n        self.transformer = Transformer(\n            width=transformer_width,\n            layers=transformer_layers,\n            heads=transformer_heads,\n            attn_mask=self.build_attention_mask()\n        )\n\n        self.vocab_size = vocab_size\n        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n        self.ln_final = LayerNorm(transformer_width)\n\n        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n        self.initialize_parameters()\n\n    def initialize_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        if isinstance(self.visual, ModifiedResNet):\n            if self.visual.attnpool is not None:\n                std = self.visual.attnpool.c_proj.in_features ** -0.5\n                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n\n            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n                for name, param in resnet_block.named_parameters():\n                    if name.endswith(\"bn3.weight\"):\n                        nn.init.zeros_(param)\n\n        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width ** -0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    @property\n    def dtype(self):\n        return self.visual.conv1.weight.dtype\n\n    def encode_image(self, image):\n        return self.visual(image.type(self.dtype))\n\n    def encode_text(self, text):\n        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.type(self.dtype)\n        x = x.permute(1, 0, 2)  # NLD -&gt; LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -&gt; NLD\n        x = self.ln_final(x).type(self.dtype)\n\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n\n        return x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n\n        # normalized features\n        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n\n        # cosine similarity as logits\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * image_features @ text_features.t()\n        logits_per_text = logits_per_image.t()\n\n        # shape = [global_batch_size, global_batch_size]\n        return logits_per_image, logits_per_text\n\n\ndef convert_weights(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n\n    def _convert_weights_to_fp16(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n\n        if isinstance(l, nn.MultiheadAttention):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr)\n                if tensor is not None:\n                    tensor.data = tensor.data.half()\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name):\n                attr = getattr(l, name)\n                if attr is not None:\n                    attr.data = attr.data.half()\n\n    model.apply(_convert_weights_to_fp16)\n\n\ndef build_model(state_dict: dict):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_resolution = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_resolution = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n\n    model = CLIP(\n        embed_dim,\n        image_resolution, vision_layers, vision_width, vision_patch_size,\n        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        if key in state_dict:\n            del state_dict[key]\n\n    convert_weights(model)\n    model.load_state_dict(state_dict)\n    return model.eval()\n</code></pre>"},{"location":"sci-paper/cs/DeTi/","title":"DeTi:Training data-efficient image transformers &amp; distillation through attention","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aTraining data-efficient image transformers &amp; distillation through attention</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/facebookresearch/deit</p> <p></p>"},{"location":"sci-paper/cs/DeTi/#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p>\u5728\u5bf9 ViT \u7684\u4ecb\u7ecd\u4e2d\uff0c\u6211\u4eec\u4e86\u89e3\u5230\uff0cViT \u7b97\u6cd5\u8981\u60f3\u53d6\u5f97\u4e00\u4e2a\u8f83\u597d\u7684\u6307\u6807\uff0c\u9700\u8981\u5148\u4f7f\u7528 JFT-300 \u6216\u8005 ImageNet-21K \u8fd9\u6837\u7684\u8d85\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u518d\u8fc1\u79fb\u5230\u5176\u4ed6\u4e2d\u7b49\u6216\u8f83\u5c0f\u89c4\u6a21\u7684\u6570\u636e\u96c6\u4e0a\u3002\u800c\u5f53\u4e0d\u4f7f\u7528\u50cf JFT-300 \u8fd9\u6837\u7684\u5de8\u5927\u7684\u6570\u636e\u96c6\u65f6\uff0c\u6548\u679c\u662f\u4e0d\u5982 CNN \u6a21\u578b\u7684\uff0c\u4e5f\u5c31\u53cd\u6620\u51fa Transformer \u7ed3\u6784\u5728 CV \u9886\u57df\u7684\u4e00\u4e2a\u5c40\u9650\u6027\u3002\u5bf9\u4e8e\u5927\u591a\u6570\u7684\u7814\u7a76\u8005\u800c\u8a00\uff0c\u4f7f\u7528\u5982\u6b64\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u610f\u5473\u7740\u9700\u8981\u5f88\u6602\u8d35\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e00\u65e6\u65e0\u6cd5\u83b7\u53d6\u5230\u8fd9\u4e9b\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e0d\u80fd\u4f7f\u7528\u8fd9\u4e48\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5c31\u65e0\u6cd5\u590d\u73b0\u51fa\u7b97\u6cd5\u5e94\u6709\u7684\u6548\u679c\u3002\u6240\u4ee5\uff0c\u51fa\u4e8e\u8fd9\u4e2a\u52a8\u673a\uff0c\u7814\u7a76\u8005\u9488\u5bf9 ViT \u7b97\u6cd5\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u63d0\u51fa\u4e86DeiT\u3002</p> <p>\u5728 DeiT \u4e2d\uff0c\u4f5c\u8005\u5728 ViT \u7684\u57fa\u7840\u4e0a\u6539\u8fdb\u4e86\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u4e86\u84b8\u998f\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u53ea\u9700\u8981\u5728 ImageNet \u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u6709\u7ade\u4e89\u529b\u7684 Transformer \u6a21\u578b\uff0c\u800c\u4e14\u5728\u5355\u53f0\u8ba1\u7b97\u673a\u4e0a\uff0c\u8bad\u7ec3\u65f6\u95f4\u4e0d\u52303\u5929\u5373\u53ef\u3002</p>"},{"location":"sci-paper/cs/Distilling-the-knowledge/","title":"Distilling the Knowledge in a Neural Network","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aDistilling the Knowledge in a Neural Network</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/HobbitLong/RepDistiller</p> <p></p>"},{"location":"sci-paper/cs/Distilling-the-knowledge/#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p><code>Knowledge Distillaion</code>\u662f\u4e00\u79cd\u4f7f\u7528\u7ecf\u8fc7\u8bad\u7ec3\u7684\u5927\u578b\u7f51\u7edc\u4e2d\u7684\u77e5\u8bc6\u6765\u8bad\u7ec3\u5c0f\u578b\u7f51\u7edc\u7684\u65b9\u6cd5\uff1b\u5373\u4ece\u5927\u578b\u7f51\u7edc\u4e2d\u63d0\u70bc\u77e5\u8bc6\u3002</p> <p>\u76f4\u63a5\u5728\u6570\u636e\u548c\u6807\u7b7e\u4e0a\u8bad\u7ec3\u65f6\uff0c\u5177\u6709\u6b63\u5219\u5316\u6216\u6a21\u578b\u96c6\u5408\uff08\u4f7f\u7528 dropout\uff09\u7684\u5927\u578b\u6a21\u578b\u6bd4\u5c0f\u578b\u6a21\u578b\u7684\u6982\u5316\u6548\u679c\u66f4\u597d\u3002\u4f46\u662f\uff0c\u5728\u5927\u578b\u6a21\u578b\u7684\u5e2e\u52a9\u4e0b\uff0c\u53ef\u4ee5\u8bad\u7ec3\u5c0f\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u8fdb\u884c\u6982\u62ec\u3002\u8f83\u5c0f\u7684\u6a21\u578b\u5728\u751f\u4ea7\u4e2d\u66f4\u597d\uff1a\u901f\u5ea6\u66f4\u5feb\u3001\u8ba1\u7b97\u66f4\u5c11\u3001\u5185\u5b58\u66f4\u5c11\u3002</p> <p>\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u8f93\u51fa\u6982\u7387\u6bd4\u6807\u7b7e\u63d0\u4f9b\u7684\u4fe1\u606f\u66f4\u591a\uff0c\u56e0\u4e3a\u5b83\u4e5f\u4f1a\u4e3a\u9519\u8bef\u7684\u7c7b\u5206\u914d\u975e\u96f6\u6982\u7387\u3002\u8fd9\u4e9b\u6982\u7387\u544a\u8bc9\u6211\u4eec\uff0c\u6837\u672c\u6709\u53ef\u80fd\u5c5e\u4e8e\u67d0\u4e9b\u7c7b\u522b\u3002\u4f8b\u5982\uff0c\u5728\u5bf9\u6570\u5b57\u8fdb\u884c\u5206\u7c7b\u65f6\uff0c\u5f53\u7ed9\u5b9a\u6570\u5b57 7 \u7684\u56fe\u50cf\u65f6\uff0c\u5e7f\u4e49\u6a21\u578b\u4f1a\u7ed9\u51fa7\u7684\u9ad8\u6982\u7387\uff0c\u7ed92\u7684\u6982\u7387\u5f88\u5c0f\u4f46\u4e0d\u662f\u96f6\uff0c\u800c\u7ed9\u5176\u4ed6\u6570\u5b57\u5206\u914d\u51e0\u4e4e\u4e3a\u96f6\u7684\u6982\u7387\u3002\u84b8\u998f\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6765\u66f4\u597d\u5730\u8bad\u7ec3\u5c0f\u578b\u6a21\u578b\u3002</p>"},{"location":"sci-paper/cs/Distilling-the-knowledge/#\u4e3b\u8981\u5185\u5bb9","title":"\u4e3b\u8981\u5185\u5bb9","text":"<p>\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u4f7f\u7528<code>softmax</code>\u6765\u751f\u6210\u7c7b\u6982\u7387\uff0c\u6211\u4eec\u5f15\u5165\u84b8\u998f\u6e29\u5ea6\\(T\\)\u4f7f\u5f97\u5728\u5206\u7c7b\u4e0a\u4ea7\u751f\u66f4\u67d4\u548c\uff08softer\uff09\u7684\u6982\u7387\u5206\u5e03:</p> \\[ q_i = \\frac{exp(z_i/T)}{\\sum_j exp(z_j/T)} \\] <ul> <li> <p>\\(T=1\\)\uff0c\u8868\u793a\u7f51\u7edc\u8f93\u51fa<code>Softmax</code>\u7684\u7c7b\u6982\u7387\uff0c\u5f53\\(T&lt;1\\)\u65f6\uff0c\u6982\u7387\u5206\u5e03\u6bd4\u539f\u59cb\u66f4 \u201c\u9661\u5ced\u201d\uff0c \u5f53\\(T\u21920\\) \u65f6, <code>Softmax</code>\u7684\u8f93\u51fa\u503c\u4f1a\u63a5\u8fd1\u4e8e<code>hard-target</code>\uff0c\\(T&gt;1\\)\u65f6, \u6982\u7387\u5206\u5e03\u6bd4\u539f\u59cb\u66f4\u201c\u5e73\u7f13\"</p> </li> <li> <p>\\(T=+\\infty\\)\uff0c\u6b64\u65f6\u8868\u793a\u7f51\u7edc\u8f93\u51fa\u7684\u903b\u8f91\u5355\u5143\uff0c\u6b64\u65f6<code>softmax</code>\u7684\u503c\u662f\u5e73\u5747\u5206\u5e03\u7684</p> </li> </ul> <p></p> <p>\u5982\u56fe\u53ef\u77e5\uff0c\u4f7f\u7528\u5927\u4e8e1\u7684\u84b8\u998f\u6e29\u5ea6\\(T\\)\uff0c<code>softmax</code>\u7684\u8f93\u51fa\u5206\u5e03\u8d8a\u6765\u8d8a\u5e73\u6ed1\uff0c\u4fe1\u606f\u71b5\u4e5f\u4f1a\u8d8a\u6765\u8d8a\u5927\uff0c\u90a3\u4e48\u5728<code>student</code>\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u4e8e\u8d1f\u6807\u7b7e\u7684\u5173\u6ce8\u4e5f\u4f1a\u589e\u52a0\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u6982\u7387\u503c\u663e\u8457\u9ad8\u4e8e\u5e73\u5747\u6982\u7387\u503c\u7684\u8d1f\u6807\u7b7e\uff0c\u5373\uff1a</p> <ul> <li> <p>\u5f53<code>student</code>\u6a21\u578b\u8f83\u5c0f\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u628a\u6e29\u5ea6\u8c03\u4f4e\uff0c\u8fd9\u6837\u8d1f\u6807\u7b7e\u7684\u5e72\u6270\u5c31\u4f1a\u51cf\u5c11</p> </li> <li> <p>\u5f53\u60f3\u4ece\u8d1f\u6807\u7b7e\u4e2d\u5b66\u5230\u4e00\u4e9b\u4fe1\u606f\u91cf\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u628a\u6e29\u5ea6\\(T\\)\u8c03\u9ad8</p> </li> </ul> <p>\u200b\u5982\u5728<code>MNIST</code>\u6570\u636e\u96c6\u4e2d\u505a\u624b\u5199\u4f53\u6570\u5b57\u8bc6\u522b\u4efb\u52a1\uff0c\u5047\u8bbe\u67d0\u4e2a\u8f93\u5165\u7684\u201c2\u201d\u66f4\u52a0\u5f62\u4f3c\"3\"\uff0c<code>softmax</code>\u7684\u8f93\u51fa\u503c\u4e2d\"3\"\u5bf9\u5e94\u7684\u6982\u7387\u4f1a\u6bd4\u5176\u4ed6\u8d1f\u6807\u7b7e\u7c7b\u522b\u9ad8\uff1b\u800c\u53e6\u4e00\u4e2a\"2\"\u66f4\u52a0\u5f62\u4f3c\"7\"\uff0c\u5219\u8fd9\u4e2a\u6837\u672c\u5206\u914d\u7ed9\"7\"\u5bf9\u5e94\u7684\u6982\u7387\u4f1a\u6bd4\u5176\u4ed6\u8d1f\u6807\u7b7e\u7c7b\u522b\u9ad8\u3002\u8fd9\u4e24\u4e2a\"2\"\u5bf9\u5e94\u7684<code>Hard-target</code>\u7684\u503c\u662f\u76f8\u540c\u7684\uff0c\u4f46\u662f\u5b83\u4eec\u7684<code>Soft-target</code>\u5374\u662f\u4e0d\u540c\u7684\uff0c\u7531\u6b64\u6211\u4eec\u53ef\u89c1<code>Soft-target</code>\u8574\u542b\u7740\u6bd4<code>Hard-target</code>\u66f4\u591a\u7684\u4fe1\u606f\u3002</p> <p></p> <p><code>teacher</code>\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u5927\u7684\u590d\u6742\u6a21\u578b\uff0c\u6548\u679c\u597d\uff0c<code>student</code>\u6a21\u578b\u662f\u4e00\u4e2a\u8f7b\u91cf\u578b\u7684\u6a21\u578b\uff0c\u6211\u4eec\u7684\u76ee\u7684\u662f\u5c06<code>student</code>\u6a21\u578b\u7ecf\u8fc7\u8bad\u7ec3\u540e\u8fbe\u5230<code>teacher</code>\u6a21\u578b\u7684\u6548\u679c\uff0c\u6216\u8005\u6bd4<code>teacher</code>\u6a21\u578b\u66f4\u597d\u3002\u5bf9\u4e8e\u8bad\u7ec3<code>student</code>\u6a21\u578b\u4e2d\u635f\u5931\u51fd\u6570\u4e3b\u8981\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff0c\u4e00\u90e8\u5206\u4f7f<code>teacher</code>\u6a21\u578b\u7ecf\u8fc7\u84b8\u998f\u6e29\u5ea6\\(T\\)\u540e\u5f97\u5230<code>soft-loss</code>\uff0c\u8fd8\u6709\u81ea\u5df1\u6a21\u578b\u4e0e\u771f\u5b9e\u6807\u7b7e\u7684\u666e\u901a\u8bad\u7ec3\u540e\u5f97\u5230\u7684<code>hard-loss</code>\u3002\u603b\u7684<code>loss = soft-loss + hard-loss</code>\u3002</p> \\[ Loss = (1-\\alpha) T^2 L_{soft} + \\alpha L_{hard} \\] <p></p> <p>\u6211\u4eec\u53d1\u73b0\u6700\u597d\u7684\u7ed3\u679c\u662f\u5728\u7b2c\u4e8c\u4e2a\u76ee\u6807\u51fd\u6570\u4e0a\u4f7f\u7528\u8f83\u4f4e\u7684\u6743\u503c\uff08\\(\\alpha\\)\u8f83\u5c0f\uff09\u3002\u7531\u4e8e\u8f6f\u76ee\u6807\u4ea7\u751f\u7684\u68af\u5ea6\u7f29\u653e\uff08\\(1/T^{2}\\)\uff09 \uff0c\u56e0\u800c\u5728\u540c\u65f6\u4f7f\u7528\u8f6f\u76ee\u6807\u548c\u786c\u76ee\u6807\u65f6\uff0c\u5c06\u8f6f\u76ee\u6807\u7684\u68af\u5ea6\u4e58\u4ee5\\(T^{2}\\)\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002\u8fd9\u786e\u4fdd\u4e86\u5728\u5b9e\u9a8c\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u7528\u4e8e\u84b8\u998f\u7684\u6e29\u5ea6\u53d1\u751f\u6539\u53d8\uff0c\u90a3\u4e48\u786c\u76ee\u6807\u548c\u8f6f\u76ee\u6807\u7684\u76f8\u5bf9\u8d21\u732e\u5927\u81f4\u4fdd\u6301\u4e0d\u53d8\u3002 </p> <p></p> <p>\u5047\u8bbeP\u4e3a\u6559\u5e08\u6a21\u578b\u7684\u8f6f\u6807\u7b7e\uff0cQ\u4e3a\u5b66\u751f\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u5219\u4e24\u8005\u7684KL\u6563\u5ea6\u4f5c\u4e3aloss\uff1a</p> \\[ D_{KL}(P||Q) = \\sum_{i} P_{i} \\log (\\frac{P_{i}}{Q_{i}}) = \\sum_{i} P_{i} (\\log(P_i) - \\log(Q_i))  \\] <pre><code>class DistillKL(nn.Module):\n    \"\"\"Distilling the Knowledge in a Neural Network\"\"\"\n    def __init__(self, T):\n        super(DistillKL, self).__init__()\n        self.T = T # \u84b8\u998f\u6e29\u5ea6\n\n    def forward(self, y_s, y_t):\n        '''\n        y_s: student model output\n        y_t: teacher model output\n        '''\n        p_s = F.log_softmax(y_s/self.T, dim=1)\n        p_t = F.softmax(y_t/self.T, dim=1)\n        loss = F.kl_div(p_s, p_t, size_average=False) * (self.T**2) / y_s.shape[0]\n        return loss\n</code></pre>"},{"location":"sci-paper/cs/Distilling-the-knowledge/#\u7b80\u6d01\u5b9e\u73b0nnlabmlai","title":"\u7b80\u6d01\u5b9e\u73b0\uff08nn.labml.ai\uff09","text":"<pre><code>import torch\nimport torch.nn.functional\nfrom torch import nn\n\nclass Configs(CIFAR10Configs):\n    \"\"\"\n    ## Configurations\n\n    This extends from [`CIFAR10Configs`](../experiments/cifar10.html) which defines all the\n    dataset related configurations, optimizer, and a training loop.\n    \"\"\"\n    # The small model\n    model: SmallModel\n    # The large model\n    large: LargeModel\n    # KL Divergence loss for soft targets\n    kl_div_loss = nn.KLDivLoss(log_target=True) # KL\u6563\u5ea6\u635f\u5931\n    # Cross entropy loss for true label loss\n    loss_func = nn.CrossEntropyLoss()\n    # Temperature, $T$\n    temperature: float = 5.\n    # Weight for soft targets loss.\n    #\n    # The gradients produced by soft targets get scaled by $\\frac{1}{T^2}$.\n    # To compensate for this the paper suggests scaling the soft targets loss\n    # by a factor of $T^2$\n    soft_targets_weight: float = 100.\n    # Weight for true label cross entropy loss\n    label_loss_weight: float = 0.5\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training/validation step\n\n        We define a custom training/validation step to include the distillation\n        \"\"\"\n\n        # Training/Evaluation mode for the small model\n        self.model.train(self.mode.is_train)\n        # Large model in evaluation mode\n        self.large.eval()\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Update global step (number of samples processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Get the output logits, $v_i$, from the large model\n        with torch.no_grad():\n            large_logits = self.large(data)\n\n        # Get the output logits, $z_i$, from the small model\n        output = self.model(data)\n\n        # Soft targets\n        # $$p_i = \\frac{\\exp (\\frac{v_i}{T})}{\\sum_j \\exp (\\frac{v_j}{T})}$$\n        soft_targets = nn.functional.log_softmax(large_logits / self.temperature, dim=-1)\n        # Temperature adjusted probabilities of the small model\n        # $$q_i = \\frac{\\exp (\\frac{z_i}{T})}{\\sum_j \\exp (\\frac{z_j}{T})}$$\n        soft_prob = nn.functional.log_softmax(output / self.temperature, dim=-1)\n\n        # Calculate the soft targets loss\n        soft_targets_loss = self.kl_div_loss(soft_prob, soft_targets)\n        # Calculate the true label loss\n        label_loss = self.loss_func(output, target)\n        # Weighted sum of the two losses\n        loss = self.soft_targets_weight * soft_targets_loss + self.label_loss_weight * label_loss\n        # Log the losses\n        tracker.add({\"loss.kl_div.\": soft_targets_loss,\n                     \"loss.nll\": label_loss,\n                     \"loss.\": loss})\n\n        # Calculate and log accuracy\n        self.accuracy(output, target)\n        self.accuracy.track()\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward() \n            # Take optimizer step\n            self.optimizer.step()\n            # Log the model parameters and gradients on last batch of every epoch\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n\n@option(Configs.large)\ndef _large_model(c: Configs):\n    \"\"\"\n    ### Create large model\n    \"\"\"\n    return LargeModel().to(c.device)\n\n\n@option(Configs.model)\ndef _small_student_model(c: Configs):\n    \"\"\"\n    ### Create small model\n    \"\"\"\n    return SmallModel().to(c.device)\n\n\ndef get_saved_model(run_uuid: str, checkpoint: int):\n    \"\"\"\n    ### Load [trained large model](large.html)\n    \"\"\"\n\n    from labml_nn.distillation.large import Configs as LargeConfigs\n\n    # In evaluation mode (no recording)\n    experiment.evaluate()\n    # Initialize configs of the large model training experiment\n    conf = LargeConfigs()\n    # Load saved configs\n    experiment.configs(conf, experiment.load_configs(run_uuid))\n    # Set models for saving/loading\n    experiment.add_pytorch_models({'model': conf.model})\n    # Set which run and checkpoint to load\n    experiment.load(run_uuid, checkpoint)\n    # Start the experiment - this will load the model, and prepare everything\n    experiment.start()\n\n    # Return the model\n    return conf.model\n\n\ndef main(run_uuid: str, checkpoint: int):\n    \"\"\"\n    Train a small model with distillation\n    \"\"\"\n    # Load saved model\n    large_model = get_saved_model(run_uuid, checkpoint)\n    # Create experiment\n    experiment.create(name='distillation', comment='cifar10')\n    # Create configurations\n    conf = Configs()\n    # Set the loaded large model\n    conf.large = large_model\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n        'model': '_small_student_model',\n    })\n    # Set model for saving/loading\n    experiment.add_pytorch_models({'model': conf.model})\n    # Start experiment from scratch\n    experiment.load(None, None)\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main('d46cd53edaec11eb93c38d6538aee7d6', 1_000_000)\n</code></pre>"},{"location":"sci-paper/cs/EfficientNet/","title":"EfficientNet","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aEfficientNet:Rethinking Model Scaling for Convolutional Neural Networks</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/lukemelas/EfficientNet-PyTorch/</p> <p>\u672c\u9875\u5185\u5bb9\u662f\u5bf9<code>EfficientNet</code>\u7684\u6587\u7ae0\u603b\u7ed3/\u4ee3\u7801\u9605\u8bfb(\u4fa7\u91cd\u4ee3\u7801\u5b66\u4e60)</p> <p>\u8fd9\u662f\u6211\u9605\u8bfb\u7684\u7b2c\u4e00\u7bc7DL\u8bba\u6587\u4e0e\u6e90\u7801</p> <p></p> <p>\u6587\u7ae0\u6458\u8981</p> <p> <p>EfficientNet\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u7f29\u653e\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ed4\u7ec6\u5e73\u8861\u7f51\u7edc\u6df1\u5ea6\uff08<code>depth</code>\uff09\u3001\u5bbd\u5ea6(<code>width</code>)\u548c\u5206\u8fa8\u7387(<code>resolutinon</code>)\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u590d\u6742\u5ea6\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002</p> <p></p> <p></p> <p>\u4f20\u7edf\u7684\u65b9\u6cd5\u5927\u591a\u5c06\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u89c4\u6a21\u5206\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u7ef4\u5ea6\u4e4b\u4e00:</p> <ul> <li> <p><code>Depth (d)</code>\uff1a\u66f4\u6df1\u5c42\u6b21\u7684<code>ConvNet</code>\u80fd\u591f\u6355\u83b7\u66f4\u4e30\u5bcc\u3001\u66f4\u590d\u6742\u7684\u7279\u6027\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6982\u62ec\u65b0\u4efb\u52a1\u3002\u7136\u800c\uff0c\u7531\u4e8e\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u66f4\u6df1\u7684\u7f51\u7edc\u4e5f\u66f4\u96be\u4ee5\u8bad\u7ec3\u3002\u867d\u7136<code>skip connections</code>\u548c<code>BatchNorm</code>\u7b49\u6280\u672f\u7f13\u89e3\u4e86\u8bad\u7ec3\u95ee\u9898\uff0c\u4f46\u662f\u975e\u5e38\u6df1\u7684\u7f51\u7edc\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u964d\u4f4e</p> </li> <li> <p><code>Width (w)</code>\uff1a\u66f4\u5e7f\u6cdb\u7684\u7f51\u7edc\u5f80\u5f80\u80fd\u591f\u6355\u83b7\u66f4\u591a\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u66f4\u5bb9\u6613\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u6781\u5bbd\u4f46\u8f83\u6d45\u7684\u7f51\u7edc\u5f80\u5f80\u96be\u4ee5\u6355\u83b7\u66f4\u9ad8\u5c42\u6b21\u7684\u7279\u5f81\u3002\u5f53\u7f51\u7edc\u53d8\u5f97\u66f4\u5bbd\u65f6\uff0c\u968f\u7740w\u7684\u589e\u5927\uff0c\u51c6\u786e\u7387\u5f88\u5feb\u9971\u548c</p> </li> <li> <p><code>Resolution (r)</code>\uff1a\u589e\u52a0\u8f93\u5165\u7f51\u7edc\u7684\u56fe\u50cf\u5206\u8fa8\u7387\u80fd\u591f\u6f5c\u5728\u5f97\u83b7\u5f97\u66f4\u9ad8\u7ec6\u7c92\u5ea6(fine-grained)\u7684\u7279\u5f81\u6a21\u677f\uff0c\u4f46\u5bf9\u4e8e\u975e\u5e38\u9ad8\u7684\u8f93\u5165\u5206\u8fa8\u7387\uff0c\u51c6\u786e\u7387\u7684\u589e\u76ca\u4e5f\u4f1a\u51cf\u5c0f\u3002\u5e76\u4e14\u5927\u5206\u8fa8\u7387\u56fe\u50cf\u4f1a\u589e\u52a0\u8ba1\u7b97\u91cf\u3002</p> </li> </ul> <p></p> <p>\u7531\u4e0a\u56fe\u53ef\u77e5\uff0c\u6269\u5927\u7f51\u7edc\u5bbd\u5ea6\u3001\u6df1\u5ea6\u6216\u5206\u8fa8\u7387\u7684\u4efb\u4f55\u7ef4\u5ea6\u53ef\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u4e8e\u8f83\u5927\u7684\u6a21\u578b\uff0c\u7cbe\u5ea6\u589e\u76ca\u4f1a\u964d\u4f4e\u3002</p> <p>\u56e0\u6b64\uff0c\u4e3a\u4e86\u8ffd\u6c42\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5728<code>ConvNet</code>\u7f29\u653e\u8fc7\u7a0b\u4e2d\uff0c\u5e73\u8861\u7f51\u7edc\u5bbd\u5ea6\u3001\u6df1\u5ea6\u548c\u5206\u8fa8\u7387\u7684\u6240\u6709\u7ef4\u5ea6\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002</p> <p></p> <p>\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f29\u653e\u65b9\u6cd5\uff0c\u79f0\u4e3a\u590d\u5408\u7f29\u653e\u65b9\u6cd5 \uff08<code>Compound Scaling</code>\uff09\uff0c\u5b83\u901a\u8fc7\u540c\u65f6\u7f29\u653e\u6df1\u5ea6\u3001\u5bbd\u5ea6\u548c\u5206\u8fa8\u7387\uff0c\u4ee5\u4fdd\u6301\u6a21\u578b\u590d\u6742\u5ea6\u6052\u5b9a\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u6a21\u578b\u590d\u6742\u5ea6\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002</p> <p>\u8be5\u65b9\u6cd5\u4f7f\u7528\u590d\u5408\u7cfb\u6570\\(\\phi\\)\u4ee5\u7279\u5b9a\u7684\u65b9\u5f0f\u5747\u5300\u7f29\u653e\u7f51\u7edc\u5bbd\u5ea6\u3001\u6df1\u5ea6\u548c\u5206\u8fa8\u7387\uff1a</p> \\[     \\begin{aligned}     \\text{depth\uff1a} &amp;d = \\alpha^\\phi    \\\\     \\text{width\uff1a} &amp;w = \\beta^\\phi     \\\\     \\text{resolution\uff1a} &amp;r = \\gamma^\\phi  \\\\     s.t. \\; &amp;\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\\\     &amp;\\alpha \u2265 1\uff0c\\beta \u2265 1\uff0c\\gamma \u2265 1  \\\\     \\end{aligned} \\] <p>\u5176\u4e2d\uff0c\\(\\phi\\)\u662f\u4e00\u4e2auser-specified\u7f29\u653e\u7cfb\u6570\uff0c\u53d7\u9650\u4e8e\u53ef\u7528\u7684\u8d44\u6e90\u6570\u91cf\uff0c\u800c\\(\\alpha,\\beta,\\gamma\\)\u662f\u53ef\u4ee5\u901a\u8fc7small grid search\u786e\u5b9a\u7684\u5e38\u6570\u3002</p> <p><code>\u5377\u79ef\u5c42\u7684FLOPs</code></p> <p> <p><code>FLOPs\uff08floating point operations\uff09</code>\u7528\u6765\u8ba1\u7b97\u6574\u4e2a\u7f51\u7edc\u6a21\u578b\u4e2d\u4e58\u6cd5/\u52a0\u6cd5\u7684\u8fd0\u884c\u6b21\u6570\uff0c\u53ef\u4ee5\u7528\u6765\u8861\u91cf\u4e00\u4e2a\u7b97\u6cd5/\u6a21\u578b\u7b49\u7684\u590d\u6742\u5ea6\u3002</p> <p>\u5bf9\u4e8e\u5e38\u89c4\u7684\u5377\u79ef\u64cd\u4f5c\u7684<code>FLOPs</code>\u4e0e\\(d,w^2.r^2\\)\u6210\u6b63\u6bd4\uff0c\u5373\u7f51\u7edc\u6df1\u5ea6\u52a0\u500d\u4f1a\u4f7f<code>FLOPs</code>\u52a0\u500d\uff0c\u800c\u7f51\u7edc\u5bbd\u5ea6\u3001\u5206\u8fa8\u7387\u52a0\u500d\u4f1a\u4f7f<code>FLOPs</code>\u589e\u52a04\u500d\u3002</p> <p>\u5bf9\u4e8e\u4e00\u4e2a\u5377\u79ef\u5c42:</p> \\[ params = C_o \\times (k_w \\times k_h \\times C_i + 1) \\] <p>\u82e5\u4e3a\u6b63\u65b9\u5f62\u5377\u79ef\u6838\u5219\uff1a</p> \\[ params = C_o \\times (k^2 \\times C_i + 1) \\] <p>\u4f7f\u7528<code>Batch Norm</code>\u65f6\u4e0d\u9700\u8981<code>bias</code>\uff0c\u6b64\u65f6\u8ba1\u7b97\u5f0f\u4e2d\u7684<code>+1</code>\u9879\u53bb\u9664\u3002</p> \\[ FLOPs = C_o \\times H \\times W \\times [(C_i \\times k_w \\times k_h) + (C_i \\times k_w \\times k_h - 1) + 1] \\] <p>\u5176\u4e2d\uff0c\\(C_i \\times k_w \\times k_h\\)\u8868\u793a\u4e00\u6b21\u5377\u79ef\u64cd\u4f5c\u4e2d\u7684\u4e58\u6cd5\u8fd0\u7b97\u91cf\uff0c\\(C_i \\times k_w \\times k_h - 1\\)\u8868\u793a\u4e00\u6b21\u5377\u79ef\u64cd\u4f5c\u4e2d\u7684\u52a0\u6cd5\u8fd0\u7b97\u91cf\u3002</p> <p>\u5bf9\u4e8e\u6b63\u65b9\u5f62\u5377\u79ef\u6838\uff1a</p> \\[ FLOPs = 2 \\times C_i \\times C_o \\times H \\times W \\times k^2 \\] <p>\u5728\u8bba\u6587\u4e2d\uff0c\u5e38\u5e38\u5c06\u4e00\u4e2a\u2019\u4e58-\u52a0\u2019\u7ec4\u5408\u89c6\u4e3a\u4e00\u6b21\u6d6e\u70b9\u8fd0\u7b97\uff08<code>Multi-Add</code>\uff09\u5373\uff1a</p> <p>$$ FLOPs = C_i \\times C_o \\times H \\times W \\times k^2 $$ </p> <p>\u672c\u6587\u4e2d\u63d0\u51fa\\(FOLPs \\propto (\\alpha \\times \\beta^2 \\times \\gamma^2)^\\phi\\)\uff0c\u5e76\u4e14\u7ea6\u675f\\(\\alpha \\times \\beta^2 \\times \\gamma^2 \\approx 2\\)\u4f7f\u5f97\\(FOLPs \\propto 2^\\phi\\)</p> <p>\u5047\u8bbe\u5377\u79ef\u6838\u5927\u5c0f\uff08\\(k^2\\)\uff09\u548c\u8f93\u5165\u901a\u9053\u6570(\\(c_i\\))\u90fd\u662f\u5e38\u6570\uff0c\u5219\uff1a</p> \\[ FLOPs \\propto H \\times W \\times C_o \\] <p>\u7531\u4e8e\\(d\\)\u8868\u793a\u7f51\u7edc\u6df1\u5ea6\uff08\u5377\u79ef\u5c42\u7684\u6570\u91cf\uff09\uff0c\\(w\\)\u8868\u793a\u7f51\u7edc\u5bbd\u5ea6\uff08\u8f93\u51fa\u901a\u9053\u6570\u7684\u500d\u6570\uff09\uff0c\\(r\\)\u8868\u793a\u5206\u8fa8\u7387\uff08\\(h\\)\u548c\\(w\\)\u7684\u500d\u6570\uff09\uff1a</p> \\[ FLOPs \\propto d \\times w^2 \\times r^2 \\] <p>\u7f51\u7edc\u6df1\u5ea6d\u6ca1\u6709\u5e73\u65b9\u662f\u56e0\u4e3a\u5b83\u53ea\u5f71\u54cd\u5377\u79ef\u5c42\u7684\u6570\u91cf\uff0c\u800c\u4e0d\u5f71\u54cd\u6bcf\u4e2a\u5377\u79ef\u5c42\u7684\u8ba1\u7b97\u91cf</p> <p></p> <p></p> <p>\u5f53\u56fe\u50cf\u5206\u8fa8\u7387\u589e\u5927\u65f6\uff0c<code>EfficientNet</code>\u7684\u6240\u6709\u5377\u79ef\u5c42\u548c\u6c60\u5316\u5c42\u7684\u8f93\u5165\u548c\u8f93\u51fa\u5c3a\u5bf8\u4e5f\u4f1a\u76f8\u5e94\u5730\u589e\u5927\u3002\u4f46\u662f\uff0c<code>EfficientNet</code>\u7684\u5377\u79ef\u5c42\u548c\u6c60\u5316\u5c42\u7684<code>kernel_size</code>\u548c<code>stride</code>\u4e0d\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u8fd9\u610f\u5473\u7740\uff0c\u5f53\u56fe\u50cf\u5206\u8fa8\u7387\u589e\u5927\u65f6\uff0c<code>EfficientNet</code>\u7684\u5377\u79ef\u5c42\u548c\u6c60\u5316\u5c42\u7684\u611f\u53d7\u91ce\u4e5f\u4f1a\u76f8\u5e94\u5730\u589e\u5927\u3002\u8fd9\u6709\u52a9\u4e8e\u7f51\u7edc\u6355\u6349\u66f4\u591a\u7684\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002</p> <p><code>EfficientNet</code>\u7684\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u5165\u5c3a\u5bf8\u4e5f\u4f1a\u968f\u7740\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u589e\u5927\u800c\u589e\u5927\uff0c\u4f46\u662f\u8f93\u51fa\u5c3a\u5bf8\u4e0d\u4f1a\u53d8\u5316\u3002\u8fd9\u610f\u5473\u7740\uff0c\u5f53\u56fe\u50cf\u5206\u8fa8\u7387\u589e\u5927\u65f6\uff0c<code>EfficientNet</code>\u7684\u5168\u8fde\u63a5\u5c42\u9700\u8981\u66f4\u591a\u7684\u53c2\u6570\u6765\u5904\u7406\u66f4\u591a\u7684\u7279\u5f81\u3002</p> <p>\u53e6\u5916\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528<code>AdvProp</code>\uff08\u5bf9\u6297\u6837\u672c\uff09\uff0c\uff08<code>Noisy Student</code>\uff09\uff0c<code>AutoAugment</code>\uff0c<code>label smoothing</code>\u7b49\u6280\u672f\u6765\u589e\u5f3a\u8bad\u7ec3\u6548\u679c\u3002</p>"},{"location":"sci-paper/cs/EfficientNet/#utilspy","title":"<code>utils.py</code>","text":"<ul> <li> <p>\u4f7f\u7528<code>collections.namedtuple()</code>\u81ea\u5b9a\u4e49\u9700\u8981\u8f93\u5165\u7684\u53c2\u6570\u7ed3\u6784\uff0c\u975e\u5e38\u65b9\u4fbf\uff1a</p> <p> Code <p><pre><code>import collections \n# Parameters for the entire model (stem, all blocks, and head)\nGlobalParams = collections.namedtuple('GlobalParams', [\n    'width_coefficient', 'depth_coefficient', 'image_size', 'dropout_rate',\n    'num_classes', 'batch_norm_momentum', 'batch_norm_epsilon',\n    'drop_connect_rate', 'depth_divisor', 'min_depth'])\n\n# Parameters for an individual model block\nBlockArgs = collections.namedtuple('BlockArgs', [\n    'num_repeat', 'kernel_size', 'stride', 'expand_ratio',\n    'input_filters', 'output_filters', 'se_ratio', 'id_skip'])\n\n# Set GlobalParams and BlockArgs's defaults (\u53c2\u6570\u521d\u59cb\u5316)\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n</code></pre> </p> <li> <p>\u81ea\u5b9a\u4e49<code>Swish</code>\u6fc0\u6d3b\u51fd\u6570\uff08\u6ce8\u610f<code>torch.autograd.Funciton</code>\u7684\u4f7f\u7528\uff09\uff1a</p> <p> Code\uff1aSwish <pre><code># A memory-efficient implementation of Swish function\nclass SwishImplementation(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_tensors[0]\n        sigmoid_i = torch.sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\nclass MemoryEfficientSwish(nn.Module):\n    def forward(self, x):\n        return SwishImplementation.apply(x)\n</code></pre> </p> </li> <li> <p>\u81ea\u5b9a\u4e49\u7684<code>Dropconnect</code>\u5c42:</p> <p> Code\uff1adrop_connect <pre><code>def drop_connect(inputs, p, training):\n    \"\"\"Drop connect.\n\n    Args:\n        input (tensor: BCWH): Input of this structure.\n        p (float: 0.0~1.0): Probability of drop connection.\n        training (bool): The running mode.\n\n    Returns:\n        output: Output after drop connection.\n    \"\"\"\n    assert p &gt;= 0 and p &lt;= 1, 'p must be in range of [0,1]'\n    # assert \u5148\u5224\u65ad\u53c2\u6570\u6761\u4ef6\uff0c\u5f88\u5e38\u89c1\u7684\u64cd\u4f5c\n\n    if not training:\n        return inputs\n        # \u5224\u65ad\u82e5\u5904\u5728\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5219\u542f\u7528dropout\n\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n\n    # generate binary_tensor mask according to probability (p for 0, 1-p for 1)\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device) # .rand() [0,1)\u7684\u5747\u5300\u5206\u5e03\n    binary_tensor = torch.floor(random_tensor) # \u63a9\u7801\u77e9\u9635\n\n    output = inputs / keep_prob * binary_tensor\n    return output\n</code></pre> <p> </p> <li> <p>\u7531\u4e8e<code>EfficientNet</code>\u6e90\u7801\u662f\u4f7f\u7528<code>tensorflow</code>\u5b9e\u73b0\u7684\uff0c\u5728\u5377\u79ef\u65b9\u5f0f\u4e0a\u5b58\u5728\u4e0d\u540c\u4e4b\u5904\uff0c\u5373<code>'SAME'</code>\u4e0e<code>'VALID'</code>\u3002</p> <p>\u82e5\u5377\u79ef\u65b9\u5f0f\u8bbe\u4e3a<code>'SAME'</code>\uff0c\u5219\u5377\u79ef\u540e\u8f93\u51fa\u7684\u7279\u5f81\u56fe\u4e0e<code>kernel_size</code>\u65e0\u5173\uff0c\u53ea\u8ddf<code>stride</code>\u6709\u5173\u3002\u82e5<code>stride=1</code>\uff0c\u524d\u540e\u7279\u5f81\u56fe\u5927\u5c0f\u4fdd\u6301\u4e0d\u53d8\uff0c\u5373\u81ea\u52a8<code>padding</code>\u8865\u5168\u3002</p> <p> Code1\uff1aget_shape <pre><code>def calculate_output_image_size(input_image_size, stride):\n    \"\"\"\u5b9e\u73b0'SAME'\u65b9\u6cd5\u4e0b\u8f93\u51fa\u5377\u79ef\u7279\u5f81\u56fe\u5927\u5c0f\uff08\u6839\u636estride\uff09\n\n    Args:\n        input_image_size (int, tuple or list): Size of input image.\n        stride (int, tuple or list): Conv2d operation's stride.\n\n    Returns:\n        output_image_size: A list [H,W].\n    \"\"\"\n    if input_image_size is None:\n        return None\n    image_height, image_width = get_width_and_height_from_size(input_image_size)\n    stride = stride if isinstance(stride, int) else stride[0]\n    # \u68c0\u67e5\u8f93\u5165\uff0c\u5728\u9879\u76ee\u4e2d\u5f88\u5e38\u89c1\n\n    image_height = int(math.ceil(image_height / stride))\n    image_width = int(math.ceil(image_width / stride))\n    # \u6839\u636e\u7ed9\u5b9a\u7684stride\u8ba1\u7b97\u7279\u5f81\u56fe\u5927\u5c0f\n\n    return [image_height, image_width]\n</code></pre> <p> </p> <p> Code2\uff1aConv2d_Dynamic_Same_Padding <pre><code>class Conv2dDynamicSamePadding(nn.Conv2d):\n    \"\"\"2D Convolutions like TensorFlow, for a dynamic image size.\n    The padding is operated in forward function by calculating dynamically.\n    \"\"\"\n    # Tips for 'SAME' mode padding.\n    #     Given the following:\n    #         i: width or height\n    #         s: stride\n    #         k: kernel size\n    #         d: dilation\n    #         p: padding\n    #     Output after Conv2d:\n    #         o = floor((i+p-((k-1)*d+1))/s+1)\n    # If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),\n    # =&gt; p = (i-1)*s+((k-1)*d+1)-i\n\n    # \u52a8\u6001\u586b\u5145\u64cd\u4f5c\uff0c\u6bcf\u6b21\u524d\u5411\u4f20\u64ad\u65f6\u90fd\u9700\u8981\u91cd\u65b0\u8ba1\u7b97\u586b\u5145\u91cf\uff0c\u9002\u7528\u4e8e\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\u4e0d\u56fa\u5b9a\u7684\u60c5\u51b5\u3002\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n        # stride -&gt; list \u5206\u4e3a\u6c34\u5e73/\u5782\u76f4\u65b9\u5411\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]  # input's height,width\n        kh, kw = self.weight.size()[-2:] # self.weight\u4ecenn.Conv2d\u7ee7\u627f\uff0c\u4ee3\u8868\u5377\u79ef\u6838\n        sh, sw = self.stride # \u6c34\u5e73/\u5782\u76f4\u65b9\u5411stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw) # change the output size according to stride ! ! !\n\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n\n        if pad_h &gt; 0 or pad_w &gt; 0:\n            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n            # [\u5de6\u586b\u5145\u6570\uff0c \u53f3\u586b\u5145\u6570\uff0c \u4e0a\u586b\u5145\u6570\uff0c \u4e0b\u586b\u5145\u6570]\n\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\nclass Conv2dStaticSamePadding(nn.Conv2d):\n    \"\"\"2D Convolutions like TensorFlow's 'SAME' mode, with the given input image size.\n    The padding module is calculated in construction function, then used in forward.\n    \"\"\"\n\n    # With the same calculation as Conv2dDynamicSamePadding\n\n    # \u9759\u6001\u586b\u5145\uff0c\u5df2\u77e5\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\uff0c\u5728\u521d\u59cb\u5316\u65f6\u5c31\u8ba1\u7b97\u586b\u5145\u91cf\u5e76\u4e14\u56fa\u5b9a\uff0c\u9002\u7528\u4e8e\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\u56fa\u5b9a\u7684\u60c5\u51b5\u3002\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, image_size=None, **kwargs):\n        super().__init__(in_channels, out_channels, kernel_size, stride, **kwargs)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n        # Calculate padding based on image size and save it\n        assert image_size is not None # \u56fa\u5b9a\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\n\n        ih, iw = (image_size, image_size) if isinstance(image_size, int) else image_size # \u5e38\u89c1\u64cd\u4f5c\n\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n\n        if pad_h &gt; 0 or pad_w &gt; 0:\n            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n        else:\n            self.static_padding = Identity()\n\n    def forward(self, x):\n        x = self.static_padding(x)\n        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n        return x\n\ndef get_same_padding_conv2d(image_size=None):\n    \"\"\"Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n    Static padding is necessary for ONNX exporting of models.\n\n    Args:\n        image_size (int or tuple): Size of the image.\n\n    Returns:\n        Conv2dDynamicSamePadding or Conv2dStaticSamePadding.\n    \"\"\"\n    if image_size is None:\n        return Conv2dDynamicSamePadding\n    else:\n        return partial(Conv2dStaticSamePadding, image_size=image_size)\n\nclass Identity(nn.Module):\n    \"\"\"Identity mapping.\n    Send input to output directly.\n    \"\"\"\n\n    # \u4e00\u4e2a\u7b80\u5355\u7684\u6620\u5c04\u5c42\uff0c\u5176\u529f\u80fd\u662f\u5c06\u8f93\u5165\u76f4\u63a5\u4f20\u9012\u5230\u8f93\u51fa\uff0c\u800c\u4e0d\u505a\u4efb\u4f55\u4fee\u6539\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n</code></pre> <p> Code3\uff1aMaxPool2d_Same_Padding <p><pre><code>class MaxPool2dDynamicSamePadding(nn.MaxPool2d):\n    \"\"\"2D MaxPooling like TensorFlow's 'SAME' mode, with a dynamic image size.\n    The padding is operated in forward function by calculating dynamically.\n    \"\"\"\n\n    def __init__(self, kernel_size, stride, padding=0, dilation=1, return_indices=False, ceil_mode=False):\n        super().__init__(kernel_size, stride, padding, dilation, return_indices, ceil_mode)\n        self.stride = [self.stride] * 2 if isinstance(self.stride, int) else self.stride\n        self.kernel_size = [self.kernel_size] * 2 if isinstance(self.kernel_size, int) else self.kernel_size\n        self.dilation = [self.dilation] * 2 if isinstance(self.dilation, int) else self.dilation\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]\n        kh, kw = self.kernel_size\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h &gt; 0 or pad_w &gt; 0:\n            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding,\n                            self.dilation, self.ceil_mode, self.return_indices)\n\nclass MaxPool2dStaticSamePadding(nn.MaxPool2d):\n    \"\"\"2D MaxPooling like TensorFlow's 'SAME' mode, with the given input image size.\n    The padding mudule is calculated in construction function, then used in forward.\n    \"\"\"\n\n    def __init__(self, kernel_size, stride, image_size=None, **kwargs):\n        super().__init__(kernel_size, stride, **kwargs)\n        self.stride = [self.stride] * 2 if isinstance(self.stride, int) else self.stride\n        self.kernel_size = [self.kernel_size] * 2 if isinstance(self.kernel_size, int) else self.kernel_size\n        self.dilation = [self.dilation] * 2 if isinstance(self.dilation, int) else self.dilation\n\n        # Calculate padding based on image size and save it\n        assert image_size is not None\n        ih, iw = (image_size, image_size) if isinstance(image_size, int) else image_size\n        kh, kw = self.kernel_size\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h &gt; 0 or pad_w &gt; 0:\n            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n        else:\n            self.static_padding = Identity()\n\n    def forward(self, x):\n        x = self.static_padding(x)\n        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding,\n                        self.dilation, self.ceil_mode, self.return_indices)\n        return x\n\ndef get_same_padding_maxPool2d(image_size=None):\n    \"\"\"Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n    Static padding is necessary for ONNX exporting of models.\n\n    Args:\n        image_size (int or tuple): Size of the image.\n\n    Returns:\n        MaxPool2dDynamicSamePadding or MaxPool2dStaticSamePadding.\n    \"\"\"\n    if image_size is None:\n        return MaxPool2dDynamicSamePadding\n    else:\n        return partial(MaxPool2dStaticSamePadding, image_size=image_size)\n\n# \u4e3b\u8981\u7ec6\u8282\u4e0e\u4e0a\u9762\u7684conv2d\u4e00\u81f4\n</code></pre> </p> <li> <p>Helper functions for loading model params</p> <p>\u901a\u8fc7\u8fd9\u4e9b\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5728\u4ee3\u7801\u4e2d\u7075\u6d3b\u5730\u4f7f\u7528\u5b57\u7b26\u4e32\u8868\u793a\u5377\u79ef\u5757\u7684\u53c2\u6570\uff0c\u540c\u65f6\u53c8\u80fd\u65b9\u4fbf\u5730\u8fdb\u884c\u89e3\u6790\u548c\u5904\u7406</p> <p> Code4\uff1a\u67e5\u8be2\u5404\u4e2a\u6a21\u578b\u7684\u590d\u5408\u7cfb\u6570 <p><pre><code>def efficientnet_params(model_name):\n    \"\"\"Map EfficientNet model name to parameter coefficients.\n\n    Args:\n        model_name (str): Model name to be queried.\n\n    Returns:\n        params_dict[model_name]: A (width,depth,res,dropout) tuple.\n    \"\"\"\n\n    # \u67e5\u8be2\u4f5c\u8005\u8ba1\u7b97\u7684\u4e00\u7cfb\u5217\u6a21\u578b\u590d\u5408\u7cfb\u6570\n\n    params_dict = {\n        # Coefficients:   width,depth,res,dropout\n        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n        'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n        'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n    }\n    return params_dict[model_name]\n</code></pre> </p> <p> Code5\uff1a\u8bfb\u53d6\u6a21\u5757\u53c2\u6570\u5b57\u7b26\u4e32,\u4f8b\u5982'r1_k3_s11_e1_i32_o16_se0.25_noskip' <p><pre><code>class BlockDecoder(object):\n    \"\"\"Block Decoder for readability,\n    straight from the official TensorFlow repository.\n    \"\"\"\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        \"\"\"Get a block through a string notation of arguments.\n\n        Args:\n            block_string (str): A string notation of arguments.\n                                Examples: 'r1_k3_s11_e1_i32_o16_se0.25_noskip'.\n\n        Returns:\n            BlockArgs: The namedtuple defined at the top of this file.\n        \"\"\"\n\n        # \u901a\u8fc7\u53c2\u6570\u7684\u5b57\u7b26\u4e32\u8868\u793a\u6cd5\u83b7\u53d6\u4e00\u4e2a\u6a21\u5757\n\n        assert isinstance(block_string, str)\n\n        ops = block_string.split('_')\n        options = {}\n        for op in ops:\n            splits = re.split(r'(\\d.*)', op)\n            # \u901a\u8fc7\u6b63\u5219\u8868\u8fbe\u5f0f\u5c06\u6bcf\u4e2a\u53c2\u6570\u5206\u5272\u6210\u952e\u503c\u5bf9\uff0c\u4f8b\u5982 'r1' \u5206\u5272\u6210 ('r', '1')\u3002\n            if len(splits) &gt;= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        #\u68c0\u67e5 stride \u53c2\u6570\u662f\u5426\u6709\u6548\u3002\n        assert (('s' in options and len(options['s']) == 1) or\n                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n\n        return BlockArgs(\n            num_repeat=int(options['r']),\n            kernel_size=int(options['k']),\n            stride=[int(options['s'][0])],\n            expand_ratio=int(options['e']),\n            input_filters=int(options['i']),\n            output_filters=int(options['o']),\n            se_ratio=float(options['se']) if 'se' in options else None,\n            id_skip=('noskip' not in block_string)\n            )\n       # \u5c06\u89e3\u6790\u540e\u7684\u53c2\u6570\u8f6c\u6362\u6210BlockArgs(namedtuple)\uff0c\u5e76\u8fd4\u56de\n\n    @staticmethod\n    def _encode_block_string(block):\n        \"\"\"Encode a block to a string.\n\n        Args:\n            block (namedtuple): A BlockArgs type argument.\n\n        Returns:\n            block_string: A String form of BlockArgs.\n        \"\"\"\n\n        # \u5c06\u4e00\u4e2a BlockArgs(namedtuple)\u7f16\u7801\u4e3a\u5b57\u7b26\u4e32\u3002\n        args = [\n            'r%d' % block.num_repeat,\n            'k%d' % block.kernel_size,\n            's%d%d' % (block.strides[0], block.strides[1]),\n            'e%s' % block.expand_ratio,\n            'i%d' % block.input_filters,\n            'o%d' % block.output_filters\n        ]\n\n        if 0 &lt; block.se_ratio &lt;= 1:\n            args.append('se%s' % block.se_ratio)\n        if block.id_skip is False:\n            args.append('noskip')\n        return '_'.join(args)\n\n\n    @staticmethod\n    def decode(string_list):\n        \"\"\"Decode a list of string notations to specify blocks inside the network.\n\n        Args:\n            string_list (list[str]): A list of strings, each string is a notation of block.\n\n        Returns:\n            blocks_args: A list of BlockArgs namedtuples of block args.\n        \"\"\"\n        # \u89e3\u7801\u542b\u6709\u5b57\u7b26\u4e32\u7b26\u53f7\u7684\u5217\u8868\uff0c\u6765\u6307\u5b9a\u7f51\u7edc\u5185\u7684\u5757\u3002\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u5b57\u7b26\u4e32\u4ee3\u8868\u4e00\u4e2a\u4e0d\u540c\u7684\u5757\n\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n\n    @staticmethod\n    def encode(blocks_args):\n        \"\"\"Encode a list of BlockArgs to a list of strings.\n\n        Args:\n            blocks_args (list[namedtuples]): A list of BlockArgs namedtuples of block args.\n\n        Returns:\n            block_strings: A list of strings, each string is a notation of block.\n        \"\"\"\n\n        # \u4e0edecode(string_list)\u529f\u80fd\u76f8\u53cd\n\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(BlockDecoder._encode_block_string(block))\n        return block_strings\n</code></pre> </p> <p> Code7\uff1a\u4e3a\u6a21\u578b\u521b\u5efaBlockArgs\u548cGlobalParams <p><pre><code>def efficientnet(width_coefficient=None, depth_coefficient=None, image_size=None,dropout_rate=0.2, drop_connect_rate=0.2,num_classes=1000):\n    \"\"\"Create BlockArgs and GlobalParams for efficientnet model.\n\n    Args:\n        width_coefficient (float)\n        depth_coefficient (float)\n        image_size (int)\n        dropout_rate (float)\n        drop_connect_rate (float)\n        num_classes (int)\n\n        Meaning as the name suggests.\n\n    Returns:\n        blocks_args, global_params.\n    \"\"\"\n\n    # Blocks args for the whole model(efficientnet-b0 by default)\n    # It will be modified in the construction of EfficientNet Class according to model\n\n    # \u4e3a\u6574\u4e2a\u6a21\u578b\u8bbe\u7f6e Blocks args\n\n    blocks_args = [\n        'r1_k3_s11_e1_i32_o16_se0.25',\n        'r2_k3_s22_e6_i16_o24_se0.25',\n        'r2_k5_s22_e6_i24_o40_se0.25',\n        'r3_k3_s22_e6_i40_o80_se0.25',\n        'r3_k5_s11_e6_i80_o112_se0.25',\n        'r4_k5_s22_e6_i112_o192_se0.25',\n        'r1_k3_s11_e6_i192_o320_se0.25',\n    ]\n    blocks_args = BlockDecoder.decode(blocks_args)\n    # blocks_args\u4e3a\u5217\u8868\uff0c\u5217\u8868\u6bcf\u4e2a\u5143\u7d20\u4e3aBlockArgs\u5bf9\u8c61\uff0c\u5305\u542br\u3001k\u3001s\u3001e\u3001i\u3001o\u3001se\u7b49\u5c5e\u6027\n\n    global_params = GlobalParams(\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        image_size=image_size,\n        dropout_rate=dropout_rate,\n\n        num_classes=num_classes,\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        drop_connect_rate=drop_connect_rate,\n        depth_divisor=8, # \u5e38\u89c1\u7684\u6df1\u5ea6\u9664\u65708,16,64...\n        min_depth=None,\n    )\n\n    return blocks_args, global_params\n\n\ndef get_model_params(model_name, override_params):\n    \"\"\"Get the block args and global params for a given model name.\n\n    Args:\n        model_name (str): \u6a21\u578b\u540d\u79f0\uff0c\u5982\"efficientnet-b0\", \"efficientnet-b1\"\u7b49.\n        override_params (dict): \u4e00\u4e2a\u7528\u4e8e\u4fee\u6539\u5168\u5c40\u53c2\u6570global_params\u7684\u5b57\u5178.\n\n    Returns:\n        blocks_args, global_params\n    \"\"\"\n\n    # \u83b7\u53d6\u7ed9\u5b9a\u6a21\u578b\u540d\u79f0\u7684\u5757\u53c2\u6570\u548c\u5168\u5c40\u53c2\u6570\u3002\n\n    if model_name.startswith('efficientnet'):\n        w, d, s, p = efficientnet_params(model_name)\n        # \u8bfb\u53d6\u4f5c\u8005\u5df2\u51c6\u5907\u7684\u6a21\u578b\u53c2\u6570\uff0c\u5982efficientnet-b7\u7b49\n\n        # note: all models have drop connect rate = 0.2\n        blocks_args, global_params = efficientnet(\n            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n    else:\n        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n\n    if override_params:\n        # ValueError will be raised here if override_params has fields not included in global_params.\n        global_params = global_params._replace(**override_params)\n        # \u5c06override_params\u89e3\u5305\u8bfb\u5165\uff0c\u66ff\u6362\u539f\u672cglobal_params\u4e2d\u7684\u53c2\u6570\n\n        # _replace() \u662f Python \u4e2d namedtuple \u7c7b\u578b\u7684\u4e00\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 namedtuple \u5b9e\u4f8b\uff0c\u8be5\u5b9e\u4f8b\u662f\u5bf9\u539f\u6709\u5b9e\u4f8b\u8fdb\u884c\u6307\u5b9a\u5b57\u6bb5\u7684\u66ff\u6362\u540e\u751f\u6210\u7684\u3002\u5b83\u4e0d\u4f1a\u4fee\u6539\u539f\u6765\u7684 namedtuple\uff0c\u800c\u662f\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5b9e\u4f8b\u3002\n\n    return blocks_args, global_params\n</code></pre> </p> <li> <p>\u5377\u79ef\u6838\u7f29\u653e<code>round_filters</code>/<code>round_repeat</code></p> <p> Code\uff1around_filters/round_repeat <pre><code>def round_filters(filters, global_params):\n    \"\"\"Calculate and round number of filters based on width multiplier.\n    Use width_coefficient, depth_divisor and min_depth of global_params.\n\n    Args:\n        filters (int): Filters number to be calculated.\n        global_params (namedtuple): Global params of the model.\n\n    Returns:\n        new_filters: New filters number after calculating.\n    \"\"\"\n    multiplier = global_params.width_coefficient\n    if not multiplier:\n        return filters\n    # TODO: modify the params names.\n    #       maybe the names (width_divisor,min_width)\n    #       are more suitable than (depth_divisor,min_depth).\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor # pay attention to this line when using min_depth\n    # follow the formula transferred from official TensorFlow implementation\n    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n    if new_filters &lt; 0.9 * filters: # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    \"\"\"Calculate module's repeat number of a block based on depth multiplier.\n    Use depth_coefficient of global_params.\n\n    Args:\n        repeats (int): num_repeat to be calculated.\n        global_params (namedtuple): Global params of the model.\n\n    Returns:\n        new repeat: New repeat number after calculating.\n    \"\"\"\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    # follow the formula transferred from official TensorFlow implementation\n    return int(math.ceil(multiplier * repeats))\n</code></pre>"},{"location":"sci-paper/cs/EfficientNet/#modelpy","title":"<code>model.py</code>","text":"<p>\u539f\u59cb\u6a21\u578b<code>Efficient Net-B0</code>:</p> <p></p> <p>\u4e3b\u8981\u7684\u6784\u5efa\u6a21\u5757\u4e3a<code>Mobile inverted bottleneck MBConv</code>(\u51fa\u81eaMobileNet v2\u8bba\u6587)</p> <p></p> <p>\u5176\u4e2d\uff0c\u8be5\u6a21\u578b\u5c06<code>ReLU</code>\u6362\u6210\u4e86<code>Swish</code>\uff0c\u4f7f\u7528\u4e86<code>drop_connect</code>\u65b9\u6cd5\u6765\u4ee3\u66ff\u4f20\u7edf\u7684<code>dropout</code>\u65b9\u6cd5</p> <p>\\(\\alpha\uff0c\\beta\uff0c\\gamma\\)\u7f29\u653e\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u9996\u5148\u56fa\u5b9a\\(\\phi = 1\\)\uff0c\u5bf9\\(\\alpha\uff0c\\beta\uff0c\\gamma\\)\u8fdb\u884c<code>small grid search</code>,\u5728\\(\\alpha \\times \\beta^2 \\times \\gamma^2 \u2248 2\\)\u7ea6\u675f\u4e0b\uff0c\u627e\u5230\u6700\u4f18\u7684\\(\\alpha\uff0c\\beta\uff0c\\gamma\\)\uff0c\u6211\u4eec\u53d1\u73b0<code>EfficientNet-B0</code>\u7684\u6700\u4f73\u503c\u4e3a <code>\\alpha=1.2\uff0c\\beta=1.1\uff0c\\gamma=1.15</code></p> </li> <li> <p>\u5176\u6b21\uff0c\u6211\u4eec\u5c06\\(\\alpha\uff0c\\beta\uff0c\\gamma\\)\u56fa\u5b9a\u4e3a\u5e38\u6570\uff0c\u5e76\u7f29\u653e\u5177\u6709\u4e0d\u540c\\(\\phi\\)\u7684\u57fa\u7ebf\u7f51\u7edc\uff0c\u4ee5\u83b7\u5f97<code>EfficientNet-B1 \u5230 B7</code></p> </li> </ul> <p></p> <ul> <li> <p>\u4e3b\u8981\u6a21\u5757\uff1aMBConvBlock,\u5305\u542b\u5404\u79cd\u6784\u5efa\u6280\u5de7</p> <p> Code1\uff1aMBConvBlock <p><pre><code>class MBConvBlock(nn.Module):\n    \"\"\"Mobile Inverted Residual Bottleneck Block.\n\n    Args:\n        block_args (namedtuple): BlockArgs, defined in utils.py.\n        global_params (namedtuple): GlobalParam, defined in utils.py.\n        image_size (tuple or list): [image_height, image_width].\n\n    References:\n        [1] https://arxiv.org/abs/1704.04861 (MobileNet v1)\n        [2] https://arxiv.org/abs/1801.04381 (MobileNet v2)\n        [3] https://arxiv.org/abs/1905.02244 (MobileNet v3)\n    \"\"\"\n\n    def __init__(self, block_args, global_params, image_size=None):\n        super().__init__()\n        self._block_args = block_args \n        # \u5757\u53c2\u6570\n        self._bn_mom = 1 - global_params.batch_norm_momentum # pytorch's difference from tensorflow\n        self._bn_eps = global_params.batch_norm_epsilon\n        self.has_se = (self._block_args.se_ratio is not None) and (0 &lt; self._block_args.se_ratio &lt;= 1)\n        self.id_skip = block_args.id_skip  # whether to use skip connection and drop connect\n\n        # Expansion phase (Inverted Bottleneck) \u6269\u5f20\n        inp = self._block_args.input_filters  # number of input channels\n        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n\n        if self._block_args.expand_ratio != 1:\n            Conv2d = get_same_padding_conv2d(image_size=image_size) # \u9759\u6001 or \u52a8\u6001\n            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n            # image_size = calculate_output_image_size(image_size, 1) &lt;-- kernel_size=1 wouldn't modify image_size\n\n        # Depthwise convolution phase \u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        Conv2d = get_same_padding_conv2d(image_size=image_size)\n        self._depthwise_conv = Conv2d(\n            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n            kernel_size=k, stride=s, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n        image_size = calculate_output_image_size(image_size, s)\n\n        # Squeeze and Excitation layer, if desired\n        if self.has_se:\n            # \u5148AveragePooling()\n            Conv2d = get_same_padding_conv2d(image_size=(1,1))\n            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n        # Pointwise convolution phase \u9010\u70b9\u5377\u79ef\n        final_oup = self._block_args.output_filters\n        Conv2d = get_same_padding_conv2d(image_size=image_size)\n        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n        self._swish = MemoryEfficientSwish()\n\n    def forward(self, inputs, drop_connect_rate=None):\n        \"\"\"MBConvBlock's forward function.\n\n        Args:\n            inputs (tensor): Input tensor.\n            drop_connect_rate (bool): Drop connect rate (float, between 0 and 1).\n\n        Returns:\n            Output of this block after processing.\n        \"\"\"\n\n        # Expansion and Depthwise Convolution\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = self._expand_conv(inputs)\n            x = self._bn0(x)\n            x = self._swish(x)\n\n        x = self._depthwise_conv(x)\n        x = self._bn1(x)\n        x = self._swish(x)\n\n        # Squeeze and Excitation\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_reduce(x_squeezed)\n            x_squeezed = self._swish(x_squeezed)\n            x_squeezed = self._se_expand(x_squeezed)\n            x = torch.sigmoid(x_squeezed) * x\n\n        # Pointwise Convolution\n        x = self._project_conv(x)\n        x = self._bn2(x)\n\n        # Skip connection and drop connect\n        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n            # The combination of skip connection and drop connect brings about stochastic depth.\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs  # skip connection\n        return x\n\n    def set_swish(self, memory_efficient=True):\n        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n\n        Args:\n            memory_efficient (bool): Whether to use memory-efficient version of swish.\n        \"\"\"\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n</code></pre> </p> <li> <p>\u4e3b\u6a21\u578b\uff1a</p> <p> Code\uff1aEfficientNet <pre><code>class EfficientNet(nn.Module):\n    \"\"\"EfficientNet model.\n    Most easily loaded with the .from_name or .from_pretrained methods.\n\n    Args:\n        blocks_args (list[namedtuple]): A list of BlockArgs to construct blocks.\n        global_params (namedtuple): A set of GlobalParams shared between blocks.\n\n    References:\n        [1] https://arxiv.org/abs/1905.11946 (EfficientNet)\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from efficientnet.model import EfficientNet\n        &gt;&gt;&gt; inputs = torch.rand(1, 3, 224, 224)\n        &gt;&gt;&gt; model = EfficientNet.from_pretrained('efficientnet-b0')\n        &gt;&gt;&gt; model.eval()\n        &gt;&gt;&gt; outputs = model(inputs)\n    \"\"\"\n\n    def __init__(self, blocks_args=None, global_params=None):\n        super().__init__()\n        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n        assert len(blocks_args) &gt; 0, 'block args must be greater than 0'\n        self._global_params = global_params\n        self._blocks_args = blocks_args\n\n        # Batch norm parameters\n        bn_mom = 1 - self._global_params.batch_norm_momentum\n        bn_eps = self._global_params.batch_norm_epsilon\n\n        # Get stem static or dynamic convolution depending on image size\n        image_size = global_params.image_size\n        Conv2d = get_same_padding_conv2d(image_size=image_size)\n\n        # Stem\n        in_channels = 3  # rgb\n        out_channels = round_filters(32, self._global_params)  # number of output channels\n        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n        image_size = calculate_output_image_size(image_size, 2)\n\n        # Build blocks\n        self._blocks = nn.ModuleList([])\n        for block_args in self._blocks_args:\n\n            # Update block input and output filters based on depth multiplier.\n            block_args = block_args._replace(\n                input_filters=round_filters(block_args.input_filters, self._global_params),\n                output_filters=round_filters(block_args.output_filters, self._global_params),\n                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n            )\n\n            # The first block needs to take care of stride and filter size increase.\n            self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n            image_size = calculate_output_image_size(image_size, block_args.stride)\n            if block_args.num_repeat &gt; 1: # modify block_args to keep same output size\n                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n            for _ in range(block_args.num_repeat - 1):\n                self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n                # image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1\n\n        # Head\n        in_channels = block_args.output_filters  # output of final block\n        out_channels = round_filters(1280, self._global_params)\n        # \u5168\u8fde\u63a5\u5c42\u4e5f\u7f29\u653e\n        Conv2d = get_same_padding_conv2d(image_size=image_size)\n        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Final linear layer\n        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n        self._dropout = nn.Dropout(self._global_params.dropout_rate)\n        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n        self._swish = MemoryEfficientSwish()\n\n    def set_swish(self, memory_efficient=True):\n        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n\n        Args:\n            memory_efficient (bool): Whether to use memory-efficient version of swish.\n\n        \"\"\"\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n        for block in self._blocks:\n            block.set_swish(memory_efficient)\n\n    def extract_endpoints(self, inputs):\n        \"\"\"Use convolution layer to extract features\n        from reduction levels i in [1, 2, 3, 4, 5].\n\n        Args:\n            inputs (tensor): Input tensor.\n\n        Returns:\n            Dictionary of last intermediate features\n            with reduction levels i in [1, 2, 3, 4, 5].\n            Example:\n                &gt;&gt;&gt; import torch\n                &gt;&gt;&gt; from efficientnet.model import EfficientNet\n                &gt;&gt;&gt; inputs = torch.rand(1, 3, 224, 224)\n                &gt;&gt;&gt; model = EfficientNet.from_pretrained('efficientnet-b0')\n                &gt;&gt;&gt; endpoints = model.extract_endpoints(inputs)\n                &gt;&gt;&gt; print(endpoints['reduction_1'].shape)  # torch.Size([1, 16, 112, 112])\n                &gt;&gt;&gt; print(endpoints['reduction_2'].shape)  # torch.Size([1, 24, 56, 56])\n                &gt;&gt;&gt; print(endpoints['reduction_3'].shape)  # torch.Size([1, 40, 28, 28])\n                &gt;&gt;&gt; print(endpoints['reduction_4'].shape)  # torch.Size([1, 112, 14, 14])\n                &gt;&gt;&gt; print(endpoints['reduction_5'].shape)  # torch.Size([1, 1280, 7, 7])\n        \"\"\"\n        endpoints = dict()\n\n        # Stem\n        x = self._swish(self._bn0(self._conv_stem(inputs)))\n        prev_x = x\n\n        # Blocks\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self._blocks) # scale drop connect_rate\n            x = block(x, drop_connect_rate=drop_connect_rate)\n            if prev_x.size(2) &gt; x.size(2):\n                endpoints[f'reduction_{len(endpoints)+1}'] = prev_x\n            prev_x = x\n\n        # Head\n        x = self._swish(self._bn1(self._conv_head(x)))\n        endpoints[f'reduction_{len(endpoints)+1}'] = x\n\n        return endpoints\n\n    def extract_features(self, inputs):\n        \"\"\"use convolution layer to extract feature .\n\n        Args:\n            inputs (tensor): Input tensor.\n\n        Returns:\n            Output of the final convolution \n            layer in the efficientnet model.\n        \"\"\"\n        # Stem\n        x = self._swish(self._bn0(self._conv_stem(inputs)))\n\n        # Blocks\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self._blocks) # scale drop connect_rate\n                # \u6839\u636e\u7f51\u7edc\u6df1\u5ea6\u7684\u589e\u52a0\u6765\u589e\u5927 drop_connect_rate\uff0c\u5f00\u59cb\u65f6drop_connect_rate\u8f83\u5c0f\uff08drop\u6982\u7387\u5c0f\uff09,\u540e\u7eed\u4f9d\u6b21\u589e\u5927\n            x = block(x, drop_connect_rate=drop_connect_rate)\n\n        # Head\n        x = self._swish(self._bn1(self._conv_head(x)))\n\n        return x\n\n    def forward(self, inputs):\n        \"\"\"EfficientNet's forward function.\n        Calls extract_features to extract features, applies final linear layer, and returns logits.\n\n        Args:\n            inputs (tensor): Input tensor.\n\n        Returns:\n            Output of this model after processing.\n        \"\"\"\n        # Convolution layers\n        x = self.extract_features(inputs)\n\n        # Pooling and final linear layer\n        x = self._avg_pooling(x)\n        x = x.flatten(start_dim=1)\n        x = self._dropout(x)\n        x = self._fc(x)\n\n        return x\n\n    @classmethod\n    def from_name(cls, model_name, in_channels=3, **override_params):\n        # \u52a0\u8f7d\u6a21\u578b\n\n        \"\"\"create an efficientnet model according to name.\n\n        Args:\n            model_name (str): Name for efficientnet.\n            in_channels (int): Input data's channel number.\n            override_params (other key word params): \n                Params to override model's global_params.\n                Optional key:\n                    'width_coefficient', 'depth_coefficient',\n                    'image_size', 'dropout_rate',\n                    'num_classes', 'batch_norm_momentum',\n                    'batch_norm_epsilon', 'drop_connect_rate',\n                    'depth_divisor', 'min_depth'\n\n        Returns:\n            An efficientnet model.\n        \"\"\"\n        cls._check_model_name_is_valid(model_name)\n        blocks_args, global_params = get_model_params(model_name, override_params)\n        model = cls(blocks_args, global_params)\n        model._change_in_channels(in_channels)\n        return model\n\n    @classmethod\n    def from_pretrained(cls, model_name, weights_path=None, advprop=False, \n                        in_channels=3, num_classes=1000, **override_params):\n        # \u52a0\u8f7d\u9884\u8bad\u7ec3\uff08pretrained\uff09\u7684\u6a21\u578b\n\n        \"\"\"create an efficientnet model according to name.\n\n        Args:\n            model_name (str): Name for efficientnet.\n            weights_path (None or str): \n                str: path to pretrained weights file on the local disk.\n                None: use pretrained weights downloaded from the Internet.\n            advprop (bool): \n                Whether to load pretrained weights\n                trained with advprop (valid when weights_path is None).\n            in_channels (int): Input data's channel number.\n            num_classes (int): \n                Number of categories for classification.\n                It controls the output size for final linear layer.\n            override_params (other key word params): \n                Params to override model's global_params.\n                Optional key:\n                    'width_coefficient', 'depth_coefficient',\n                    'image_size', 'dropout_rate',\n                    'num_classes', 'batch_norm_momentum',\n                    'batch_norm_epsilon', 'drop_connect_rate',\n                    'depth_divisor', 'min_depth'\n\n        Returns:\n            A pretrained efficientnet model.\n        \"\"\"\n        model = cls.from_name(model_name, num_classes = num_classes, **override_params)\n        load_pretrained_weights(model, model_name, weights_path=weights_path, load_fc=(num_classes == 1000), advprop=advprop)\n        model._change_in_channels(in_channels)\n        return model\n\n    @classmethod\n    def get_image_size(cls, model_name):\n        \"\"\"Get the input image size for a given efficientnet model.\n\n        Args:\n            model_name (str): Name for efficientnet.\n\n        Returns:\n            Input image size (resolution).\n        \"\"\"\n        cls._check_model_name_is_valid(model_name)\n        _, _, res, _ = efficientnet_params(model_name) \n        # \u8fd4\u56deparameter coefficients\u7684resolution(\u56fe\u7247\u5206\u8fa8\u7387)\n        return res\n\n    @classmethod\n    def _check_model_name_is_valid(cls, model_name):\n        \"\"\"Validates model name. \n\n        Args:\n            model_name (str): Name for efficientnet.\n\n        Returns:\n            bool: Is a valid name or not.\n        \"\"\"\n        if model_name not in VALID_MODELS:\n            raise ValueError('model_name should be one of: ' + ', '.join(VALID_MODELS))\n\n    def _change_in_channels(self, in_channels):\n        \"\"\"Adjust model's first convolution layer to in_channels, if in_channels not equals 3.\n\n        Args:\n            in_channels (int): Input data's channel number.\n        \"\"\"\n        if in_channels != 3:\n            Conv2d = get_same_padding_conv2d(image_size = self._global_params.image_size)\n            out_channels = round_filters(32, self._global_params)\n            self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n</code></pre>"},{"location":"sci-paper/cs/EfficientNet/#test_modelpy","title":"<code>test_model.py</code>","text":"<ul> <li> <p>test_model</p> <p> Code4\uff1a\u67e5\u8be2\u5404\u4e2a\u6a21\u578b\u7684\u590d\u5408\u7cfb\u6570 <p><pre><code>from collections import OrderedDict\n\nimport pytest\nimport torch\nimport torch.nn as nn\n\nfrom efficientnet_pytorch import EfficientNet\n\n\n# -- fixtures -------------------------------------------------------------------------------------\n\n@pytest.fixture(scope='module', params=[x for x in range(4)])\ndef model(request):\n    return 'efficientnet-b{}'.format(request.param)\n\n\n@pytest.fixture(scope='module', params=[True, False])\ndef pretrained(request):\n    return request.param\n\n\n@pytest.fixture(scope='function')\ndef net(model, pretrained):\n    return EfficientNet.from_pretrained(model) if pretrained else EfficientNet.from_name(model)\n\n\n# -- tests ----------------------------------------------------------------------------------------\n\n@pytest.mark.parametrize('img_size', [224, 256, 512])\ndef test_forward(net, img_size):\n    \"\"\"Test `.forward()` doesn't throw an error\"\"\"\n    data = torch.zeros((1, 3, img_size, img_size))\n    output = net(data)\n    assert not torch.isnan(output).any()\n\n\ndef test_dropout_training(net):\n    \"\"\"Test dropout `.training` is set by `.train()` on parent `nn.module`\"\"\"\n    net.train()\n    assert net._dropout.training == True\n\n\ndef test_dropout_eval(net):\n    \"\"\"Test dropout `.training` is set by `.eval()` on parent `nn.module`\"\"\"\n    net.eval()\n    assert net._dropout.training == False\n\n\ndef test_dropout_update(net):\n    \"\"\"Test dropout `.training` is updated by `.train()` and `.eval()` on parent `nn.module`\"\"\"\n    net.train()\n    assert net._dropout.training == True\n    net.eval()\n    assert net._dropout.training == False\n    net.train()\n    assert net._dropout.training == True\n    net.eval()\n    assert net._dropout.training == False\n\n\n@pytest.mark.parametrize('img_size', [224, 256, 512])\ndef test_modify_dropout(net, img_size):\n    \"\"\"Test ability to modify dropout and fc modules of network\"\"\"\n    dropout = nn.Sequential(OrderedDict([\n        ('_bn2', nn.BatchNorm1d(net._bn1.num_features)),\n        ('_drop1', nn.Dropout(p=net._global_params.dropout_rate)),\n        ('_linear1', nn.Linear(net._bn1.num_features, 512)),\n        ('_relu', nn.ReLU()),\n        ('_bn3', nn.BatchNorm1d(512)),\n        ('_drop2', nn.Dropout(p=net._global_params.dropout_rate / 2))\n    ]))\n    fc = nn.Linear(512, net._global_params.num_classes)\n\n    net._dropout = dropout\n    net._fc = fc\n\n    data = torch.zeros((2, 3, img_size, img_size))\n    output = net(data)\n    assert not torch.isnan(output).any()\n\n\n@pytest.mark.parametrize('img_size', [224, 256, 512])\ndef test_modify_pool(net, img_size):\n    \"\"\"Test ability to modify pooling module of network\"\"\"\n\n    class AdaptiveMaxAvgPool(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            self.ada_avgpool = nn.AdaptiveAvgPool2d(1)\n            self.ada_maxpool = nn.AdaptiveMaxPool2d(1)\n\n        def forward(self, x):\n            avg_x = self.ada_avgpool(x)\n            max_x = self.ada_maxpool(x)\n            x = torch.cat((avg_x, max_x), dim=1)\n            return x\n\n    avg_pooling = AdaptiveMaxAvgPool()\n    fc = nn.Linear(net._fc.in_features * 2, net._global_params.num_classes)\n\n    net._avg_pooling = avg_pooling\n    net._fc = fc\n\n    data = torch.zeros((2, 3, img_size, img_size))\n    output = net(data)\n    assert not torch.isnan(output).any()\n\n\n@pytest.mark.parametrize('img_size', [224, 256, 512])\ndef test_extract_endpoints(net, img_size):\n    \"\"\"Test `.extract_endpoints()` doesn't throw an error\"\"\"\n    data = torch.zeros((1, 3, img_size, img_size))\n    endpoints = net.extract_endpoints(data)\n    assert not torch.isnan(endpoints['reduction_1']).any()\n    assert not torch.isnan(endpoints['reduction_2']).any()\n    assert not torch.isnan(endpoints['reduction_3']).any()\n    assert not torch.isnan(endpoints['reduction_4']).any()\n    assert not torch.isnan(endpoints['reduction_5']).any()\n    assert endpoints['reduction_1'].size(2) == img_size // 2\n    assert endpoints['reduction_2'].size(2) == img_size // 4\n    assert endpoints['reduction_3'].size(2) == img_size // 8\n    assert endpoints['reduction_4'].size(2) == img_size // 16\n    assert endpoints['reduction_5'].size(2) == img_size // 32\n</code></pre> &lt;\\details&gt; </p>"},{"location":"sci-paper/cs/EfficientNet/#mainpy","title":"<code>main.py</code>","text":"<ul> <li> <p>argparse\u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570</p> <p> Code1\uff1aMBConvBlock <p><pre><code>import argparse\nfrom efficientnet_pytorch import EfficientNet\n\nparser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n\nparser.add_argument('data', metavar='DIR',\n                    help='path to dataset')\n# \u6307\u5b9a\u6570\u636e\u96c6\u7684\u8def\u5f84\nparser.add_argument('-a', '--arch', metavar='ARCH', default='resnet18',\n                    help='model architecture (default: resnet18)')\n# \u6307\u5b9a\u6a21\u578b\u7684\u67b6\u6784\nparser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n                    help='number of data loading workers (default: 4)')\n# \u6307\u5b9a\u6570\u636e\u52a0\u8f7d\u7684\u5de5\u4f5c\u7ebf\u7a0b\u6570\u91cf\uff0c\u9ed8\u8ba4\u503c\u662f4\nparser.add_argument('--epochs', default=90, type=int, metavar='N',\n                    help='number of total epochs to run')\n# \u6307\u5b9a\u8981\u8fd0\u884c\u7684\u603b\u8bad\u7ec3\u8f6e\u6570\uff0c\u9ed8\u8ba4\u503c\u662f90\nparser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n                    help='manual epoch number (useful on restarts)')\n# \u6307\u5b9a\u624b\u52a8\u8bbe\u7f6e\u7684\u521d\u59cb\u8bad\u7ec3\u5468\u671f\uff0c\u9ed8\u8ba4\u503c\u662f0\uff0c\u7528\u4e8e\u8bad\u7ec3\u91cd\u542f\u65f6\nparser.add_argument('-b', '--batch-size', default=256, type=int,\n                    metavar='N',\n                    help='mini-batch size (default: 256), this is the total '\n                        'batch size of all GPUs on the current node when '\n                        'using Data Parallel or Distributed Data Parallel')\n# \u6307\u5b9a\u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u7684\u5927\u5c0f\uff0c\u9ed8\u8ba4\u503c\u662f256\nparser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n                    metavar='LR', help='initial learning rate', dest='lr')\n# \u6307\u5b9a\u521d\u59cb\u5b66\u4e60\u7387\uff0c\u9ed8\u8ba4\u503c\u662f0.1\nparser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n                    help='momentum')\n# \u6307\u5b9a\u52a8\u91cf\uff0c\u9ed8\u8ba4\u503c\u662f0.9\nparser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n                    metavar='W', help='weight decay (default: 1e-4)',\n                    dest='weight_decay')\n# \u6307\u5b9a\u6743\u91cd\u8870\u51cf\uff08\u6b63\u5219\u5316\uff09\uff0c\u9ed8\u8ba4\u503c\u662f1e-4\nparser.add_argument('-p', '--print-freq', default=10, type=int,\n                    metavar='N', help='print frequency (default: 10)')\n# \u6307\u5b9a\u6253\u5370\u9891\u7387\uff08\u591a\u5c11\u6279\u6b21\u6253\u5370\u4e00\u6b21\uff09\uff0c\u9ed8\u8ba4\u503c\u662f10\nparser.add_argument('--resume', default='', type=str, metavar='PATH',\n                    help='path to latest checkpoint (default: none)')\n# \u6307\u5b9a\u6700\u65b0\u68c0\u67e5\u70b9\u7684\u8def\u5f84\uff0c\u9ed8\u8ba4\u503c\u662f\u7a7a\u5b57\u7b26\u4e32\nparser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n                    help='evaluate model on validation set')\n# \u6307\u5b9a\u662f\u5426\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\nparser.add_argument('--pretrained', dest='pretrained', action='store_true',\n                    help='use pre-trained model')\n# \u6307\u5b9a\u662f\u5426\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\nparser.add_argument('--world-size', default=-1, type=int,\n                    help='number of nodes for distributed training')\n# \u6307\u5b9a\u7528\u4e8e\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u8282\u70b9\u6570\uff0c\u9ed8\u8ba4\u503c\u662f-1\nparser.add_argument('--rank', default=-1, type=int,\n                    help='node rank for distributed training')\n# \u6307\u5b9a\u7528\u4e8e\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u8282\u70b9\u6392\u540d\uff0c\u9ed8\u8ba4\u503c\u662f-1\nparser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,\n                    help='url used to set up distributed training')\n# \u6307\u5b9a\u7528\u4e8e\u8bbe\u7f6e\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684URL\uff0c\u9ed8\u8ba4\u503c\u662ftcp://224.66.41.62:23456\nparser.add_argument('--dist-backend', default='nccl', type=str,\n                    help='distributed backend')\n# \u6307\u5b9a\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u540e\u7aef\uff0c\u9ed8\u8ba4\u503c\u662fnccl\nparser.add_argument('--seed', default=None, type=int,\n                    help='seed for initializing training. ')\n#\u6307\u5b9a\u521d\u59cb\u5316\u8bad\u7ec3\u7684\u79cd\u5b50\u503c\nparser.add_argument('--gpu', default=None, type=int,\n                    help='GPU id to use.')\n# \u6307\u5b9a\u4f7f\u7528\u7684GPU ID\nparser.add_argument('--image_size', default=224, type=int,\n                    help='image size')\n# \u6307\u5b9a\u56fe\u50cf\u5927\u5c0f\uff0c\u9ed8\u8ba4\u503c\u662f224\nparser.add_argument('--advprop', default=False, action='store_true',\n                    help='use advprop or not')\n# \u6307\u5b9a\u662f\u5426\u4f7f\u7528advprop\nparser.add_argument('--multiprocessing-distributed', action='store_true',\n                    help='Use multi-processing distributed training to launch '\n                        'N processes per node, which has N GPUs. This is the '\n                        'fastest way to use PyTorch for either single node or '\n                        'multi node data parallel training')\n# \u6307\u5b9a\u662f\u5426\u4f7f\u7528\u591a\u8fdb\u7a0b\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8N\u4e2a\u8fdb\u7a0b\uff0c\u6bcf\u4e2a\u8282\u70b9\u6709N\u4e2aGPU\u3002\u8fd9\u662f\u4f7f\u7528PyTorch\u8fdb\u884c\u5355\u8282\u70b9\u6216\u591a\u8282\u70b9\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u7684\u6700\u5feb\u65b9\u5f0f\u3002\n\nbest_acc1 = 0\n</code></pre> </p> <li> <p>\u5b66\u4e60\u7387\u8870\u51cf<code>learning_rate</code></p> <p> Code\uff1aadjust_learning_rate <p><pre><code>\"\"\"optimizer.param_groups-&gt;list\uff0c\u5176\u4e2d\u7684\u5143\u7d20\u662f\u5b57\u5178\uff0c\u6bcf\u4e2a\u5b57\u5178\u4ee3\u8868\u4e00\u4e2a\u53c2\u6570\u7ec4\uff0c\u6bcf\u4e2a\u53c2\u6570\u7ec4\u5305\u542b\u4f18\u5316\u5668\u7279\u6709\u7684\u5143\u6570\u636e\uff0c\u4f8b\u5982\u5b66\u4e60\u7387\u548c\u6743\u91cd\u8870\u51cf\uff0c\u4ee5\u53ca\u7ec4\u4e2d\u53c2\u6570\u7684\u53c2\u6570 ID \u5217\u8868\n\n \u6bcf\u4e2a\u53c2\u6570\u7ec4\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a\n    'params'\uff1a\u4e00\u4e2a\u53c2\u6570\u5217\u8868\u6216\u5143\u7ec4\uff0c\u5305\u542b\u4e86\u9700\u8981\u4f18\u5316\u7684\u53c2\u6570\u3002\n    'lr'\uff1a\u5b66\u4e60\u7387\uff0c\u7528\u4e8e\u66f4\u65b0\u53c2\u6570\u3002\n    'weight_decay'\uff1a\u6743\u91cd\u8870\u51cf\uff08\u6b63\u5219\u5316\uff09\uff0c\u7528\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\u3002\n    'momentum'\uff1a\u52a8\u91cf\uff0c\u7528\u4e8e\u52a0\u901f\u68af\u5ea6\u4e0b\u964d\u3002\n    ...\n\"\"\"\n\ndef adjust_learning_rate(optimizer, epoch, args):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n    lr = args.lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr # \u66f4\u65b0\u5b66\u4e60\u7387\n</code></pre> </p> <li> <p><code>Top-K</code>\u7cbe\u5ea6\u8ba1\u7b97</p> <p> Code\uff1aTop-K <p><pre><code>def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\n\n        topk (tuple): \u4e00\u4e2a\u5305\u542b\u591a\u4e2a k \u503c\u7684\u5143\u7ec4\uff0c\u7528\u4e8e\u8ba1\u7b97\u524d k \u4e2a\u9884\u6d4b\u7684\u51c6\u786e\u7387\u3002\u9ed8\u8ba4\u503c\u662f (1,)\uff0c\u5373\u53ea\u8ba1\u7b97 top-1 \u51c6\u786e\u7387\u3002\n\n    \"\"\"\n    with torch.no_grad():  # \u5fc5\u8981\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True) # \u8fd4\u56de\u524d maxk \u4e2a\u9884\u6d4b\u7ed3\u679c\u53ca\u5176\u5bf9\u5e94\u7684\u7d22\u5f15\n        # torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)\n        # .topk()'s output: (values, indices)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n</code></pre> </p> <li> <p><code>main</code>\u51fd\u6570</p> <p><code>main</code>\u4e3b\u7a0b\u5e8f\uff0c\u5176\u4e2d\u6d89\u53ca\u53c2\u6570\u89e3\u6790\u3001\u968f\u673a\u79cd\u5b50\u8bbe\u7f6e\u3001\u8bbe\u5907\u8bbe\u7f6e\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3\u8bbe\u7f6e\u3001\u6a21\u578b\u521b\u5efa\u3001\u4f18\u5316\u5668\u8bbe\u7f6e\u3001\u6570\u636e\u52a0\u8f7d\u3001\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7b49\u6b65\u9aa4</p> <p> Code\uff1aaccuracy <p><pre><code>def main():\n    args = parser.parse_args()\n\n    if args.seed is not None:\n        random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        cudnn.deterministic = True # \u5f3a\u5236cuDNN\u4f7f\u7528\u786e\u5b9a\u6027\u7684\u7b97\u6cd5,\u4ece\u800c\u4fdd\u8bc1\u7ed3\u679c\u7684\u4e00\u81f4\u6027\n        warnings.warn('You have chosen to seed training. '\n                    'This will turn on the CUDNN deterministic setting, '\n                    'which can slow down your training considerably! '\n                    'You may see unexpected behavior when restarting '\n                    'from checkpoints.')\n\n    if args.gpu is not None:\n        warnings.warn('You have chosen a specific GPU. This will completely '\n                    'disable data parallelism.')\n\n    if args.dist_url == \"env://\" and args.world_size == -1:\n        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n\n    args.distributed = args.world_size &gt; 1 or args.multiprocessing_distributed\n\n    ngpus_per_node = torch.cuda.device_count()\n    if args.multiprocessing_distributed:\n        # Since we have ngpus_per_node processes per node, the total world_size\n        # needs to be adjusted accordingly\n        args.world_size = ngpus_per_node * args.world_size\n        # Use torch.multiprocessing.spawn to launch distributed processes: the\n        # main_worker process function\n        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n    else:\n        # Simply call main_worker function\n        main_worker(args.gpu, ngpus_per_node, args)\n\ndef main_worker(gpu, ngpus_per_node, args):\n    global best_acc1\n    args.gpu = gpu\n\n    if args.gpu is not None:\n        print(\"Use GPU: {} for training\".format(args.gpu))\n\n    if args.distributed:\n        if args.dist_url == \"env://\" and args.rank == -1:\n            args.rank = int(os.environ[\"RANK\"])\n        if args.multiprocessing_distributed:\n            # For multiprocessing distributed training, rank needs to be the\n            # global rank among all the processes\n            args.rank = args.rank * ngpus_per_node + gpu\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                world_size=args.world_size, rank=args.rank)\n    # create model\n    if 'efficientnet' in args.arch:  # NEW\n        if args.pretrained:\n            model = EfficientNet.from_pretrained(args.arch, advprop=args.advprop)\n            print(\"=&gt; using pre-trained model '{}'\".format(args.arch))\n        else:\n            print(\"=&gt; creating model '{}'\".format(args.arch))\n            model = EfficientNet.from_name(args.arch)\n\n    else:\n        if args.pretrained:\n            print(\"=&gt; using pre-trained model '{}'\".format(args.arch))\n            model = models.__dict__[args.arch](pretrained=True)\n        else:\n            print(\"=&gt; creating model '{}'\".format(args.arch))\n            model = models.__dict__[args.arch]()\n\n    if args.distributed:\n        # For multiprocessing distributed, DistributedDataParallel constructor\n        # should always set the single device scope, otherwise,\n        # DistributedDataParallel will use all available devices.\n        if args.gpu is not None:\n            torch.cuda.set_device(args.gpu)\n            model.cuda(args.gpu)\n            # When using a single GPU per process and per\n            # DistributedDataParallel, we need to divide the batch size\n            # ourselves based on the total number of GPUs we have\n            args.batch_size = int(args.batch_size / ngpus_per_node)\n            args.workers = int(args.workers / ngpus_per_node)\n            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n        else:\n            model.cuda()\n            # DistributedDataParallel will divide and allocate batch_size to all\n            # available GPUs if device_ids are not set\n            model = torch.nn.parallel.DistributedDataParallel(model)\n    elif args.gpu is not None:\n        torch.cuda.set_device(args.gpu)\n        model = model.cuda(args.gpu)\n    else:\n        # DataParallel will divide and allocate batch_size to all available GPUs\n        if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n            model.features = torch.nn.DataParallel(model.features)\n            model.cuda()\n        else:\n            model = torch.nn.DataParallel(model).cuda()\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n\n    optimizer = torch.optim.RMSprop(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(\"=&gt; loading checkpoint '{}'\".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint['epoch']\n            best_acc1 = checkpoint['best_acc1']\n            if args.gpu is not None:\n                # best_acc1 may be from a checkpoint from a different GPU\n                best_acc1 = best_acc1.to(args.gpu)\n            model.load_state_dict(checkpoint['state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            print(\"=&gt; loaded checkpoint '{}' (epoch {})\"\n                .format(args.resume, checkpoint['epoch']))\n        else:\n            print(\"=&gt; no checkpoint found at '{}'\".format(args.resume))\n\n    cudnn.benchmark = True\n\n    # Data loading code\n    traindir = os.path.join(args.data, 'train')\n    valdir = os.path.join(args.data, 'val')\n    if args.advprop:\n        normalize = transforms.Lambda(lambda img: img * 2.0 - 1.0)\n    else:\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                        std=[0.229, 0.224, 0.225])\n        # \u5e38\u7528\u7684 ImageNet \u5f52\u4e00\u5316\u53c2\u6570\n\n    if 'efficientnet' in args.arch:\n        image_size = EfficientNet.get_image_size(args.arch)\n    else:\n        image_size = args.image_size\n\n    # \u7684\u56fe\u50cf\u9884\u5904\u7406\u548c\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\n    train_dataset = datasets.ImageFolder(\n        traindir,\n        transforms.Compose([\n            transforms.RandomResizedCrop(image_size), # \u968f\u673a\u88c1\u526a\n            transforms.RandomHorizontalFlip(), # \u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    if args.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    else:\n        train_sampler = None\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n        num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n\n    val_transforms = transforms.Compose([\n        transforms.Resize(image_size, interpolation=PIL.Image.BICUBIC), # \u53cc\u7acb\u65b9\u63d2\u503c\u6cd5\n        transforms.CenterCrop(image_size),\n        transforms.ToTensor(),\n        normalize,\n    ])\n    print('Using image size', image_size)\n\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(valdir, val_transforms),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    # 'pin_memory' \u53c2\u6570\u51b3\u5b9a\u662f\u5426\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u56fa\u5b9a\u5185\u5b58\uff08\u9875\u9501\u5b9a\u5185\u5b58\uff09\u4e2d\u3002\n    # \u8bbe\u7f6e\u4e3a True \u65f6\uff0c\u6570\u636e\u5c06\u52a0\u8f7d\u5230\u56fa\u5b9a\u5185\u5b58\u4e2d\uff0c\u7136\u540e\u518d\u4f20\u8f93\u5230 GPU\uff0c\u8fd9\u6837\u53ef\u4ee5\u52a0\u5feb\u6570\u636e\u4f20\u8f93\u901f\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528 GPU \u8bad\u7ec3\u65f6\u3002\n\n    if args.evaluate:\n        res = validate(val_loader, model, criterion, args)\n        with open('res.txt', 'w') as f:\n            print(res, file=f)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            train_sampler.set_epoch(epoch)\n        adjust_learning_rate(optimizer, epoch, args) # \u8c03\u6574\u5b66\u4e60\u7387\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch, args)\n\n        # evaluate on validation set\n        acc1 = validate(val_loader, model, criterion, args)\n\n        # remember best acc@1 and save checkpoint\n        is_best = acc1 &gt; best_acc1\n        best_acc1 = max(acc1, best_acc1)\n\n        if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n                and args.rank % ngpus_per_node == 0):\n\n            save_checkpoint({\n                'epoch': epoch + 1,\n                'arch': args.arch,\n                'state_dict': model.state_dict(),\n                'best_acc1': best_acc1,\n                'optimizer' : optimizer.state_dict(),\n            }, is_best)\n\n            # 'save_checkpoint'\u51fd\u6570\u4f1a\u6839\u636e'is_best'\u53c2\u6570\u51b3\u5b9a\u662f\u5426\u66f4\u65b0\u6700\u4f73\u6a21\u578b\u6587\u4ef6\u3002\n</code></pre> </p> <li> <p>\u8bad\u7ec3\u4e0e\u6d4b\u8bd5</p> <p> Code\uff1atrain/validate <pre><code>def train(train_loader, model, criterion, optimizer, epoch, args):\n    batch_time = AverageMeter('Time', ':6.3f')\n    data_time = AverageMeter('Data', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    progress = ProgressMeter(len(train_loader), batch_time, data_time, losses, top1,\n                            top5, prefix=\"Epoch: [{}]\".format(epoch))\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (images, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if args.gpu is not None:\n            images = images.cuda(args.gpu, non_blocking=True)\n        target = target.cuda(args.gpu, non_blocking=True)\n\n        # compute output \u524d\u5411\u4f20\u64ad\u548c\u635f\u5931\u8ba1\u7b97\uff1a\n        output = model(images)\n        loss = criterion(output, target)\n\n        # measure accuracy and record loss \u8ba1\u7b97\u51c6\u786e\u7387\u5e76\u8bb0\u5f55\u635f\u5931\uff1a\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), images.size(0))\n        top1.update(acc1[0], images.size(0))\n        top5.update(acc5[0], images.size(0))\n\n        # compute gradient and do SGD step \u53cd\u5411\u4f20\u64ad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time \u6d4b\u91cf\u6279\u6b21\u8017\u65f6\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            progress.print(i)\n\n\ndef validate(val_loader, model, criterion, args):\n    batch_time = AverageMeter('Time', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    progress = ProgressMeter(len(val_loader), batch_time, losses, top1, top5,\n                            prefix='Test: ')\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        end = time.time()\n        for i, (images, target) in enumerate(val_loader):\n            if args.gpu is not None:\n                images = images.cuda(args.gpu, non_blocking=True)\n            target = target.cuda(args.gpu, non_blocking=True)\n\n            # compute output\n            output = model(images)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            losses.update(loss.item(), images.size(0))\n            top1.update(acc1[0], images.size(0))\n            top5.update(acc5[0], images.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                progress.print(i)\n\n        # TODO: this should also be done with the ProgressMeter\n        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n            .format(top1=top1, top5=top5))\n\n    return top1.avg\n</code></pre> <li> <p>\u8bad\u7ec3\u8ddf\u8e2a\u8fdb\u5ea6</p> <p> Code <p><pre><code>class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    #\u7528\u4e8e\u7edf\u8ba1\u548c\u5b58\u50a8\u5e73\u5747\u503c\u548c\u5f53\u524d\u503c\u7684\u7c7b`AverageMeter`,\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u5e38\u7528\u4e8e\u8bb0\u5f55\u635f\u5931\u3001\u51c6\u786e\u7387\u7b49\u6307\u6807\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset() # \u91cd\u7f6e\u6240\u6709\u7edf\u8ba1\u53d8\u91cf\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n\n    # \u7528\u4e8e\u663e\u793a\u8bad\u7ec3\u8fdb\u5ea6\u7684\u7c7b`ProgressMeter`\uff0c\u53ef\u4ee5\u5e2e\u52a9\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ee5\u683c\u5f0f\u5316\u7684\u65b9\u5f0f\u8f93\u51fa\u6bcf\u4e2a\u6279\u6b21\u7684\u4fe1\u606f\uff0c\u5305\u62ec\u6279\u6b21\u7f16\u53f7\u548c\u5404\u79cd\u6307\u6807\uff08\u5982\u635f\u5931\u3001\u51c6\u786e\u7387\u7b49\uff09\n    def __init__(self, num_batches, *meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def print(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n</code></pre> </p> <p>\u4e8b\u5b9e\u4e0a\uff0c<code>EfficientNet</code>\u6a21\u578b\u867d\u7136\u6709\u7740\u5f88\u4f4e\u7684<code>FLOPs</code>\uff0c\u5374\u4f34\u968f\u7740\u8f83\u9ad8\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u6bd4\u5982B3\u7248\u672c\u7684FLOPs\u4e0d\u5230ResNet50\u7684\u4e00\u534a\uff0c\u63a8\u7406\u901f\u5ea6\u5374\u662fResNet50\u7684\u4e24\u500d</p> <p>\u53ef\u77e5\uff0c<code>EfficientNet</code>\u6a21\u578b\u662f\u4e00\u79cd\u4f4eFLOPs\u3001\u9ad8\u6570\u636e\u8bfb\u5199\u91cf\u7684\u6a21\u578b\uff0c\u7531\u4e8e<code>Depthwise Conv</code>\u64cd\u4f5c\uff0c\u4f7f\u5f97\u6a21\u578b\u628a\u5927\u91cf\u7684\u65f6\u95f4\u6d6a\u8d39\u5728\u4e86\u4ece\u663e\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\u4e0a\uff0c\u4ece\u800c\u5bfc\u81f4GPU\u7684\u7b97\u529b\u6ca1\u6709\u5f97\u5230\u201c\u5145\u5206\u5229\u7528\u201d</p> <p>\u53c2\u8003\uff1a\u4e3a\u4ec0\u4e48shuffleNetv2\u3001EfficientNet\u7b49\u8f7b\u91cf\u5316\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u53cd\u800c\u6bd4\u8f83\u6162\u5462\uff1f</p>"},{"location":"sci-paper/cs/GAN/","title":"GAN\uff1aGenerative Adversarial Nets","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aGenerative Adversarial Nets</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:</p> <p>\u8d44\u6e90\uff1a</p> <p></p>"},{"location":"sci-paper/cs/GAN/#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p>\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684framework\uff0c\u901a\u8fc7\u4e00\u4e2a\u5bf9\u6297\u7684\u8fc7\u7a0b\u6765\u4f30\u8ba1\u4e00\u4e2a\u751f\u6210\u6a21\u578b\uff0c\u540c\u65f6\u8bad\u7ec3\u4e24\u4e2a\u6a21\u578b\uff1a\u4e00\u4e2a\u751f\u6210\u6a21\u578b\\(G\\)\uff0c\u53e6\u4e00\u4e2a\u662f\u5224\u522b\u6a21\u578b\\(D\\)\uff0c\u5176\u4e2d\u751f\u6210\u6a21\u578b\\(G\\)\u662f\u901a\u8fc7\u6355\u83b7\u6837\u672c\u6570\u636e\u5206s\u5e03\u6765\u751f\u6210\u65b0\u7684\u6570\u636e\uff0c\u5224\u522b\u6a21\u578b\\(D\\)\u662f\u7528\u6765\u4f30\u8ba1\u6837\u672c\u6765\u81ea\u8bad\u7ec3\u6570\u636e\u800c\u4e0d\u662f\\(G\\)\u7684\u6982\u7387\uff0c\u5176\u4e2d\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u7684\u8fc7\u7a0b\u5c31\u662f\u4e3a\u4e86\u4f7f\u5224\u522b\u6a21\u578b\u51fa\u9519\u7684\u6982\u7387\u6700\u5927\u5316\uff08\u4e00\u4e2a\u535a\u5f08\u7684\u8fc7\u7a0b\uff09\u3002</p> <p>\u5728\\(G\\)\u548c\\(D\\)\u90fd\u662f\u7531\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u5b9a\u4e49\u7684\u60c5\u51b5\u4e0b\uff0c\u6574\u4e2a\u7cfb\u7edf\u53ef\u4ee5\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u8bad\u7ec3\u3002</p>"},{"location":"sci-paper/cs/GAN/#\u57fa\u672c\u539f\u7406","title":"\u57fa\u672c\u539f\u7406","text":"<p>\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08Generative Adversarial Network, GAN\uff09\u7531\u4e00\u4e2a\u751f\u6210\u5668\u548c\u4e00\u4e2a\u5224\u522b\u5668\u7ec4\u6210\u3002\u751f\u6210\u5668\u4ece\u6f5c\u5728\u7a7a\u95f4\u968f\u673a\u53d6\u6837\u4f5c\u4e3a\u8f93\u5165\uff0c\u5176\u8f93\u51fa\u7ed3\u679c\u9700\u8981\u5c3d\u91cf\u6a21\u4eff\u8bad\u7ec3\u96c6\u4e2d\u7684\u771f\u5b9e\u6837\u672c\u3002\u5224\u522b\u5668\u7684\u8f93\u5165\u5219\u4e3a\u771f\u5b9e\u6837\u672c\u6216\u751f\u6210\u5668\u7684\u8f93\u51fa\uff0c\u5176\u76ee\u7684\u662f\u5c06\u751f\u6210\u5668\u7684\u8f93\u51fa\u4ece\u771f\u5b9e\u6837\u672c\u4e2d\u5c3d\u53ef\u80fd\u5206\u522b\u51fa\u6765\u3002\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u76f8\u4e92\u5bf9\u6297\u3001\u4e0d\u65ad\u5b66\u4e60\uff0c\u6700\u7ec8\u76ee\u7684\u4f7f\u5f97\u5224\u522b\u5668\u65e0\u6cd5\u5224\u65ad\u751f\u6210\u5668\u7684\u8f93\u51fa\u7ed3\u679c\u662f\u5426\u771f\u5b9e\u3002</p> <p>\u4e3a\u4e86\u5b66\u4e60Generator\u5728\u6570\u636e\\(x\\)\u4e0a\u7684\u5206\u5e03\\(p_g\\)\uff0c\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u8f93\u5165\u566a\u58f0\u53d8\u91cf\\(p_z(z)\\)\uff0c\u5c06\u6620\u5c04\u5230\u6570\u636e\u7a7a\u95f4\u7684\u6620\u5c04\u8868\u793a\u4e3a\\(G(z;\\theta_g)\\)\uff0c\\(G\\)\u4e3a\u4e00\u4e2amultilayer perceptron\uff0c\\(theta_g\\)\u4e3a\u5176\u53c2\u6570\u3002\u6211\u4eec\u4e5f\u5b9a\u4e49\u7b2c\u4e8c\u4e2amultilayer perceptron \\(D(x;\\theta_d)\\)\uff0c\u5176\u8f93\u51fa\u4e3a\u6807\u91cf\uff0c\u8868\u793a\u6570\u636e\\(x\\)\u6765\u81ea\u771f\u5b9e\u6570\u636e\u7684\u6982\u7387\u3002</p> <p></p> <p>\u5bf9\u4e8e\u5224\u522b\u5668\\(D\\)\uff0c\u6211\u4eec\u901a\u8fc7maximize the probability of assigning the correct label to both training examples and samples from \\(G\\)\u7684\u65b9\u5f0f\u6765\u8bad\u7ec3\\(D\\)\uff1b\u5bf9\u4e8e\u751f\u6210\u5668\\(G\\),\u6211\u4eec\u901a\u8fc7minimize \\(log(1-D(G(z)))\\)\u7684\u65b9\u5f0f\u8bad\u7ec3\u3002\\(G\\)\u4e0e\\(D\\)\u540c\u65f6\u4f5c\u5bf9\u6297\u8bad\u7ec3\u3002</p> <p>\u5373\\(D\\) and \\(G\\) play the following two-player minimax game with value function \\(V (G, D)\\):</p> \\[ \\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)} [\\log (1-D(G(z)))] \\] <p>\u5728\u5b9e\u9a8c\u4e2d\uff0c\u4e0a\u5f0f\u5e76\u4e0d\u80fd\u4e3a\\(G\\)\u63d0\u4f9b\u8db3\u591f\u7684\u68af\u5ea6\u53bb\u5b66\u4e60\uff0c\u5728\u5b66\u4e60\u65e9\u671f\uff0c\u56e0\u4e3a\u771f\u5b9e\u6837\u672c\u4e0e\\(G\\)\u751f\u6210\u7684\u6837\u672c\u6709\u7740\u660e\u663e\u5dee\u522b\uff0c\u56e0\u6b64\\(D\\) reject samples with high confidence\uff0c\\(log(1-D(G(z)))\\)\u5b58\u5728\u9971\u548c\u73b0\u8c61\u3002\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e0b\u65b9\u6cd5\uff1aRather than training \\(G\\) to minimize \\(log(1-D(G(z)))\\), we can train \\(G\\) to maximize \\(log(D(z))\\)\u3002 </p> <p></p> <p>\u5982\u4e0a\u56fe\u6240\u793a\uff1a\u865a\u7ebf\u70b9\u4e3a\u771f\u5b9e\u6570\u636e\u5206\u5e03\uff0c\u84dd\u8272\u865a\u7ebf\u662f\u5224\u522b\u5668D\u7684\u5206\u5e03\uff0c\u7eff\u8272\u5b9e\u7ebf\u4e3a\u751f\u6210\u5668G\u7684\u5206\u5e03\u3002\u521d\u59cb\u8bad\u7ec3\u51fa\u751f\u6210\u5668\u7f51\u7edcG\u548c\u5224\u522b\u5668\u7f51\u7edcD\uff1b\u4ecea\u5230d\u662f\u6211\u4eec\u5e0c\u671b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002</p> <p>\u5bf9\u4e8e\u4efb\u610f\u7ed9\u5b9a\u7684\u751f\u6210\u5668\\(G\\)\uff0c\u5219\u6700\u4f18\u7684\u5224\u522b\u5668\\(D^*\\)\u4e3a\uff1a</p> \\[ D^*_G(x) = \\max V(G,D) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \\] <p>\u5176\u4e2d\\(p_g\\)\u4e0e\\(p_{data}\\)\u8868\u793a\\(x\\)\u5728\u751f\u6210\u5668\\(G\\)\u62df\u5408\u7684\u5206\u5e03\uff0c\u548c\u771f\u5b9e\u6570\u636e\u7684\u5206\u5e03\u91cc\u7684\u6982\u7387\u3002\u5f53\\(p_g=p_{data}\\)\u65f6\uff0c\u7ed3\u679c\u4e3a\\(\\frac{1}{2}\\)\uff0c\u8868\u793a\u4e24\u4e2a\u5206\u5e03\u5b8c\u5168\u76f8\u540c\uff0c\u6700\u4f18\u7684\u5224\u522b\u5668\u4e5f\u65e0\u6cd5\u5206\u8fa8\u51fa\u6765\u3002</p> <p></p> <p>\u7531\u4e0a\u56fe\uff1a</p> \\[ Loss_D = CrossEntryLoss(D(x),1) + CrossEntryLoss(D(G(z)),0) \\] \\[ Loss_G = CrossEntryLoss(D(G(z)),1) \\]"},{"location":"sci-paper/cs/GAN/#gan\u7684\u7f3a\u70b9","title":"GAN\u7684\u7f3a\u70b9","text":"<ul> <li> <p>\u96be\u4ee5\u6536\u655b\uff08non-convergence\uff09\u3002\u76ee\u524d\u9762\u4e34\u7684\u57fa\u672c\u95ee\u9898\u662f\uff1a\u6240\u6709\u7684\u7406\u8bba\u90fd\u8ba4\u4e3a GAN \u5e94\u8be5\u5728\u7eb3\u4ec0\u5747\u8861\uff08Nash equilibrium\uff09\u4e0a\u6709\u5353\u8d8a\u7684\u8868\u73b0\uff0c\u4f46\u68af\u5ea6\u4e0b\u964d\u53ea\u6709\u5728\u51f8\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\u624d\u80fd\u4fdd\u8bc1\u5b9e\u73b0\u7eb3\u4ec0\u5747\u8861\u3002\u5f53\u535a\u5f08\u53cc\u65b9\u90fd\u7531\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u65f6\uff0c\u5728\u6ca1\u6709\u5b9e\u9645\u8fbe\u5230\u5747\u8861\u7684\u60c5\u51b5\u4e0b\uff0c\u8ba9\u5b83\u4eec\u6c38\u8fdc\u4fdd\u6301\u5bf9\u81ea\u5df1\u7b56\u7565\u7684\u8c03\u6574\u662f\u53ef\u80fd\u7684\u3002</p> </li> <li> <p>\u96be\u4ee5\u8bad\u7ec3\uff1a\u5d29\u6e83\u95ee\u9898\uff08collapse problem\uff09\u3002GAN\u6a21\u578b\u88ab\u5b9a\u4e49\u4e3a\u6781\u5c0f\u6781\u5927\u95ee\u9898\uff0c\u6ca1\u6709\u635f\u5931\u51fd\u6570\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f88\u96be\u533a\u5206\u662f\u5426\u6b63\u5728\u53d6\u5f97\u8fdb\u5c55\u3002GAN\u7684\u5b66\u4e60\u8fc7\u7a0b\u53ef\u80fd\u53d1\u751f\u5d29\u6e83\u95ee\u9898\uff08collapse problem\uff09\uff0c\u751f\u6210\u5668\u5f00\u59cb\u9000\u5316\uff0c\u603b\u662f\u751f\u6210\u540c\u6837\u7684\u6837\u672c\u70b9\uff0c\u65e0\u6cd5\u7ee7\u7eed\u5b66\u4e60\u3002\u5f53\u751f\u6210\u6a21\u578b\u5d29\u6e83\u65f6\uff0c\u5224\u522b\u6a21\u578b\u4e5f\u4f1a\u5bf9\u76f8\u4f3c\u7684\u6837\u672c\u70b9\u6307\u5411\u76f8\u4f3c\u7684\u65b9\u5411\uff0c\u8bad\u7ec3\u65e0\u6cd5\u7ee7\u7eed\u3002</p> </li> <li> <p>\u65e0\u9700\u9884\u5148\u5efa\u6a21\uff0c\u6a21\u578b\u8fc7\u4e8e\u81ea\u7531\u4e0d\u53ef\u63a7\u3002\u4e0e\u5176\u4ed6\u751f\u6210\u5f0f\u6a21\u578b\u76f8\u6bd4\uff0cGAN\u4e0d\u9700\u8981\u6784\u9020\u5206\u5e03\u51fd\u6570\uff0c\u800c\u662f\u4f7f\u7528\u4e00\u79cd\u5206\u5e03\u76f4\u63a5\u8fdb\u884c\u91c7\u6837\uff0c\u4ece\u800c\u771f\u6b63\u8fbe\u5230\u7406\u8bba\u4e0a\u53ef\u4ee5\u5b8c\u5168\u903c\u8fd1\u771f\u5b9e\u6570\u636e\uff0c\u8fd9\u4e5f\u662fGAN\u6700\u5927\u7684\u4f18\u52bf\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u4e0d\u9700\u8981\u9884\u5148\u5efa\u6a21\u7684\u65b9\u6cd5\u7f3a\u70b9\u662f\u592a\u8fc7\u81ea\u7531\u4e86\uff0c\u5bf9\u4e8e\u8f83\u5927\u7684\u56fe\u7247\uff0c\u8f83\u591a\u7684pixel\u7684\u60c5\u5f62\uff0c\u57fa\u4e8e\u7b80\u5355 GAN \u7684\u65b9\u5f0f\u5c31\u4e0d\u592a\u53ef\u63a7\u4e86(\u8d85\u9ad8\u7ef4)\u3002</p> </li> </ul>"},{"location":"sci-paper/cs/MAE/","title":"MAE\uff1aMasked Autoencoders Are Scalable Vision Learners","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aMasked Autoencoders Are Scalable Vision Learners</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/facebookresearch/mae/</p> <p>\u8d44\u6e90\uff1a</p> <p></p>"},{"location":"sci-paper/cs/MAE/#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p>MAE\u662f\u4e00\u79cd\u7b80\u5355\u3001\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684 \u7528\u4e8e\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u975e\u5bf9\u79f0\u7f16\u7801\u5668-\u89e3\u7801\u5668\u4f53\u7cfb\u7ed3\u6784\uff0c\u5176\u4e2d\u7684\u7f16\u7801\u5668\u53ea\u5bf9\u53ef\u89c1\u7684patch\u5b50\u96c6\uff08\u6ca1\u6709mask\u7684token\uff09\u8fdb\u884c\u64cd\u4f5c\uff0c\u540c\u65f6\u8fd8\u6709\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u89e3\u7801\u5668\uff0c\u8be5\u89e3\u7801\u5668\u4ece\u6f5c\u5728\u8868\u793a\u548cmask token\u4e2d\u91cd\u5efa\u539f\u59cb\u56fe\u50cf\u3002\u5176\u6b21\uff0c\u4f5c\u8005\u53d1\u73b0mask\u9ad8\u6bd4\u4f8b\u7684\u8f93\u5165\u56fe\u50cfpatch\uff08\u4f8b\u598275%\uff09\u4f1a\u53d8\u6210\u4e00\u4e2a\u4e0d\u9519\u4e14\u6709\u610f\u4e49\u7684\u81ea\u76d1\u7763\u4efb\u52a1\u3002\u5c06\u8fd9\u4e24\u79cd\u8bbe\u8ba1\u7ed3\u5408\u8d77\u6765\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u8bad\u7ec3\u5927\u578b\u6a21\u578b\uff1a\u672c\u6587\u7684\u6a21\u578b\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\uff083\u500d\u6216\u66f4\u591a\uff09\u5e76\u63d0\u9ad8\u7cbe\u5ea6\u3002</p> <p>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u975e\u5bf9\u79f0\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5176\u4e2d\u4e00\u4e2a\u7f16\u7801\u5668\u53ea\u5bf9\u53ef\u89c1\u7684patches\u5b50\u96c6\u8fdb\u884c\u64cd\u4f5c\uff08\u6ca1\u6709\u63a9\u7801\u6807\u8bb0\uff09\uff0c\u4ee5\u53ca\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\uff0c\u53ef\u4ee5\u4ece\u6f5c\u5728\u8868\u793a\u548c\u63a9\u7801\u6807\u8bb0\u91cd\u5efa\u539f\u59cb\u56fe\u50cf\u3002\u6211\u4eec\u53d1\u73b0\u5c4f\u853d\u5927\u90e8\u5206\u8f93\u5165\u56fe\u50cf\uff0c\u4f8b\u5982 75%\uff0c\u4f1a\u4ea7\u751f\u91cd\u8981\u4e14\u6709\u610f\u4e49\u7684\u81ea\u76d1\u7763\u4efb\u52a1\u3002\u5c06\u8fd9\u4e24\u79cd\u8bbe\u8ba1\u7ed3\u5408\u8d77\u6765\u4f7f\u6211\u4eec\u80fd\u591f\u9ad8\u6548\u5730\u8bad\u7ec3\u5927\u578b\u6a21\u578b\uff0c\u5c06\u8bad\u7ec3\u901f\u5ea6\u63d0\u9ad8 3 \u500d\u6216\u66f4\u591a\uff0c\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002</p> <p></p> <p>\u8bba\u6587\u5728 <code>ImageNet-1K</code> \u8bad\u7ec3\u96c6\u4e0a\u8fdb\u884c\u4e86\u81ea\u76d1\u7763\u7684\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u8fdb\u884c\u4e86\u76d1\u7763\u8bad\u7ec3\uff0c\u4ee5\u901a\u8fc7\u7aef\u5230\u7aef\u7684\u5fae\u8c03\u6216\u7ebf\u6027\u63a2\u6d4b\u6765\u8bc4\u4f30\u8868\u793a\u3002\u4ed6\u4eec\u4f7f\u7528<code>ViT-Large (ViT-L/16)</code>\u4f5c\u4e3a\u4ed6\u4eec\u7684\u6a21\u578b\u5e76\u9a8c\u8bc1Top1\u51c6\u786e\u6027\u3002</p> <p>\u7ed3\u679c\u8868\u660e\uff0cMAE \u5b66\u4e60\u4e86\u975e\u5e38\u9ad8\u5bb9\u91cf\u7684\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e5f\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u3002\u4f7f\u7528\u666e\u901a\u7684<code>ViT-Huge</code>\u6a21\u578b\uff0c<code>MAE</code> \u5728 <code>ImageNet-1K</code> \u4e0a\u8fdb\u884c\u5fae\u8c03\u65f6\u8fbe\u5230\u4e86 <code>87.8%</code> \u7684\u51c6\u786e\u7387\u3002</p> <p></p>"},{"location":"sci-paper/cs/MAE/#\u65b9\u6cd5","title":"\u65b9\u6cd5","text":"<p>MAE\u4f5c\u4e3a\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u7ed9\u5b9a\u4e00\u4e2a\u5b8c\u6574\u7684\u8f93\u5165\u56fe\u50cf\uff0c\u968f\u673a\u906e\u6321\u4e00\u90e8\u5206patch\uff0c\u7136\u540e\u901a\u8fc7\u672a\u906e\u6321\u7684patch\u6765\u9884\u6d4b\u88ab\u906e\u6321\u7684patch\u3002\u5e76\u4e14\uff0c\u6a21\u578b\u91c7\u7528\u4e86\u4e00\u79cd\u975e\u5bf9\u79f0\u8bbe\u8ba1\uff0c\u4ece\u8f93\u5165\u56fe\u50cf\u4e2d\u968f\u673amask\u90e8\u5206patch\uff0c\u5229\u7528\u89e3\u7801\u5668\u5728\u50cf\u7d20\u7a7a\u95f4\u4e2d\u91cd\u6784\u4e22\u5931\u7684patch\u3002\u7f16\u7801\u5668\u4ec5\u4ec5\u5bf9\u53ef\u89c1\u7684patch\u8fdb\u884c\u8ba1\u7b97\u3002\u89e3\u7801\u5668\u662f\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u7f51\u7edc\uff0c\u5728\u89e3\u7801\u5668\u4e2d\u4f7f\u7528mask token\u4e0e\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4e00\u540c\u91cd\u6784\u50cf\u7d20\uff0c\u8fd9\u6837\u8ba1\u7b97\u91cf\u5927\u5927\u51cf\u5c0f\u3002</p> <ul> <li>Masking</li> </ul> <p>\u5bf9\u4e8e\u4ecepatch\u4e2d\u9009\u53d6mask\uff0c\u4f5c\u8005\u91c7\u7528\u4e86\u673a\u62bd\u6837\u7684\u65b9\u6cd5\uff0c\u5728\u5bf9\u539f\u672c\u987a\u5e8f\u7684patch\u8fdb\u884cshuffle\u540e\u9009\u53d6\u524d25%\u4f5c\u4e3a\u53ef\u89c1\u7684patch\uff0c\u5176\u4f59\u768475%\u4f5c\u4e3amask\u3002\u8fd9\u6837\u9ad8\u906e\u76d6\u7387\u7684\u968f\u673a\u91c7\u6837\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u6d88\u9664\u4e86\u5197\u4f59(redundancy)\uff0c\u56e0\u6b64\u521b\u5efa\u4e86\u4e00\u4e2a\u65e0\u6cd5\u901a\u8fc7\u4ece\u53ef\u89c1\u76f8\u90bbpatch\u6765\u8f7b\u677e\u89e3\u51b3\u7684\u4efb\u52a1\u3002\u5747\u5300\u5206\u5e03\u53ef\u9632\u6b62\u6f5c\u5728\u7684\u4e2d\u5fc3\u504f\u79fb\uff08\u5373\u56fe\u50cf\u4e2d\u5fc3\u9644\u8fd1\u6709\u66f4\u591a\u906e\u76d6patch\uff09\u3002\u6700\u540e\uff0c\u9ad8\u5ea6\u7a00\u758f\u7684\u8f93\u5165\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u7f16\u7801\u5668\u521b\u9020\u4e86\u673a\u4f1a\u3002</p> <ul> <li>MAE Encoder</li> </ul> <p></p> <p>MAE\u7684\u7f16\u7801\u5668\u91c7\u7528<code>ViT</code>\u7ed3\u6784\uff0c\u5bf9\u53ef\u89c1\u7684patch\u8fdb\u884c\u7f16\u7801\uff0c\u53bb\u9664\u4e86mask patch\uff0c\u4e14\u4e0d\u4f7f\u7528mask token\uff0c\u800cmask\u7684\u90e8\u5206\uff0c\u4f1a\u4ea4\u7ed9\u66f4\u8f7b\u91cf\u7ea7\u7684decoder\u6765\u5904\u7406\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u7528\u4e00\u5c0f\u90e8\u5206\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6765\u8bad\u7ec3\u975e\u5e38\u5927\u7684\u7f16\u7801\u5668\u3002</p> <ul> <li>MAE Decoder</li> </ul> <p>MAE\u7684\u89e3\u7801\u5668\u91c7\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684<code>Decoder</code>\uff0c\u4e3b\u4f53\u5305\u542b\u51e0\u4e2a<code>transformer blocks</code>\uff0c\u800c\u6700\u540e\u4e00\u5c42\u662f\u4e00\u4e2alinear\u5c42\uff0c\u7528\u6765\u76f4\u63a5\u9884\u6d4bmask patch\u7684\u50cf\u7d20\u503c\uff0c\u5176\u8f93\u51fa\u901a\u9053\u6570\u662f\u548c\u4e00\u4e2apatch\u50cf\u7d20\u6570\u4e00\u81f4\uff0c\u8f93\u51fa\u7ecf\u8fc7reshape\u4ee5\u540e\u5f62\u6210\u91cd\u5efa\u56fe\u50cf\u3002\u89e3\u7801\u5668\u7684\u8f93\u5165\u4fe1\u606f\u662f\u6240\u6709token\uff0c\u7531\u7f16\u7801\u5668\u8f93\u51fa\u7684\u7f16\u7801\u5411\u91cf\u548cmask token\u7ec4\u6210\u3002\u6bcf\u4e2amask token\u90fd\u662f\u4e00\u4e2a\u5171\u4eab\u7684\u3001\u53ef\u5b66\u4e60\u7684\u5411\u91cf\uff0c\u8868\u793a\u8981\u9884\u6d4b\u7684\u7f3a\u5931patch\u3002\u800c\u540e\u7ed9\u6240\u6709token\u6dfb\u52a0<code>positional embedding</code></p> <p>\u8bad\u7ec3\u7684loss\u91c7\u7528\u91cd\u5efa\u56fe\u50cf\u548c\u539f\u59cb\u56fe\u50cf\u4e4b\u95f4\u7684\u7684\u5747\u65b9\u8bef\u5dee\uff08<code>MSE</code>\uff09\uff1a\u8ba1\u7b97\u9884\u6d4b\u50cf\u7d20\u503c\u548c\u539f\u59cb\u50cf\u7d20\u503c\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u4e0d\u8fc7loss\u53ea\u8ba1\u7b97mask patch\u4e0a\u7684\u635f\u5931\u3002</p>"},{"location":"sci-paper/cs/MAE/#\u5b9e\u9a8c","title":"\u5b9e\u9a8c","text":"<p>\u8bba\u6587\u4f7f\u7528\u4e86<code>ViT-Large\uff08ViT-L/16\uff09</code>\u4f5c\u4e3a\u7f16\u7801\u5668\u5728<code>ImageNet-1K</code>\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u4e86\u4f7f\u7528MAE\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u4e0e\u4ece\u5934\u8bad\u7ec3\u540e\u6a21\u578b\uff0c\u6765\u8bc4\u4f30encoder\u7684\u8868\u5f81\u80fd\u529b\u3002</p> <p></p> <p>\u53ef\u77e5\uff0c<code>baseline MAE</code>\u7684\u65b9\u6cd5\uff08\u7ecf<code>MAE</code>\u9884\u8bad\u7ec3\u540e<code>fine-tune</code>\uff09\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8bf4\u660e<code>MAE</code>\u9884\u8bad\u7ec3\u80fd\u591f\u63d0\u5347\u6a21\u578b\u7684\u8868\u5f81\u80fd\u529b\u3002</p> <ul> <li><code>Masking ratio</code></li> </ul> <p></p> <p>\u4f5c\u8005\u8ba4\u4e3a\uff1a\u56fe\u50cf\u662f\u4f4e\u4fe1\u606f\u5bc6\u5ea6\u7684\uff0c\u5176\u4e2d\u4fe1\u606f\u5197\u4f59\u975e\u5e38\u5927\uff0c\u56e0\u6b64\u9700\u8981\u8f83\u5927\u6bd4\u4f8b\u7684Mask\u3002\u6839\u636e\u5b9e\u9a8c\u53ef\u77e5\uff0c<code>Masking ratio</code>\u5927\u4e8e50%\u662f\u8f83\u597d\u7684\uff0c\u80fd\u8ba9\u6a21\u578b\u5b66\u4e60\u5230\u6bd4\u8f83\u6df1\u523b\u7684\u4fe1\u606f\u3002\u800c\u5bf9\u4e8enlp\u4efb\u52a1\u7684\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u7684\u4efb\u52a1\uff0cBERT\u4f7f\u752815%\u7684mask\u6bd4\u4f8b\u5df2\u7ecf\u8db3\u77e3\u3002</p> <p>\u5373\u4fe1\u606f\u5bc6\u5ea6\u8d8a\u5927\uff0c\u6211\u4eec\u9700\u8981mask\u7684\u6bd4\u4f8b\u8d8a\u5c11\uff1b\u4fe1\u606f\u5bc6\u5ea6\u8d8a\u7a00\u758f\uff0c\u6211\u4eec\u9700\u8981mask\u7684\u6bd4\u4f8b\u8d8a\u591a\u3002</p> <p>\u540c\u65f6\uff0c\u5b9e\u9a8c\u4e5f\u8bc1\u5b9e\u4e86\u4f7f\u7528\u968f\u673a\u91c7\u6837(random masking)\u7684\u6548\u679c\u8f83\u597d\uff1a</p> <p></p>"},{"location":"sci-paper/cs/MAE/#fine-tuning","title":"fine-tuning","text":"<p>\u5bf9\u4e8e<code>ViT architecture</code>\uff1a</p> <p>\u6211\u4eec\u9075\u5faa\u6807\u51c6\u7684ViT\u67b6\u6784\u3002\u5b83\u6709\u4e00\u5806<code>Transformer</code>\u5757\uff0c\u6bcf\u4e2a\u5757\u7531\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5757\u548cMLP\u5757\u7ec4\u6210\uff0c\u4e24\u8005\u90fd\u5177\u6709<code>LayerNorm (LN)</code>\u3002\u7f16\u7801\u5668\u4ee5<code>LN</code>\u7ed3\u675f\u3002\u7531\u4e8e<code>MAE</code>\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u5bbd\u5ea6\u4e0d\u540c\uff0c\u6211\u4eec\u5728\u7f16\u7801\u5668\u4e4b\u540e\u91c7\u7528<code>linear projection layer</code>\u6765\u5339\u914d\u5b83\u3002<code>MAE</code>\u5c06<code>positional embeddings</code>\uff08the sine-cosine version\uff09\u6dfb\u52a0\u5230\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u8f93\u5165\u4e2d\u3002\u6211\u4eec\u7684 MAE \u4e0d\u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u6216\u5c42\u7f29\u653e\u3002\u6211\u4eec\u4ece\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u63d0\u53d6\u7279\u5f81\u8fdb\u884c<code>Fine-tuning</code>\u548c<code>Linear Probing</code>\u3002\u7531\u4e8e<code>ViT</code>\u6709\u4e00\u4e2a\u7c7b\u6807\u8bb0<code>[CLS]</code>\uff0c\u4e3a\u4e86\u9002\u5e94\u8fd9\u79cd\u8bbe\u8ba1\uff0c\u5728\u6211\u4eec\u7684<code>MAE</code>\u9884\u8bad\u7ec3\u4e2d\uff0c\u6211\u4eec\u5c06\u8f85\u52a9\u865a\u62df\u6807\u8bb0\u9644\u52a0\u5230\u7f16\u7801\u5668\u8f93\u5165\u4e2d\u3002\u8be5\u6807\u8bb0\u5c06\u88ab\u89c6\u4e3a\u7528\u4e8e\u5728<code>Linear Probing</code>\u548c<code>Fine-tuning</code>\u4e2d\u8bad\u7ec3\u5206\u7c7b\u5668\u7684\u7c7b\u6807\u8bb0\u3002\u6211\u4eec\u7684<code>MAE</code>\u5728\u6ca1\u6709\u8fd9\u4e2a\u6807\u8bb0\uff08\u4f7f\u7528average pooling\u4ee3\u66ff\uff09\u7684\u60c5\u51b5\u4e0b\u4e5f\u540c\u6837\u6709\u6548\u3002</p> <ul> <li>Pre-training</li> </ul> <p>\u6211\u4eec\u4e0d\u4f7f\u7528<code>color jittering, drop path, gradient clip</code>\u3002\u6211\u4eec\u4f7f\u7528<code>xavier uniform</code>\u6765\u521d\u59cb\u5316\u6240\u6709<code>Transformer</code>\u5757\uff0c\u9075\u5faaViT\u7684\u5b98\u65b9\u4ee3\u7801\u3002\u6211\u4eec\u4f7f\u7528\u7ebf\u6027lr\u7f29\u653e\u89c4\u5219 [20]\uff1a\\(lr = base_lr \\times batchsize / 256\\)\u3002</p> <p></p> <ul> <li>End-to-end fine-tuning</li> </ul> <p>\u6211\u4eec\u7684\u5fae\u8c03\u9075\u5faa<code>supervised ViT</code>\u8bad\u7ec3\u7684\u5e38\u89c1\u505a\u6cd5\uff0c\u9ed8\u8ba4\u8bbe\u7f6e\u5982\u8868\u6240\u793a\u3002\u6211\u4eec\u4f7f\u7528\u5206\u5c42lr\u8870\u51cf\uff08<code>layer-wise lr decay</code>\uff09\u3002</p> <p></p> <ul> <li>Linear probing</li> </ul> <p>\u6211\u4eec\u89c2\u5bdf\u5230<code>Linear Probing</code>\u9700\u8981\u4e0e<code>End-to-end fine-tuning</code>\u975e\u5e38\u4e0d\u540c\u7684\u65b9\u6848\u3002\u7279\u522b\u5728\u4e8e\uff0c\u6b63\u5219\u5316\u901a\u5e38\u5bf9<code>Linear Probing</code>\u6709\u5bb3\u3002\u6211\u4eec\u7981\u7528\u4e86\u8bb8\u591a\u5e38\u89c1\u7684\u6b63\u5219\u5316\u7b56\u7565:\u6211\u4eec\u4e0d\u4f7f\u7528<code>mixup\u3001cutmix\u3001droppath</code>\u6216<code>color jittering</code>\uff0c\u5e76\u4e14\u6211\u4eec\u8bbe\u7f6e\u6743\u503c\u8870\u51cf\u4e3a\u96f6\u3002</p> <p></p>"},{"location":"sci-paper/cs/MAE/#mask\u4fee\u590d\u6548\u679c\u5c55\u793a\u539f\u6587","title":"mask\u4fee\u590d\u6548\u679c\u5c55\u793a\uff08\u539f\u6587\uff09","text":""},{"location":"sci-paper/cs/MAE/#\u4ee3\u7801\u5b9e\u73b0kaiming","title":"\u4ee3\u7801\u5b9e\u73b0\uff08kaiming\uff09","text":"utils models_mae.py <pre><code>from functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.models.vision_transformer import PatchEmbed, Block\n\nfrom util.pos_embed import get_2d_sincos_pos_embed\n\n\nclass MaskedAutoencoderViT(nn.Module):\n    \"\"\" Masked Autoencoder with VisionTransformer backbone\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n                 embed_dim=1024, depth=24, num_heads=16,\n                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n        super().__init__()\n\n        # --------------------------------------------------------------------------\n        # MAE encoder specifics\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n\n        self.blocks = nn.ModuleList([\n            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n        # --------------------------------------------------------------------------\n\n        # --------------------------------------------------------------------------\n        # MAE decoder specifics\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n\n        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n\n        self.decoder_blocks = nn.ModuleList([\n            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n            for i in range(decoder_depth)])\n\n        self.decoder_norm = norm_layer(decoder_embed_dim)\n        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n        # --------------------------------------------------------------------------\n\n        self.norm_pix_loss = norm_pix_loss\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        # initialization\n        # initialize (and freeze) pos_embed by sin-cos embedding\n        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n\n        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n\n        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n        w = self.patch_embed.proj.weight.data\n        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n        torch.nn.init.normal_(self.cls_token, std=.02)\n        torch.nn.init.normal_(self.mask_token, std=.02)\n\n        # initialize nn.Linear and nn.LayerNorm\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            # we use xavier_uniform following official JAX ViT:\n            torch.nn.init.xavier_uniform_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def patchify(self, imgs):\n        \"\"\"\n        imgs: (N, 3, H, W)\n        x: (N, L, patch_size**2 *3)\n        \"\"\"\n        p = self.patch_embed.patch_size[0]\n        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n\n        h = w = imgs.shape[2] // p\n        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n        x = torch.einsum('nchpwq-&gt;nhwpqc', x)\n        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n        return x\n\n    def unpatchify(self, x):\n        \"\"\"\n        x: (N, L, patch_size**2 *3)\n        imgs: (N, 3, H, W)\n        \"\"\"\n        p = self.patch_embed.patch_size[0]\n        h = w = int(x.shape[1]**.5)\n        assert h * w == x.shape[1]\n\n        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n        x = torch.einsum('nhwpqc-&gt;nchpwq', x)\n        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n        return imgs\n\n    def random_masking(self, x, mask_ratio):\n        \"\"\"\n        Perform per-sample random masking by per-sample shuffling.\n        Per-sample shuffling is done by argsort random noise.\n        x: [N, L, D], sequence\n        \"\"\"\n        N, L, D = x.shape  # batch, length, dim\n        len_keep = int(L * (1 - mask_ratio))\n\n        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n\n        # sort noise for each sample\n        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n\n        # keep the first subset\n        ids_keep = ids_shuffle[:, :len_keep]\n        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n\n        # generate the binary mask: 0 is keep, 1 is remove\n        mask = torch.ones([N, L], device=x.device)\n        mask[:, :len_keep] = 0\n        # unshuffle to get the binary mask\n        mask = torch.gather(mask, dim=1, index=ids_restore)\n\n        return x_masked, mask, ids_restore\n\n    def forward_encoder(self, x, mask_ratio):\n        # embed patches\n        x = self.patch_embed(x)\n\n        # add pos embed w/o cls token\n        x = x + self.pos_embed[:, 1:, :]\n\n        # masking: length -&gt; length * mask_ratio\n        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n\n        # append cls token\n        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # apply Transformer blocks\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n\n        return x, mask, ids_restore\n\n    def forward_decoder(self, x, ids_restore):\n        # embed tokens\n        x = self.decoder_embed(x)\n\n        # append mask tokens to sequence\n        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n\n        # add pos embed\n        x = x + self.decoder_pos_embed\n\n        # apply Transformer blocks\n        for blk in self.decoder_blocks:\n            x = blk(x)\n        x = self.decoder_norm(x)\n\n        # predictor projection\n        x = self.decoder_pred(x)\n\n        # remove cls token\n        x = x[:, 1:, :]\n\n        return x\n\n    def forward_loss(self, imgs, pred, mask):\n        \"\"\"\n        imgs: [N, 3, H, W]\n        pred: [N, L, p*p*3]\n        mask: [N, L], 0 is keep, 1 is remove, \n        \"\"\"\n        target = self.patchify(imgs)\n        if self.norm_pix_loss:\n            mean = target.mean(dim=-1, keepdim=True)\n            var = target.var(dim=-1, keepdim=True)\n            target = (target - mean) / (var + 1.e-6)**.5\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n\n        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n        return loss\n\n    def forward(self, imgs, mask_ratio=0.75):\n        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n        loss = self.forward_loss(imgs, pred, mask)\n        return loss, pred, mask\n\n\ndef mae_vit_base_patch16_dec512d8b(**kwargs):\n    model = MaskedAutoencoderViT(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef mae_vit_large_patch16_dec512d8b(**kwargs):\n    model = MaskedAutoencoderViT(\n        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef mae_vit_huge_patch14_dec512d8b(**kwargs):\n    model = MaskedAutoencoderViT(\n        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n</code></pre>"},{"location":"sci-paper/cs/MoE/","title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/davidmrau/mixture-of-experts</p> <p>\u53c2\u8003\u5b66\u4e60\uff1a\u3010\u8bba\u6587\u7cbe\u70bc\u3011OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER | \u8d85\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\uff1a\u7a00\u758f\u95e8\u63a7\u4e13\u5bb6\u6df7\u5408\u5c42</p> <p></p>"},{"location":"sci-paper/cs/MoE/#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p>\u795e\u7ecf\u7f51\u7edc\u7684\u5438\u6536\u4fe1\u606f\u7684\u5bb9\u91cf\uff08The capacity of a neural network to absorb information\uff09\u53d7\u9650\u4e8e\u53c2\u6570\u6570\u76ee\u3002</p> <p>\u6761\u4ef6\u8ba1\u7b97\uff08conditional computation\uff09\u9488\u5bf9\u4e8e\u6bcf\u4e2a\u6837\u672c\uff0c\u6fc0\u6d3b\u7f51\u7edc\u7684\u90e8\u5206\u5b50\u7f51\u7edc\u8fdb\u884c\u8ba1\u7b97\uff0c\u5b83\u5728\u7406\u8bba\u4e0a\u5df2\u8bc1\u660e\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u663e\u8457\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u7684\u65b9\u6cd5\u3002</p> <p>\u5728\u5b9e\u9645\u4e2d\uff0c\u6211\u4eec\u5728\u727a\u7272\u5c11\u91cf\u8ba1\u7b97\u6548\u7387\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86 1000 \u500d\u7684\u6a21\u578b\u5bb9\u91cf\uff08model capacity\uff09\u7684\u63d0\u5347\u3002</p> <p>\u6211\u4eec\u5f15\u5165\u4e86\u7a00\u758f\u95e8\u63a7\u4e13\u5bb6\u6df7\u5408\u5c42\uff08Sparsely-Gated Mixture-of-Experts Layer\uff09\uff0c\u5305\u62ec\u6570\u4ee5\u5343\u8ba1\u7684\u524d\u9988\u5b50\u7f51\u7edc\u3002\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u6837\u672c\uff0c\u6709\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u95e8\u63a7\u7f51\u7edc\uff08gating network\uff09\u4f1a\u8ba1\u7b97\u8fd9\u4e9b\u4e13\u5bb6\uff08\u6307\u524d\u9988\u5b50\u7f51\u7edc\uff09\u7684\u7a00\u758f\u7ec4\u5408\u3002\u5728\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u67b6\u6784\u91cc\uff0cMoE \u5305\u542b 1370 \u4ebf\u4e2a\u53c2\u6570\uff0c\u4ee5\u5377\u79ef\u7684\u65b9\u5f0f\u653e\u5728\u5806\u53e0 LSTM \u5c42\u4e4b\u95f4\u3002\u6211\u4eec\u628a\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u5e94\u7528\u4e8e\u8bed\u8a00\u5efa\u6a21\u548c\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u5bf9\u4e8e\u8fd9\u4e9b\u4efb\u52a1\uff0c\u4ece\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u5438\u6536\u7684\u5de8\u91cf\u77e5\u8bc6\uff0c\u662f\u5341\u5206\u5173\u952e\u7684\u3002</p> <p>\u5728\u5927\u578b\u8bed\u8a00\u5efa\u6a21\u548c\u53ca\u5176\u7ffb\u8bd1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6a21\u578b\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u597d\u7684\u7ed3\u679c\u3002</p>"},{"location":"sci-paper/cs/MoE/#\u65b9\u6cd5","title":"\u65b9\u6cd5","text":"<p>\u672c\u6587\u7684\u6761\u4ef6\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5c31\u662f\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u901a\u7528\u795e\u7ecf\u7f51\u7edc\u7ec4\u4ef6\u7c7b\u578b\uff1a\u7a00\u758f\u95e8\u63a7\u4e13\u5bb6\u6df7\u5408\u5c42\uff08<code>MoE</code>\uff09\uff1a</p> <p><code>MoE</code>\u7531\u8bb8\u591a\u4e13\u5bb6(experts)(\u6bcf\u4e2a\u4e13\u5bb6\u90fd\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc)\uff0c\u4ee5\u53ca\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u95e8\u63a7\u7f51\u7edc\u7ec4\u6210\uff0c\uff08\u95e8\u63a7\u7f51\u7edc\u53ef\u4ee5\u9009\u62e9\u4e13\u5bb6\u7684\u7a00\u758f\u7ec4\u5408\u6765\u5904\u7406\u6bcf\u4e2a\u8f93\u5165\uff09\u3002</p> <p></p> <p>\u672c\u6587\u805a\u7126\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u628a<code>MoE</code>\u4ee5\u5377\u79ef\u7684\u65b9\u5f0f\u653e\u5728\u591a\u5c42<code>LSTM</code>\u5c42\u4e4b\u95f4\u3002\u5728\u6587\u672c\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u4e0a\uff0c\u5c31\u4f1a\u8c03\u7528<code>MoE</code>\u4e00\u6b21\uff0c\u8fdb\u800c\u53ef\u80fd\u9009\u62e9\u4e0d\u540c\u7684\u4e13\u5bb6\u7ec4\u5408\u3002\u4e0d\u540c\u7684\u4e13\u5bb6\u4f1a\u503e\u5411\u4e8e\u53d8\u5f97\u9ad8\u5ea6\u4e13\u4e1a\u5316\uff08\u57fa\u4e8e\u8bed\u6cd5\u548c\u8bed\u4e49\uff09\u3002</p> <p>\u4e00\u4e2a\u6807\u51c6\u7684<code>MoE</code>\u5c42\u5305\u62ec\uff1a</p> <ul> <li> <p>\u591a\u4f4d\u4e13\u5bb6\uff08experts\uff09:\\(E_1, E_2,... ,E_n\\)</p> </li> <li> <p>\u4e00\u4e2a\u95e8\u63a7\u7f51\u7edc\\(G\\)\uff0c\u5176\u8f93\u51fa\u662f\u4e00\u4e2a\u7a00\u758f\u7684n\u7ef4\u5411\u91cf</p> </li> </ul> <p>\u56e0\u6b64<code>MoE</code>\u5c42\u7684\u8f93\u51fa\u4e3a\uff1a</p> \\[ \\hat{y} = \\sum_{i=1}^{n} G_i(x) E_i(x) \\] <p>\u5176\u4e2d\\(G(x)\\)\u662f\u95e8\u63a7\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\\(E_i(x)\\)\u662f\u7b2ci\u4e2a\u4e13\u5bb6\u7684\u8f93\u51fa\u3002</p> <p>\u7531\u4e8e\\(G(x)\\)\u5177\u6709\u7a00\u758f\u6027\uff0c\u56e0\u6b64\u53ef\u4ee5\u5927\u5927\u8282\u7701\u8ba1\u7b97\u91cf\uff08\u5373\u5f53\\(G_i(x)=0\\)\u65f6\uff0c\u5bf9\u5e94\u7684\\(E_i(x)\\)\u4e0d\u9700\u8981\u8ba1\u7b97\uff09</p>"},{"location":"sci-paper/cs/MoE/#\u95e8\u63a7\u7f51\u7edcg","title":"\u95e8\u63a7\u7f51\u7edc\\(G\\)","text":"\\[ G(x) = \\text{Softmax}(x \\text{W}_G) \\] <p>\u4e3a\u4e86\u786e\u4fdd\u8f93\u51fa\u7684\u7a00\u758f\u6027\uff0c\u6211\u4eec\u9009\u62e9\u4fdd\u7559top-k\u7684\u503c\u3002\u6211\u4eec\u4e5f\u52a0\u5165\u4e86\u53ef\u8c03\u7684\u9ad8\u65af\u566a\u58f0\uff0c\u4e3a\u4e86\u5e2e\u52a9\u8d1f\u8f7d\u5747\u8861\uff08load balancing\uff09</p> <p>\u5373\uff1a</p> \\[ G(x)=\\operatorname{Softmax}(\\operatorname{KeepTopK}(H(x), k)) \\] \\[  H(x)_{i}=\\left(x \\cdot W_{g}\\right)_{i}+\\text { StandardNormal }() \\cdot \\operatorname{Softplus}\\left(\\left(x \\cdot W_{\\text {noise }}\\right)_{i}\\right) \\]"},{"location":"sci-paper/cs/MoE/#\u5e73\u8861\u4e13\u5bb6\u7684\u5229\u7528\u7387balancing-expert-utilization","title":"\u5e73\u8861\u4e13\u5bb6\u7684\u5229\u7528\u7387(BALANCING EXPERT UTILIZATION)","text":"<p>\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u95e8\u63a7\u7f51\u7edc\u503e\u5411\u4e8e\u6536\u655b\u5230\u4e00\u79cd\u4e0d\u597d\u7684\u72b6\u6001\uff0c\u5373\u5bf9\u5c11\u91cf\u4e13\u5bb6\uff0c\u603b\u662f\u4f1a\u5f97\u5230\u8f83\u5927\u7684\u6743\u91cd\u3002\u8fd9\u79cd\u4e0d\u5e73\u8861\u662f\u4e0d\u65ad\u81ea\u6211\u5f3a\u5316\uff08self-reinforcing\uff09\u7684\uff0c\u968f\u7740\u66f4\u597d\u7684\u4e13\u5bb6\u4e0d\u65ad\u8bad\u7ec3\u5b66\u4e60\uff0c\u5b83\u4eec\u66f4\u6709\u53ef\u80fd\u88ab\u95e8\u63a7\u7f51\u7edc\u9009\u4e2d\u3002\u9762\u5bf9\u8fd9\u79cd\u95ee\u9898\uff0c\u8fc7\u53bb\u6587\u732e\u6709\u7684\u7528\u786c\u6027\u7ea6\u675f\uff0c\u6709\u7684\u7528\u8f6f\u6027\u7ea6\u675f\u3002</p> <p>\u6211\u4eec\u91c7\u7528\u8f6f\u6027\u7ea6\u675f\u65b9\u6cd5\u3002\u6211\u4eec\u5b9a\u4e49\u5bf9\u4e8e\u4e00\u4e2a\u6279\u6b21\u8bad\u7ec3\u6837\u672c\u7684\u4e13\u5bb6\u91cd\u8981\u5ea6\uff08the importance of an expert\uff09\uff0c\u5373\u8be5\u4e13\u5bb6\u5728\u4e00\u4e2a\u6279\u6b21\u4e0a\u7684\u95e8\u63a7\u8f93\u51fa\u503c\u7684\u548c\u3002\u5e76\u4e14\u5b9a\u4e49\u635f\u5931\u9879\\(L_{importance}\\)\uff0c\u52a0\u5165\u5230\u6a21\u578b\u7684\u603b\u635f\u5931\u4e0a\u3002\u8be5\u635f\u5931\u9879\u7b49\u4e8e\u6240\u6709\u4e13\u5bb6\u91cd\u8981\u5ea6\u7684\u65b9\u5dee\u7684\u5e73\u65b9\uff0c\u518d\u52a0\u4e0a\u4e00\u4e2a\u624b\u5de5\u8c03\u8282\u7684\u6bd4\u4f8b\u56e0\u5b50\\(w_{importance}\\)\u3002\u8fd9\u4e2a\u635f\u5931\u9879\u4f1a\u9f13\u52b1\u6240\u6709\u4e13\u5bb6\u6709\u76f8\u540c\u7684\u91cd\u8981\u5ea6\u3002 \\[ Importance(X) = \\sum_{x \\in X} G(x) \\] \\[ L_{importance}(X) = w_{importance} \\cdot CV(Importance(X))^2 \\] <p>\u5c3d\u7ba1\u73b0\u5728\u7684\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u4fdd\u8bc1\u76f8\u540c\u7684\u91cd\u8981\u5ea6\uff0c\u4e13\u5bb6\u4ecd\u7136\u53ef\u80fd\u63a5\u6536\u5230\u5dee\u5f02\u5f88\u5927\u7684\u6837\u672c\u6570\u76ee\u3002\u4f8b\u5982\uff0c\u67d0\u4e9b\u4e13\u5bb6\u53ef\u80fd\u63a5\u6536\u5230\u5c11\u91cf\u7684\u5927\u6743\u91cd\u7684\u6837\u672c\uff1b\u800c\u67d0\u4e9b\u4e13\u5bb6\u53ef\u80fd\u63a5\u6536\u5230\u66f4\u591a\u7684\u5c0f\u6743\u91cd\u7684\u6837\u672c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7b2c\u4e8c\u4e2a\u635f\u5931\u51fd\u6570\uff1a\\(L_{load}\\)\uff0c\u5b83\u53ef\u4ee5\u4fdd\u8bc1\u8d1f\u8f7d\u5747\u8861\u3002</p>"},{"location":"sci-paper/cs/MoE/#\u8d1f\u8f7d\u5e73\u8861l_load","title":"\u8d1f\u8f7d\u5e73\u8861\\(L_{load}\\)","text":"<p>\u51fa\u4e8e\u8d1f\u8f7d\u5e73\u8861\u7684\u76ee\u7684\uff0c\u6211\u4eec\u5e0c\u671b\u5b9a\u4e49\u4e00\u4e2a\u989d\u5916\u7684\u635f\u5931\u51fd\u6570\u6765\u9f13\u52b1\u4e13\u5bb6\u63a5\u6536\u5927\u81f4\u76f8\u540c\u6570\u91cf\u7684\u8bad\u7ec3\u793a\u4f8b\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u4e13\u5bb6\u63a5\u6536\u5230\u7684\u793a\u4f8b\u6570\u91cf\u662f\u4e00\u4e2a\u79bb\u6563\u7684\u6570\u91cf\uff0c\u56e0\u6b64\u5b83\u4e0d\u80fd\u7528\u4e8e\u53cd\u5411\u4f20\u64ad\u3002</p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u4e3a\u4e00\u6279\\(X\\)\u7684\u8f93\u5165\u5b9a\u4e49\u4e86\u5206\u914d\u7ed9\u6bcf\u4e2a\u4e13\u5bb6\u7684\u793a\u4f8b\u6570\u91cf\u7684\u5e73\u6ed1\u4f30\u8ba1\u5668\\(Load(X)\\)\uff0c\u5e73\u6ed1\u5ea6\uff08smoothness\uff09\u5141\u8bb8\u6211\u4eec\u901a\u8fc7\u4f30\u8ba1\u5668\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u3002</p> <p>\u5c06\\(P(x,i)\\)\u5b9a\u4e49\u4e3a\\(G_i(x)\\)\u975e\u96f6\uff08the probability of nonzero\uff09\u7684\u6982\u7387\uff0c\u5e76\u4e14\u5728\u5143\u7d20i\u4e0a\u52a0\u5165\u968f\u673a\u566a\u58f0\u9009\u62e9\uff0c\u4f46\u5c06\u5df2\u7ecf\u91c7\u6837\u7684\u566a\u58f0\u9009\u62e9\u4fdd\u6301\u5728\u5176\u4ed6\u5143\u7d20\u4e0a\u3002\\(G_i(x)\\)\u975e\u96f6\u5f53\u4e14\u4ec5\u5f53\\(H_i(x)\\)\u5927\u4e8e\\(H(x)\\)\u7684\u7b2ck\u5927\u7684\u5143\u7d20\uff08\u4e0d\u5305\u62ec\u81ea\u8eab\uff09\uff0c\u5373\u4e3a\uff1a</p> \\[ P(x,i) = Pr((xW_g)_i + StandardNormal() \\cdot Softplus((xW_{noise})_i)  $$ \\] <p>kth _ excluding(H(x),k,i)) $$ \u5176\u4e2d\\(kth \\_ excluding(H(x),k,i)\\)\u8868\u793a\\(H(x)\\)\u7684\u7b2ck\u5927\u7684\u5143\u7d20\uff08\u4e0d\u5305\u62eci\uff09\uff0c\u7b80\u5316\u5f97\uff1a</p> \\[ P(x,i) = \\Phi (\\frac{ (xW_g)_i - kth \\_ excluding(H(x),k,i)) }{Softplus((xW_{noise})_i)}) \\] \\[ Load(x)_i = \\sum_{x \\in X} P(x,i) \\] <p>\u5c06\u8d1f\u8f7d\u635f\u5931\u5b9a\u4e49\u4e3a\u8d1f\u8f7d\u5411\u91cf\u53d8\u5f02\u7cfb\u6570\u7684\u5e73\u65b9\uff0c\u4e58\u4ee5\u624b\u52a8\u8c03\u6574\u7684\u6bd4\u4f8b\u56e0\u5b50 \\(w_{load}\\)\u3002</p> \\[ L_{load}(X) = w_{load} \\cdot CV(Load(X))^2 \\]"},{"location":"sci-paper/cs/MoE/#\u5206\u5c42\u6df7\u5408\u4e13\u5bb6hierachical-mixture-of-experts","title":"\u5206\u5c42\u6df7\u5408\u4e13\u5bb6\uff08HIERACHICAL MIXTURE OF EXPERTS\uff09","text":"<p>\u5f53\u4e13\u5bb6\u6570\u91cf\u8f83\u5927\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7\u5206\u5c42MoE\u6765\u8ba1\u7b97\uff1a\u5728\u5206\u5c42MoE\u4e2d\uff0c\u4e3b\u95e8\u63a7\u7f51\u7edc\u9009\u62e9\u201c\u4e13\u5bb6\u201d\u7684\u7a00\u758f\u52a0\u6743\u7ec4\u5408\uff0c\u6bcf\u4e2a\u7ec4\u5408\u672c\u8eab\u5c31\u662f\u4e00\u4e2a\u5177\u6709\u81ea\u5df1\u95e8\u63a7\u7f51\u7edc\u7684\u4e8c\u6b21\u4e13\u5bb6\u6df7\u5408\u3002\u6211\u4eec\u7528\\(G_{primary}\\)\u4ee3\u8868\u4e3b\u95e8\u63a7\u7f51\u7edc\uff0c\u7528(\\(G_1,G_2,...,G_a\\))\u4ee3\u8868\u4e8c\u6b21\u95e8\u63a7\u7f51\u7edc\uff0c\u4ee5\u53ca\u5bf9\u5e94\u7684\u4e13\u5bb6\u7f51\u7edc(\\(E_{0,0},E_{0,1},...,E_{a,b}\\)):</p> \\[ y_H = \\sum_{i=1}^{a} \\sum_{j=1}^{b} G_{primary}(x)_i  G_i(x)_j E_{i,j}(x) \\] <p>\u4e13\u5bb6\u5229\u7528\u6307\u6807(metrics of expert utilization)\u66f4\u6539\u4e3a\u4ee5\u4e0b\u5185\u5bb9\uff1a</p> \\[ Importance_H(X)_{i,j} = \\sum_{x \\in X} G_{primary}(x)_i \\cdot G_i(x)_j \\] \\[ Load_H(X)_{i,j} = \\frac{Load_{primary}(x)_i \\cdot Load_i(X^{(i)})_j}{|X^{(i)}|} \\]"},{"location":"sci-paper/cs/MobileNet./","title":"MoblieNet","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/lukemelas/EfficientNet-PyTorch/</p> <p>\u672c\u9875\u5185\u5bb9\u662f\u5bf9<code>MoblieNet</code>\u7684\u6587\u7ae0\u603b\u7ed3/\u4ee3\u7801\u9605\u8bfb(\u4fa7\u91cd\u4ee3\u7801\u5b66\u4e60)</p> <p></p>"},{"location":"sci-paper/cs/Moco/","title":"MoCo: Momentum Contrast for Unsupervised Visual Representation Learning","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aMomentum Contrast for Unsupervised Visual Representation Learning</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/facebookresearch/moco</p> <p>\u8d44\u6e90\uff1a</p> <p></p>"},{"location":"sci-paper/cs/Moco/#\u6982\u8ff0","title":"\u6982\u8ff0","text":""},{"location":"sci-paper/cs/Noisy-students/","title":"Self-training with Noisy Student improves ImageNet classification","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aSelf-training with Noisy Student improves ImageNet classification</p> <p>\u4ee3\u7801\uff08tensorflow\u7248\uff09:https://github.com/google-research/noisystudent</p> <p>\u8d44\u6e90\uff1a</p> <p></p>"},{"location":"sci-paper/cs/Noisy-students/#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p><code>Noisy student</code>\u662f\u4e00\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u4e3b\u8981\u57fa\u4e8e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u6559\u5e08\u6a21\u578b\uff09\u751f\u6210\u566a\u58f0\u6570\u636e\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u6570\u636e\u53bb\u8bad\u7ec3\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684\u5b66\u751f\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u3002\u8fd9\u79cd\u65b9\u6cd5\u7684\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u5b83\u6253\u7834\u4e86\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5141\u8bb8\u6a21\u578b\u5728\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u8fdb\u884c\u81ea\u6211\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u6027\u80fd\u3002</p>"},{"location":"sci-paper/cs/Noisy-students/#\u65b9\u6cd5","title":"\u65b9\u6cd5","text":"<p>\u5728\u8be5\u6846\u67b6\u4e0b\uff0c\u6211\u4eec\u51c6\u5907\u6807\u7b7e\u4e0e\u672a\u6807\u7b7e\u7684\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528\u6807\u7b7e\u56fe\u50cf\u6765\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u672a\u6807\u7b7e\u56fe\u50cf\u751f\u6210\u4f2a\u6807\u7b7e\uff08soft or hard\uff09,\u518d\u4f7f\u7528\u6240\u6709\u7684\u56fe\u50cf\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u6700\u540e\u901a\u8fc7\u5c06\u5b66\u751f\u4f5c\u4e3a\u6559\u5e08\u6765\u8fed\u4ee3\u8be5\u8fc7\u7a0b\u4ee5\u751f\u6210\u65b0\u7684\u4f2a\u6807\u7b7e\u5e76\u8bad\u7ec3\u65b0\u5b66\u751f\u3002</p> <p>\u5047\u8bbe\u5df2\u6807\u7b7e\u56fe\u50cf\\(\\{ {(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}  \\}\\)\uff0c\u672a\u6807\u7b7e\u56fe\u50cf$ { \\tilde{x_{1}},\\tilde{x_{2}},...\\tilde{x_{m}} }$\u3002</p> <ul> <li>\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\uff0c\u4f7f\u6807\u8bb0\u56fe\u50cf\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u6700\u5c0f\u5316</li> </ul> \\[ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{L} (y_i, f^{noised}(x_i, \\theta^{teacher})) \\] <ul> <li>\u4f7f\u7528\u65e0\u566a\u58f0\u7684\u6559\u5e08\u6a21\u578b\u4e3a\u672a\u6807\u8bb0\u7684\u56fe\u50cf\u751f\u6210soft or hard\u7684\u4f2a\u6807\u7b7e</li> </ul> \\[ \\tilde{y_i} = f(\\tilde{x_i}, \\theta^{teacher}_*), \\quad \u2200i = 1,2,...,m \\] <ul> <li>\u8bad\u7ec3\u66f4\u5927\uff08equal-or-larger\uff09\u7684\u5b66\u751f\u6a21\u578b\\(\\theta^{student}_*\\)\uff0c\u4f7f\u6807\u8bb0\u56fe\u50cf\u548c\u672a\u6807\u8bb0\u56fe\u50cf\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u6700\u5c0f\u5316\uff0c\u566a\u58f0\u88ab\u6dfb\u52a0\u5230\u5b66\u751f\u6a21\u578b\u4e2d\u3002</li> </ul> \\[ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{L} (y_i, f^{noised}(x_i, \\theta^{student})) + \\frac{1}{m} \\sum_{i=1}^{n} \\mathbf{L} (\\tilde{y_i}, f^{noised}(\\tilde{x_i}, \\theta^{student})) \\] <ul> <li>\u8fed\u4ee3\u8bad\u7ec3</li> </ul> <p></p> <p>\u663e\u7136<code>Noisy student</code>\u662f\u4e00\u79cd\u81ea\u6211\u8bad\u7ec3\u7684\u534a\u76d1\u7763(semi-supervised)\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5728\u672a\u6807\u6ce8\u6570\u636e\u4e0a\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u6765\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u540c\u4e8e<code>Knowledge Distillation</code>\uff0c\u5176\u4e2d\u6dfb\u52a0\u566a\u58f0\u4e0d\u662f\u6838\u5fc3\u95ee\u9898\uff0c\u800c\u5c0f\u6a21\u578b\u7ecf\u5e38\u88ab\u7528\u4f5c\u5b66\u751f\u6bd4\u8001\u5e08\u66f4\u5feb\u3002\u4eba\u4eec\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u89c6\u4e3a\u77e5\u8bc6\u6269\u5c55\uff0c\u6211\u4eec\u5e0c\u671b\u5b66\u751f\u5728\u566a\u58f0\u65b9\u9762\u8d4b\u4e88\u5b66\u751f\u6a21\u578b\u66f4\u591a\u7684\u5bb9\u91cf\u548c\u56f0\u96be\u7684\u73af\u5883\u6765\u5b66\u4e60\uff0c\u4ece\u800c\u6bd4\u8001\u5e08\u66f4\u597d\u3002</p> <ul> <li><code>Noising Student</code></li> </ul> <p>\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u6dfb\u52a0\u4e86\u4e24\u79cd\u566a\u97f3\uff1a\u8f93\u5165\u566a\u58f0\u548c\u6a21\u578b\u566a\u58f0\uff08input noise and model noise\uff09\u3002\u5bf9\u4e8e\u8f93\u5165\u566a\u97f3\uff0c\u4f7f\u7528<code>RandAugment</code>\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff1b\u5bf9\u4e8e\u6a21\u578b\u566a\u97f3\uff0c\u6211\u4eec\u4f7f\u7528 <code>dropout</code> \u4e0e <code>stochastic depth</code>\uff08\u968f\u673a\u6df1\u5ea6\uff09\u3002</p> <p><code>Noisy Student</code>\u901a\u8fc7\u4e00\u4e2a\u989d\u5916\u7684\u6280\u5de7\u4f7f\u5f97\u6548\u679c\u66f4\u597d\uff1a\u6570\u636e\u8fc7\u6ee4\u548c\u5e73\u8861(data filtering and balancing)\uff0c\u5373\u901a\u8fc7\u6ee4\u53bb\u7f6e\u4fe1\u5ea6\u8f83\u4f4e\u7684\u56fe\u50cf\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u56fe\u50cf\u901a\u5e38\u662f\u57df\u5916\u6570\u636e(out-of-domain)\uff0c\u4e3a\u4e86\u786e\u4fdd\u672a\u6807\u8bb0\u56fe\u50cf\u7684\u5206\u5e03\u4e0e\u8bad\u7ec3\u96c6\u7684\u5206\u5e03\u76f8\u5339\u914d\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u5e73\u8861\u6bcf\u4e2a\u7c7b\u7684\u672a\u6807\u8bb0\u56fe\u50cf\u7684\u6570\u91cf\u3002\u5bf9\u4e8e\u6211\u4eec\u6709\u592a\u591a\u56fe\u50cf\u7684\u7c7b\uff0c\u6211\u4eec\u53d6\u7f6e\u4fe1\u5ea6\u6700\u9ad8\u7684\u56fe\u50cf\u3002</p> <p>\u4f5c\u8005\u5728\u6587\u4e2d\u8bf4\u660e\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u53ef\u4ee5\u662fsoft\u6216hard\u7684\uff0c\u56e0\u4e3a\u5b9e\u9a8c\u89c2\u5bdf\u5230soft\u548chard\u5bf9\u4e8e\u6a21\u578b\u7ed3\u679c\u90fd\u5f88\u6709\u6548\uff0c\u5176\u4e2dsoft\u5bf9\u4e8e\u57df\u5916\u672a\u6807\u8bb0\u6570\u636e\uff08out of domain unlabeled data\uff09\u7684\u5de5\u4f5c\u7a0d\u597d\uff09</p>"},{"location":"sci-paper/cs/Noisy-students/#\u5b9e\u9a8c","title":"\u5b9e\u9a8c","text":"<p>\u6587\u7ae0\u5c06<code>Noisy student</code>\u4e0e\u5f53\u65f6\u7684SOTA\u6bd4\u8f83\u4e86\u5728<code>ImageNet</code>\u4e0a\u7684\u7ed3\u679c\u3002\u6700\u540e\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u9c81\u68d2\u6027\u6570\u636e\u96c6\uff08ImageNet-A\u3001C\u3001P\uff09\u4ee5\u53ca\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u9762\u7684\u60ca\u4eba\u6539\u8fdb\u3002</p> <p>\u5728\u6a21\u578b\u67b6\u6784\u4e0a\u4f7f\u7528\u4e86\u5f53\u65f6\u5148\u8fdb\u7684<code>EfficientNets</code>\u4f5c\u4e3abaseline\uff0c\u5e76\u4e14\u8fdb\u4e00\u6b65\u6269\u5927\u4e86<code>EfficientNetB7</code>\uff0c\u5e76\u83b7\u5f97\u4e86<code>EfficientNet-L2</code>\u3002<code>EfficientNet-L2</code>\u6bd4<code>EfficientNet-B7</code>\u66f4\u5e7f\u6cdb\u548c\u66f4\u6df1\uff0c\u4f46\u4f7f\u7528\u4e86\u8f83\u4f4e\u7684\u5206\u8fa8\u7387\uff0c\u8fd9\u4f7f\u5f97\u5b83\u6709\u66f4\u591a\u7684\u53c2\u6570\u6765\u62df\u5408\u5927\u91cf\u7684\u672a\u6807\u8bb0\u56fe\u50cf\u3002\u7531\u4e8e\u6a21\u578b\u5c3a\u5bf8\u8f83\u5927\uff0c<code>EfficientNet-L2</code>\u7684\u8bad\u7ec3\u65f6\u95f4\u5927\u7ea6\u662f<code>EfficientNet-B7</code>\u8bad\u7ec3\u65f6\u95f4\u7684\u4e94\u500d\u3002</p> <p></p> <p>\u5728ImageNet-A\u3001C\u3001P\u4e0a\u7684\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5728\u5e76\u6ca1\u6709\u523b\u610f\u9488\u5bf9\u9c81\u68d2\u6027\u8fdb\u884c\u4f18\u5316\u7684\u60c5\u51b5\u4e0b\uff0c<code>Noisy Student</code>\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6548\u679c\uff0c\u8fd9\u8868\u660e\u5373\u4f7f\u6ca1\u6709\u76f4\u63a5\u4f18\u5316\u9c81\u68d2\u6027\uff0c<code>Noisy student</code>\u4e5f\u80fd\u5927\u5927\u63d0\u9ad8\u9c81\u68d2\u6027\u3002</p> <p></p> <p>\u5728\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u7684\u5178\u578b\u81ea\u6211\u8bad\u7ec3\uff08self-training with the teacher-student framework\uff09\u4e2d\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e0d\u4f7f\u7528\u5bf9\u5b66\u751f\u8fdb\u884c\u566a\u58f0\u6ce8\u5165\uff0c\u6216\u8005\u566a\u58f0\u7684\u4f5c\u7528\u5c1a\u672a\u5b8c\u5168\u4e86\u89e3\u6216\u8bc1\u660e\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u548c\u5148\u524d\u5de5\u4f5c\u7684\u4e3b\u8981\u533a\u522b\u5728\u4e8e\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u566a\u58f0\u7684\u91cd\u8981\u6027\uff0c\u5e76\u79ef\u6781\u5730\u6ce8\u5165\u566a\u58f0\u4ee5\u4f7f\u5b66\u751f\u66f4\u597d\u3002</p> <p><code>Knowledge Distillation</code>\u7684\u4e3b\u8981\u7528\u9014\u662f\u901a\u8fc7\u4f7f\u5b66\u751f\u6a21\u578b\u66f4\u5c0f\u6765\u8fdb\u884c\u6a21\u578b\u538b\u7f29\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u548c<code>Knowledge Distillation</code>\u7684\u4e3b\u8981\u533a\u522b\u5728\u4e8e<code>Knowledge Distillation</code>\u6ca1\u6709\u8003\u8651\u672a\u6807\u8bb0\u7684\u6570\u636e\uff0c\u4e14<code>Noisy Student</code>\u7684\u76ee\u6807\u4e0d\u662f\u6539\u8fdb\u5b66\u751f\u6a21\u578b\uff0c\u800c\u662f\u901a\u8fc7\u4e24\u4e2a\u6a21\u578b\u57fa\u4e8eteacher-student\u7684\u6846\u67b6\u4ea4\u6362\u8fed\u4ee3\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002"},{"location":"sci-paper/cs/Noisy-students/#\u603b\u7ed3","title":"\u603b\u7ed3","text":"<p>NoisyStudent\u662f\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u4e00\u79cd\u521b\u65b0\u5c1d\u8bd5\uff0c\u5b83\u4ee5\u81ea\u6211\u76d1\u7763\u7684\u65b9\u5f0f\u6316\u6398\u672a\u6807\u6ce8\u6570\u636e\u7684\u4ef7\u503c\uff0c\u63d0\u4f9b\u4e86\u5728\u5927\u6570\u636e\u65f6\u4ee3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u65b0\u9014\u5f84\u3002</p>"},{"location":"sci-paper/cs/ViT/","title":"ViT\uff1aAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","text":"<p>\u76f8\u5173\u4fe1\u606f</p> <p> <p>\u8bba\u6587\u5730\u5740\uff1aAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</p> <p>\u4ee3\u7801\uff08Pytorch\u7248\uff09:https://github.com/huggingface/pytorch-image-models/</p> <p>Labml.ai\u6ce8\u91ca\u5b9e\u73b0\uff1ahttps://nn.labml.ai/zh/transformers/vit/index.html</p> <p>https://github.com/yangyunfeng-cyber/Useful-DL-Projects-for-Exercise/blob/main/VIT/vit_model.py</p> <p></p> <p></p> <p>\u6587\u7ae0\u6458\u8981</p> <p> <p><code>ViT</code>\u662f2020\u5e74Google\u56e2\u961f\u63d0\u51fa\u7684\u5c06<code>Transformer</code>\u5e94\u7528\u5728\u56fe\u50cf\u5206\u7c7b\u7684\u6a21\u578b\uff0c\u867d\u7136\u4e0d\u662f\u7b2c\u4e00\u7bc7\u5c06<code>transformer</code>\u5e94\u7528\u5728\u89c6\u89c9\u4efb\u52a1\u7684\u8bba\u6587\uff0c\u4f46\u662f\u56e0\u4e3a\u5176\u6a21\u578b\u201c\u7b80\u5355\u201d\u4e14\u6548\u679c\u597d\uff0c\u53ef\u6269\u5c55\u6027\u5f3a\uff08scalable\uff0c\u6a21\u578b\u8d8a\u5927\u6548\u679c\u8d8a\u597d\uff09\uff0c\u57fa\u4e8e<code>Transformer</code>\u7684\u6a21\u578b\u5728\u89c6\u89c9\u9886\u57df\u7684\u5f00\u7bc7\u4e4b\u4f5c\u3002ViT\u6a21\u578b\u662f\u57fa\u4e8e<code>Transformer Encoder</code>\u6a21\u578b\u7684\u3002</p> <p>\u76f8\u5173\u6280\u672f\uff1a</p> <p>Big Transfer (BiT): General Visual Representation Learning</p> <p>Self-training with Noisy Student improves ImageNet classification</p> <p></p>"},{"location":"sci-paper/cs/ViT/#vit-\u67b6\u6784","title":"ViT \u67b6\u6784","text":"<p>\u8be5\u7b97\u6cd5\u5728\u4e2d\u7b49\u89c4\u6a21\uff08\u4f8b\u5982<code>ImageNet</code>\uff09\u4ee5\u53ca\u5927\u89c4\u6a21\uff08\u4f8b\u5982<code>ImageNet-21K</code>\u3001<code>JFT-300M</code>\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u53d1\u73b0\uff1a</p> <ul> <li> <p><code>Tranformer</code>\u76f8\u8f83\u4e8e<code>CNN</code>\u7ed3\u6784\uff0c\u7f3a\u5c11\u4e00\u5b9a\u7684\u5e73\u79fb\u4e0d\u53d8\u6027Translation Equivariance\u548c\u5c40\u90e8\u611f\u77e5\u6027Locality(\u5f52\u7eb3\u504f\u7f6eInductive Bias)\uff0c\u56e0\u6b64\u5728\u6570\u636e\u91cf\u4e0d\u5145\u5206\u65f6\uff0c\u5f88\u96be\u8fbe\u5230\u540c\u7b49\u7684\u6548\u679c\u3002\u5177\u4f53\u8868\u73b0\u4e3a\u4f7f\u7528\u4e2d\u7b49\u89c4\u6a21\u7684ImageNet\u8bad\u7ec3\u7684<code>Tranformer</code>\u4f1a\u6bd4<code>ResNet</code>\u5728\u7cbe\u5ea6\u4e0a\u4f4e\u51e0\u4e2a\u767e\u5206\u70b9\u3002</p> </li> <li> <p>\u5f53\u6709\u5927\u91cf\u7684\u8bad\u7ec3\u6837\u672c\u65f6\uff0c\u7ed3\u679c\u5219\u4f1a\u53d1\u751f\u6539\u53d8\u3002\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\u540e\uff0c\u518d\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u7684\u65b9\u5f0f\u5e94\u7528\u5230\u5176\u4ed6\u6570\u636e\u96c6\u4e0a\uff0c\u53ef\u4ee5\u8fbe\u5230\u6216\u8d85\u8d8a\u5f53\u524d\u7684SOTA\u6c34\u5e73\u3002</p> </li> </ul> <p>\u56e0\u4e3a<code>Tranformer</code>\u672c\u8eab\u5e76\u4e0d\u80fd\u8f93\u5165\u4e8c\u7ef4\u7684\u56fe\u50cf\u6570\u636e\uff0c\u56e0\u6b64\u5148\u5c06\u56fe\u50cf\u5212\u5206\u4e3a<code>patches</code>\uff0c\u5373\u8f93\u5165\u56fe\u50cf\\(\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}\\)\u88ab\u5212\u5206\u4e3a\u5927\u5c0f\u4e3a\\(P \\times P\\)\u7684<code>patch</code>\uff0c\u5f62\u6210\u957f\u5ea6\u4e3a\\(N=\\frac{HW}{P^2}\\)\u4e2a\u56fe\u50cf\u5757\u7684\u5e8f\u5217\uff0c\u6bcf\u4e2a<code>patch</code>\u8868\u793a\u4e3a\\(\\mathbf{x}_p^i \\in \\mathbb{R}^{1 \\times (P^2 \\times C)} ,i \\in \\{1,...,N \\}\\)\u3002 </p> <p>\u4e3a\u4e86\u8f93\u5165\u8fdb<code>Tranformer</code>\uff0c\u5c06\u6bcf\u4e2a<code>patch</code>\u62c9\u5e73\u4e3a\u4e00\u7ef4\u5411\u91cf\uff0c\u518d\u901a\u8fc7\u4e00\u4e2a\u7ebf\u6027\u5c42\u8fdb\u884c\u6620\u5c04\u4e3a\\(\\mathbf{E} = \\mathbb{R}^{(P^2 \\times C) \\times D}\\)\uff0c\u6620\u5c04\u4e3a\u4e00\u4e2a\u7ef4\u5ea6\u4e3aD\u7684\u4e00\u7ef4\u5411\u91cf\u3002</p> <p>\u5728\u6240\u6709<code>patch</code>\u4e4b\u524d\uff0c\u751f\u6210\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684<code>[Class] Token</code>(\u4e00\u4e2a\u968f\u673a\u7684\u4e00\u7ef4\u5411\u91cf)\uff0c\u5c06\u5176\u6dfb\u52a0\u5230\u56fe\u50cf\u5757\u6295\u5f71\u5e8f\u5217\u7684\u524d\u9762\uff0c\u8be5<code>class token</code>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u4e0d\u65ad\u66f4\u65b0\uff0c\u7528\u4e8e\u8868\u793a\u6574\u4e2a\u56fe\u50cf\u7684\u4fe1\u606f\u3002</p> <p>\u540c\u65f6\uff0c\u4e3a\u4e86\u53cd\u5e94\u6bcf\u4e2a\u56fe\u50cf\u5757\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u7ed9\u6bcf\u4e2a\u56fe\u50cf\u5757\u5d4c\u5165\u6dfb\u52a0\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\u3002\u56e0\u6b64\u5171\u6709N+1\u4e2a\u5e8f\u5217\uff0c\u6240\u4ee5\u53ef\u5b66\u4e60\u4f4d\u7f6e\u7f16\u7801\u8868\u793a\u4e3a\\(\\mathbf{E}_{pos} = \\mathbb{R}^{(N+1) \\times D}\\)\u3002</p> \\[ \\mathbf{z}_0 = [\\mathbf{x}_{class}; \\mathbf{x}_p^1 \\mathbf{E}; \\mathbf{x}_p^2 \\mathbf{E}; \\cdots; \\mathbf{x}_p^N \\mathbf{E}] + \\mathbf{E}_{pos}  \\] \\[ \\mathbf{z}'_{l} = MSA(LN(\\mathbf{z}_{l-1})) + \\mathbf{z}_{l-1}  \\] \\[ \\mathbf{z}_{l} = MLP(LN(\\mathbf{z}'_{l})) + \\mathbf{z}'_{l}  \\] \\[ \\mathbf{y} = LN(\\mathbf{z}_L^0) \\]"},{"location":"sci-paper/cs/ViT/#vit-\u6a21\u578b\u7ed3\u6784","title":"ViT \u6a21\u578b\u7ed3\u6784","text":"ViT\u6846\u67b6\u7b80\u6d01\u5b9e\u73b0\uff08lucidrains\uff09 <pre><code>import torch\nfrom torch import nn\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass FeedForward(nn.Module): \n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.ffnet = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(), # GELU\u6539\u8fdb\u4e86ReLU\u5728x=0\u5904\u4e0d\u53ef\u5bfc\uff0c\u51fd\u6570\u66f2\u7ebf\u4e0d\u5e73\u6ed1\u7684\u7f3a\u70b9\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        return self.ffnet(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        '''\n        dim\uff1a\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7279\u5f81\u7ef4\u5ea6\n        heads\uff1a\u591a\u5934\u6ce8\u610f\u529b\u7684\u5934\u6570\n        dim_head\uff1a\u6bcf\u4e2a\u5934\u7684\u7ef4\u5ea6\n        '''\n\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.norm = nn.LayerNorm(dim)\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim),\n                                    nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        x = self.norm(x)\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -&gt; b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n                FeedForward(dim, mlp_dim, dropout = dropout)\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n\n        return self.norm(x)\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n        super().__init__()\n\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n\n        # \u56fe\u50cf\u5c3a\u5bf8\u88ab\u5757\u5c3a\u5bf8\u6574\u9664\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        # patch\u6570\u91cf\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n\n        # patch\u5d4c\u5165\u7ef4\u5ea6\n        patch_dim = channels * patch_height * patch_width\n\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)' # ViT\u7684\u4e24\u79cd\u8f93\u51fa\u6a21\u5f0f\uff1atoken[class] or average pooling\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width), # \u91cd\u65b0\u6392\u5217\u56fe\u50cf\u6570\u636e\u4ee5\u5f62\u6210patch\uff08einops\u5e93\uff09\n            nn.LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim),\n            nn.LayerNorm(dim),\n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Linear(dim, num_classes)\n\n    def forward(self, img):\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '1 1 d -&gt; b 1 d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)\n</code></pre>"},{"location":"sci-paper/cs/ViT/#droppath\u66ff\u4ee3dropout","title":"DropPath\u66ff\u4ee3Dropout","text":"<p>\u53ef\u4ee5\u91c7\u7528<code>DropPath\uff08Stochastic Depth\uff09</code>\u6765\u4ee3\u66ff\u4f20\u7edf\u7684<code>Dropout</code>\u7ed3\u6784\u3002DropPath\u662f\u4e00\u79cd\u9488\u5bf9\u5206\u652f\u7f51\u7edc\u800c\u63d0\u51fa\u7684\u7f51\u7edc\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5176\u4f5c\u7528\u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u968f\u673a\u4e22\u5f03\u5b50\u56fe\u5c42\uff08randomly drop a subset of layers\uff09\uff0c\u800c\u5728\u9884\u6d4b\u65f6\u6b63\u5e38\u4f7f\u7528\u5b8c\u6574\u7684 Graph.\u3002\u5176\u4e2d\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u79cd<code>DropPath</code>\u65b9\u6cd5\uff1a</p> <ul> <li> <p>Local Drop\uff1a\u5bf9join\u5c42\u7684\u8f93\u5165\u5206\u652f\u6309\u4e00\u5b9a\u7684\u6982\u7387\u8fdb\u884c\u4e22\u5f03\uff0c\u4f46\u662f\u81f3\u5c11\u4fdd\u8bc1\u8981\u6709\u4e00\u4e2a\u8f93\u5165</p> </li> <li> <p>Global Drop\uff1a\u6574\u4e2a\u7f51\u7edc\u6765\u53ea\u9009\u62e9\u4e00\u6761\u8def\u5f84\uff0c\u4e14\u9650\u5236\u4e3a\u67d0\u4e2a\u5355\u72ec\u5217\uff0c\u8be5\u8def\u5f84\u5177\u6709\u72ec\u7acb\u7684\u5f3a\u9884\u6d4b\u3002</p> </li> </ul> <p></p> <p>\u53ef\u4ee5\u4f7f\u7528<code>from timm.layers import DropPath</code>\u6765\u8c03\u7528</p>"},{"location":"sci-paper/cs/ViT/#\u5b9e\u9a8c","title":"\u5b9e\u9a8c","text":"<p><code>ViT</code>\u8bba\u6587\u4e2d\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u5b9e\u9a8c\u4e3b\u8981\u91c7\u7528\u4e86\u4f20\u7edf\u8303\u5f0f\uff1a\u9996\u5148\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5b83\u5e76\u6ca1\u6709\u50cf BERT \u4e00\u6837\u8bbe\u8ba1\u51fa\u521b\u65b0\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u5f0f\u3002\u7136\u800c\uff0c\u540e\u7eed\u80af\u5b9a\u53ef\u4ee5\u4f7f\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6280\u672f\u6765\u8fdb\u4e00\u6b65\u6539\u8fdb<code>ViT</code>\u6a21\u578b\u3002</p> <p></p>"},{"location":"study-cs/","title":"\ud83d\udee0 \u8bfe\u7a0b\u5b66\u4e60\u4e3b\u9875","text":"<p>\u56e0\u4e3a\u592a\u61d2,\u8fd8\u6ca1\u5f00\u59cb/(\u3112o\u3112)/~~</p>"}]}